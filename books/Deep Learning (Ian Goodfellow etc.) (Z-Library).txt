Contents
Acknowledgments

vii

Notation

ix

1

Introduction
1.1 Who Should Read This Book? . . . . . . . . . . . . . . . . . . . .
1.2 Historical Trends in Deep Learning . . . . . . . . . . . . . . . . .

1
8
11

I

Applied Math and Machine Learning Basics

26

2

Linear Algebra
2.1 Scalars, Vectors, Matrices and Tensors . .
2.2 Multiplying Matrices and Vectors . . . . .
2.3 Identity and Inverse Matrices . . . . . . .
2.4 Linear Dependence, Span and Rank . . .
2.5 Norms . . . . . . . . . . . . . . . . . . . .
2.6 Special Kinds of Matrices and Vectors . .
2.7 Eigendecomposition . . . . . . . . . . . . .
2.8 Singular Value Decomposition . . . . . . .
2.9 The Moore-Penrose Pseudoinverse . . . .
2.10 The Trace Operator . . . . . . . . . . . .
2.11 Determinant . . . . . . . . . . . . . . . . .
2.12 Example: Principal Components Analysis

.
.
.
.
.
.
.
.
.
.
.
.

28
28
30
32
33
35
36
37
40
41
42
43
43

.
.
.
.
.

48
48
51
51
53
53

3

Probability and Information Theory
3.1 Why Probability? . . . . . . . . . .
3.2 Random Variables . . . . . . . . .
3.3 Probability Distributions . . . . . .
3.4 Marginal Probability . . . . . . . .
3.5 Conditional Probability . . . . . .
i

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.

CONTENTS

3.6
3.7
3.8
3.9
3.10
3.11
3.12
3.13
3.14
3.15
4

5

II
6

The Chain Rule of Conditional Probabilities .
Independence and Conditional Independence
Expectation, Variance and Covariance . . . .
Information Theory . . . . . . . . . . . . . . .
Common Probability Distributions . . . . . .
Useful Properties of Common Functions . . .
Bayes’ Rule . . . . . . . . . . . . . . . . . . .
Technical Details of Continuous Variables . .
Structured Probabilistic Models . . . . . . . .
Example: Naive Bayes . . . . . . . . . . . . .

Numerical Computation
4.1 Overﬂow and Underﬂow . . . .
4.2 Poor Conditioning . . . . . . .
4.3 Gradient-Based Optimization .
4.4 Constrained Optimization . . .
4.5 Example: Linear Least Squares

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

54
54
55
56
59
65
66
67
68
71

.
.
.
.
.

77
77
78
79
88
90

Machine Learning Basics
5.1 Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Example: Linear Regression . . . . . . . . . . . . . . . . . . . . .
5.3 Generalization, Capacity, Overﬁtting and Underﬁtting . . . . . .
5.4 Hyperparameters and Validation Sets . . . . . . . . . . . . . . . .
5.5 Estimators, Bias and Variance . . . . . . . . . . . . . . . . . . . .
5.6 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . .
5.7 Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . .
5.8 Supervised Learning Algorithms . . . . . . . . . . . . . . . . . . .
5.9 Unsupervised Learning Algorithms . . . . . . . . . . . . . . . . .
5.10 Weakly Supervised Learning . . . . . . . . . . . . . . . . . . . . .
5.11 Building a Machine Learning Algorithm . . . . . . . . . . . . . .
5.12 The Curse of Dimensionality and Statistical Limitations of Local
Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Modern Practical Deep Networks
Feedforward Deep Networks
6.1 Vanilla MLPs . . . . . . . . . . . . .
6.2 Estimating Conditional Statistics . .
6.3 Parametrizing a Learned Predictor .
6.4 Flow Graphs and Back-Propagation
ii

92
92
100
103
113
115
123
126
133
138
141
142
143

155
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

157
. 158
. 162
. 162
. 174

CONTENTS

6.5
6.6
6.7
6.8
7

Universal Approximation Properties and
Feature / Representation Learning . . .
Piecewise Linear Hidden Units . . . . .
Historical Notes . . . . . . . . . . . . . .

Depth
. . . .
. . . .
. . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

Regularization of Deep or Distributed Models
7.1 Regularization from a Bayesian Perspective . . . . . .
7.2 Classical Regularization: Parameter Norm Penalty . .
7.3 Classical Regularization as Constrained Optimization .
7.4 Regularization and Under-Constrained Problems . . .
7.5 Dataset Augmentation . . . . . . . . . . . . . . . . . .
7.6 Classical Regularization as Noise Robustness . . . . .
7.7 Early Stopping as a Form of Regularization . . . . . .
7.8 Parameter Tying and Parameter Sharing . . . . . . . .
7.9 Sparse Representations . . . . . . . . . . . . . . . . . .
7.10 Bagging and Other Ensemble Methods . . . . . . . . .
7.11 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . .
7.12 Multi-Task Learning . . . . . . . . . . . . . . . . . . .
7.13 Adversarial Training . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.

.
.
.
.

188
191
192
194

.
.
.
.
.
.
.
.
.
.
.
.
.

196
. 198
. 199
. 207
. 208
. 210
. 211
. 217
. 223
. 224
. 226
. 227
. 232
. 234

8

Optimization for Training Deep Models
236
8.1 Optimization for Model Training . . . . . . . . . . . . . . . . . . 236
8.2 Challenges in Optimization . . . . . . . . . . . . . . . . . . . . . 241
8.3 Optimization Algorithms I: Basic Algorithms . . . . . . . . . . . 250
8.4 Optimization Algorithms II: Adaptive Learning Rates . . . . . . 256
8.5 Optimization Algorithms III: Approximate Second-Order Methods261
8.6 Optimization Algorithms IV: Natural Gradient Methods . . . . . 262
8.7 Optimization Strategies and Meta-Algorithms . . . . . . . . . . . 262
8.8 Hints, Global Optimization and Curriculum Learning . . . . . . . 270

9

Convolutional Networks
9.1 The Convolution Operation . . . . . . . . . . .
9.2 Motivation . . . . . . . . . . . . . . . . . . . . .
9.3 Pooling . . . . . . . . . . . . . . . . . . . . . .
9.4 Convolution and Pooling as an Inﬁnitely Strong
9.5 Variants of the Basic Convolution Function . .
9.6 Structured Outputs . . . . . . . . . . . . . . . .
9.7 Convolutional Modules . . . . . . . . . . . . . .
9.8 Data Types . . . . . . . . . . . . . . . . . . . .
9.9 Eﬃcient Convolution Algorithms . . . . . . . .
9.10 Random or Unsupervised Features . . . . . . .
iii

. . . .
. . . .
. . . .
Prior
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

274
. 275
. 278
. 282
. 287
. 288
. 295
. 295
. 295
. 297
. 298

CONTENTS

9.11 The Neuroscientiﬁc Basis for Convolutional Networks . . . . . . . 299
9.12 Convolutional Networks and the History of Deep Learning . . . . 305
10 Sequence Modeling: Recurrent and Recursive Nets
308
10.1 Unfolding Flow Graphs and Sharing Parameters . . . . . . . . . . 309
10.2 Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . 311
10.3 Bidirectional RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . 322
10.4 Encoder-Decoder Sequence-to-Sequence Architectures . . . . . . 323
10.5 Deep Recurrent Networks . . . . . . . . . . . . . . . . . . . . . . 325
10.6 Recursive Neural Networks . . . . . . . . . . . . . . . . . . . . . 326
10.7 Auto-Regressive Networks . . . . . . . . . . . . . . . . . . . . . . 328
10.8 Facing the Challenge of Long-Term Dependencies . . . . . . . . . 334
10.9 Handling Temporal Dependencies with n-grams, HMMs, CRFs
and Other Graphical Models . . . . . . . . . . . . . . . . . . . . . 347
10.10 Combining Neural Networks and Search . . . . . . . . . . . . . . 358
11 Practical methodology
364
11.1 Basic Machine Learning Methodology . . . . . . . . . . . . . . . 364
11.2 Selecting Hyperparameters . . . . . . . . . . . . . . . . . . . . . . 365
11.3 Debugging Strategies . . . . . . . . . . . . . . . . . . . . . . . . . 373
12 Applications
12.1 Large Scale Deep Learning . . . .
12.2 Computer Vision . . . . . . . . .
12.3 Speech Recognition . . . . . . . .
12.4 Natural Language Processing and
12.5 Structured Outputs . . . . . . . .
12.6 Other Applications . . . . . . . .

III

Deep Learning Research

. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
Neural Language
. . . . . . . . . .
. . . . . . . . . .

. . . . .
. . . . .
. . . . .
Models
. . . . .
. . . . .

.
.
.
.
.
.

.
.
.
.
.
.

376
. 376
. 384
. 391
. 393
. 408
. 409

410

13 Structured Probabilistic Models for Deep Learning
412
13.1 The Challenge of Unstructured Modeling . . . . . . . . . . . . . . 413
13.2 Using Graphs to Describe Model Structure . . . . . . . . . . . . . 417
13.3 Advantages of Structured Modeling . . . . . . . . . . . . . . . . . 431
13.4 Learning About Dependencies . . . . . . . . . . . . . . . . . . . . 432
13.5 Inference and Approximate Inference Over Latent Variables . . . 434
13.6 The Deep Learning Approach to Structured Probabilistic Models 435
14 Monte Carlo Methods
440
14.1 Markov Chain Monte Carlo Methods . . . . . . . . . . . . . . . . 440
iv

CONTENTS

14.2 The Diﬃculty of Mixing Between Well-Separated Modes . . . . . 442
15 Linear Factor Models and Auto-Encoders
15.1 Regularized Auto-Encoders . . . . . . . . . . .
15.2 Denoising Auto-encoders . . . . . . . . . . . . .
15.3 Representational Power, Layer Size and Depth
15.4 Reconstruction Distribution . . . . . . . . . . .
15.5 Linear Factor Models . . . . . . . . . . . . . . .
15.6 Probabilistic PCA and Factor Analysis . . . . .
15.7 Reconstruction Error as Log-Likelihood . . . .
15.8 Sparse Representations . . . . . . . . . . . . . .
15.9 Denoising Auto-Encoders . . . . . . . . . . . .
15.10 Contractive Auto-Encoders . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

444
. 445
. 448
. 450
. 451
. 452
. 453
. 457
. 458
. 463
. 468

16 Representation Learning
471
16.1 Greedy Layerwise Unsupervised Pre-Training . . . . . . . . . . . 472
16.2 Transfer Learning and Domain Adaptation . . . . . . . . . . . . . 479
16.3 Semi-Supervised Learning . . . . . . . . . . . . . . . . . . . . . . 486
16.4 Semi-Supervised Learning and Disentangling Underlying Causal
Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
16.5 Assumption of Underlying Factors and Distributed Representation489
16.6 Exponential Gain in Representational Eﬃciency from Distributed
Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493
16.7 Exponential Gain in Representational Eﬃciency from Depth . . . 495
16.8 Priors Regarding The Underlying Factors . . . . . . . . . . . . . 497
17 The
17.1
17.2
17.3
17.4
17.5

Manifold Perspective on Representation Learning
501
Manifold Interpretation of PCA and Linear Auto-Encoders . . . 509
Manifold Interpretation of Sparse Coding . . . . . . . . . . . . . 512
The Entropy Bias from Maximum Likelihood . . . . . . . . . . . 512
Manifold Learning via Regularized Auto-Encoders . . . . . . . . 513
Tangent Distance, Tangent-Prop, and Manifold Tangent Classiﬁer 514

18 Confronting the Partition Function
18.1 The Log-Likelihood Gradient of Energy-Based Models . . .
18.2 Stochastic Maximum Likelihood and Contrastive Divergence
18.3 Pseudolikelihood . . . . . . . . . . . . . . . . . . . . . . . .
18.4 Score Matching and Ratio Matching . . . . . . . . . . . . .
18.5 Denoising Score Matching . . . . . . . . . . . . . . . . . . .
18.6 Noise-Contrastive Estimation . . . . . . . . . . . . . . . . .
18.7 Estimating the Partition Function . . . . . . . . . . . . . .
v

.
.
.
.
.
.
.

.
.
.
.
.
.
.

518
. 519
. 521
. 528
. 530
. 532
. 532
. 534

CONTENTS

19 Approximate inference
19.1 Inference as Optimization . . . . . . . . . . . . . . . . .
19.2 Expectation Maximization . . . . . . . . . . . . . . . . .
19.3 MAP Inference: Sparse Coding as a Probabilistic Model
19.4 Variational Inference and Learning . . . . . . . . . . . .
19.5 Stochastic Inference . . . . . . . . . . . . . . . . . . . .
19.6 Learned Approximate Inference . . . . . . . . . . . . . .
20 Deep Generative Models
20.1 Boltzmann Machines . . . . . . . . . . . . .
20.2 Restricted Boltzmann Machines . . . . . . .
20.3 Training Restricted Boltzmann Machines . .
20.4 Deep Belief Networks . . . . . . . . . . . . .
20.5 Deep Boltzmann Machines . . . . . . . . . .
20.6 Boltzmann Machines for Real-Valued Data .
20.7 Convolutional Boltzmann Machines . . . . .
20.8 Other Boltzmann Machines . . . . . . . . .
20.9 Directed Generative Nets . . . . . . . . . .
20.10 A Generative View of Autoencoders . . . .
20.11 Generative Stochastic Networks . . . . . . .
20.12 Methodological Notes . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.

542
. 544
. 545
. 546
. 547
. 551
. 551

.
.
.
.
.
.
.
.
.
.
.
.

553
. 553
. 556
. 559
. 563
. 566
. 577
. 580
. 581
. 581
. 583
. 589
. 591

Bibliography

595

Index

637

vi

Acknowledgments
This book would not have been possible without the contributions of many people.
We would like to thank those who commented on our proposal for the book
and helped plan its contents and organization: Hugo Larochelle, Guillaume Alain,
Kyunghyun Cho, Çağlar Gülçehre, Razvan Pascanu, David Krueger and Thomas
Rohée.
We would like to thank the people who oﬀered feedback on the content of
the book itself. Some oﬀered feedback on many chapters: Julian Serban, Laurent
Dinh, Guillaume Alain, Kelvin Xu, Meire Fortunato, Ilya Sutskever, Vincent
Vanhoucke, David Warde-Farley, Augustus Q. Odena, Matko Bošnjak, Stephan
Dreseitl, Jurgen Van Gael, Dustin Webb, Johannes Roith, Ion Androutsopoulos,
Stephan Dreiseitl, Karl Pichotta, Pawel Chilinski, Halis Sak, Frédéric Francis,
Jonathan Hunt, and Grigory Sapunov.
We would also like to thank those who provided us with useful feedback on
individual chapters:
• Chapter 1, Introduction: Johannes Roith, Eric Morris, Samira Ebrahimi,
Ozan Çağlayan, Martı́n Abadi, and Sebastien Bratieres.
• Chapter 2, Linear Algebra: Pierre Luc Carrier, Li Yao, Thomas Rohée,
Colby Toland, Amjad Almahairi, Sergey Oreshkov, István Petrás, Dennis
Prangle, and Alessandro Vitale.
• Chapter 3, Probability and Information Theory: Rasmus Antti, Stephan
Gouws, Vincent Dumoulin, Artem Oboturov, Li Yao, John Philip Anderson,
and Kai Arulkumaran.
• Chapter 4, Numerical Computation: Tran Lam An.
• Chapter 5, Machine Learning Basics: Dzmitry Bahdanau, and Zheng Sun.
• Chapter 6, Feedforward Deep Networks: David Krueger.
• Chapter 8, Optimization for Training Deep Models: James Martens and
Marcel Ackermann.
vii

CONTENTS

• Chapter 9, Convolutional Networks: Mehdi Mirza, Çağlar Gülçehre, and
Martı́n Arjovsky.
• Chapter 10, Sequence Modeling: Recurrent and Recursive Nets: Mihaela
Rosca, Razvan Pascanu, Dmitriy Serdyuk, and Dongyu Shi.
• Chapter 18, Confronting the Partition Function: Sam Bowman, and Ozan
Çağlayan.
• Chapter 20, Deep Generative Models: Fady Medhat
• Bibliography, Leslie N. Smith.
We also want to thank those who allowed us to reproduce images, ﬁgures
or data from their publications: David Warde-Farley, Matthew D. Zeiler, Rob
Fergus, Chris Olah, Jason Yosinski, Nicolas Chapados and James Bergstra. We
indicate their contributions in the ﬁgure captions throughout the text.
Finally, we would like to thank Google for allowing Ian Goodfellow to work
on the book as his 20% project while working at Google. In particular, we would
like to thank Ian’s former manager, Greg Corrado, and his subsequent manager,
Samy Bengio, for their support of this eﬀort.

viii

Notation
This section provides a concise reference describing the notation used throughout
this book. If you are unfamiliar with any of these mathematical concepts, this
notation reference may seem intimidating. However, do not despair, we describe
most of these ideas in chapters 1-3.
Numbers and Arrays
a

A scalar (integer or real) value with the name “a”

a

A vector with the name “a”

A

A matrix with the name “A”

A

A tensor with the name “A”

In

Identity matrix with n rows and n columns

I

Identity matrix with dimensionality implied by context

ei

Standard basis vector [0, . . . , 0, 1, 0, . . . , 0] with a 1 at position i.

diag(a)

A square, diagonal matrix with entries given by a

a

A scalar random variable with the name “a”

a

A vector-valued random variable with the name “a”

A

A matrix-valued random variable with the name “A”

ix

CONTENTS

Sets and Graphs
A

A set with the name “A”

R

The set of real numbers

{0, 1}

The set containing 0 and 1

{0, 1, . . . , n}

The set of all integers between 0 and n

[a, b]

The real interval including a and b

(a, b]

The real interval excluding a but including b

A\B

Set subtraction, i.e., the elements of A that are not in B

G

A graph with the name “G”

P a G (xi )

The parents of xi in G .
Indexing

ai

Element i of vector a, with indexing starting at 1

a−i

All elements of vector a except for element i

Ai,j

Element i, j of matrix A

Ai,:

Row i of matrix A

A:,i

Column i of matrix A

Ai,j,k

Element (i, j, k) of a 3-D tensor A

A :,:,i

2-D slice of a 3-D tensor

ai

Element i of the random vector a
Linear Algebra Operations

A>

Transpose of matrix A

A+

Moore-Penrose pseudoinverse of A

AB

Element-wise (Hadamard) product of A and B

x

CONTENTS

Calculus
dy
dx
∂y
∂x
∇x y

Derivative of y with respect to x
Partial derivative of y with respect to x
Gradient of y with respect to x

∇ Xy
∂f
∂x
H (f )(x)
Z
f (x)dx
Z
f (x)dx

Matrix derivatives of y with respect to x
Jacobian matrix J ∈ Rm×n of a function f : Rn → Rm
The Hessian matrix of f at input point x
Deﬁnite integral over the entire domain of x
Deﬁnite integral with respect to x over the set S

S

Probability and Information Theory

a⊥b

The random variables a and b are independent.

a⊥b | c

They are are conditionally independent given c.

E x∼P[f (x)] or Ef (x)
Var(f (x))

Expectation of f (x) with respect to P (x)
Variance of f (x) under P (x)

Cov(f (x), g(x))

Covariance of f (x) and g(x) under P (x, y)

H (x)

Shannon entropy of the random variable x

D KL(P kQ)

Kullback-Leibler divergence of P and Q

xi

CONTENTS

Functions
f ◦g
f (x; θ)

Composition of the functions f and g
A function of x parameterized by θ

log x

Natural logarithm of x

σ(x)

Logistic sigmoid, 1/(1 + exp(−x))

ζ(x)

Softplus, log(1 + exp(x))

||x|| p

Lp norm of x

x+

Positive part of x, i.e., max(0, x)

1 condition is 1 if the condition is true, 0 otherwise.
Sometimes we write f (x), f (X), or f (X), when f is a function of a scalar rather
than a vector, matrix, or tensor. In this case, we mean to apply f to the array
element-wise. For example, if C = σ(X), then C i,j,k = σ(Xi,j,k ) for all valid values
of i, j and k.
Datasets and distributions
X
x(i)

A set of training examples
The i-th example (input) from a dataset

y (i) or y (i)

The target associated with x(i) for supervised learning

X

The m × n matrix with input example x(i) in row Xi,:

xii

Chapter 1

Introduction
Inventors have long dreamed of creating machines that think. Ancient Greek
myths tell of intelligent objects, such as animated statues of human beings and
tables that arrive full of food and drink when called.
When programmable computers were ﬁrst conceived, people wondered whether
they might become intelligent, over a hundred years before one was built (Lovelace,
1842). Today, artiﬁcial intelligence (AI) is a thriving ﬁeld with many practical
applications and active research topics. We look to intelligent software to automate routine labor, understand speech or images, make diagnoses in medicine
and to support basic scientiﬁc research.
In the early days of artiﬁcial intelligence, the ﬁeld rapidly tackled and solved
problems that are intellectually diﬃcult for human beings but relatively straightforward for computers—problems that can be described by a list of formal, mathematical rules. The true challenge to artiﬁcial intelligence proved to be solving
the tasks that are easy for people to perform but hard for people to describe
formally—problems that we solve intuitively, that feel automatic, like recognizing
spoken words or faces in images.
This book is about a solution to these more intuitive problems. This solution
is to allow computers to learn from experience and understand the world in terms
of a hierarchy of concepts, with each concept deﬁned in terms of its relation
to simpler concepts. By gathering knowledge from experience, this approach
avoids the need for human operators to formally specify all of the knowledge that
the computer needs. The hierarchy of concepts allows the computer to learn
complicated concepts by building them out of simpler ones. If we draw a graph
showing how these concepts are built on top of each other, the graph is deep, with
many layers. For this reason, we call this approach to AI deep learning.
Many of the early successes of AI took place in relatively sterile and formal
environments and did not require computers to have much knowledge about the
1

CHAPTER 1. INTRODUCTION

world. For example, IBM’s Deep Blue chess-playing system defeated world champion Garry Kasparov in 1997 (Hsu, 2002). Chess is of course a very simple world,
containing only sixty-four locations and thirty-two pieces that can move in only
rigidly circumscribed ways. Devising a successful chess strategy is a tremendous
accomplishment, but the challenge is not due to the diﬃculty of describing the
relevant concepts to the computer. Chess can be completely described by a very
brief list of completely formal rules, easily provided ahead of time by the programmer.
Ironically, abstract and formal tasks that are among the most diﬃcult mental
undertakings for a human being are among the easiest for a computer. Computers have long been able to defeat even the best human chess player, but are
only recently matching some of the abilities of average human beings to recognize objects or speech. A person’s everyday life requires an immense amount of
knowledge about the world. Much of this knowledge is subjective and intuitive,
and therefore diﬃcult to articulate in a formal way. Computers need to capture
this same knowledge in order to behave in an intelligent way. One of the key
challenges in artiﬁcial intelligence is how to get this informal knowledge into a
computer.
Several artiﬁcial intelligence projects have sought to hard-code knowledge
about the world in formal languages. A computer can reason about statements in
these formal languages automatically using logical inference rules. This is known
as the knowledge base approach to artiﬁcial intelligence. None of these projects
has lead to a major success. One of the most famous such projects is Cyc (Lenat
and Guha, 1989). Cyc is an inference engine and a database of statements in
a language called CycL. These statements are entered by a staﬀ of human supervisors. It is an unwieldy process. People struggle to devise formal rules with
enough complexity to accurately describe the world. For example, Cyc failed to
understand a story about a person named Fred shaving in the morning (Linde,
1992). Its inference engine detected an inconsistency in the story: it knew that
people do not have electrical parts, but because Fred was holding an electric razor,
it believed the entity “FredWhileShaving” contained electrical parts. It therefore
asked whether Fred was still a person while he was shaving.
The diﬃculties faced by systems relying on hard-coded knowledge suggest that
AI systems need the ability to acquire their own knowledge, by extracting patterns
from raw data. This capability is known as machine learning. The introduction
of machine learning allowed computers to tackle problems involving knowledge
of the real world and make decisions that appear subjective. A simple machine
learning algorithm called logistic regression can determine whether to recommend
cesarean delivery (Mor-Yosef et al., 1990). A simple machine learning algorithm
called naive Bayes can separate legitimate e-mail from spam e-mail.
2

CHAPTER 1. INTRODUCTION

The performance of these simple machine learning algorithms depends heavily
on the representation of the data they are given. For example, when logistic
regression is used to recommend cesarean delivery, the AI system does not examine
the patient directly. Instead, the doctor tells the system several pieces of relevant
information, such as the presence or absence of a uterine scar. Each piece of
information included in the representation of the patient is known as a feature.
Logistic regression learns how each of these features of the patient correlates with
various outcomes. However, it cannot inﬂuence the way that the features are
deﬁned in any way. If logistic regression was given a 3-D MRI image of the
patient, rather than the doctor’s formalized report, it would not be able to make
useful predictions. Individual voxels1 in an MRI scan have negligible correlation
with any complications that might occur during delivery.
This dependence on representations is a general phenomenon that appears
throughout computer science and even daily life. In computer science, operations
such as searching a collection of data can proceed exponentially faster if the collection is structured and indexed intelligently. People can easily perform arithmetic
on Arabic numerals, but ﬁnd arithmetic on Roman numerals much more time
consuming. It is not surprising that the choice of representation has an enormous
eﬀect on the performance of machine learning algorithms. For a simple visual
example, see Fig. 1.1.
Many artiﬁcial intelligence tasks can be solved by designing the right set of
features to extract for that task, then providing these features to a simple machine
learning algorithm. For example, a useful feature for speaker identiﬁcation from
sound is the pitch. The pitch can be formally speciﬁed—it is the lowest frequency
major peak of the spectrogram. It is useful for speaker identiﬁcation because it
is determined by the size of the vocal tract, and therefore gives a strong clue as
to whether the speaker is a man, woman, or child.
However, for many tasks, it is diﬃcult to know what features should be extracted. For example, suppose that we would like to write a program to detect
cars in photographs. We know that cars have wheels, so we might like to use the
presence of a wheel as a feature. Unfortunately, it is diﬃcult to describe exactly
what a wheel looks like in terms of pixel values. A wheel has a simple geometric
shape but its image may be complicated by shadows falling on the wheel, the sun
glaring oﬀ the metal parts of the wheel, the fender of the car or an object in the
foreground obscuring part of the wheel, and so on.
One solution to this problem is to use machine learning to discover not only
the mapping from representation to output but also the representation itself.
This approach is known as representation learning. Learned representations of1

A voxel is the value at a single point in a 3-D scan, much as a pixel as the value at a single
point in an image.
3

CHAPTER 1. INTRODUCTION

Figure 1.1: Example of diﬀerent representations: suppose we want to separate two categories of data by drawing a line between them in a scatterplot. In the plot on the left, we
represent some data using Cartesian coordinates, and the task is impossible. In the plot
on the right, we represent the data with polar coordinates and the task becomes simple
to solve with a vertical line. (Figure credit: David Warde-Farley)

ten result in much better performance than can be obtained with hand-designed
representations. They also allow AI systems to rapidly adapt to new tasks, with
minimal human intervention. A representation learning algorithm can discover a
good set of features for a simple task in minutes, or a complex task in hours to
months. Manually designing features for a complex task requires a great deal of
human time and eﬀort; it can take decades for an entire community of researchers.
The quintessential example of a representation learning algorithm is the autoencoder. An autoencoder is the combination of an encoder function that converts
the input data into a diﬀerent representation, and a decoder function that converts
the new representation back into the original format. Autoencoders are trained
to preserve as much information as possible when an input is run through the
encoder and then the decoder, but are also trained to make the new representation have various nice properties. Diﬀerent kinds of autoencoders aim to achieve
diﬀerent kinds of properties.
When designing features or algorithms for learning features, our goal is usually
to separate the factors of variation that explain the observed data. In this context,
we use the word “factors” simply to refer to separate sources of inﬂuence; the
factors are usually not combined by multiplication. Such factors are often not
quantities that are directly observed but they may exist either as unobserved
objects or forces in the physical world that aﬀect observable quantities, or they
are constructs in the human mind that provide useful simplifying explanations
4

CHAPTER 1. INTRODUCTION

or inferred causes of the observed data. They can be thought of as concepts or
abstractions that help us make sense of the rich variability in the data. When
analyzing a speech recording, the factors of variation include the speaker’s age,
their sex, their accent and the words that they are speaking. When analyzing an
image of a car, the factors of variation include the position of the car, its color,
and the angle and brightness of the sun.
A major source of diﬃculty in many real-world artiﬁcial intelligence applications is that many of the factors of variation inﬂuence every single piece of data
we are able to observe. The individual pixels in an image of a red car might be
very close to black at night. The shape of the car’s silhouette depends on the
viewing angle. Most applications require us to disentangle the factors of variation
and discard the ones that we do not care about.
Of course, it can be very diﬃcult to extract such high-level, abstract features
from raw data. Many of these factors of variation, such as a speaker’s accent,
can only be identiﬁed using sophisticated, nearly human-level understanding of
the data. When it is nearly as diﬃcult to obtain a representation as to solve the
original problem, representation learning does not, at ﬁrst glance, seem to help
us.
Deep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations. Deep learning allows the computer to build complex concepts out of
simpler concepts. Fig. 1.2 shows how a deep learning system can represent the
concept of an image of a person by combining simpler concepts, such as corners
and contours, which are in turn deﬁned in terms of edges.
The quintessential example of a deep learning model is the feedforward deep
network or multilayer perceptron (MLP). A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function
is formed by composing many simpler functions. We can think of each application of a diﬀerent mathematical function as providing a new representation of the
input.
The idea of learning the right representation for the data provides one perspective on deep learning. Another perspective on deep learning is that it allows
the computer to learn a multi-step computer program. Each layer of the representation can be thought of as the state of the computer’s memory after executing
another set of instructions in parallel. Networks with greater depth can execute
more instructions in sequence. Being able to execute instructions sequentially offers great power because later instructions can refer back to the results of earlier
instructions. According to this view of deep learning, not all of the information
in a layer’s representation of the input necessarily encodes factors of variation
that explain the input. The representation is also used to store state information
5

CHAPTER 1. INTRODUCTION

CAR

PERSON

ANIMAL

Output
(object identity)

3rd hidden layer
(object parts)

2nd hidden layer
(corners and
contours)

1st hidden layer
(edges)

Visible layer
(input pixels)
Figure 1.2: Illustration of a deep learning model. It is diﬃcult for a computer to understand the meaning of raw sensory input data, such as this image represented as a
collection of pixel values. The function mapping from a set of pixels to an object identity
is very complicated. Learning or evaluating this mapping seems insurmountable if tackled directly. Deep learning resolves this diﬃculty by breaking the desired complicated
mapping into a series of nested simple mappings, each described by a diﬀerent layer of
the model. The input is presented at the visible layer, so named because it contains the
variables that we are able to observe. Then a series of hidden layers extracts increasingly
abstract features from the image. These layers are called “hidden” because their values
are not given in the data; instead the model must determine which concepts are useful
for explaining the relationships in the observed data. The images here are visualizations
of the kind of feature represented by each hidden unit. Given the pixels, the ﬁrst layer
can easily identify edges, by comparing the brightness of neighboring pixels. Given the
ﬁrst hidden layer’s description of the edges, the second hidden layer can easily search for
corners and extended contours, which are recognizable as collections of edges. Given the
second hidden layer’s description of the image in terms of corners and contours, the third
hidden layer can detect entire parts of speciﬁc objects, by ﬁnding speciﬁc collections of
contours and corners. Finally, this description of the image in terms of the object parts
it contains can be used to recognize the objects present in the image. Images reproduced
with permission from Zeiler and Fergus (2014).
6

CHAPTER 1. INTRODUCTION



Element set

+
⇥


w

⇥
1

Element set

+
x

1

w

⇥
2

Logistic
Regression

x

2

Logistic
Regression

w

x

Figure 1.3: Illustration of computational ﬂow graphs mapping an input to an output
where each node performs an operation. Depth is the length of the longest path from input
to output but depends on the deﬁnition of what constitutes a possible computational step.
The computation depicted in these graphs is the output of a logistic regression model,
σ(w T x), where σ is the logistic sigmoid function. If we use addition, multiplication and
logistic sigmoids as the elements of our computer language, then this model has depth
three. If we view logistic regression as an element itself, then this model has depth one.

that helps to execute a program that can make sense of the input. This state
information could be analogous to a counter or pointer in a traditional computer
program. It has nothing to do with the content of the input speciﬁcally, but it
helps the model to organize its processing.
There are two main ways of measuring the depth of a model.
The ﬁrst view is based on the number of sequential instructions that must
be executed to evaluate the architecture. We can think of this as the length of
the longest path through a ﬂow chart that describes how to compute each of the
model’s outputs given its inputs. Just as two equivalent computer programs will
have diﬀerent lengths depending on which language the program is written in, the
same function may be drawn as a ﬂow chart with diﬀerent depths depending on
which functions we allow to be used as individual steps in the ﬂow chart. Fig. 1.3
illustrates how this choice of language can give two diﬀerent measurements for
the same architecture.
Another approach, used by deep probabilistic models, illustrates not the depth
of the computational graph but the depth of the graph describing how concepts are
related to each other. In this case, the depth of the ﬂow-chart of the computations
needed to compute the representation of each concept may be much deeper than
the graph of the concepts themselves. This is because the system’s understanding
of the simpler concepts can be reﬁned given information about the more complex
concepts. For example, an AI system observing an image of a face with one eye in
7

CHAPTER 1. INTRODUCTION

shadow may initially only see one eye. After detecting that a face is present, it can
then infer that a second eye is probably present as well. In this case, the graph of
concepts only includes two layers—a layer for eyes and a layer for faces—but the
graph of computations includes 2n layers if we reﬁne our estimate of each concept
given the other n times.
Because it is not always clear which of these two views—the depth of the
computational graph, or the depth of the probabilistic modeling graph—is most
relevant, and because diﬀerent people choose diﬀerent sets of smallest elements
from which to construct their graphs, there is no single correct value for the depth
of an architecture, just as there is no single correct value for length of a computer
program. Nor is there a consensus about how much depth a model requires to
qualify as “deep.” However, deep learning can safely be regarded as the study of
models that either involve a greater amount of composition of learned functions
or learned concepts than traditional machine learning does.
To summarize, deep learning, the subject of this book, is an approach to AI.
Speciﬁcally, it is a type of machine learning, a technique that allows computer
systems to improve with experience and data. According to the authors of this
book, machine learning is the only viable approach to building AI systems that can
operate in complicated, real-world environments. Deep learning is a particular
kind of machine learning that achieves great power and ﬂexibility by learning
to represent the world as a nested hierarchy of concepts and representations,
with each concept deﬁned in relation to simpler concepts, and more abstract
representations computed in terms of less abstract ones. Fig. 1.4 illustrates the
relationship between these diﬀerent AI disciplines. Fig. 1.5 gives a high-level
schematic of how each works.

1.1

Who Should Read This Book?

This book can be useful for a variety of readers, but we wrote it with two main
target audiences in mind. One of these target audiences is university students (undergraduate or graduate) learning about machine learning, including those who
are beginning a career in deep learning and artiﬁcial intelligence research. The
other target audience is software engineers who do not have a machine learning or
statistics background, but want to rapidly acquire one and begin using deep learning in their product or platform. Software engineers working in a wide variety of
industries are likely to ﬁnd deep learning to be useful, as it has already proven
successful in many areas including computer vision, speech and audio processing,
natural language processing, robotics, bioinformatics and chemistry, video games,
search engines, online advertising and ﬁnance.
This book has been organized into three parts in order to best accommodate
8

CHAPTER 1. INTRODUCTION

Deep learning
Example:
MLPs

Example:
Shallow
autoencoders

Example:
Logistic
regression

Example:
Knowledge
bases

Representation learning
Machine learning

AI

Figure 1.4: A Venn diagram showing how deep learning is a kind of representation learning, which is in turn a kind of machine learning, which is used for many but not all
approaches to AI. Each section of the Venn diagram includes an example of an AI technology.

9

CHAPTER 1. INTRODUCTION

Figure 1.5: Flow-charts showing how the diﬀerent parts of an AI system relate to each
other within diﬀerent AI disciplines. Shaded boxes indicate components that are able to
learn from data.

10

CHAPTER 1. INTRODUCTION

a variety of readers. Part 1 introduces basic mathematical tools and machine
learning concepts. Part 2 describes the most established deep learning algorithms
that are essentially solved technologies. Part 3 describes more speculative ideas
that are widely believed to be important for future research in deep learning.
Readers should feel free to skip parts that are not relevant given their interests
or background. Readers familiar with linear algebra, probability, and fundamental
machine learning concepts can skip part 1, for example, while readers who just
want to implement a working system need not read beyond part 2.
We do assume that all readers come from a computer science background. We
assume familiarity with programming, a basic understanding of computational
performance issues, complexity theory, introductory level calculus and some of
the terminology of graph theory.

1.2

Historical Trends in Deep Learning

It is easiest to understand deep learning with some historical context. Rather
than providing a detailed history of deep learning, we identify a few key trends:
• Deep learning has had a long and rich history, but has gone by many names
reﬂecting diﬀerent philosophical viewpoints, and has waxed and waned in
popularity.
• Deep learning has become more useful as the amount of available training
data has increased.
• Deep learning models have grown in size over time as computer hardware
and software infrastructure for deep learning has improved.
• Deep learning has solved increasingly complicated applications with increasing accuracy over time.

1.2.1

The Many Names and Changing Fortunes of Neural Networks

We expect that many readers of this book have heard of deep learning as an
exciting new technology, and are surprised to see a mention of “history” in a
book about an emerging ﬁeld. In fact, deep learning has a long and rich history.
Deep learning only appears to be new, because it was relatively unpopular for
several years preceding its current popularity, and because it has gone through
many diﬀerent names. While the term “deep learning” is relatively new, the ﬁeld
dates back to the 1950s. The ﬁeld has been rebranded many times, reﬂecting the
inﬂuence of diﬀerent researchers and diﬀerent perspectives.
11

CHAPTER 1. INTRODUCTION

A comprehensive history of deep learning is beyond the scope of this pedagogical textbook. However, some basic context is useful for understanding deep
learning. Broadly speaking, there have been three waves of development of deep
learning: deep learning known as cybernetics in the 1940s-1960s, deep learning
known as connectionism in the 1980s-1990s, and the current resurgence under the
name deep learning beginning in 2006. See Figure 1.6 for a basic timeline.

Figure 1.6: The three historical waves of artiﬁcial neural nets research, starting with
cybernetics in the 1940-1960’s, with the perceptron (Rosenblatt, 1958) to train a
single neuron, then the connectionist approach of the 1980-1995 period, with backpropagation (Rumelhart et al., 1986a) to train a neural network with one or two hidden
layers, and the current wave, deep learning, started around 2006 (Hinton et al., 2006;
Bengio et al., 2007a; Ranzato et al., 2007a), which allows us to train very deep networks.

Some of the earliest learning algorithms we recognize today were intended
to be computational models of biological learning, i.e. models of how learning
happens or could happen in the brain. As a result, one of the names that deep
learning has gone by is artiﬁcial neural networks (ANNs). The corresponding
perspective on deep learning models is that they are engineered systems inspired
by the biological brain (whether the human brain or the brain of another animal). The neural perspective on deep learning is motivated by two main ideas.
One idea is that the brain provides a proof by example that intelligent behavior
is possible, and a conceptually straightforward path to building intelligence is to
reverse engineer the computational principles behind the brain and duplicate its
functionality. Another perspective is that it would be deeply interesting to understand the brain and the principles that underlie human intelligence, so machine
learning models that shed light on these basic scientiﬁc questions are useful apart
from their ability to solve engineering applications.
12

CHAPTER 1. INTRODUCTION

The modern term “deep learning” goes beyond the neuroscientiﬁc perspective
on the current breed of machine learning models. It appeals to a more general
principle of learning multiple levels of composition, which can be applied in machine learning frameworks that are not necessarily neurally inspired.
The earliest predecessors of modern deep learning were simple linear models
motivated from a neuroscientiﬁc perspective. These models were designed to
take a set of n input values x1, . . . , xn and associate them with an output y.
These models would learn a set of weights w 1, . . . , wn and compute their output
f (x, w) = x1 w1 + · · · + xn w n. This ﬁrst wave of neural networks research was
known as cybernetics (see Fig. 1.6).
The McCulloch-Pitts Neuron (McCulloch and Pitts, 1943) was an early model
of brain function. This linear model could recognize two diﬀerent categories of
inputs by testing whether f (x, w) is positive or negative. Of course, for the model
to correspond to the desired deﬁnition of the categories, the weights needed to be
set correctly. These weights could be set by the human operator. In the 1950s,
the perceptron (Rosenblatt, 1958, 1962) became the ﬁrst model that could learn
the weights deﬁning the categories given examples of inputs from each category.
The Adaptive Linear Element (ADALINE), which dates from about the same
time, simply returned the value of f (x) itself to predict a real number (Widrow
and Hoﬀ, 1960), and could also learn to predict these numbers from data.
These simple learning algorithms greatly aﬀected the modern landscape of machine learning. The training algorithm used to adapt the weights of the ADALINE
was a special case of an algorithm called stochastic gradient descent. Slightly modiﬁed versions of the stochastic gradient descent algorithm remain the dominant
training algorithms for deep learning models today.
Models based on the f (x, w) used by the perceptron and ADALINE are called
linear models. These models remain some of the most widely used machine learning models, though in many cases they are trained in diﬀerent ways than the
original models were trained.
Linear models have many limitations. Most famously, they cannot learn the
XOR function, where f([0, 1], w) = 1 and f ([1, 0], w) = 1 but f ([1, 1], w) = 0
and f ([0, 0], w) = 0. Critics who observed these ﬂaws in linear models caused
a backlash against biologically inspired learning in general (Minsky and Papert,
1969). This is the ﬁrst dip in the popularity of neural networks in our broad
timeline (Fig. 1.6).
Today, neuroscience is regarded as an important source of inspiration for deep
learning researchers, but it is no longer the predominant guide for the ﬁeld.
The main reason for the diminished role of neuroscience in deep learning
research today is that we simply do not have enough information about the brain
to use it as a guide. To obtain a deep understanding of the actual algorithms
13

CHAPTER 1. INTRODUCTION

used by the brain, we would need to be able to monitor the activity of (at the
very least) thousands of interconnected neurons simultaneously. Because we are
not able to do this, we are far from understanding even some of the most simple
and well-studied parts of the brain (Olshausen and Field, 2005).
Neuroscience has given us a reason to hope that a single deep learning algorithm can solve many diﬀerent tasks. Neuroscientists have found that ferrets can
learn to “see” with the auditory processing region of their brain if their brains
are rewired to send visual signals to that area (Von Melchner et al., 2000). This
suggests that much of the mammalian brain might use a single algorithm to solve
most of the diﬀerent tasks that the brain solves. Before this hypothesis, machine
learning research was more fragmented, with diﬀerent communities of researchers
studying natural language processing, vision, motion planning and speech recognition. Today, these application communities are still separate, but it is common
for deep learning research groups to study many or even all of these application
areas simultaneously.
We are able to draw some rough guidelines from neuroscience. The basic idea
of having many computational units that become intelligent only via their interactions with each other is inspired by the brain. The Neocognitron (Fukushima,
1980) introduced a powerful model architecture for processing images that was
inspired by the structure of the mammalian visual system and later became the
basis for the modern convolutional network (LeCun et al., 1998b), as we will see
in Chapter 9.11. Most neural networks today are based on a model neuron called
the rectiﬁed linear unit. These units were developed from a variety of viewpoints,
with (Nair and Hinton, 2010b) and Glorot et al. (2011a) citing neuroscience as an
inﬂuence, and Jarrett et al. (2009a) citing more engineering-oriented inﬂuences.
While neuroscience is an important source of inspiration, it need not be taken
as a rigid guide. We know that actual neurons compute very diﬀerent functions
than modern rectiﬁed linear units, but greater neural realism has not yet found
a machine learning value or interpretation. Also, while neuroscience has successfully inspired several neural network architectures, we do not yet know enough
about biological learning for neuroscience to oﬀer much guidance for the learning
algorithms we use to train these architectures.
Media accounts often emphasize the similarity of deep learning to the brain.
While it is true that deep learning researchers are more likely to cite the brain
as an inﬂuence than researchers working in other machine learning ﬁelds such
as kernel machines or Bayesian statistics, one should not view deep learning as
an attempt to simulate the brain. Modern deep learning draws inspiration from
many ﬁelds, especially applied math fundamentals like linear algebra, probability, information theory, and numerical optimization. While some deep learning
researchers cite neuroscience as an important inﬂuence, others are not concerned
14

CHAPTER 1. INTRODUCTION

with neuroscience at all.
It is worth noting that the eﬀort to understand how the brain works on an
algorithmic level is alive and well. This endeavor is primarily known as “computational neuroscience” and is a separate ﬁeld of study from deep learning. It is
common for researchers to move back and forth between both ﬁelds. The ﬁeld
of deep learning is primarily concerned with how to build computer systems that
are able to successfully solve tasks requiring intelligence, while the ﬁeld of computational neuroscience is primarily concerned with building more accurate models
of how the brain actually works.
In the 1980s, the second wave of neural network research emerged in great part
via a movement called connectionism or parallel distributed processing (Rumelhart
et al., 1986d). Connectionism arose in the context of cognitive science. Cognitive
science is an interdisciplinary approach to understanding the mind, combining
multiple diﬀerent levels of analysis. During the early 1980s, most cognitive scientists studied models of symbolic reasoning. Despite their popularity, symbolic
models were diﬃcult to explain in terms of how the brain could actually implement them using neurons. The connectionists began to study models of cognition
that could actually be grounded in neural implementations, reviving many ideas
dating back to the work of psychologist Donald Hebb in the 1940s (Hebb, 1949).
The central idea in connectionism is that a large number of simple computational units can achieve intelligent behavior when networked together. This
insight applies equally to neurons in biological nervous systems and to hidden
units in computational models.
Several key concepts arose during the connectionism movement of the 1980s
that remain central to today’s deep learning.
One of these concepts is that of distributed representation. This is the idea that
each input to a system should be represented by many features, and each feature
should be involved in the representation of many possible inputs. For example,
suppose we have a vision system that can recognize cars, trucks, and birds and
these objects can each be red, green, or blue. One way of representing these inputs
would be to have a separate neuron or hidden unit that activates for each of the
nine possible combinations: red truck, red car, red bird, green truck, and so on.
This requires nine diﬀerent neurons, and each neuron must independently learn
the concept of color and object identity. One way to improve on this situation is
to use a distributed representation, with three neurons describing the color and
three neurons describing the object identity. This requires only six neurons total
instead of nine, and the neuron describing redness is able to learn about redness
from images of cars, trucks and birds, not only from images of one speciﬁc category
of objects. The concept of distributed representation is central to this book, and
will be described in greater detail in Chapter 16.
15

CHAPTER 1. INTRODUCTION

Another major accomplishment of the connectionist movement was the successful use of back-propagation to train deep neural networks with internal representations and the popularization of the back-propagation algorithm (Rumelhart
et al., 1986a; LeCun, 1987). This algorithm has waxed and waned in popularity
but as of this writing is currently the dominant approach to training deep models.
The second wave of neural networks research lasted until the mid-1990s. At
that point, the popularity of neural networks declined again. This was in part due
to a negative reaction to the failure of neural networks (and AI research in general)
to fulﬁll excessive promises made by a variety of people seeking investment in
neural network-based ventures, but also due to improvements in other ﬁelds of
machine learning: kernel machines (Boser et al., 1992; Cortes and Vapnik, 1995;
Schölkopf et al., 1999) and graphical models (Jordan, 1998).
Kernel machines enjoy many nice theoretical guarantees. In particular, training a kernel machine is a convex optimization problem (this will be explained in
more detail in Chapter 4) which means that the training process can be guaranteed to ﬁnd the optimal model eﬃciently. This made kernel machines very
amenable to software implementations that “just work” without much need for
the human operator to understand the underlying ideas. Soon, most machine
learning applications consisted of manually designing good features to provide to
a kernel machine for each diﬀerent application area.
During this time, neural networks continued to obtain impressive performance
on some tasks (LeCun et al., 1998c; Bengio et al., 2001a). The Canadian Institute
for Advanced Research (CIFAR) helped to keep neural networks research alive
via its Neural Computation and Adaptive Perception research initiative. This
program united machine research groups led by Geoﬀrey Hinton at University of
Toronto, Yoshua Bengio at University of Montreal, and Yann LeCun at New York
University. It had a multi-disciplinary nature that also included neuroscientists
and experts in human and computer vision.
At this point in time, deep networks were generally believed to be very diﬃcult
to train. We now know that algorithms that have existed since the 1980s work
quite well, but this was not apparent circa 2006. The issue is perhaps simply that
these algorithms were too computationally costly to allow much experimentation
with the hardware available at the time.
The third wave of neural networks research began with a breakthrough in
2006. Geoﬀrey Hinton showed that a kind of neural network called a deep belief network could be eﬃciently trained using a strategy called greedy layer-wise
pretraining (Hinton et al., 2006), which will be described in more detail in Chapter 16.1. The other CIFAR-aﬃliated research groups quickly showed that the
same strategy could be used to train many other kinds of deep networks (Bengio
et al., 2007a; Ranzato et al., 2007a) and systematically helped to improve gen16

CHAPTER 1. INTRODUCTION

eralization on test examples. This wave of neural networks research popularized
the use of the term deep learning to emphasize that researchers were now able to
train deeper neural networks than had been possible before, and to emphasize the
theoretical importance of depth (Bengio and LeCun, 2007a; Delalleau and Bengio, 2011; Pascanu et al., 2014a; Montufar et al., 2014). Deep neural networks
displaced kernel machines with manually designed features for several important
application areas during this time—in part because the time and memory cost
of training a kernel machine is quadratic in the size of the dataset, and datasets
grew to be large enough for this cost to outweigh the beneﬁts of convex optimization. This third wave of popularity of neural networks continues to the time of
this writing, though the focus of deep learning research has changed dramatically
within the time of this wave. The third wave began with a focus on new unsupervised learning techniques and the ability of deep models to generalize well from
small datasets, but today there is more interest in much older supervised learning
algorithms and the ability of deep models to leverage large labeled datasets.

1.2.2

Increasing Dataset Sizes

One may wonder why deep learning has only recently become recognized as a
crucial technology if it has existed since the 1950s. Deep learning has been successfully used in commercial applications since the 1990s, but was often regarded
as being more of an art than a technology and something that only an expert could
use, until recently. It is true that some skill is required to get good performance
from a deep learning algorithm. Fortunately, the amount of skill required reduces as the amount of training data increases. The learning algorithms reaching
human performance on complex tasks today are nearly identical to the learning
algorithms that struggled to solve toy problems in the 1980s, though the models
we train with these algorithms have undergone changes that simplify the training of very deep architectures. The most important new development is that
today we can provide these algorithms with the resources they need to succeed.
Fig. 1.7 shows how the size of benchmark datasets has increased remarkably over
time. This trend is driven by the increasing digitization of society. As more and
more of our activities take place on computers, more and more of what we do
is recorded. As our computers are increasingly networked together, it becomes
easier to centralize these records and curate them into a dataset appropriate for
machine learning applications. The age of “Big Data” has made machine learning
much easier because the key burden of statistical estimation—generalizing well
to new data after observing only a small amount of data—has been considerably
lightened. As of 2015, a rough rule of thumb is that a supervised deep learning
algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when
17

CHAPTER 1. INTRODUCTION

trained with a dataset containing at least 10 million labeled examples. Working
successfully with datasets smaller than this is an important research area, focusing in particular on how we can take advantage of large quantities of unlabeled
examples, with unsupervised or semi-supervised learning.

1.2.3

Increasing Model Sizes

Another key reason that neural networks are wildly successful today after enjoying comparatively little success since the 1980s is that we have the computational
resources to run much larger models today. One of the main insights of connectionism is that animals become intelligent when many of their neurons work
together. An individual neuron or small collection of neurons is not particularly
useful.
Biological neurons are not especially densely connected. As seen in Fig. 1.9,
our machine learning models have had a number of connections per neuron that
was within an order of magnitude of even mammalian brains for decades.
In terms of the total number of neurons, neural networks have been astonishingly small until quite recently, as shown in Fig. 1.10. Since the introduction
of hidden units, artiﬁcial neural networks have doubled in size roughly every 2.4
years. This growth is driven by faster computers with larger memory and by the
availability of larger datasets. Larger networks are able to achieve higher accuracy
on more complex tasks. This trend looks set to continue for decades. Unless new
technologies allow faster scaling, artiﬁcial neural networks will not have the same
number of neurons as the human brain until at least the 2050s. Biological neurons may represent more complicated functions than current artiﬁcial neurons, so
biological neural networks may be even larger than this plot portrays.
In retrospect, it is not particularly surprising that neural networks with fewer
neurons than a leech were unable to solve sophisticated artiﬁcial intelligence problems. Even today’s networks, which we consider quite large from a computational
systems point of view, are smaller than the nervous system of even relatively primitive vertebrate animals like frogs.
The increase in model size over time, due to the availability of faster CPUs, the
advent of general purpose GPUs, faster network connectivity and better software
infrastructure for distributed computing, is one of the most important trends in
the history of deep learning. This trend is generally expected to continue well
into the future.

18

Dataset size (number examples, logarithmic scale)

CHAPTER 1. INTRODUCTION

Increasing dataset size over time

109

Canadian Hansard

108

WMT English→French

107

ImageNet10k

106

ImageNet
Public SVHN

105
104

Sports-1M

ILSVRC 2014

MNIST

Criminals

CIFAR-10

103
Iris

102

T vs G vs F

Rotated T vs C

101
100

1900

1950

1985

2000

2015

Year (logarithmic scale)

Figure 1.7: Dataset sizes have increased greatly over time. In the early 1900s, statisticians
studied datasets using hundreds or thousands of manually compiled measurements (Garson, 1900; Gosset, 1908; Anderson, 1935; Fisher, 1936). In the 1950s through 1980s,
the pioneers of biologically-inspired machine learning often worked with small, synthetic
datasets, such as low-resolution bitmaps of letters, that were designed to incur low computational cost and demonstrate that neural networks were able to learn speciﬁc kinds
of functions (Widrow and Hoﬀ, 1960; Rumelhart et al., 1986b). In the 1980s and 1990s,
machine learning became more statistical in nature and began to leverage larger datasets
containing tens of thousands of examples such as the MNIST dataset (show in Fig. 1.8) of
scans of handwritten numbers (LeCun et al., 1998c). In the ﬁrst decade of the 2000s, more
sophisticated datasets of this same size, such as the CIFAR-10 dataset (Krizhevsky and
Hinton, 2009) continued to be produced. Toward the end of that decade and throughout
the ﬁrst half of the 2010s, signiﬁcantly larger datasets, containing hundreds of thousands
to tens of millions of examples, completely changed what was possible with deep learning. These datasets included the public Street View House Numbers dataset(Netzer et al.,
2011), various versions of the ImageNet dataset (Deng et al., 2009, 2010a; Russakovsky
et al., 2014a), and the Sports-1M dataset (Karpathy et al., 2014). At the top of the
graph, we see that datasets of translated sentences, such as IBM’s dataset constructed
from the Canadian Hansard (Brown et al., 1990) and the WMT 2014 dataset (Schwenk,
2014) are typically far ahead of other dataset sizes.
19

CHAPTER 1. INTRODUCTION

Figure 1.8: Example inputs from the MNIST dataset. The “NIST” stands for National
Institute of Standards and Technology, the agency that originally collected this data.
The “M” stands for “modiﬁed,” since the data has been preprocessed for easier use with
machine learning algorithms. The MNIST dataset consists of scans of handwritten digits
and associated labels describing which digit 0-9 is contained in each image. This simple
classiﬁcation problem is one of the simplest and most widely used tests in deep learning
research. It remains popular despite being quite easy for modern techniques to solve.
Geoﬀrey Hinton has described it as “the drosophila of machine learning,” meaning that
it allows machine learning researchers to study their algorithms in controlled laboratory
conditions, much as biologists often study fruit ﬂies.

20

Connections per neuron (logarithmic scale)

CHAPTER 1. INTRODUCTION

Number of connections per neuron over time
104

Human
9

Cat

6
7
10

3

4
2

Mouse

10
5
8

102

Fruit ﬂy
3
1

101

1950

1985

2000

2015

Year

Figure 1.9: Initially, the number of connections between neurons in artiﬁcial neural networks was limited by hardware capabilities. Today, the number of connections between
neurons is mostly a design consideration. Some artiﬁcial neural networks have nearly as
many connections per neuron as a cat, and it is quite common for other neural networks
to have as many connections per neuron as smaller mammals like mice. Even the human
brain does not have an exorbitant amount of connections per neuron. The sparse connectivity of biological neural networks means that our artiﬁcial networks are able to match
the performance of biological neural networks despite limited hardware. Modern neural
networks are much smaller than the brains of any vertebrate animal, but we typically
train each network to perform just one task, while an animal’s brain has diﬀerent areas
devoted to diﬀerent tasks. Biological neural network sizes from Wikipedia (2015).
1. Adaptive Linear Element (Widrow and Hoﬀ, 1960)
2. Neocognitron (Fukushima, 1980)
3. GPU-accelerated convolutional network (Chellapilla et al., 2006)
4. Deep Boltzmann machines (Salakhutdinov and Hinton, 2009a)
5. Unsupervised convolutional network (Jarrett et al., 2009b)
6. GPU-accelerated multilayer perceptron (Ciresan et al., 2010)
7. Distributed autoencoder (Le et al., 2012)
8. Multi-GPU convolutional network (Krizhevsky et al., 2012a)
9. COTS HPC unsupervised convolutional network (Coates et al., 2013)
10. GoogLeNet (Szegedy et al., 2014a)

21

CHAPTER 1. INTRODUCTION

1.2.4

Increasing Accuracy, Application Complexity and Real-World
Impact

Since the 1980s, deep learning has consistently improved in its ability to provide
accurate recognition or prediction. Moreover, deep learning has consistently been
applied with success to broader and broader sets of applications.
The earliest deep models were used to recognize individual objects in tightly
cropped, extremely small images (Rumelhart et al., 1986a). Since then there
has been a gradual increase in the size of images neural networks could process.
Modern object recognition networks process rich high-resolution photographs and
do not have a requirement that the photo be cropped near the object to be recognized (Krizhevsky et al., 2012b). Similarly, the earliest networks could only
recognize two kinds of objects (or in some cases, the absence or presence of a single kind of object), while these modern networks typically recognize at least 1,000
diﬀerent categories of objects. The largest contest in object recognition is the ImageNet Large-Scale Visual Recognition Competition held each year. A dramatic
moment in the meteoric rise of deep learning came when a convolutional network
won this challenge for the ﬁrst time and by a wide margin, bringing down the
state-of-the-art error rate from 26.1% to 15.3% (Krizhevsky et al., 2012b). Since
then, these competitions are consistently won by deep convolutional nets, and as
of this writing, advances in deep learning had brought the latest error rate in this
contest down to 6.5% as shown in Fig. 1.11, using even deeper networks (Szegedy
et al., 2014a). Outside the framework of the contest, this error rate has now
dropped to below 5% (Ioﬀe and Szegedy, 2015; Wu et al., 2015).
Deep learning has also had a dramatic impact on speech recognition. After
improving throughout the 1990s, the error rates for speech recognition stagnated
starting in about 2000. The introduction of deep learning (Dahl et al., 2010;
Deng et al., 2010b; Seide et al., 2011; Hinton et al., 2012a) to speech recognition
resulted in a sudden drop of error rates by up to half! We will explore this history
in more detail in Chapter 12.3.1.
Deep networks have also had spectacular successes for pedestrian detection
and image segmentation (Sermanet et al., 2013; Farabet et al., 2013a; Couprie et al., 2013) and yielded superhuman performance in traﬃc sign classiﬁcation (Ciresan et al., 2012).
At the same time that the scale and accuracy of deep networks has increased,
so has the complexity of the tasks that they can solve. Goodfellow et al. (2014d)
showed that neural networks could learn to output an entire sequence of characters
transcribed from an image, rather than just identifying a single object. Previously,
it was widely believed that this kind of learning required labeling of the individual
elements of the sequence (Gülçehre and Bengio, 2013). Since this time, a neural
network designed to model sequences, the Long Short-Term Memory or LSTM
22

CHAPTER 1. INTRODUCTION

(Hochreiter and Schmidhuber, 1997), has enjoyed an explosion in popularity.
LSTMs and related models are now used to model relationships between sequences
and other sequences rather than just ﬁxed inputs. This sequence-to-sequence
learning seems to be on the cusp of revolutionizing another application: machine
translation (Sutskever et al., 2014a; Bahdanau et al., 2014).
This trend of increasing complexity has been pushed to its logical conclusion
with the introduction of the Neural Turing Machine (Graves et al., 2014a), a
neural network that can learn entire programs. This neural network has been
shown to be able to learn how to sort lists of numbers given examples of scrambled
and sorted sequences. This self-programming technology is in its infancy, but in
the future could in principle be applied to nearly any task.
Many of these applications of deep learning are highly proﬁtable, given enough
data to apply deep learning to. Deep learning is now used by many top technology
companies including Google, Microsoft, Facebook, IBM, Baidu, Apple, Adobe,
Netﬂix, NVIDIA and NEC.
Deep learning has also made contributions back to other sciences. Modern
convolutional networks for object recognition provide a model of visual processing
that neuroscientists can study (DiCarlo, 2013). Deep learning also provides useful
tools for processing massive amounts of data and making useful predictions in
scientiﬁc ﬁelds. It has been successfully used to predict how molecules will interact
in order to help pharmaceutical companies design new drugs (Dahl et al., 2014),
to search for subatomic particles (Baldi et al., 2014), and to automatically parse
microscope images used to construct a 3-D map of the human brain (KnowlesBarley et al., 2014). We expect deep learning to appear in more and more scientiﬁc
ﬁelds in the future.
In summary, deep learning is an approach to machine learning that has drawn
heavily on our knowledge of the human brain, statistics and applied math as it
developed over the past several decades. In recent years, it has seen tremendous
growth in its popularity and usefulness, due in large part to more powerful computers, larger datasets and techniques to train deeper networks. The years ahead
are full of challenges and opportunities to improve deep learning even further and
bring it to new frontiers.

23

Number of neurons (logarithmic scale)

CHAPTER 1. INTRODUCTION

1011
10

Increasing neural network size over time

Human

10

17

109
108
10

16

19

20
Octopus
18

14

Frog

7

8

106

11

Bee
Ant

105

3

104
10

3

10

2

101

Leech
13
12
1

6

9
5

100
10 −1
10 −2

Roundworm

2

1950

1985

15

10

4

7

2000

2015

2056

Sponge

Year
Figure 1.10: Since the introduction of hidden units, artiﬁcial neural networks have doubled in size roughly every 2.4 years. Biological neural network sizes from Wikipedia
(2015).
1. Perceptron (Rosenblatt, 1958, 1962)
2. Adaptive Linear Element (Widrow and Hoﬀ, 1960)
3. Neocognitron (Fukushima, 1980)
4. Early backpropagation network (Rumelhart et al., 1986b)
5. Recurrent neural network for speech recognition (Robinson and Fallside, 1991)
6. Multilayer perceptron for speech recognition (Bengio et al., 1991)
7. Mean ﬁeld sigmoid belief network (Saul et al., 1996)
8. LeNet-5 (LeCun et al., 1998b)
9. Echo state network (Jaeger and Haas, 2004)
10. Deep belief network (Hinton et al., 2006)
11. GPU-accelerated convolutional network (Chellapilla et al., 2006)
12. Deep Boltzmann machines (Salakhutdinov and Hinton, 2009a)
13. GPU-accelerated deep belief network (Raina et al., 2009)
14. Unsupervised convolutional network (Jarrett et al., 2009b)
15. GPU-accelerated multilayer perceptron (Ciresan et al., 2010)
16. OMP-1 network (Coates and Ng, 2011)
17. Distributed autoencoder (Le et al., 2012)
18. Multi-GPU convolutional network (Krizhevsky et al., 2012a)
19. COTS HPC unsupervised convolutional network (Coates et al., 2013)
20. GoogLeNet (Szegedy et al., 2014a)

24

CHAPTER 1. INTRODUCTION

Figure 1.11: Since deep networks reached the scale necessary to compete in the ImageNet
Large Scale Visual Recognition, they have consistently won the competition every year,
and yielded lower and lower error rates each time. Data from Russakovsky et al. (2014b).

25

Part I

Applied Math and Machine
Learning Basics

26

This part of the book introduces the basic mathematical concepts needed to
understand deep learning. We begin with general ideas from applied math, that
allow us to deﬁne functions of many variables, ﬁnd the highest and lowest points
on these functions and quantify degrees of belief.
Next, we describe the fundamental goals of machine learning. We describe how
to accomplish these goals by specifying a model that represents certain beliefs,
designing a cost function that measures how well those beliefs correspond with
reality and using a training algorithm to minimize that cost function.
This elementary framework is the basis for a broad variety of machine learning
algorithms, including approaches to machine learning that are not deep. In the
subsequent parts of the book, we develop deep learning algorithms within this
framework.

27

Chapter 2

Linear Algebra
Linear algebra is a branch of mathematics that is widely used throughout science
and engineering. However, because linear algebra is a form of continuous rather
than discrete mathematics, many computer scientists have little experience with
it. A good understanding of linear algebra is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms.
We therefore begin the technical content of the book with a focused presentation
of the key linear algebra ideas that are most important in deep learning.
If you are already familiar with linear algebra, feel free to skip this chapter.
If you have previous experience with these concepts but need a detailed reference
sheet to review key formulas, we recommend The Matrix Cookbook (Petersen and
Pedersen, 2006). If you have no exposure at all to linear algebra, this chapter
will teach you enough to read this book, but we highly recommend that you also
consult another resource focused exclusively on teaching linear algebra, such as
(Shilov, 1977). This chapter will completely omit many important linear algebra
topics that are not essential for understanding deep learning.

2.1

Scalars, Vectors, Matrices and Tensors

The study of linear algebra involves several types of mathematical objects:
• Scalars: A scalar is just a single number, in contrast to most of the other
objects studied in linear algebra, which are usually arrays of multiple numbers. We write scalars in italics. We usually give scalars lower-case variable
names. When we introduce them, we specify what kind of number they
are. For example, we might say “Let s ∈ R be the slope of the line,” while
deﬁning a real-valued scalar, or “Let n ∈ N be the number of units,” while
deﬁning a natural number scalar.
28

CHAPTER 2. LINEAR ALGEBRA

• Vectors: A vector is an array of numbers. The numbers have an order to
them, and we can identify each individual number by its index in that ordering. Typically we give vectors lower case names written in bold typeface,
such as x. The elements of the vector are identiﬁed by writing its name in
italic typeface, with a subscript. The ﬁrst element of x is x1 , the second
element is x2 and so on. We also need to say what kind of numbers are
stored in the vector. If each element is in R, and the vector has n elements,
then the vector lies in the set formed by taking the Cartesian product of R
n times, denoted as Rn . When we need to explicitly identify the elements
of a vector, we write them as a column enclosed in square brackets:


x1
 x2 


x =  . .
 .. 
xn

We can think of vectors as identifying points in space, with each element
giving the coordinate along a diﬀerent axis.
Sometimes we need to index a set of elements of a vector. In this case, we
deﬁne a set containing the indices and write the set as a subscript. For
example, to access x1 , x3 and x6, we deﬁne the set S = {1, 3, 6} and write
xS . We use the − sign to index the complement of a set. For example
x−1 is the vector containing all elements of x except for x1 , and x−S is the
vector containing all of the elements of x except for x1 , x 3 and x6.
• Matrices: A matrix is a 2-D array of numbers, so each element is identiﬁed
by two indices instead of just one. We usually give matrices upper-case
variable names with bold typeface, such as A. If a real-valued matrix A
has a height of m and a width of n, then we say that A ∈ Rm×n . We usually
identify the elements of a matrix using its name in italic but not bold font,
and the indices are listed with separating commas. For example, A1,1 is the
upper left entry of A and Am,n is the bottom right entry. We can identify all
of the numbers with vertical coordinate i by writing a “:” for the horizontal
coordinate. For example, Ai,: denotes the horizontal cross section of A with
vertical coordinate i. This is known as the i-th row of A. Likewise, A:,i is
the i-th column of A. When we need to explicitly identify the elements of
a matrix, we write them as an array enclosed in square brackets:


A1,1 A1,2
.
A2,1 A2,2
29

CHAPTER 2. LINEAR ALGEBRA

2

3

a1,1 a1,2
a 1,1 a2,1
>
A = 4 a2,1 a2,2 5 ⇒ A =
a 1,2 a2,2
a3,1 a3,2

a3,1
a3,2

&

Figure 2.1: The transpose of the matrix can be thought of as a mirror image across the
main diagonal.

Sometimes we may need to index matrix-valued expressions that are not
just a single letter. In this case, we use subscripts after the expression, but
do not convert anything to lower case. For example, f (A)i,j gives element
(i, j) of the matrix computed by applying the function f to A.
• Tensors: In some cases we will need an array with more than two axes. In
the general case, an array of numbers arranged on a regular grid with a
variable number of axes is known as a tensor. We denote a tensor named
“A” with this typeface: A. We identify the element of A at coordinates
(i, j, k) by writing A i,j,k.
One important operation on matrices is the transpose. The transpose of a
matrix is the mirror image of the matrix across a diagonal line, called the main
diagonal, running down and to the right, starting from its upper left corner. See
Fig. 2.1 for a graphical depiction of this operation. We denote the transpose of a
matrix A as A>, and it is deﬁned such that
(A> )i,j = Aj,i .
Vectors can be thought of as matrices that contain only one column. The
transpose of a vector is therefore a matrix with only one row. Sometimes we
deﬁne a vector by writing out its elements in the text inline as a row matrix,
then using the transpose operator to turn it into a standard column vector, e.g.
x = [x1 , x2 , x3 ]>.
We can add matrices to each other, as long as they have the same shape, just
by adding their corresponding elements: C = A + B where Ci,j = A i,j + Bi,j .
We can also add a scalar to a matrix or multiply a matrix by a scalar, just
by performing that operation on each element of a matrix: D = a · B + c where
D i,j = a · Bi,j + c.

2.2

Multiplying Matrices and Vectors

One of the most important operations involving matrices is multiplication of two
matrices. The matrix product of matrices A and B is a third matrix C . In order
30

CHAPTER 2. LINEAR ALGEBRA

for this product to be deﬁned, A must have the same number of columns as B
has rows. If A is of shape m × n and B is of shape n × p, then C is of shape
m × p. We can write the matrix product just by placing two or more matrices
together, e.g.
C = AB.
The product operation is deﬁned by
X
ci,j =
ai,k bk,j .
k

Note that the standard product of two matrices is not just a matrix containing
the product of the individual elements. Such an operation exists and is called the
element-wise product or Hadamard product, and is denoted in this book1as A B.
The dot product between two vectors x and y of the same dimensionality is the
matrix product x> y. We can think of the matrix product C = AB as computing
ci,j as the dot product between row i of A and column j of B.
Matrix product operations have many useful properties that make mathematical analysis of matrices more convenient. For example, matrix multiplication is
distributive:
A(B + C) = AB + AC.
It is also associative:
A(BC) = (AB)C.
Matrix multiplication is not commutative, unlike scalar multiplication.
The transpose of a matrix product also has a simple form:
(AB)> = B >A > .
Since the focus of this textbook is not linear algebra, we do not attempt to develop
a comprehensive list of useful properties of the matrix product here, but the reader
should be aware that many more exist.
We now know enough linear algebra notation to write down a system of linear
equations:
Ax = b
(2.1)
where A ∈ R m×n is a known matrix, b ∈ Rm is a known vector, and x ∈ Rn is a
vector of unknown variables we would like to solve for. Each element xi of x is
one of these unknowns to solve for. Each row of A and each element of b provide
another constraint. We can rewrite equation 2.1 as:
A 1,: x = b1
1 The

element-wise product is used relatively rarely, so the notation for it is not as standardized
as the other operations described in this chapter.
31

CHAPTER 2. LINEAR ALGEBRA




1 0 0
0 1 0 
0 0 1
Figure 2.2: Example identity matrix: This is I3 .

A 2,: x = b2
...
A m,:x = bm
or, even more explicitly, as:
a1,1 x1 + a 1,2 x 2 + · · · + a1,n xn = b1
a2,1 x1 + a 2,2 x 2 + · · · + a2,n xn = b2
...
am,1 x 1 + am,2x 2 + · · · + a m,nxn = bm .
Matrix-vector product notations provides a more compact representation for
equations of this form.

2.3

Identity and Inverse Matrices

Linear algebra oﬀers a powerful tool called matrix inversion that allows us to
solve equation 2.1 for many values of A.
To describe matrix inversion, we ﬁrst need to deﬁne the concept of an identity
matrix. An identity matrix is a matrix that does not change any vector when
we multiply that vector by that matrix. We denote the n-dimensional identity
matrix as I n. Formally,
∀x ∈ R n, In x = x.
The structure of the identity matrix is simple: all of the entries along the main
diagonal are 1, while all of the other entries are zero. See Fig. 2.2 for an example.
The matrix inverse of A is denoted as A−1 , and it is deﬁned as the matrix
such that
A−1 A = In .
We can now solve equation 2.1 by the following steps:
Ax = b
32

CHAPTER 2. LINEAR ALGEBRA

A −1Ax = A −1b
In x = A −1b
x = A−1 b.
Of course, this depends on it being possible to ﬁnd A −1. We discuss the
conditions for the existence of A −1 in the following section.
When A−1 exists, several diﬀerent algorithms exist for ﬁnding it in closed
form. In theory, the same inverse matrix can then be used to solve the equation
many times for diﬀerent values of b. However, A −1 is primarily useful as a
theoretical tool, and should not actually be used in practice for most software
applications. Because A−1 can only be represented with limited precision on a
digital computer, algorithms that make use of the value of b can usually obtain
more accurate estimates of x.

2.4

Linear Dependence, Span and Rank

In order for A−1 to exist, equation 2.1 must have exactly one solution for every
value of b. However, it is also possible for the system of equations to have no
solutions or inﬁnitely many solutions for some values of b. It is not possible to
have more than one but less than inﬁnitely many solutions for a particular b; if
both x and y are solutions then
z = αx + (1 − α)y
is also a solution for any real α.
To analyze how many solutions the equation has, we can think of the columns
of A as specifying diﬀerent directions we can travel from the origin (the point
speciﬁed by the vector of all zeros), and determine how many ways there are of
reaching b. In this view, each element of x speciﬁes how far we should travel
in each of these directions, i.e. x i speciﬁes how far to move in the direction of
column i:
X
Ax =
x i A:,i .
i

In general, this kind of operation is called a linear combination. Formally, a linear
combination of some set of vectors {v (1) , . . . , v(n)} is given by multiplying each
vector v (i) by a corresponding scalar coeﬃcient and adding the results:
X
c i v(i) .
i

The span of a set of vectors is the set of all points obtainable by linear combination
of the original vectors.
33

CHAPTER 2. LINEAR ALGEBRA

Determining whether Ax = b has a solution thus amounts to testing whether
b is in the span of the columns of A. This particular span is known as the column
space or the range of A.
In order for the system Ax = b to have a solution for all values of b ∈ Rm ,
we therefore require that the column space of A be all of Rm . If any point in R m
is excluded from the column space, that point is a potential value of b that has
no solution. This implies immediately that A must have at least m columns, i.e.,
n ≥ m. Otherwise, the dimensionality of the column space must be less than m.
For example, consider a 3 × 2 matrix. The target b is 3-D, but x is only 2-D, so
modifying the value of x at best allows us to trace out a 2-D plane within R 3.
The equation has a solution if and only if b lies on that plane.
Having n ≥ m is only a necessary condition for every point to have a solution.
It is not a suﬃcient condition, because it is possible for some of the columns to
be redundant. Consider a 2 × 2 matrix where both of the columns are equal to
each other. This has the same column space as a 2 × 1 matrix containing only
one copy of the replicated column. In other words, the column space is still just
a line, and fails to encompass all of R 2, even though there are two columns.
Formally, this kind of redundancy is known as linear dependence. A set of
vectors is linearly independent if no vector in the set is a linear combination of
the other vectors. If we add a vector to a set that is a linear combination of the
other vectors in the set, the new vector does not add any points to the set’s span.
This means that for the column space of the matrix to encompass all of R m , the
matrix must have at least m linearly independent columns. This condition is both
necessary and suﬃcient for equation 2.1 to have a solution for every value of b.
In order for the matrix to have an inverse, we additionally need to ensure that
equation 2.1 has at most one solution for each value of b. To do so, we need to
ensure that the matrix has at most m columns. Otherwise there is more than one
way of parametrizing each solution.
Together, this means that the matrix must be square, that is, we require that
m = n and that all of the columns must be linearly independent. A square matrix
with linearly dependent columns is known as singular.
If A is not square or is square but singular, it can still be possible to solve the
equation. However, we can not use the method of matrix inversion to ﬁnd the
solution.
So far we have discussed matrix inverses as being multiplied on the left. It is
also possible to deﬁne an inverse that is multiplied on the right:
AA−1 = I.
For square matrices, the left inverse and right inverse are equal.
34

CHAPTER 2. LINEAR ALGEBRA

2.5

Norms

Sometimes we need to measure the size of a vector. In machine learning, we
usually measure the size of vectors using an L p norm:
||x||p =

X
i

!1

p

|xi | p

for p ∈ R, p ≥ 1.
Norms, including the L p norm, are functions mapping vectors to non-negative
values, satisfying these properties that make them behave like distances between
points:
• f (x) = 0 ⇒ x = 0
• f (x + y) ≤ f(x) + f (y) (the triangle inequality)
• ∀α ∈ R, f (αx) = |α|f (x)
The L2 norm, with p = 2, is known as the Euclidean norm. It is simply the
Euclidean distance from the origin to the point identiﬁed by x. This is probably
the most common norm used in machine learning. It is also common to measure
the size of a vector using the squared L2 norm, which can be calculated simply
as x >x.
The squared L2 norm is more convenient to work with mathematically and
computationally than the L2 norm itself. For example, the derivatives of the
squared L2 norm with respect to each element of x each depend only on the
corresponding element of x, while all of the derivatives of the L2 norm depend
on the entire vector. In many contexts, the squared L2 norm may be undesirable
because it increases very slowly near the origin. In several machine learning
applications, it is important to discriminate between elements that are exactly
zero and elements that are small but nonzero. In these cases, we turn to a
function that grows at the same rate in all locations, but retains mathematical
simplicity: the L1 norm. The L 1 norm may be simpliﬁed to
X
||x||1 =
|xi |.
i

The L 1 norm is commonly used in machine learning when the diﬀerence between
zero and nonzero elements is very important. Every time an element of x moves
away from 0 by , the L 1 norm increases by .
We sometimes measure the size of the vector by counting its number of nonzero
elements (and when we use the L 1 norm, we often use it as a proxy for this
35

CHAPTER 2. LINEAR ALGEBRA

function). Some authors refer to this function as the “l 0 norm,” but this is
incorrect terminology, because scaling the vector by α does not change the number
of nonzero entries.
One other norm that commonly arises in machine learning is the l∞ norm,
also known as the max norm. This norm simpliﬁes to
||x||∞ = max |x i|,
i

e.g., the absolute value of the element with the largest magnitude in the vector.
Sometimes we may also wish to measure the size of a matrix. In the context
of deep learning, the most common way to do this is with the otherwise obscure
Frobenius norm
sX
||A||F =
a 2i,j ,
i,j

which is analogous to the L 2 norm of a vector.
The dot product of two vectors can be rewritten in terms of norms. Speciﬁcally,
x> y = ||x||2||y||2 cos θ
where θ is the angle between x and y.

2.6

Special Kinds of Matrices and Vectors

Some special kinds of matrices and vectors are particularly useful.
Diagonal matrices only have non-zero entries along the main diagonal. Formally, a matrix D is diagonal if and only if di,j = 0 for all i 6
= j. We’ve already
seen one example of a diagonal matrix: the identity matrix, where all of the diagonal entries are 1. In this book2 , we write diag(v) to denote a square diagonal
matrix whose diagonal entries are given by the entries of the vector v. Diagonal
matrices are of interest in part because multiplying by a diagonal matrix is very
computationally eﬃcient. To compute diag(v)x, we only need to scale each element xi by vi . In other words, diag(v)x = v  x. Inverting a square diagonal
matrix is also eﬃcient. The inverse exists only if every diagonal entry is nonzero,
and in that case, diag(v)−1 = diag([1/v 1, . . . , 1/v n ]>). In many cases, we may derive some very general machine learning algorithm in terms of arbitrary matrices,
but obtain a less expensive (and less descriptive) algorithm by restricting some
matrices to be diagonal.
Note that not all diagonal matrices need be square. It is possible to construct
a rectangular diagonal matrix. Non-square diagonal matrices do not have inverses
2

There is not a standardized notation for constructing a diagonal matrix from a vector.
36

CHAPTER 2. LINEAR ALGEBRA

but it is still possible to multiply by them cheaply. For a non-square diagonal
matrix D, the product Dx will involving scaling each element of x, and either
concatenating some zeros to the result if D is taller than it is wide, or discarding
some of the last elements of the vector if D is wider than it is tall.
A symmetric matrix is any matrix that is equal to its own transpose:
A = A> .
Symmetric matrices often arise when the entries are generated by some function of
two arguments that does not depend on the order of the arguments. For example,
if A is a matrix of distance measurements, with ai,j giving the distance from point
i to point j, then ai,j = aj,i because distance functions are symmetric.
A unit vector is a vector with unit norm:
||x||2 = 1.
A vector x and a vector y are orthogonal to each other if x >y = 0. If both
vectors have nonzero norm, this means that they are at 90 degree angles to each
other. In R n, at most n vectors may be mutually orthogonal with nonzero norm.
If the vectors are not only orthogonal but also have unit norm, we call them
orthonormal.
An orthogonal matrix is a square matrix whose rows are mutually orthonormal
and whose columns are mutually orthonormal:
A> A = AA> = I.
This implies that

A−1 = A> ,

so orthogonal matrices are of interest because their inverse is very cheap to compute. Pay careful attention to the deﬁnition of orthogonal matrices. Counterintuitively, their rows are not merely orthogonal but fully orthonormal. There
is no special term for a matrix whose rows or columns are orthogonal but not
orthonormal.

2.7

Eigendecomposition

Many mathematical objects can be understood better by breaking them into
constituent parts, or ﬁnding some properties of them that are universal, not caused
by the way we choose to represent them.
For example, integers can be decomposed into prime factors. The way we
represent the number 12 will change depending on whether we write it in base
37

CHAPTER 2. LINEAR ALGEBRA

ten or in binary, but it will always be true that 12 = 2 × 2 × 3. From this
representation we can conclude useful properties, such as that 12 is not divisible
by 5, or that any integer multiple of 12 will be divisible by 3.
Much as we can discover something about the true nature of an integer by
decomposing it into prime factors, we can also decompose matrices in ways that
show us information about their functional properties that is not obvious from
the representation of the matrix as an array of elements.
One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and
eigenvalues.
An eigenvector of a square matrix A is a non-zero vector v such that multiplication by A alters only the scale of v:
Av = λv.
The scalar λ is known as the eigenvalue corresponding to this eigenvector. (One
can also ﬁnd a left eigenvector such that v > A = λv, but we are usually concerned
with right eigenvectors).
Note that if v is an eigenvector of A, then so is any rescaled vector sv for
s ∈ R, s 6
= 0. Moreover, sv still has the same eigenvalue. For this reason, we
usually only look for unit eigenvectors.
We can represent the matrix A using an eigendecomposition, with eigenvectors
(1)
{v , . . . , v (n)} and corresponding eigenvalues {λ1 , . . . , λn} by concatenating the
eigenvectors into a matrix V = [v (1), . . . , v(n) ], (i.e. one column per eigenvector)
and concatenating the eigenvalues into a vector λ. Then the matrix
A = V diag(λ)V

−1

has the desired eigenvalues and eigenvectors. If we make V an orthogonal matrix,
then we can think of A as scaling space by λi in direction v (i). See Fig. 2.3 for
an example.
We have seen that constructing matrices with speciﬁc eigenvalues and eigenvectors allows us to stretch space in desired directions. However, we often want
to decompose matrices into their eigenvalues and eigenvectors. Doing so can help
us to analyze certain properties of the matrix, much as decomposing an integer
into its prime factors can help us understand the behavior of that integer.
Not every matrix can be decomposed into eigenvalues and eigenvectors. In
some cases, the decomposition exists, but may involve complex rather than real
numbers. Fortunately, in this book, we usually need to decompose only a speciﬁc class of matrices that have a simple decomposition. Speciﬁcally, every real
symmetric matrix can be decomposed into an expression using only real-valued
eigenvectors and eigenvalues:
A = QΛQ> ,
38

CHAPTER 2. LINEAR ALGEBRA

Figure 2.3: An example of the eﬀect of eigenvectors and eigenvalues. Here, we have
a matrix A with two orthonormal eigenvectors, v (1) with eigenvalue λ1 and v(2) with
eigenvalue λ2 . Left) We plot the set of all unit vectors u ∈ R 2 as a unit circle. Right) We
plot the set of all points Au. By observing the way that A distorts the unit circle, we
can see that it scales space in direction v (i) by λ i.

39

CHAPTER 2. LINEAR ALGEBRA

where Q is an orthogonal matrix composed of eigenvectors of A, and Λ is a
diagonal matrix. The eigenvalue Λi,i is associated with the eigenvector in column
i of Q, denoted as Q:,i .
While any real symmetric matrix A is guaranteed to have an eigendecomposition, the eigendecomposition is not unique. If any two or more eigenvectors
share the same eigenvalue, then any set of orthogonal vectors lying in their span
are also eigenvectors with that eigenvalue, and we could equivalently choose a Q
using those eigenvectors instead. By convention, we usually sort the entries of
Λ in descending order. Under this convention, the eigendecomposition is unique
only if all of the eigenvalues are unique.
The eigendecomposition of a matrix tells us many useful facts about the matrix. The matrix is singular if and only if any of the eigenvalues are 0. The
eigendecomposition can also be used to optimize quadratic expressions of the
form f (x) = x >Ax subject to ||x||2 = 1. Whenever x is equal to an eigenvector
of A, f takes on the value of the corresponding eigenvalue. The maximum value
of f within the constraint region is the maximum eigenvalue and its minimum
value within the constraint region is the minimum eigenvalue.
A matrix whose eigenvalues are all positive is called positive deﬁnite. A matrix
whose eigenvalues are all positive or zero-valued is called positive semideﬁnite.
Likewise, if all eigenvalues are negative, the matrix is negative deﬁnite, and if
all eigenvalues are negative or zero-valued, it is negative semideﬁnite. Positive
semideﬁnite matrices are interesting because they guarantee that ∀x, x >Ax ≥ 0.
Positive deﬁnite matrices additionally guarantee that x> Ax = 0 ⇒ x = 0.

2.8

Singular Value Decomposition

In Sec. 2.7, we saw how to decompose a matrix into eigenvectors and eigenvalues. The singular value decomposition (SVD) provides another way to factorize a
matrix, into singular vectors and singular values. The SVD allows us to discover
some of the same kind of information as the eigendecomposition. However, the
SVD is more generally applicable. Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. For example,
if a matrix is not square, the eigendecomposition is not deﬁned, and we must use
a singular value decomposition instead.
Recall that the eigendecomposition involves analyzing a matrix A to discover
a matrix V of eigenvectors and a vector of eigenvalues λ such that we can rewrite
A as
A = V diag(λ)V −1 .
The singular value decomposition is similar, except this time we will write A
40

CHAPTER 2. LINEAR ALGEBRA

as a product of three matrices:
A = U DV > .
Suppose that A is an m ×n matrix. Then U is deﬁned to be an m ×m matrix,
D to be an m × n matrix, and V to be an n × n matrix.
Each of these matrices is deﬁned to have a special structure. The matrices U
and V are both deﬁned to be orthogonal matrices. The matrix D is deﬁned to
be a diagonal matrix. Note that D is not necessarily square.
The elements along the diagonal of D are known as the singular values of the
matrix A. The columns of U are known as the left-singular vectors. The columns
of V are known as as the right-singular vectors.
We can actually interpret the singular value decomposition of A in terms of
the eigendecomposition of functions of A. The left-singular vectors of A are the
eigenvectors of AA >. The right-singular vectors of A are the eigenvectors of
A >A. The non-zero singular values of A are the square roots of the eigenvalues
of A> A. The same is true for AA>.
Perhaps the most useful feature of the SVD is that we can use it to partially
generalize matrix inversion to non-square matrices, as we will see in the next
section.

2.9

The Moore-Penrose Pseudoinverse

Matrix inversion is not deﬁned for matrices that are not square. Suppose we want
to make a left-inverse B of a matrix A, so that we can solve a linear equation
Ax = y
by left-multiplying each side to obtain
x = By.
Depending on the structure of the problem, it may not be possible to design a
unique mapping from A to B.
If A is taller than it is wide, then it is possible for this equation to have
no solution. If A is wider than it is tall, then there could be multiple possible
solutions.
The Moore-Penrose Pseudoinverse allows us to make some headway in these
cases. The pseudoinverse of A is deﬁned as a matrix
A+ = lim (A> A + αI)−1 A> .
α&0

41

CHAPTER 2. LINEAR ALGEBRA

Practical algorithms for computing the pseudoinverse are not based on this deﬁnition, but rather the formula
A+ = V D+ U >,
where U , D and V are the singular value decomposition of A, and the pseudoinverse D + of a diagonal matrix D is obtained by taking the reciprocal of its
non-zero elements then taking the transpose of the resulting matrix.
When A has more rows than columns, then solving a linear equation using
pseudoinverse provides one of the many possible solutions. Speciﬁcally, it provides
the solution x = A+ y with minimal Euclidean norm ||x||2 among all possible
solutions.
When A has more columns than rows, it is possible for there to be no solution.
In this case, using the pseudoinverse gives us the x for which Ax is as close as
possible to y in terms of Euclidean norm ||Ax − y||2.

2.10

The Trace Operator

The trace operator gives the sum of all of the diagonal entries of a matrix:
X
Tr(A) =
ai,i.
i

The trace operator is useful for a variety of reasons. Some operations that
are diﬃcult to specify without resorting to summation notation can be speciﬁed
using matrix products and the trace operator. For example, the trace operator
provides an alternative way of writing the Frobenius norm of a matrix:
q
||A|| F = Tr(A>A).
The trace operator also has many useful properties that make it easy to manipulate expressions involving the trace operator. For example, the trace operator
is invariant to the transpose operator:
Tr(A) = Tr(A> ).
The trace of a square matrix composed of many factors is also invariant to
moving the last factor into the ﬁrst position:
Tr(ABC) = Tr(C AB) = Tr(BCA)
or more generally,
n

Tr(

n−1

F

(i)

) = Tr(F

i=1

Y

(n)

F (i)).
i=1

42

Y

CHAPTER 2. LINEAR ALGEBRA

Another useful fact to keep in mind is that a scalar is its own trace, i.e.
a = T r(a). This can be useful when wishing to manipulate inner products. Let
a and b be two column vectors in Rn
a>b = Tr(a >b) = Tr(ba >).

2.11

Determinant

The determinant of a square matrix, denoted det(A), is a function mapping matrices to real scalars. The determinant is equal to the product of all the matrix’s
eigenvalues. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space. If
the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the
transformation is volume-preserving.

2.12

Example: Principal Components Analysis

One simple machine learning algorithm, principal components analysis (PCA)
can be derived using only knowledge of basic linear algebra.
Suppose we have a collection of m points {x(1) , . . . , x(m)} in Rn . Suppose we
would like to apply lossy compression to these points, i.e. we would like to ﬁnd a
way of storing the points that requires less memory but may lose some precision.
We would like to lose as little precision as possible.
One way we can encode these points is to represent a lower-dimensional version
of them. For each point x (i) ∈ Rn we will ﬁnd a corresponding code vector
c(i) ∈ Rl . If l is smaller than n, it will take less memory to store the code
points than the original data. We will want to ﬁnd some encoding function that
produces the code for an input, f (x) = c and a decoding function that produces
the reconstructed input given its code, i.e., x ≈ g(f (x)).
PCA is deﬁned by our choice of the decoding function. Speciﬁcally, to make
the decoder very simple, we choose to use matrix multiplication to map the code
back into Rn . Let g(c) = Dc, where D ∈ Rn×l is the matrix deﬁning the
decoding.
Computing the optimal code for this decoder could be a diﬃcult problem.
To keep the encoding problem easy, PCA constrains the columns of D to be
orthogonal to each other. (Note that D is still not technically “an orthogonal
matrix” unless l = n)
With the problem as described so far, many solutions are possible, because
we can increase the scale of D :,i if we decrease ci proportionally for all points. To
43

CHAPTER 2. LINEAR ALGEBRA

give the problem a unique solution, we constrain all of the columns of D to have
unit norm.
In order to turn this basic idea into an algorithm we can implement, the ﬁrst
thing we need to do is ﬁgure out how to generate the optimal code point c∗ for
each input point x. One way to do this is to minimize the distance between the
input point x and its reconstruction, g(c∗ ). We can measure this distance using
a norm. In the principal components algorithm, we use the L 2 norm:
c ∗ = arg min ||x − g(c)||2.
c

We can switch to the squared L 2 norm instead of the L2 norm itself, because
both are minimized by the same value of c. This is because the L2 norm is nonnegative and the squaring operation is monotonically increasing for non-negative
arguments.
c∗ = arg min ||x − g(c)||22
c

The function being minimized simpliﬁes to
(x − g(c)) > (x − g(c))
(by the deﬁnition of the L2 norm)
= x >x − x>g(c) − g(c) >x + g(c) >g(c)
(by the distributive property)
= x >x − 2x >g(c) + g(c)> g(c)
(because a scalar is equal to the transpose of itself ).
We can now change the function being minimized again, to omit the ﬁrst term,
since this term does not depend on c:
c∗ = arg min −2x> g(c) + g(c)> g(c).
c

To make further progress, we must substitute in the deﬁnition of g(c):
c ∗ = arg min −2x> Dc + c> D > Dc
c

= arg min −2x >Dc + c >I lc
c

(by the orthogonality and unit norm constraints on D)
= arg min −2x>Dc + c>c
c

44

CHAPTER 2. LINEAR ALGEBRA

We can solve this optimization problem using vector calculus (see section 4.3
if you do not know how to do this):
∇ c(−2x > Dc + c>c) = 0
−2D>x + 2c = 0
c = D>x.

This is good news: we can optimally encode x just using a matrix-vector
operation. To encode a vector, we apply the encoder function
f (x) = D> x.
Using a further matrix multiplication, we can also deﬁne the PCA reconstruction
operation:
r(x) = g (f (x)) = DD >x.
(2.2)
Next, we need to choose the encoding matrix D. To do so, we revisit the
idea of minimizing the L2 distance between inputs and reconstructions. However,
since we will use the same matrix D to decode all of the points, we can no longer
consider the points in isolation. Instead, we must minimize the Frobenius norm
of the matrix of errors computed over all dimensions and all points:
s
2
X  (i)
D ∗ = arg min
x j − r(x(i) )j subject to D >D = Il
(2.3)
D

i,j

To derive the algorithm for ﬁnding D∗ , we will start by considering the case
where l = 1. In this case, D is just a single vector, d. Substituting Eq. 2.2 into
Eq. 2.3 and simplifying D into d, the problem reduces to
X
d ∗ = arg min
||x(i) − dd> x(i)||22 subject to ||d|| 2 = 1.
d

i

The above formulation is the most direct way of performing the substitution,
but is not the most stylistically pleasing way to write the equation. It places the
scalar value d >x(i) on the right of the vector d. It is more conventional to write
scalar coeﬃcients on the left of vector they operate on. We therefore usually write
such a formula as
X
||x(i) − d >x(i) d||22 subject to ||d|| 2 = 1,
d ∗ = arg min
d

i

or, exploiting the fact that a scalar is its own tranpose, as
d ∗ = arg min
d

i

X

||x(i) − x (i)>dd||22 subject to ||d|| 2 = 1.
45

CHAPTER 2. LINEAR ALGEBRA

The reader should aim to become familiar with such cosmetic rearrangements.
At this point, it can be helpful to rewrite the problem in terms of a single
design matrix of examples, rather than as a sum over separate example vectors.
This will allow us to use more compact notation. Let X ∈ Rm×n be the matrix
>
deﬁned by stacking all of the vectors describing the points, such that Xi,: = x (i) .
We can now rewrite the problem as
d∗ = arg min ||X − Xdd> || 2F subject to d >d = 1.
d

Disregarding the constraint for the moment, we can simplify the Frobenius norm
portion as follows:
arg min ||X − Xdd> ||2F
d

= arg min Tr
d



> 

>
X − Xdd
X − Xdd
>

(by the alternate deﬁnition of the Frobenius norm)

= arg min Tr(X > X − X> Xdd > − dd >X >X + dd> X> Xdd> )
d

= arg min Tr(X> X) − Tr(X> Xdd >) − Tr(dd >X >X) + Tr(dd >X >Xdd >)
d

= arg min − Tr(X >Xdd>) − Tr(dd >X >X) + Tr(dd >X >Xdd> )
d

(because terms not involving d do not aﬀect the arg min)
= arg min −2 Tr(X >Xdd > ) + Tr(dd>X >Xdd> )
d

(because we can cycle the order of the matrices inside a trace)
= arg min −2 Tr(X >Xdd > ) + Tr(X> Xdd> dd> )
d

(using the same property again)
At this point, we re-introduce the constraint:
arg min −2 Tr(X > Xdd >) + Tr(X > Xdd> dd>) subject to d> d = 1
d

= arg min −2 Tr(X >Xdd >) + Tr(X >Xdd >) subject to d >d = 1
d

(due to the constraint)
= arg min − Tr(X >Xdd >) subject to d > d = 1
d

46

CHAPTER 2. LINEAR ALGEBRA

= arg max Tr(X> Xdd> ) subject to d>d = 1
d

= arg max Tr(d>X > Xd) subject to d>d = 1
d

This optimization problem may be solved using eigendecomposition. Specifically, the optimal d is given by the eigenvector of X> X corresponding to the
largest eigenvalue.
In the general case, where l > 1, D is given by the l eigenvectors corresponding
to the largest eigenvalues. This may be shown using proof by induction. We
recommend writing this proof as an exercise.

47

Chapter 3

Probability and Information
Theory
In this chapter, we describe probability theory. Probability theory is a mathematical framework for representing uncertain statements. It provides a means
of quantifying uncertainty and axioms for deriving new uncertain statements. In
artiﬁcial intelligence applications, we use probability theory in two major ways.
First, the laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using
probability theory. Second, we can use probability and statistics to theoretically
analyze the behavior of proposed AI systems.
Probability theory is a fundamental tool of many disciplines of science and
engineering. We provide this chapter to ensure that readers whose background is
primarily in software engineering with limited exposure to probability theory can
understand the material in this book. If you are already familiar with probability
theory, feel free to skip this chapter. If you have absolutely no prior experience with probability, this chapter should be suﬃcient to successfully carry out
deep learning research projects, but we do suggest that you consult an additional
resource, such as (Jaynes, 2003).

3.1

Why Probability?

Many branches of computer science deal mostly with entities that are entirely
deterministic and certain. A programmer can usually safely assume that a CPU
will execute each machine instruction ﬂawlessly. Errors in hardware do occur, but
are rare enough that most software applications do not need to be designed to
account for them. Given that many computer scientists and software engineers
work in a relatively clean and certain environment, it can be surprising that
48

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

machine learning makes heavy use of probability theory.
This is because machine learning must always deal with uncertain quantities,
and sometimes may also need to deal with stochastic (non-deterministic) quantities. Uncertainty and stochasticity can arise from many sources. Researchers
have made compelling arguments for quantifying uncertainty using probability
since at least the 1980s. Many of the arguments presented here are summarized
from or inspired by Pearl (1988).
Nearly all activities require some ability to reason in the presence of uncertainty. In fact, beyond mathematical statements that are true by deﬁnition, it is
diﬃcult to think of any proposition that is absolutely true or any event that is
absolutely guaranteed to occur.
There are three possible sources of uncertainty:
1. Inherent stochasticity in the system being modeled. For example, most
interpretations of quantum mechanics describe the dynamics of subatomic
particles as being probabilistic. We can also create theoretical scenarios that
we postulate to have random dynamics, such as a hypothetical card game
where we assume that the cards are truly shuﬄed into a random order.
2. Incomplete observability. Even deterministic systems can appear stochastic
when we cannot observe all of the variables that drive the behavior of the
system. For example, in the Monty Hall problem, a game show contestant is
asked to choose between three doors and wins a prize held behind the chosen
door. Two doors lead to a goat while a third leads to a car. The outcome
given the contestant’s choice is deterministic, but from the contestant’s point
of view, the outcome is uncertain.
3. Incomplete modeling. When we use a model that must discard some of the
information we have observed, the discarded information results in uncertainty in the model’s predictions. For example, suppose we build a robot
that can exactly observe the location of every object around it. If the robot
discretizes space when predicting the future location of these objects, then
the discretization makes the robot immediately become uncertain about
the precise position of objects: each object could be anywhere within the
discrete cell that it was observed to occupy.
In many cases, it is more practical to use a simple but uncertain rule rather
than a complex but certain one, even if the true rule is deterministic and our
modeling system has the ﬁdelity to accommodate a complex rule. For example,
the simple rule “Most birds ﬂy” is cheap to develop and is broadly useful, while a
rule of the form, “Birds ﬂy, except for very young birds that have not yet learned
to ﬂy, sick or injured birds that have lost the ability to ﬂy, ﬂightless species of birds
49

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

including the cassowary, ostrich and kiwi. . . ” is expensive to develop, maintain
and communicate, and after all of this eﬀort is still very brittle and prone to
failure.
Given that we need a means of representing and reasoning about uncertainty,
it is not immediately obvious that probability theory can provide all of the tools
we want for artiﬁcial intelligence applications. Probability theory was originally
developed to analyze the frequencies of events. It is easy to see how probability
theory can be used to study events like drawing a certain hand of cards in a game of
poker. These kinds of events are often repeatable. When we say that an outcome
has a probability p of occurring, it means that if we repeated the experiment (e.g.,
draw a hand of cards) inﬁnitely many times, then proportion p of the repetitions
would result in that outcome. This kind of reasoning does not seem immediately
applicable to propositions that are not repeatable. If a doctor analyzes a patient
and says that the patient has a 40% chance of having the ﬂu, this means something
very diﬀerent—we can not make inﬁnitely many replicas of the patient, nor is there
any reason to believe that diﬀerent replicas of the patient would present with the
same symptoms yet have varying underlying conditions. In the case of the doctor
diagnosing the patient, we use probability to represent a degree of belief, with 1
indicating absolute certainty and 0 indicating absolute uncertainty. The former
kind of probability, related directly to the rates at which events occur, is known as
frequentist probability , while the latter, related to qualitative levels of certainty,
is known asBayesian probability. Bayesian probability
It turns out that if we list several properties that we expect common sense
reasoning about uncertainty to have, then the only way to satisfy those properties
is to treat Bayesian probabilities as behaving exactly the same as frequentist
probabilities. For example, if we want to compute the probability that a player
will win a poker game given that she has a certain set of cards, we use exactly the
same formulas as when we compute the probability that a patient has a disease
given that she has certain symptoms. For more details about why a small set of
common sense assumptions implies that the same axioms must control both kinds
of probability, see (Ramsey, 1926).
Probability can be seen as the extension of logic to deal with uncertainty.
Logic provides a set of formal rules for determining what propositions are implied
to be true or false given the assumption that some other set of propositions is
true or false. Probability theory provides a set of formal rules for determining the
likelihood of a proposition being true given the likelihood of other propositions.

50

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

3.2

Random Variables

A random variable is a variable that can take on diﬀerent values randomly. We
typically denote the random variable itself with a lower case letter in plain typeface, and the values it can take on with lower case script letters. For example,
x1 and x2 are both possible values that the random variable x can take on. For
vector-valued variables, we would write the random variable as x and one of its
values as x. On its own, a random variable is just a description of the states that
are possible; it must be coupled with a probability distribution that speciﬁes how
likely each of these states are.
Random variables may be discrete or continuous. A discrete random variable
is one that has a ﬁnite or countably inﬁnite number of states. Note that these
states are not necessarily the integers; they can also just be named states that
are not considered to have any numerical value. A continuous random variable is
associated with a real value.

3.3

Probability Distributions

A probability distribution is a description of how likely a random variable or set
of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or
continuous.

3.3.1

Discrete Variables and Probability Mass Functions

A probability distribution over discrete variables may be described using a probability mass function (PMF). We typically denote probability mass functions with
a capital P . Often we associate each random variable with a diﬀerent probability mass function and the reader must infer which probability mass function to
use based on the identity of the random variable, rather than the name of the
function; P (x) is usually not the same as P (y).
The probability mass function maps from a state of a random variable to
the probability of that random variable taking on that state. P (x) denotes the
probability that x = x, with a probability of 1 indicating that x = x is certain and
a probability of 0 indicating that x = x is impossible. Sometimes to disambiguate
which PMF to use, we write the name of the random variable explicitly: P (x =
x). Sometimes we deﬁne a variable ﬁrst, then use ∼ notation to specify which
distribution it follows later: x ∼ P (x).
Probability mass functions can act on many variables at the same time. Such
a probability distribution over many variables is known as a joint probability
51

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

distribution. P (x = x, y = y) denotes the probability that x = x and y = y
simultaneously. We may also write P (x, y) for brevity.
To be a probability mass function on a set of random variables x, a function
P must meet the following properties:
• The domain of P must be the set of all possible states of x.
• ∀x ∈ x, 0 ≤ P (x) ≤ 1. An impossible event has probability 0 and no state
can be less probable than that. Likewise, an event that is guaranteed to
happen has probability 1, and no state can have a greater chance of occurring.
P
•
x∈x P (x) = 1. (P must guarantee that some state occurs.)

For example, consider a single discrete random variable x with k diﬀerent
states. We can place a uniform distribution on x—that is, make each of its states
equally likely—by setting its probability mass function to
P (x = xi ) =

1
k

for all i. We can see that this ﬁts the requirements for a probability massP
function.
1
The value k is positive because k is a positive integer. We also see that i P (x =
P
xi ) = i 1k = kk = 1, so the distribution is properly normalized.

3.3.2

Continuous Variables and Probability Density Functions

When working with continuous random variables, we describe probability distributions using a probability density function (PDF) rather than a probability
mass function. To be a probability density function, a function p must satisfy the
following properties:
• The domain of p must be the set of all possible states of x.
• ∀x ∈ x, p(x) ≥ 0. Note that we do not require p(x) ≤ 1.
R
• p(x)dx = 1.

A probability density function p(x) does not give the probability of a speciﬁc
state directly, instead the probability of landing inside an inﬁnitesimal region with
volume δx is given by p(x)δx.
We can integrate the density function to ﬁnd the actual probability mass of
a set of points. Speciﬁcally, the probability that x lies in some set S is given by
the integral of p(x) over that set. In the univariate example, the probability that
x lies in the interval [a, b] is given by [a,b] p(x)dx.
R

52

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

For an example of a probability density function corresponding to a speciﬁc
probability density over a continuous random variable, consider a uniform distribution on an interval of the real numbers. We can do this with a function
u(x; a, b), where a and b are the endpoints of the interval, with b > a. (The
“;” notation means “parametrized by”; we consider x to be the argument of the
function, while a and b are parameters that deﬁne the function) To ensure that
there is no probability mass outside the interval, we say u(x; a, b) = 0 for all
1 . We can see that this is nonnegative
x 6
∈ [a, b]. Within [a, b], u(x; a, b) = b−a
everywhere. Additionally, it integrates to 1. We often denote that x follows the
uniform distribution on [a, b] by writing x ∼ U (a, b).

3.4

Marginal Probability

Sometimes we know the probability distribution over a set of variables and we
want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution.
For example, suppose we have discrete random variables x and y, and we know
P (x, y). We can ﬁnd P (x) with the sum rule:
X
∀x ∈ x, P (x = x) =
P (x = x, y = y).
y

The name “marginal probability” comes from the process of computing marginal
probabilities on paper. When the values of P (x, y) are written in a grid with different values of x in rows and diﬀerent values of y in columns, it is natural to sum
across a row of the grid, then write P (x) in the margin of the paper just to the
right of the row.
For continuous variables, we need to use integration instead of summation:
Z
p(x) = p(x, y)dy.

3.5

Conditional Probability

In many cases, we are interested in the probability of some event, given that some
other event has happened. This is called a conditional probability. We denote the
conditional probability that y = y given x = x as P (y = y | x = x). This
conditional probability can be computed with the formula
P (y = y | x = x) = P (y = y, x = x)/P (x = x).
53

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Note that this is only deﬁned when P (x = x) > 0. We cannot compute the
conditional probability conditioned on an event that never happens.
It is important not to confuse conditional probability with computing what
would happen if some action were undertaken. The conditional probability that
a person is from Germany given that they speak German is quite high, but if
a randomly selected person is taught to speak German, their country of origin
does not change. Computing the consequences of an action is called making an
intervention query. Intervention queries are the domain of causal modeling, which
we do not explore in this book.

3.6

The Chain Rule of Conditional Probabilities

Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:
P (x (1), . . . , x(n) ) = P (x(1) )Πni=2P (x(i) | x(1) , . . . , x(i−1) ).
This observation is known as the chain rule or product rule of probability. It
follows immediately from the deﬁnition of conditional probability. For example,
applying the deﬁnition twice, we get
P (a, b, c) = P (a | b, c)P (b, c)
P (b, c) = P (b | c)P (c)

P (a, b, c) = P (a | b, c)P (b | c)P (c).
Note how every statement about probabilities remains true if we add conditions
(stuﬀ on the right-hand side of the vertical bar) consistently on all the “P”’s in
the statement. We can use this to derive the same thing diﬀerently:
P (a, b | c) = P (a | b, c)P (b | c)
P (a, b, c) = P (a, b | c)P (c) = P (a | b, c)P (b | c)P (c).

3.7

Independence and Conditional Independence

Two random variables x and y are independent if their probability distribution can
be expressed as a product of two factors, one involving only x and one involving
only y:
∀x ∈ x, y ∈ y, p(x = x, y = y) = p(x = x)p(y = y).
54

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Two random variables x and y are conditionally independent given a random
variable z if the conditional probability distribution over x and y factorizes in this
way for every value of z:
∀x ∈ x, y ∈ y, z ∈ z, p(x = x, y = y | z = z) = p(x = x | z = z)p(y = y | z = z).
We can denote independence and conditional independence with compact notation: x⊥y means that x and y are independent, while x⊥y | z means that x and
y are conditionally independent given z.

3.8

Expectation, Variance and Covariance

The expectation or expected value of some function f(x) with respect to a probability distribution P (x) is the average or mean value that f takes on when x is
drawn from P . For discrete variables this can be computed with a summation:
X
E x∼P [f (x)] =
P (x)f(x),
x

while for continuous variables, it is computed with an integral:
Z
Ex∼P [f(x)] = p(x)f(x)dx.
When the identity of the distribution is clear from the context, we may simply
write the name of the random variable that the expectation is over, e.g. Ex [f(x)].
If it is clear which random variable the expectation is over, we may omit the
subscript entirely, e.g. E[f (x)]. By default, we can assume that E[·] averages over
the values of all the random variables inside the brackets. Likewise, when there
is no ambiguity, we may omit the square brackets.
Expectations are linear, for example, E[αf(x) +βg(x)] = αE[f(x)] +βE[g(x)],
when α and β are ﬁxed (not random and not depending on x).
The variance gives a measure of how much the diﬀerent values of a function
are spread apart:
h
i
Var(f(x)) = E (f(x) − E[f(x)])2 .
When the variance is low, the values of f(x) cluster near their expected value.
The square root of the variance is known as the standard deviation.
The covariance gives some sense of how much two values are linearly related
to each other, as well as the scale of these variables:
Cov(f (x), g(y)) = E [(f(x) − E [f(x)]) (g(y) − E [g(y)])] .
55

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

High absolute values of the covariance mean that the values change a lot and
are both far from their respective means at the same time. If the sign of the
covariance is positive, then the values tend to change in the same direction, while
if it is negative, they tend to change in opposite directions. Other measures such
as correlation normalize the contribution of each variable in order to measure only
how much the variables are related, rather than also being aﬀected by the scale
of the separate variables.
The notions of covariance and dependence are conceptually related, but are in
fact distinct concepts. Two random variables that have non-zero covariance are
dependent. However, they may have zero covariance without being independent.
For example, suppose we ﬁrst generate x, then generate s ∈ {−1, 1} with each
state having probability 0.5, then generate y as s(x − E[x]). Clearly, x and y
are not independent, because y only has two possible values given x. However,
Cov(x, y) = 0.
The covariance matrix of a random vector x ∈ Rn is an n × n matrix, such
that
Cov(x) i,j = Cov(xi , xj ).
Note that the diagonal elements give Cov(x i, x i) = Var(x i).

3.9

Information Theory

Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented
to study sending messages from discrete alphabets over a noisy channel, such as
communication via radio transmission. In this context, information theory tells
how to design optimal codes and calculate the expected length of messages sampled from speciﬁc probability distributions using various encoding schemes. In
the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply.
This ﬁeld is fundamental to many areas of electrical engineering and computer
science. In this textbook, we mostly use a few key ideas from information theory
to characterize probability distributions or quantify similarity between probability distributions. For more detail on information theory, see (Cover and Thomas,
2006; MacKay, 2003).
The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has
occurred. A message saying “the sun rose this morning” is so uninformative as
to be unnecessary to send, but a message saying “there was a solar eclipse this
morning” is very informative.
56

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

We would like to quantify information in a way that formalizes this intuition.
Speciﬁcally,
• Likely events should have low information content, and in the extreme case,
events that are guaranteed to happen should have no information content
whatsoever.
• Less likely events should have higher information content.
• Independent events should have additive information. For example, ﬁnding
out that a tossed coin has come up as heads twice should convey twice as
much information as ﬁnding out that a tossed coin has come up as heads
once.
In order to satisfy all three of these properties, we deﬁne the self-information
of an event x = x to be
I(x) = − log P (x).
(3.1)
In this book, we always use log to mean the natural logarithm, with base e. Our
deﬁnition of I(x) is therefore written in units of nats. One nat is the amount of
information gained by observing an event of probability 1e . Other texts use base-2
logarithms and units called bits or shannons; information measured in bits is just
a rescaling of information measured in nats.
When x is continuous, we use the same deﬁnition of information by analogy,
but some of the properties from the discrete case are lost. For example, an event
with unit density still has zero information, despite not being an event that is
guaranteed to occur.
Self-information deals only with a single outcome. We can quantify the
amount of uncertainty in an entire probability distribution using the Shannon
entropy 1:
H(x) = E x∼P[I(x)] = −E x∼P[log P (x)].
(3.2)
also denoted H(P ). In other words, the Shannon entropy of a distribution is
the expected amount of information in an event drawn from that distribution. It
actually gives a lower bound on the number of bits (if the logarithm is base 2,
otherwise the units are diﬀerent) needed in average to encode symbols drawn from
a distribution P. Distributions that are nearly deterministic (where the outcome
is nearly certain) have low entropy; distributions that are closer to uniform have
high entropy. See Fig. 3.1 for a demonstration. When x is continous, the Shannon
entropy is known as the diﬀerential entropy.
1

Shannon entropy is named for Claude Shannon, the father of information theory (Shannon,
1948, 1949). For an interesting biographical account of Shannon and some of his contemporaries,
see Fortune’s Formula by William Poundstone (Poundstone, 2005).
57

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Figure 3.1: This plot shows how distributions that are closer to deterministic have low
Shannon entropy while distributions that are close to uniform have high Shannon entropy.
On the horizontal axis, we plot p, the probability of a binary random variable being equal
to 1. When p is near 0, the distribution is nearly deterministic, because the random
variable is nearly always 0. When p is near 1, the distribution is nearly deterministic,
because the random variable is nearly always 1. When p = 0.5, the entropy is maximal,
because the distribution is uniform over the two outcomes.

58

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

If we have two separate probability distributions P (x) and Q(x) over the same
random variable x, we can measure how diﬀerent these two distributions are using
the Kullback-Leibler (KL) divergence:


P (x)
D KL(P kQ) = E x∼P log
= Ex∼P [log P (x) − log Q(x)] .
(3.3)
Q(x)
In the case of discrete variables, it is the extra amount of information (measured in bits if we use the base 2 logarithm, but in machine learning we usually
use nats and the natural logarithm) needed to send a message containing symbols
drawn from probability distribution P , when we use a code that was designed to
minimize the length of messages drawn from probability distribution Q.
The KL divergence has many useful properties, most notably that it is nonnegative. The KL divergence is 0 if and only if P and Q are the same distribution
in the case of discrete variables, or equal “almost everywhere” in the case of
continuous variables (see section 3.13 for details). Because the KL divergence
is non-negative and measures the diﬀerence between two distributions, it is often conceptualized as measuring some sort of distance between these distributions. However, it is not a true distance measure because it is not symmetric, i.e.
D KL(P kQ) 6
= D KL(QkP ) for some P and Q.
A quantity that is closely related to the KL divergence is the cross entropy
H(P, Q) = H(P ) + DKL (P kQ), which is similar to the KL divergence but lacking
the term on the left:
H(P, Q) = Ex∼P log Q(x).
When computing many of these quantities, it is common to encounter expressions of the form 0 log 0. By convention, in the context of information theory, we
treat these expressions as lim x→0 x log x = 0.

3.10

Common Probability Distributions

Several simple probability distributions are useful in many contexts in machine
learning.

3.10.1

Bernoulli Distribution

The Bernoulli distribution is a distribution over a single binary random variable.
It is controlled by a single parameter φ ∈ [0, 1], which gives the probability of the
random variable being equal to 1. It has the following properties:
P (x = 1) = φ
59

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

P (x = 0) = 1 − φ

P (x = x) = φx (1 − φ) 1−x
Ex [x] = φ
Varx (x) = φ(1 − φ)
H(x) = (φ − 1) log(1 − φ) − φ log φ.

3.10.2

Multinoulli Distribution

The multinoulli or categorical distribution is a distribution over a single discrete
variable with k diﬀerent states, where k is ﬁnite2 . The multinoulli distribution
is parametrized by a vector p ∈ [0, 1]k−1 , where pi gives the probability of the
i-th state. The ﬁnal, k-th state’s probability is given by 1 − 1 > p. Note that
we must constrain 1> p ≤ 1. Multinoulli distributions are often used to refer to
distributions over categories of objects, so we do not usually assume that state 1
has numerical value 1, etc. For this reason, we do not usually need to compute
the expectation or variance of multinoulli-distributed random variables.
The Bernoulli and multinoulli distributions are suﬃcient to describe any distribution over their domain. This is because they model discrete variables for which
it is feasible to simply enumerate all of the states. When dealing with continuous
variables, there are uncountably many states, so any distribution described by a
small number of parameters must impose strict limits on the distribution.

3.10.3

Gaussian Distribution

The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:
r


1
1
N (x | µ, σ2 ) =
exp − 2(x − µ)2 .
2πσ2
2σ
See Fig. 3.2 for a schematic.
The two parameters µ ∈ R and σ ∈ R+ control the normal distribution. µ
gives the coordinate of the central peak. This is also the mean of the distribution,
i.e. E[x] = µ. The standard deviation of the distribution is given by σ, i.e.
Var(x) = σ2 .
2

“Multinoulli” is a recently coined term. The multinoulli distribution is a special case of
the multinomial distribution. A multinomial distribution is the distribution over vectors in
k
{0, . . . , n} representing how many times each of the k categories is visited when n samples
are drawn from a multinoulli distribution. Many texts use the term “multinomial” to refer to
multinoulli distributions without clarifying that they refer only to the n = 1 case.
60

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Figure 3.2: The normal distribution: The normal distribution N (x | µ, σ 2 ) exhibits a
classic “bell curve” shape, with the x coordinate of its central peak given by µ, and
the width of its peak controlled by σ. In this example, we depict the standard normal
distribution, with µ = 0 and σ = 1.

61

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Note that when we evaluate the PDF, we need to square and invert σ. When
we need to frequently evaluate the PDF with diﬀerent parameter values, a more
eﬃcient way of parametrizing the distribution is to use a parameter β ∈ R+ to
control the precision or inverse variance of the distribution:
r


β
1
N (x | µ, β−1 ) =
exp − β(x − µ)2 .
2π
2
Normal distributions are a sensible choice for many applications. In the absence of prior knowledge about what form a distribution over the real numbers
should take, the normal distribution is a good default choice for two major reasons.
First, many distributions we wish to model are truly close to being normal
distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that
in practice, many complicated systems can be modeled successfully as normally
distributed noise, even if the system can be decomposed into parts with more
structured behavior.
Second, the normal distribution in some sense makes the fewest assumptions
of any distribution over the reals, so choosing to use it inserts the least amount of
prior knowledge into a model. Out of all distributions with the same variance, the
normal distribution has the highest entropy. It is not possible to place a uniform
distribution on all of R. The closest we can come to doing so is to use a normal
distribution with high variance.
The normal distribution generalizes to Rn, in which case it is known as the
multivariate normal distribution. It may be parametrized with a positive deﬁnite
symmetric matrix Σ:
s


1
1
> −1
N (x | µ, Σ) =
exp − (x − µ) Σ (x − µ) .
(2π) n det(Σ)
2
The parameter µ still gives the mean of the distribution, though now it is
vector-valued. The parameter Σ gives the covariance matrix of the distribution.
As in the univariate case, the covariance is not necessarily the most computationally eﬃcient way to parametrize the distribution, since we need to invert Σ to
evaluate the PDF. We can instead use a precision matrix β:
s


det(β)
1
−1
>
N (x | µ, β ) =
exp − (x − µ) β(x − µ) .
(2π) n
2

62

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

3.10.4

Exponential and Laplace Distributions

In the context of deep learning, we often want to have a probability distribution
with a sharp point at x = 0. To accomplish this, we can use the exponential
distribution:
p(x; λ) = λ1 x≥0 exp (−λx) .
The exponential distribution uses the indicator function 1 x≥0 to assign probability
zero to all negative values of x.
A closely related probability distribution that allows us to place a sharp peak
of probability mass at an arbitrary point µ is the Laplace distribution


|x − µ|
1
p(x; µ, λ) =
exp −
.
2λ
λ
The Laplace distribution is sometimes also called the double exponential distribution because it can be written as the sum of two exponential distributions, with
one ﬂipped and both shifted to reposition the mode to x = µ. The term “Laplace
distribution” is preferred because “double exponential distribution” also has other
meanings.

3.10.5

Dirac Distribution

In some cases, we wish to specify that all of the mass in a probability distribution
clusters around a single point. This can be accomplished by deﬁning a PDF using
the Dirac delta function, δ(x):
p(x) = δ(x − µ).
The Dirac delta function is deﬁned such that it is zero-valued everywhere but 0,
yet integrates to 1. By deﬁning p(x) to be δ shifted by −µ we obtain an inﬁnitely
narrow and inﬁnitely high peak of probability mass where x = µ.
A common use of the Dirac delta distribution is as a component of the so-called
empirical distribution,
n
1X
p̂(x) =
δ(x − xi )
(3.4)
n
i=1

which puts probability mass 1n on each of the n points x 1, . . . x n forming a given
data set or collection of samples. The Dirac delta distribution is only necessary to
deﬁne the empirical distribution over continuous variables. For discrete variables,
the situation is simpler: an empirical distribution can be conceptualized as a
multinoulli distribution, with a probability associated to each possible input value
that is simply equal to the empirical frequency of that value in the training set.
63

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

We can view the empirical distribution formed from a dataset of training examples as specifying the distribution that we sample from when we train a model
on this dataset. Another important perspective on the empirical distribution is
that it is the probability density that maximizes the likelihood of the training data
(see Section 5.6). Many machine learning algorithms can be conﬁgured to have
arbitrarily high capacity. If given enough capacity, these algorithms will simply
learn the empirical distribution. This is a bad outcome because the model does
not generalize at all and assigns inﬁnitesimal probability to any point in space
that did not occur in the training set. A central problem in machine learning is
studying how to limit the capacity of a model in a way that prevents it from simply learning the empirical distribution while also allowing it to learn complicated
functions.
The empirical distribution is a particular form of mixture, discussed next.

3.10.6

Mixtures of Distributions

It is also common to deﬁne probability distributions by composing other simpler probability distributions. One common way of combining distributions is to
construct a mixture distribution. A mixture distribution is made up of several
component distributions. On each trial, the choice of which component distribution generates the sample is determined by sampling a component identity from
a multinoulli distribution:
X
P (x) =
P (c = i)P (x | c = i)
i

where P (c) is the multinoulli distribution over component identities. In chapter 13, we explore the art of building complex probability distributions from simple ones in more detail. Note that we can think of the variable c as a non-observed
(or latent) random variable that is related to x through their joint distribution
P (x, c) = P (x | c)P (c). Latent variables are discussed further in Section 13.4.2.
A very powerful and common type of mixture model is the Gaussian mixture
model, in which the components P (x | c = i) are Gaussians, each with its mean µi
and covariance Σi . Some mixtures can have more constraints, for example, the covariances could be shared across components, i.e., Σi = Σj = Σ, or the covariance
matrices could be constrained to be diagonal or simply equal to a scalar times the
identity. A Gaussian mixture model is a universal approximator of densities, in
the sense that any smooth density can be approximated to a particular precision
by a Gaussian mixture model with enough components. Gaussian mixture models
have been used in many settings, and are particularly well known for their use as
acoustic models in speech recognition (Bahl et al., 1987).
64

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

3.11

Useful Properties of Common Functions

Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models.
One of these functions is the logistic sigmoid:
σ(x) =

1
.
1 + exp(−x)

The logistic sigmoid is commonly used to produce the φ parameter of a Bernoulli
distribution because its range is (0, 1), which lies within the valid range of values
for the φ parameter. See Fig. 3.3 for a graph of the sigmoid function.

Figure 3.3: The logistic sigmoid function.

Another commonly encountered function is the softplus function (Dugas et al.,
2001):
ζ(x) = log (1 + exp(x)) .
The softplus function can be useful for producing the β or σ parameter of a normal
distribution because its range is R+ . It also arises commonly when manipulating
expressions involving sigmoids, as it is the primitive of the sigmoid, i.e., the
integral from −∞ to x of the sigmoid. The name of the softplus function comes
from the fact that it is a smoothed or “softened” version of
x+ = max(0, x).
See Fig. 3.4 for a graph of the softplus function.
The following properties are all useful enough that you may wish to memorize
them:
65

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Figure 3.4: The softplus function.

σ(x) =

exp(x)
exp(x) + exp(0)

d
σ(x) = σ(x)(1 − σ(x))
dx
1 − σ(x) = σ(−x)
log σ(x) = −ζ(−x)
d
ζ(x) = σ(x)
dx

∀x ∈ (0, 1), σ

−1

(x) = log




x
1−x

∀x > 0, ζ−1 (x) = log (exp(x) − 1)
ζ(x) − ζ(−x) = x

The function σ −1(x) is called the logit in statistics, but this term is more rarely
used in machine learning. The ﬁnal property provides extra justiﬁcation for the
name “softplus”, since x+ − x− = x.

3.12

Bayes’ Rule

We often ﬁnd ourselves in a situation where we know P (y | x) and need to know
P (x | y). Fortunately, if we also know P (x), we can compute the desired quantity
66

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

using Bayes’ rule:
P (x | y) =

P (x)P (y | x)
.
P (y)

Note that
P while P (y) appears in the formula, it is usually feasible to compute
P (y) = x P (y | x)P (x), so we do not need to begin with knowledge of P (y).
Bayes’ rule is straightforward to derive from the deﬁnition of conditional probability, but it is useful to know the name of this formula since many texts refer to
it by name. It is named after the Reverend Thomas Bayes, who ﬁrst discovered a
special case of the formula. The general version presented here was independently
discovered by Pierre-Simon Laplace.

3.13

Technical Details of Continuous Variables

A proper formal understanding of continuous random variables and probability
density functions requires developing probability theory in terms of a branch of
mathematics known as measure theory. Measure theory is beyond the scope of
this textbook, but we can brieﬂy sketch some of the issues that measure theory
is employed to resolve.
In section 3.3.2, we saw that the probability of a continuous vector-valued x
lying in some set S is given by the integral of p(x) over the set S. Some choices
of set S can produce paradoxes. For example, it is possible to construct two sets
S1 and S2 such that P (S 1) + P (S2 ) > 1 but S1 ∩ S2 = ∅. These sets are generally
constructed making very heavy use of the inﬁnite precision of real numbers, for
example by making fractal-shaped sets or sets that are deﬁned by transforming
the set of rational numbers 3. One of the key contributions of measure theory is to
provide a characterization of the set of sets that we can compute the probability
of without encountering paradoxes. In this book, we only integrate over sets with
relatively simple descriptions, so this aspect of measure theory never becomes a
relevant concern.
For our purposes, measure theory is more useful for describing theorems that
apply to most points in Rn but do not apply to some corner cases. Measure theory
provides a rigorous way of describing that a set of points is negligibly small. Such
a set is said to have “measure zero”. We do not formally deﬁne this concept in this
textbook. However, it is useful to understand the intuition that a set of measure
zero occupies no volume in the space we are measuring. For example, within R2,
a line has measure zero, while a ﬁlled polygon has positive measure. Likewise, an
individual point has measure zero. Any union of countably many sets that each
have measure zero also has measure zero (so the set of all the rational numbers
has measure zero, for instance).
3

The Banach-Tarski theorem provides a fun example of such sets.
67

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Another useful term from measure theory is “almost everywhere”. A property
that holds almost everywhere holds throughout all of space except for on a set
of measure zero. Because the exceptions occupy a negligible amount of space,
they can be safely ignored for many applications. Some important results in
probability theory hold for all discrete values but only hold “almost everywhere”
for continuous values.
One other detail we must be aware of relates to handling random variables
that are deterministic functions of one another. Suppose we have two random
variables, x and y, such that y = g(x). You might think that py (y) = px(g −1 (y)).
This is actually not the case.
Suppose y = x2 and x ∼ U(0, 1). If we use the rule p y (y) = px (2y) then p y
will be 0 everywhere except the interval [0, 12], and it will be 1 on this interval.
This means
Z
1
py (y)dy =
2
which violates the deﬁnition of a probability distribution.
This common mistake is wrong because it fails to account for the distortion
of space introduced by the function g(x). Recall that the probability of x lying
in an inﬁnitesimally small region with volume δx is given by p(x)δx. Since g can
expand or contract space, the inﬁnitesimal volume surrounding x in x space may
have diﬀerent volume in y space. To correct the problem, we need to preserve
the property
|py (g(x))dy| = |px (x)dx|.
Solving from this, we obtain
∂x
py (y) = px (g −1 (y))| |
∂y
or equivalently
∂g(x)
|.
(3.5)
∂x
In higher dimensions, the absolute value of the derivative generalizes to the dei
terminant of the Jacobian matrix — the matrix with Ji,j = ∂x
.
∂y j
p x(x) = p y (g(x))|

3.14

Structured Probabilistic Models

Machine learning algorithms often involve probability distributions over a very
large number of random variables. Often, these probability distributions involve
direct interactions between relatively few variables. Using a single function to
describe the entire joint probability distribution can be very ineﬃcient (both
computationally and statistically).
68

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Instead of using a single function to represent a probability distribution, we
can split a probability distribution into many factors that we multiply together.
For example, suppose we have three random variables, a, b, and c. Suppose that
a inﬂuences the value of b and b inﬂuences the value of c, but that a and c are
independent given b. We can represent the probability distribution over all three
variables as a product of probability distributions over two variables:
p(a, b, c) = p(a)p(b | a)p(c | a).
These factorizations can greatly reduce the number of parameters needed to
describe the distribution. Each factor uses a number of parameters that is exponential in the number of variables in the factor. This means that we can greatly
reduce the cost of representing a distribution if we are able to ﬁnd a factorization
into distributions over fewer variables.
We can describe these kinds of factorizations using graphs. Here we use the
word “graph” in the sense of graph theory, i.e. a set of vertices that may be
connected to each other with edges. When we represent the factorization of a
probability distribution with a graph, we call it a structured probabilistic model
or graphical model.
There are two main kinds of structured probabilistic models: directed and
undirected. Both kinds of graphical models use a graph in which each node in
the graph corresponds to a random variable, and an edge connecting two random variables means that the probability distribution is able to represent direct
interactions between those two random variables.
Directed models use graphs with directed edges, and they represent factorizations into conditional probability distributions, as in the example above. Specifically, a directed model contains one factor for every random variable x i in the
distribution, and that factor consists of the conditional distribution over x i given
the parents of x i :
Y
p(x) =
p (xi | P aG (xi )) .
i

See Fig. 3.5 for an example of a directed graph and the factorization of probability
distributions it represents.
Undirected models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are
usually not probability distributions of any kind. Any set of nodes that are all
connected to each other in G is called a clique. Each clique C (i) in an undirected
model is associated with a factor φ (i)(C (i) ). These factors are just functions, not
probability distributions. The output of each factor must be non-negative, but
there is no constraint that the factor must sum or integrate to 1 like a probability
distribution.
69

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

a

b
c

d

e
Figure 3.5: A directed graphical model over random variables a, b, c, d and e. This
graph corresponds to probability distributions that can be factored as p(a, b, c, d, e) =
p(a)p(b)p(c | a, b)p(d | b)p(e | c). This graph allows us to quickly see some properties
of the distribution. For example, a and c interact directly, but a and e interact only
indirectly via c.

70

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

The probability of a conﬁguration of random variables is proportional to the
product of all of these factors—assignments that result in larger factor values are
more likely. Of course, there is no guarantee that this product will sum to 1. We
therefore divide by a normalizing constant Z, deﬁned to be the sum or integral
over all states of the product of the φ functions, in order to obtain a normalized
probability distribution:
1 Y (i)  (i) 
C
p(x) =
φ
.
Z i
See Fig. 3.6 for an example of an undirected graph and the factorization of probability distributions it represents.
Keep in mind that these graphical representations of factorizations are a language for describing probability distributions. They are not mutually exclusive
families of probability distributions. Being directed or undirected is not a property of a probability distribution; it is a property of a particular description of
a probability distribution, but any probability distribution may be described in
both ways.
Throughout part I and part II of this book, we will use structured probabilistic
models merely as a language to describe which direct probabilistic relationships
diﬀerent machine learning algorithms choose to represent. No further understanding of structured probabilistic models is needed until the discussion of research
topics, in part III, where we will explore structured probabilistic models in much
greater detail.

3.15

Example: Naive Bayes

We now know enough probability theory that we can perform some simple applications with a probabilistic model. In this example, we will show how to infer the
probability that a patient has the ﬂu using a simple probabilistic model. For now,
we will assume that we just know the correct model somehow. Later chapters will
cover the concepts needed to learn the model from data.
The Naive Bayes model is a simple probabilistic model that is often used to
recognize patterns. The model consists of one random variable c representing a
category and a set of random variables F = {f (1), . . . , f (n)} representing features
of objects in each category. In this example, we’ll use Naive Bayes to diagnose
patients as having the ﬂu or not. The random variable c can thus have two
values: c 0 representing the category of patients who do not have the ﬂu, and c 1
representing the category of patients who do. Suppose f(1) is the random variable
representing whether the patient has a sore throat, with f0(1) representing no sore
71

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

a

b
c

d

e
Figure 3.6: An undirected graphical model over random variables a, b, c, d and e. This
graph corresponds to probability distributions that can be factored as p(a, b, c, d, e) =
1 (1)
φ (a, b, c)φ(2) (b, d)φ(3) (c, e). This graph allows us to quickly see some properties of
Z
the distribution. For example, a and c interact directly, but a and e interact only indirectly
via c.

72

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

throat, and f1(1) representing a sore throat. Suppose f (2) ∈ R is the patient’s
temperature in degrees Celsius.
When using the Naive Bayes model, we assume that all of the features are
independent from each other given the category:
Y
P (c, f(1) , . . . , f(n)) = P (c)
P (f(i) | c).
i

See Fig. 3.7 for a directed graphical model that expresses these conditional independence assumptions. These assumptions are very strong and unlikely to be
true in naturally occuring situations, hence the name “naive”. Surprisingly, Naive
Bayes often produces good predictions in practice (even though the assumptions
do not hold precisely), and is a good baseline model to start with when tackling
a new problem.
Beyond these conditional independence assumptions, the Naive Bayes framework does not specify anything about the probability distribution. The speciﬁc
choice of distributions is left up to the designer. In our ﬂu example, let’s make
P (c) a Bernoulli distribution, with P (c = c 1 ) = φ(c). We can also make P (f(1) | c)
a Bernoulli distribution, with
(1)

P (f(1) = f 1 | c = c) = φ fc .
In other words, the Bernoulli parameter changes depending on the value of c.
Finally, we need to choose the distribution over f(2). Since f (2) is real-valued, a
normal distribution is a good choice. Because f(2) is a temperature, there are
hard limits to the values it can take on—it cannot go below 0K, for example.
Fortunately, these values are so far from the values measured in human patients
that we can safely ignore these hard limits. Values outside the hard limits will
receive extremely low probability under the normal distribution so long as the
mean and variance are set correctly. As with f(1) , we need to use diﬀerent parameters for diﬀerent values of c, to represent that patients with the ﬂu have diﬀerent
temperatures than patients without it:
f (2) ∼ N(f(2) | µ c , σ2c ).
Now we are ready to determine how likely a patient is to have the ﬂu. To do
this, we want to compute P (c | F), but we know P (c) and P (F | c). This suggests
that we should use Bayes’ rule to determine the desired distribution. The word
“Bayes” in the name “Naive Bayes” comes from this frequent use of Bayes’ rule
in conjunction with the model. We begin by applying Bayes’ rule:
P (c | F) =

P (c)P (F | c)
.
P (F)
73

(3.6)

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

c
(1)

f

f

(2)

Figure 3.7: A directed graphical model depicting the conditional independence assumptions used by the Naive Bayes model.

74

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

We do not know P (F). Fortunately, it is easy to compute:
X
P (F) =
P (c = c, F) (by the sum rule)
c∈c

=

X

c∈c

P (c = c)P (F | c = c) (by the chain rule).

Substituting this result back into equation 3.6, we obtain
P (c)P (F | c)
c∈c P (c = c)P (F | c = c)

P (c | F) = P
=P

P (c)ΠiP (f (i) | c)

c∈c

P (c = c)Πi P (f (i) | c = c)

by the Naive Bayes assumptions. This is as far as we can simplify the expression
for a general Naive Bayes model.
We can simplify the expression further by substituting in the deﬁnitions of the
particular probability distributions we have deﬁned for our ﬂu diagnosis example:

where

P (c = c | f(1) = f1 , f(2) = f2 ) = P

g(c)
0
c0 ∈c g(c )

g(c) = P (c = c)P (f(1) = f(1) | c = c)P (f (2) = f (2) | c = c).

Since c only has two possible values in our example, we can simplify this to:
P (c = 1 | f(1) = f1 , f(2) = f2 ) =
=
=

g(1)
g(0) + g(1)

1
1+

g(0)
g(1)

1
1 + exp (log g(0) − log g(1))
= σ (log g(1) − log g(0)) .

(3.7)

To go further, let’s simplify log g(i):
log g(i) = log

φ (c)i(1

(f)f
− φ(c) )1−iφ 1 1 (1

(f)
− φ1 ) 1−f1

s

1
exp
2πσ2i



!
1
− 2 (f2 − µi ) 2
2σi

1
1
1
(f)
(f)
2
−
= i log φ(c) +(1−i) log 1 − φ (c) +f1 log φ i +(1−f 1) log(1−φ i )+ log
2
2 (f2 − µi ) .
2
2πσ i 2σi




75

CHAPTER 3. PROBABILITY AND INFORMATION THEORY

Substituting this back into equation 3.7, we obtain

σ



P (c = c | f (1) = f1, f (2) = f2) =
(f)

(f)

log φ(c) − log(1 − φ(c) ) + f1 log φ 1 + (1 − f 1) log(1 − φ 1 )

(f)
−f1 log φ (f)
0 + (1 − f1 ) log(1 − φ 0 )

1
1
1
1
2
2
2
2
− log 2πσ1 + log 2πσ 0 − 2 (f2 − µ1) + 2 (f 2 − µ0) .
2
2
2σ1
2σ0

From this formula, we can read oﬀ various intuitive properties of the Naive
Bayes classiﬁer’s behavior on this example problem, regarding the inference that
can be drawn from a trained model. The probability of the patient having the ﬂu
grows like a sigmoidal curve. We move farther to the left as f2 , the patient’s temperature, moves farther away from µ1, the average temperature of a ﬂu patient.

76

Chapter 4

Numerical Computation
Machine learning algorithms usually require a high amount of numerical computation. This typically refers to algorithms that solve mathematical problems by
methods that iteratively update estimates of the solution, rather than analytically
deriving a formula providing a symbolic expression for the correct solution. Common operations include solving systems of linear equations and ﬁnding the value
of an argument that minimizes a function. Even just evaluating a mathematical
function on a digital computer can be diﬃcult when the function involves real
numbers, which cannot be represented precisely using a ﬁnite amount of memory.

4.1

Overﬂow and Underﬂow

The fundamental diﬃculty in performing continuous math on a digital computer
is that we need to represent inﬁnitely many real numbers with a ﬁnite number
of bit patterns. This means that for almost all real numbers, we incur some
approximation error when we represent the number in the computer. In many
cases, this is just rounding error. Rounding error is problematic, especially when
it compounds across many operations, and can cause algorithms that work in
theory to fail in practice if they are not designed to minimize the accumulation
of rounding error.
One form of rounding error that is particularly devastating is underﬂow. Underﬂow occurs when numbers near zero are rounded to zero. Many functions
behave qualitatively diﬀerently when their argument is zero rather than a small
positive number. For example, we usually want to avoid division by zero (some
software environments will raise exceptions when this occurs, otherwise will return a result with a placeholder not-a-number value) or taking the logarithm of
zero (this is usually treated as −∞, which then becomes not-a-number if it is
used for further arithmetic).
77

CHAPTER 4. NUMERICAL COMPUTATION

Another highly damaging form of numerical error is overﬂow. Overﬂow occurs
when numbers with large magnitude are approximated as ∞ or −∞. Further
arithmetic will usually change this inﬁnite values into not-a-number values.
For an example of the need to design software implementations to deal with
overﬂow and underﬂow, consider the softmax function, typically used to predict
the probabilities associated with a multinoulli distribution:
exp(xi )
softmax(x)i = Pn
.
exp(x
)
j
j

Consider what happens when all of the xi are equal to some constant c. Analytically, we can see that all of the outputs should be equal to 1n . Numerically, this
may not occur when c has large magnitude. If c is very negative, then exp(c) will
underﬂow. This means the denominator of the softmax will become 0, so the ﬁnal
result is undeﬁned. When c is very large and positive, exp(c) will overﬂow, again
resulting in the expression as a whole being undeﬁned. Both of these diﬃculties
can be resolved by instead evaluating softmax(z) where z = x − maxi xi . Simple
algebra shows that the value of the softmax function is not changed analytically
by adding or subtracting a scalar from the input vector. Subtracting maxi xi
results in the largest argument to exp being 0, which rules out the possibility of
overﬂow. Likewise, at least one term in the denominator has a value of 1, which
rules out the possibility of underﬂow in the denominator leading to a division by
zero.
There is still one small problem. Underﬂow in the numerator can still cause
the expression as a whole to evaluate to zero. This means that if we implement
log softmax(x) by ﬁrst running the softmax subroutine then passing the result to
the log function, we could erroneously obtain −∞. Instead, we must implement
a separate function that calculates log softmax in a numerically stable way. The
log softmax function can be stabilized using the same trick as we used to stabilize
the softmax function.
For the most part, we do not explicitly detail all of the numerical considerations involved in implementing the various algorithms described in this book.
Implementors should keep numerical issues in mind when developing implementations. Many numerical issues can be avoided by using Theano (Bergstra et al.,
2010a; Bastien et al., 2012), a software package that automatically detects and
stabilizes many common numerically unstable expressions that arise in the context
of deep learning.

4.2

Poor Conditioning

Conditioning refers to how rapidly a function changes with respect to small
changes in its inputs. Functions that change rapidly when their inputs are per78

CHAPTER 4. NUMERICAL COMPUTATION

turbed slightly can be problematic for scientiﬁc computation because rounding
errors in the inputs can result in large changes in the output.
Consider the function f (x) = A −1x. When A ∈ Rn×n has an eigenvalue
decomposition, its condition number is
max |
i,j

λi
|,
λj

i.e. the ratio of the magnitude of the largest and smallest eigenvalue. When this
number is large, matrix inversion is particularly sensitive to error in the input.
Note that this is an intrinsic property of the matrix itself, not the result
of rounding error during matrix inversion. Poorly conditioned matrices amplify
pre-existing errors when we multiply by the true matrix inverse. In practice,
the error will be compounded further by numerical errors in the inversion process
itself. With iterative algorithms such as solving a linear system (or the worked-out
example of linear least square by gradient descent, Section 4.5) ill-conditioning
(in that case of the linear system matrix) yields very slow convergence of the
iterative algorithm, i.e., more iterations are needed to achieve some given degree
of approximation to the ﬁnal solution.

4.3

Gradient-Based Optimization

Most deep learning algorithms involve optimization of some sort. Optimization
refers to the task of either minimizing or maximizing some function f (x) by altering x. We usually phrase most optimization problems in terms of minimizing
f (x). Maximization may be accomplished via a minimization algorithm by minimizing −f (x).
The function we want to minimize or maximize is called the objective function
or criterion. When we are minimizing it, we may also call it the cost function,
loss function, or error function. In this book, we use these terms interchangeably,
though some machine learning publications assign special meaning to some of
these terms.
We often denote the value that minimizes or maximizes a function with a
superscript ∗. For example, we might say x ∗ = arg min f (x).
We assume the reader is already familiar with calculus, but provide a brief
review of how calculus concepts relate to optimization here.
Suppose we have a function y = f (x), where both x and y are real numbers.
0
The derivative of this function is denoted as f 0 (x) or as dy
dx . The derivative f (x)
gives the slope of f (x) at the point x. In other words, it speciﬁes how to scale
a small change in the input in order to obtain the corresponding change in the
output: f (x + ) ≈ f (x) + f 0 (x).
79

CHAPTER 4. NUMERICAL COMPUTATION

Figure 4.1: An illustration of how the derivatives of a function can be used to follow the
function downhill to a minimum. This technique is called gradient descent.

The derivative is therefore useful for minimizing a function because it tells us
how to change x in order to make a small improvement in y. For example, we
know that f (x −  sign(f 0 (x))) is less than f (x) for small enough . We can thus
reduce f (x) by moving x in small steps with opposite sign of the the derivative.
This technique is called gradient descent (Cauchy, 1847a). See Fig. 4.1 for an
example of this technique.
When f 0 (x) = 0, the derivative provides no information about which direction
to move. Points where f 0 (x) = 0 are known as critical points or stationary points.
A local minimum is a point where f (x) is lower than at all neighboring points,
so it is no longer possible to decrease f (x) by making inﬁnitesimal steps. A local
maximum is a point where f (x) is higher than at all neighboring points, so it is
not possible to increase f (x) by making inﬁnitesimal steps. Some critical points
are neither maxima nor minima. These are known as sadd le points. See Fig. 4.2
for examples of each type of critical point.
A point that obtains the absolute lowest value of f (x) is a global minimum. It
80

CHAPTER 4. NUMERICAL COMPUTATION

Figure 4.2: Examples of each of the three types of critical points in 1-D. A critical point is
a point with zero slope. Such a point can either be a local minimum, which is lower than
the neighboring points, a local maximum, which is higher than the neighboring points, or
a saddle point, which has neighbors that are both higher and lower than the point itself.
The situation in higher dimension is qualitatively diﬀerent, especially for saddle points:
see Figures 4.4 and 4.5.

81

CHAPTER 4. NUMERICAL COMPUTATION

Figure 4.3: Optimization algorithms may fail to ﬁnd a global minimum when there are
multiple local minima or plateaus present. In the context of deep learning, we generally
accept such solutions even though they are not truly minimal, so long as they correspond
to signiﬁcantly low values of the cost function.

is possible for there to be only one global minimum or multiple global minima of
the function. It is also possible for there to be local minima that are not globally
optimal. In the context of deep learning, we optimize functions that may have
many local minima that are not optimal, and many saddle points surrounded by
very ﬂat regions. All of this makes optimization very diﬃcult, especially when the
input to the function is multidimensional. We therefore usually settle for ﬁnding
a value of f that is very low, but not necessarily minimal in any formal sense.
See Fig. 4.3 for an example.
We often minimize functions that have multiple inputs: f : Rn → R. Note
that for the concept of “minimization” to make sense, there must still be only
one output.
For these functions, we must make use of the concept of partial derivatives.
The partial derivative ∂x∂ f (x) measures how f changes as only the variable xi
i

82

CHAPTER 4. NUMERICAL COMPUTATION

increases at point x. The gradient generalizes the notion of derivative to the
case where the derivative is with respect to a vector: f is the vector containing
all of the partial derivatives, denoted ∇x f (x). Element i of the gradient is the
partial derivative of f with respect to x i. In multiple dimensions, critical points
are points where every element of the gradient is equal to zero.
The directional derivative in direction u (a unit vector) is the slope of the
function f in direction u. In other words, the derivative of the function f (x + αu)
with respect to α, evaluated at α = 0. Using the chain rule, we can see that this
is u> ∇x f (x).
To minimize f , we would like to ﬁnd the direction in which f decreases the
fastest. We can do this using the directional derivative:
min
>

u,u u=1

=

min

u,u >u=1

u> ∇x f (x)

||u||2||∇ x f (x)||2 cos θ

where θ is the angle between u and the gradient. Substituting in ||u||2 = 1
and ignoring factors that don’t depend on u, this simpliﬁes to minu cos θ. This
is minimized when u points in the opposite direction as the gradient. In other
words, the gradient points directly uphill, and the negative gradient points directly
downhill. We can decrease f by moving in the direction of the negative gradient.
This is known as the method of steepest descent or gradient descent.
Steepest descent proposes a new point
x0 = x − ∇ x f (x)
where  is the size of the step. We can choose  in several diﬀerent ways. A
popular approach is to set  to a small constant. Sometimes, we can solve for
the step size that makes the directional derivative vanish. Another approach is to
evaluate f (x − ∇x f (x)) for several values of  and choose the one that results
in the smallest objective function value. This last strategy is called a line search.
Steepest descent converges when every element of the gradient is zero (or, in
practice, very close to zero). In some cases, we may be able to avoid running
this iterative algorithm, and just jump directly to the critical point by solving the
equation ∇ xf (x) = 0 for x.
Sometimes we need to ﬁnd all of the partial derivatives of all of the elements
of a vector-valued function. The matrix containing all such partial derivatives is
known as a Jacobian matrix. Speciﬁcally, if we have a function f : R m → Rn ,
then the Jacobian matrix J ∈ Rn×m of f is deﬁned such that Ji,j = ∂x∂j f (x)i.
We are also sometimes interested in a derivative of a derivative. This is known
as a second derivative. For example, for a function f : R n → R, the derivative
2
with respect to xi of the derivative of f with respect to xj is denoted as ∂x∂i ∂xj f .
83

CHAPTER 4. NUMERICAL COMPUTATION

2

d
00
In a single dimension, we can denote dx
2 f by f (x).
The second derivative tells us how the ﬁrst derivative will change as we vary
the input. This means it can be useful for determining whether a critical point
is a local maximum, a local minimum, or saddle point. Recall that on a critical
point, f 0 (x) = 0. When f 00 (x) > 0, this means that f 0 (x) increases as we move to
the right, and f 0 (x) decreases as we move to the left. This means f0(x − ) < 0
and f 0 (x + ) > 0 for small enough . In other words, as we move right, the slope
begins to point uphill to the right, and as we move left, the slope begins to point
uphill to the left. Thus, when f 0 (x) = 0 and f00 (x) > 0, we can conclude that x is
a local minimum. Similarly, when f 0 (x) = 0 and f 00 (x) < 0, we can conclude that
x is a local maximum. This is known as the second derivative test. Unfortunately,
when f 00 (x) = 0, the test is inconclusive. In this case x may be a saddle point, or
a part of a ﬂat region.
In multiple dimensions, we need to examine all of the second derivatives of
the function. These derivatives can be collected together into a matrix called the
Hessian matrix. The Hessian matrix H (f )(x) is deﬁned such that

H (f )(x)i,j =

∂2
f (x).
∂xi∂x j

Equivalently, the Hessian is the Jacobian of the gradient.
Anywhere that the second partial derivatives are continuous, the diﬀerential
operators are commutative, i.e. their order can be swapped:
∂2
∂2
f (x) =
f (x).
∂xi ∂xj
∂xj ∂xi
This implies that H i,j = H j,i, so the Hessian matrix is symmetric at such points.
Most of the functions we encounter in the context of deep learning have a symmetric Hessian almost everywhere. Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis
of eigenvectors.
Using the eigendecomposition of the Hessian matrix, we can generalize the second derivative test to multiple dimensions. At a critical point, where ∇ xf (x) = 0,
we can examine the eigenvalues of the Hessian to determine whether the critical
point is a local maximum, local minimum, or saddle point. When the Hessian is
positive deﬁnite 1, the point is a local minimum. This can be seen by observing
that the directional second derivative in any direction must be positive, and making reference to the univariate second derivative test. Likewise, when the Hessian
is negative deﬁnite 2 , the point is a local maximum. In multiple dimensions, it is
1
2

all its eigenvalues are positive
all its eigenvalues are negative
84

CHAPTER 4. NUMERICAL COMPUTATION

actually possible to ﬁnd positive evidence of saddle points in some cases. When
at least one eigenvalue is positive and at least one eigenvalue is negative, we know
that x is a local maximum on one cross section of f but a local minimum on
another cross section. See Fig. 4.4 for an example. Finally, the multidimensional
second derivative test can be inconclusive, just like the univariate version. The
test is inconclusive whenever all of the non-zero eigenvalues have the same sign,
but at least one eigenvalue is zero. This is because the univariate second derivative
test is inconclusive in the cross section corresponding to the zero eigenvalue.
The Hessian can also be useful for understanding the performance of gradient
descent. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly,
while in another direction, it increases slowly. Gradient descent is unaware of this
change in the derivative so it does not know that it needs to explore preferentially
in the direction where the derivative remains negative for longer. See Fig. 4.5 for
an example.
This issue can be resolved by using information from the Hessian matrix to
guide the search. The simplest method for doing so is known as Newton’s method.
Newton’s method is based on using a second-order Taylor series expansion to
approximate f (x) near some point x0 , ignoring derivatives of higher order:
1
f (x) ≈ f (x0 ) + (x − x0 )> ∇ x f (x0 ) + (x − x0 )> H (f )(x 0)(x − x 0).
2
If we then solve for the critical point of this function, we obtain:
x∗ = x0 − H (f )(x 0 )−1 ∇ x f (x 0).
When the function can be locally approximated as quadratic, iteratively updating the approximation and jumping to the minimum of the approximation can
reach the critical point much faster than gradient descent would. This is a useful
property near a local minimum, but it can be a harmful property near a saddle
point. As discussed in Section 8.2.3, Newton’s method is only appropriate when
the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient descent can in principle escape a saddle point, although
it may take a lot of time if the negative eigenvalues are very small in magnitude,
producing a kind of plateau around the saddle point.
Optimization algorithms such as gradient descent that use only the gradient
are called ﬁrst-order optimization algorithms. Optimization algorithms such as
Newton’s method that also use the Hessian matrix are called second-order optimization algorithms (Nocedal and Wright, 2006).
The optimization algorithms employed in most contexts in this book are applicable to a wide variety of functions, but come with almost no guarantees. This
85

CHAPTER 4. NUMERICAL COMPUTATION

Figure 4.4: A saddle point containing both positive and negative curvature. The function
in this example is f (x) = x21 − x22 . Along the axis corresponding to x1 , the function
curves upward. This axis is an eigenvector of the Hessian and has a positive eigenvalue.
Along the axis corresponding to x2 , the function curves downward. This direction is
an eigenvector of the Hessian with negative eigenvalue. The name “saddle point” derives
from the saddle-like shape of this function. This is the quintessential example of a function
with a saddle point. Note that in more than one dimension, it is not necessary to have an
eigenvalue of 0 in order to get a saddle point: it is only necessary to have both positive
and negative eigenvalues.

86

CHAPTER 4. NUMERICAL COMPUTATION

Figure 4.5: Gradient descent fails to exploit the curvature information contained in the
Hessian matrix. Here we use gradient descent on a quadratic function whose Hessian
matrix has condition number 5 (curvature is 5 times larger in one direction than in some
other direction). The lines above the mesh indicate the path followed by gradient descent.
This very elongated quadratic function resembles a long canyon. Gradient descent wastes
time repeatedly descending canyon walls, because they are the steepest feature. Because
the step size is somewhat too large, it has a tendency to overshoot the bottom of the
function and thus needs to descend the opposite canyon wall on the next iteration. The
large positive eigenvalue of the Hessian corresponding to the eigenvector pointed in this
direction indicates that this directional derivative is rapidly increasing, so an optimization
algorithm based on the Hessian could predict that the steepest direction is not actually
a promising search direction in this context.

87

CHAPTER 4. NUMERICAL COMPUTATION

is because the family of functions used in deep learning is quite complicated. In
many other ﬁelds, the dominant approach to optimization is to design optimization algorithms for a limited family of functions. Perhaps the most successful
ﬁeld of specialized optimization is convex optimization. Convex optimization algorithms are able to provide many more guarantees, but are applicable only to
functions for which the Hessian is positive deﬁnite everywhere. Such functions
are well-behaved because they lack saddle points and all of their local minima are
necessarily global minima. However, most problems in deep learning are diﬃcult
to express in terms of convex optimization. Convex optimization is used only as
a subroutine of some deep learning algorithms. Ideas from the analysis of convex
optimization algorithms can be useful for proving the convergence of deep learning algorithms. However, in general, the importance of convex optimization is
greatly diminished in the context of deep learning. For more information about
convex optimization, see Boyd and Vandenberghe (2004) or Rockafellar (1997).

4.4

Constrained Optimization

Sometimes we wish not only to maximize or minimize a function f (x) over all
possible values of x. Instead we may wish to ﬁnd the maximal or minimal value
of f (x) for values of x in some set S. This is known as constrained optimization. Points x that lie within the set S are called feasible points in constrained
optimization terminology.
One simple approach to constrained optimization is simply to modify gradient
descent taking the constraint into account. If we use a small constant step size ,
we can make gradient descent steps, then project the result back into S. If we use
a line search (see previous section), we can search only over step sizes  that yield
new x points that are feasible, or we can project each point on the line back into
the constraint region. When possible, this method can be made more eﬃcient by
projecting the gradient into the tangent space of the feasible region before taking
the step or beginning the line search (Rosen, 1960).
A more sophisticated approach is to design a diﬀerent, unconstrained optimization problem whose solution can be converted into a solution to the original,
constrained optimization problem. For example, if we want to minimize f (x) for
x ∈ R2 with x constrained to have exactly unit L2 norm, we can instead minimize
g(θ) = f ([cos θ, sin θ]T ) with respect to θ, then return [cos θ, sin θ] as the solution
to the original problem. This approach requires creativity; the transformation
between optimization problems must be designed speciﬁcally for each case we
encounter.
The Karush–Kuhn–Tucker (KKT) approach 3 provides a very general solu3

The KKT approach generalizes the method of Lagrange multipliers which only allows equal88

CHAPTER 4. NUMERICAL COMPUTATION

tion to constrained optimization. With the KKT approach, we introduce a new
function called the generalized Lagrangian or generalized Lagrange function.
To deﬁne the Lagrangian, we ﬁrst need to describe S in terms of equations
and inequalities. We want a description of S in terms of m functions gi and n
functions h j so that S = {x | ∀i, g i(x) = 0 and ∀j, h j(x) ≤ 0}. The equations
involving gi are called the equality constraints and the inequalities involving h j
are called inequality constraints.
We introduce new variables λi and α j for each constraint, these are called the
KKT multipliers. The generalized Lagrangian is then deﬁned as
X
X
L(x, λ, α) = f (x) +
λ i gi (x) +
α jh j (x).
i

j

We can now solve a constrained minimization problem using unconstrained
optimization of the generalized Lagrangian. Observe that, so long as at least one
feasible point exists and f (x) is not permitted to have value ∞, then
min max max L(x, λ, α).
x

λ

α,α≥0

has the same optimal objective function value and set of optimal points x as
min f (x).
x∈S

This follows because any time the constraints are satisﬁed,
max max L(x, λ, α) = f (x),
λ

α,α≥0

while any time a constraint is violated,
max max L(x, λ, α) = ∞.
λ α,α≥0

These properties guarantee that no infeasible point will ever be optimal, and that
the optimum within the feasible points is unchanged.
To perform constrained maximization, we can construct the generalized Lagrange function of −f (x), which leads to this optimization problem:
X
X
min max max −f (x) +
λi g i(x) +
αj h j(x).
x

λ

α,α≥0

i

j

We may also convert this to a problem with maximization in the outer loop:
X
X
max min min f (x) +
λ igi (x) −
αj h j (x).
x

λ

α,α≥0

i

ity constraints
89

j

CHAPTER 4. NUMERICAL COMPUTATION

Note that the sign of the term for the equality constraints does not matter; we
may deﬁne it with addition or subtraction as we wish, because the optimization
is free to choose any sign for each λi .
The inequality constraints are particularly interesting. We say that a constraint h i (x) is active if h i(x∗ ) = 0. If a constraint is not active, then the solution
to the problem is the same whether or not that constraint exists. Because an inactive h i has negative value, then the solution to min x max λ maxα,α≥0 L(x, λ, α)
will have αi = 0. We can thus observe that at the solution, αh(x) = 0. In
other words, for all i, we know that at least one of the constraints αi ≥ 0 and
hi (x) ≤ 0 must be active at the solution. To gain some intuition for this idea, we
can say that either the solution is on the boundary imposed by the inequality and
we must use its KKT multiplier to inﬂuence the solution to x, or the inequality
has no inﬂuence on the solution and we represent this by zeroing out its KKT
multiplier.
The properties that the gradient of the generalized Lagrangian is zero, all
constraints on both x and the KKT multipliers are satisﬁed, and α  h(x) =
0 are called the Karush-Kuhn-Tucker (KKT) conditions (Karush, 1939; Kuhn
and Tucker, 1951). Together, these properties describe the optimal points of
constrained optimization problems.
In the case where there are no inequality constraints, the KKT approach
simpliﬁes to the method of Lagrange multipliers. For more information about the
KKT approach, see Nocedal and Wright (2006).

4.5

Example: Linear Least Squares

Suppose we want to ﬁnd the value of x that minimizes
f (x) =

1
||Ax − b|| 22.
2

There are specialized linear algebra algorithms that can solve this problem eﬃciently. However, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.
First, we need to obtain the gradient:
∇x f (x) = A >(Ax − b) = A >Ax − A > b.
We can then follow this gradient downhill, taking small steps. See Algorithm 4.1 for details.
One can also solve this problem using Newton’s method. In this case, because
the true function is quadratic, the quadratic approximation employed by Newton’s
90

CHAPTER 4. NUMERICAL COMPUTATION

Algorithm 4.1 An algorithm to minimize f (x) = 12 ||Ax − b|| 22 with respect to
x using gradient descent.
Set , the step size, and δ, the tolerance, to small, positive numbers.
>
while ||A>Ax
 >− A b|| 2 >> δ do
x ← x −  A Ax − A b
end while
method is exact, and the algorithm converges to the global minimum in a single
step.
Now suppose we wish to minimize the same function, but subject to the constraint x >x ≤ 1. To do so, we introduce the Lagrangian


>
L(x, λ) = f (x) + λ x x − 1 .
We can now solve the problem

min max L(x, λ).
x

λ,λ≥0

The solution to the unconstrained least squares problem is given by x = A +b.
If this point is feasible, then it is the solution to the constrained problem. Otherwise, we must ﬁnd a solution where the constraint is active. By diﬀerentiating
the Lagrangian with respect to x, we obtain the equation
A> Ax − A> b + 2λx = 0.
This tells us that the solution will take the form
x = (A >A + 2λI)−1 A>b.
The magnitude of λ must be chosen such that the result obeys the constraint.
We can ﬁnd this value by performing gradient ascent on λ. To do so, observe
∂
L(x, λ) = x> x − 1.
∂λ
When the norm of x exceeds 1, this derivative is positive, so to ascend the gradient
and increase the Lagrangian with respect to λ, we increase λ. This will in turn
shrink the optimal x. The process continues until x has the correct norm and the
derivative on λ is 0.

91

Chapter 5

Machine Learning Basics
Deep learning is a speciﬁc kind of machine learning. In order to understand deep
learning well, one must have a solid understanding of the basic principles of machine learning. This chapter provides a brief course in the most important general
principles that will be applied throughout the rest of the book. Novice readers or
those that want a wider perspective are encouraged to consider machine learning
textbooks with a more comprehensive coverage of the fundamentals, such as Murphy (2012) or Bishop (2006). If you are already familiar with machine learning
basics, feel free to skip ahead to Section 5.12. That section covers some perspectives on traditional machine learning techniques that have strongly inﬂuenced the
development of deep learning algorithms.

5.1

Learning Algorithms

A machine learning algorithm is an algorithm that is able to learn from data. But
what do we mean by learning? A popular deﬁnition of learning in the context of
computer programs is “A computer program is said to learn from
experience E with respect to some class of tasks T and performance measure
P , if its performance at tasks in T , as measured by P , improves with experience
E” (Mitchell, 1997). One can imagine a very wide variety of experiences E, tasks
T , and performance measures P , and we do not make any attempt in this book
to provide a formal deﬁnition of what may be used for each of these entities.
Instead, the following sections provide intuitive descriptions and examples of the
diﬀerent kinds of tasks, performance measures and experiences that can be used
to construct machine learning algorithms.

92

CHAPTER 5. MACHINE LEARNING BASICS

5.1.1

The Task, T

Machine learning is mostly interesting because of the tasks we can accomplish
with it. From an engineering point of view, machine learning allows us to tackle
tasks that are too diﬃcult to solve with ﬁxed programs written and designed by
human beings. From a scientiﬁc and philosophical point of view, machine learning
is interesting because understanding it allows us to understand the principles that
underlie intelligent behavior, and intelligent behavior is deﬁned as being able to
accomplish certain tasks.
In this relatively formal deﬁnition of the word “task,” the process of learning
itself is not the task. Learning is our means of attaining the ability to perform
the task. For example, if we want a robot to be able to walk, then walking is
the task. We could program the robot to learn to walk, or we could attempt to
directly write a program that speciﬁes how to walk manually.
Many kinds of tasks can be solved with machine learning. Some of the most
common machine learning tasks include the following:
• Classiﬁcation: In this type of task, the computer program is asked to specify
which of k categories some input belongs to. To solve this task, the learning
algorithm is usually asked to produce a function f : Rn → {1, . . . , k} which
may then be applied to any input. Here the output of f (x) can be interpreted as an estimate of the category that x belongs to. There are other
variants of the classiﬁcation task, for example, where f outputs a probability distribution over classes. An example of a classiﬁcation task is object
recognition, where the input is an image (usually described as a set of pixel
brightness values), and the output is a numeric code identifying the object
in the image. For example, the Willow Garage PR2 robot is able to act
as a waiter that can recognize diﬀerent kinds of drinks and deliver them
to people on command (Goodfellow et al., 2010). Modern object recognition is best accomplished with deep learning (Krizhevsky et al., 2012a; Ioﬀe
and Szegedy, 2015). Object recognition is the same basic technology that
allows computers to recognize faces (Taigman et al., 2014), which can be
used to automatically tag people in photo collections and allow computers
to interact more naturally with their users.
• Classiﬁcation with missing inputs : Classiﬁcation becomes more challenging
if the computer program is not guaranteed that every measurement in its
input vector will always be provided. In order to solve the classiﬁcation
task, the learning algorithm only has to deﬁne a single function mapping
from a vector input to a categorical output. When some of the inputs
may be missing, rather than providing a single classiﬁcation function, the
learning algorithm must learn a set of functions. Each function corresponds
93

CHAPTER 5. MACHINE LEARNING BASICS

to classifying x with a diﬀerent subset of its inputs missing. This kind
of situation arises frequently in medical diagnosis, because many kinds of
medical tests are expensive or invasive. One way to eﬃciently deﬁne such
a large set of functions is to learn a probability distribution over all of
the relevant variables, then solve the classiﬁcation task by marginalizing
out the missing variables. With n input variables, we can now obtain all
2n diﬀerent classiﬁcation functions needed for each possible set of missing
inputs, but we only need to learn a single function describing the joint
probability distribution. See Goodfellow et al. (2013b) for an example of
a deep probabilistic model applied to such a task in this way. Many of
the other tasks described in this section can also be generalized to work
with missing inputs; classiﬁcation with missing inputs is just one example
of what machine learning can do.
• Regression : In this type of task, the computer program is asked to predict a
numerical value given some input. To solve this task, the learning algorithm
is asked to output a function f : Rn → R. This type of task is similar to
classiﬁcation, except that the format of output is diﬀerent. An example of
a regression task is the prediction of the expected claim amount that an
insured person will make (used to set insurance premia), or the prediction
of future prices of securities. These kinds of predictions are also used for
algorithmic trading.
• Transcription : In this type of task, the machine learning system is asked to
observe a relatively unstructured representation of some kind of data and
transcribe it into discrete, textual form. For example, in optical character
recognition, the computer program is shown a photograph containing an
image of text and is asked to return this text in the form of a sequence of
characters (e.g. in ASCII or Unicode format). Google Street View uses deep
learning to process address numbers in this way Goodfellow et al. (2014d).
Another example is speech recognition, where the computer program is
provided an audio waveform and emits a sequence of characters or word ID
codes describing the words that were spoken in the audio recording. Deep
learning is a crucial component of modern speech recognition systems used
at major companies including Microsoft, IBM and Google (Hinton et al.,
2012b).
• Translation: In a translation task, the input already consists of a sequence
of symbols in some language, and the computer program must convert this
into a sequence of symbols in another language. This is commonly applied
to natural languages, such as to translate from English to French. Deep
94

CHAPTER 5. MACHINE LEARNING BASICS

learning has recently begun to have an important impact on this kind of
task (Sutskever et al., 2014a; Bahdanau et al., 2014).
• Structured output tasks involve any task where the output is a vector containing important relationships between the diﬀerent elements. This is a
broad category, and includes the transcription and translation tasks described above, but also many other tasks. One example is parsing—mapping
a natural language sentence into a tree that describes its grammatical structure and the relative role of its constituents. See Collobert (2011) for an
example of deep learning applied to a parsing task. Another example is
pixel-wise segmentation of images, where the computer program assigns every pixel in an image to a speciﬁc category. For example, deep learning can
be used to annotate the locations of roads in aerial photographs (Mnih and
Hinton, 2010). The output need not have its form mirror the structure of
the input as closely as in these annotation-style tasks. For example, in an
image captioning, the computer program observes an image and outputs a
natural language sentence describing the image (Kiros et al., 2014a,b; Mao
et al., 2015; Vinyals et al., 2015; Donahue et al., 2014; Karpathy and Li,
2015; Fang et al., 2015; Xu et al., 2015a). These tasks are called structured output tasks because the program must output several values that
are all tightly inter-related. For example, the words produced by an image
captioning program must form a valid sentence.
• Anomaly detection: In this type of task, the computer program sifts through
a set of events or objects, and ﬂags some of them as being unusual or atypical. An example of an anomaly detection task is credit card fraud detection.
By modeling your purchasing habits, a credit card company can detect misuse of your cards. If a thief steals your credit card or credit card information,
the thief’s purchases will often come from a diﬀerent probability distribution
over purchase types than your own. The credit card company can prevent
fraud by placing a hold on an account as soon as that card has been used
for an uncharacteristic purchase.
• Synthesis and sampling: In this type of task, the machine learning algorithm
is asked to generate new examples that are similar to those in the training
data. This can be useful for media applications where it can be expensive
or boring for an artist to generate large volumes of content by hand. For example, video games can automatically generate textures for large objects or
landscapes, rather than requiring an artist to manually label each pixel (Luo
et al., 2013). In some cases, we want the sampling or synthesis procedure
to generate some speciﬁc kind of output given the input. For example, in a
speech synthesis task, we provide a written sentence and ask the program
95

CHAPTER 5. MACHINE LEARNING BASICS

to emit an audio waveform containing a spoken version of that sentence.
This is a kind of structured output task, but with the added qualiﬁcation
that there is no single correct output for each input, and we explicitly desire
a large amount of variation in the output, in order for the output to seem
more natural and realistic.
• Imputation of missing values: In this type of task, the machine learning
algorithm is given a new example x ∈ Rn , but with some entries xi of
x missing. The algorithm must provide a prediction of the values of the
missing entries.
• Denoising: In this type of task, the machine learning algorithm is given
in input a corrupted example x̃ ∈ Rn obtained by an unknown corruption
process from a clean example x ∈ Rn . The learner must predict the clean
example x from its corrupted version x̃, or more generally predict the conditional probability distribution P (x | x̃).
• Density or probability function estimation: In the density estimation problem, the machine learning algorithm is asked to learn a function pmodel :
Rn → R, where pmodel (x) can be interpreted as a probability density function (if x is continuous) or a probability function (if x is discrete) on the
space that the examples were drawn from. To do such a task well (we will
specify exactly what that means when we discuss performance measures P ),
the algorithm needs to learn the structure of the data it has seen. It must
know where examples cluster tightly and where they are unlikely to occur.
Most of the tasks described above require that the learning algorithm has at
least implicitly captured the structure of the probability distribution. Density estimation allows us to explicitly capture that distribution. In principle,
we can then perform computations on that distribution in order to solve the
other tasks as well. For example, if we have performed density estimation to
obtain a probability distribution p(x), we can use that distribution to solve
the missing value imputation task. If a value xi is missing, then we know the
distribution over it is given by p(xi | x−i) . In practice, density estimation
does not always allow us to solve all of these related tasks, because in many
cases the required operations on p(x) are computationally intractable.
Of course, many other tasks and types of tasks are possible. The types of tasks
we listed here are only intended to provide examples of what machine learning
can do, not to deﬁne a rigid taxonomy of tasks.

96

CHAPTER 5. MACHINE LEARNING BASICS

5.1.2

The Performance Measure, P

In order to evaluate the abilities of a machine learning algorithm, we must design
a quantitative measure of its performance. Usually this performance measure P
is speciﬁc to the task T being carried out by the system.
For tasks such as classiﬁcation, classiﬁcation with missing inputs, and transcription, we often measure the accuracy of the model. Accuracy is just the
proportion of examples for which the model produces the correct output. We can
also obtain equivalent information by measuring the error rate, the proportion of
examples for which the model produces an incorrect output. We often refer to
the error rate as the expected 0-1 loss. The 0-1 loss on a particular example is 0
if it is correctly classiﬁed and 1 if it is not. For tasks such as density estimation,
we can measure the probability the model assigns to some examples.
Usually we are interested in how well the machine learning algorithm performs
on data that it has not seen before, since this determines how well it will work
when deployed in the real world. We therefore evaluate these performance measures using a test set of data that is separate from the data used for training the
machine learning system.
The choice of performance measure may seem straightforward and objective,
but it is often diﬃcult to choose a performance measure that corresponds well to
the desired behavior of the system.
In some cases, this is because it is diﬃcult to decide what should be measured. For example, when performing a transcription task, should we measure
the accuracy of the system at transcribing entire sequences, or should we use a
more ﬁne-grained performance measure that gives partial credit for getting some
elements of the sequence correct? When performing a regression task, should we
penalize the system more if it frequently makes medium-sized mistakes or if it
rarely makes very large mistakes? These kinds of design choices depend on the
application.
In other cases, we know what quantity we would ideally like to measure, but
measuring it is impractical. For example, this arises frequently in the context of
density estimation. Many of the best probabilistic models represent probability
distributions only implicitly. Computing the actual probability value assigned
to a speciﬁc point in space is intractable. In these cases, one must design an
alternative criterion that still corresponds to the design objectives, or design a
good approximation to the desired criterion.

5.1.3

The Experience, E

Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning
97

CHAPTER 5. MACHINE LEARNING BASICS

process.
Most of the learning algorithms in this book can be understood as being
allowed to experience an entire dataset. A dataset is a collection of many objects
called examples, with each example containing many features that have been
objectively measured. Sometimes we will also call examples data points.
One of the oldest datasets studied by statisticians and machine learning researchers is the Iris dataset (Fisher, 1936). It is a collection of measurements of
diﬀerent parts of 150 iris plants. Each individual plant corresponds to one example. The features within each example are the measurements of each of the parts
of the plant: the sepal length, sepal width, petal length and petal width. The
dataset also records which species each plant belonged to. Three diﬀerent species
are represented in the dataset.
Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context
of deep learning, we usually want to learn the entire probability distribution that
generated a dataset, whether explicitly as in density estimation or implicitly for
tasks like synthesis or denoising. Some other unsupervised learning algorithms
perform other roles, like dividing the dataset into clusters of similar examples.
Supervised learning algorithms experience a dataset containing features, but
each example is also associated with a label or target. For example, the Iris
dataset is annotated with the species of each iris plant. A supervised learning
algorithm can study the Iris dataset and learn to classify iris plants into three
diﬀerent species based on their measurements.
Roughly speaking, unsupervised learning involves observing several examples
of a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while
supervised learning involves observing several examples of a random vector x and
an associated value or vector y, and learning to predict y from x, e.g. estimating
p(y | x). The term supervised learning originates from the view of the target
y being provided by an instructor or teacher that shows the machine learning
system what to do. In unsupervised learning, there is no instructor or teacher,
and the algorithm must learn to make sense of the data without this guide.
Unsupervised learning and supervised learning are not formally deﬁned terms.
The lines between them are often blurred. Many machine learning technologies
can be used to perform both tasks. For example, the chain rule of probability
states that for a vector x ∈ Rn , the joint distribution can be decomposed as
p(x) = Πni=1 p(x i | x 1, . . . , x i−1).
This decomposition means that we can solve the ostensibly unsupervised problem
of modeling p(x) by splitting it into n supervised learning problems. Alternatively,
98

CHAPTER 5. MACHINE LEARNING BASICS

we can solve the supervised learning problem of learning p(y | x) by using traditional unsupervised learning technologies to learn the joint distribution p(x, y)
and inferring
p(x, y)
p(y | x) = P
0 .
y0 p(x, y )

Though unsupervised learning and supervised learning are not completely formal
or distinct concepts, they do help to roughly categorize some of the things we
do with machine learning algorithms. Traditionally, people refer to regression,
classiﬁcation and structured output problems as supervised learning. Density
estimation in support of other tasks is usually considered unsupervised learning.
Some machine learning algorithms do not just experience a ﬁxed dataset.
For example, reinforcement learning algorithms interact with an environment, so
there is a feedback loop between the learning system and its experiences. Such
algorithms are beyond the scope of this book.
Most machine learning algorithms simply experience a dataset. A dataset can
be described in many ways. In all cases, a dataset is a collection of examples.
Each example is a collection of observations called features collected from a different time or place. If we wish to make a system for recognizing objects from
photographs, we might use a machine learning algorithm where each example is a
photograph, and the features within the example are the brightness values of each
of the pixels within the photograph. If we wish to perform speech recognition,
we might collect a dataset where each example is a recording of a person saying
a word or sentence, and each of the features is the amplitude of the sound wave
at a particular moment in time.
One common way of describing a dataset is with a design matrix. A design
matrix is a matrix containing a diﬀerent example in each row. Each column of the
matrix corresponds to a diﬀerent feature. For instance, the Iris dataset contains
150 examples with four features for each example. This means we can represent
the dataset with a design matrix X ∈ R150×4 , where Xi,1 is the sepal length
of plant i, Xi,2 is the sepal width of plant i, etc. We will describe most of the
learning algorithms in this book in terms of how they operate on design matrix
datasets.
Of course, to describe a dataset as a design matrix, it must be possible to
describe each example as a vector, and each of these vectors must be the same size.
This is not always possible. For example, if you have a collection of photographs
with diﬀerent widths and heights, then diﬀerent photographs will contain diﬀerent
numbers of pixels, so not all of the photographs may be described with the same
length of vector. Diﬀerent sections of this book describe how to handle diﬀerent
types of heterogeneous data. In cases like these, rather than describing the dataset
as a matrix with m rows, we will describe it as a set containing m elements, e.g.
99

CHAPTER 5. MACHINE LEARNING BASICS

{x(1) , x(2) , . . . , x(m) }. This notation does not imply that any two example vectors
x(i) and x(j) have the same size.
In the case of supervised learning, the example contains a label or target as
well as a collection of features. For example, if we want to use a learning algorithm
to perform object recognition from photographs, we need to specify which object
appears in each of the photos. We might do this with a numeric code, with 0
signifying a person, 1 signifying a car, 2 signifying a cat, etc. Often when working
with a dataset containing a design matrix of feature observations X, we also
provide a vector of labels y, with yi providing the label for example i.
Of course, sometimes the label may be more than just a single number. For
example, if we want to train a speech recognition system to transcribe entire
sentences, then the label for each example sentence is a sequence of words.
Just as there is no formal deﬁnition of supervised and unsupervised learning,
there is no rigid taxonomy of datasets or experiences. The structures described
here cover most cases, but it is always possible to design new ones for new applications.

5.2

Example: Linear Regression

In the previous section, we saw that a machine learning algorithm is an algorithm
that is capable of improving a computer program’s performance at some task via
experience. Now it is time to deﬁne some speciﬁc machine learning algorithms.
Let’s begin with an example of a simple machine learning algorithm: linear
regression. In this section, we will only describe what the linear regression algorithm does. We wait until later sections of this chapter to justify the algorithm
and show more formally that it actually works.
As the name implies, linear regression solves a regression problem. In other
words, the goal is to build a system that can take a vector x ∈ Rn as input and
predict the value of a scalar y ∈ R as its output. In the case of linear regression,
the output is a linear function of the input. Let ŷ be the value that our model
predicts y should take on. We deﬁne the output to be
ŷ = w>x
where w ∈ Rn is a vector of parameters.
Parameters are values that control the behavior of the system. In this case, w i
is the coeﬃcient that we multiply by feature x i before summing up the contributions from all the features. We can think of w as a set of weights that determine
how each feature aﬀects the prediction. If a feature xi receives a positive weight
wi , then increasing the value of that feature increases the value of our prediction
ŷ. If a feature receives a negative weight, then increasing the value of that feature
100

CHAPTER 5. MACHINE LEARNING BASICS

decreases the value of our prediction. If a feature’s weight is large in magnitude,
then it has a large eﬀect on the prediction. If a feature’s weight is zero, it has no
eﬀect on the prediction.
We thus have a deﬁnition of our task T : to predict y from x by outputting
ŷ = w > x. Next we need a deﬁnition of our performance measure, P .
Let’s suppose that we have a design matrix of m example inputs that we will
not use for training, only for evaluating how well the model performs. We also
have a vector of regression targets providing the correct value of y for each of
these examples. Because this dataset will only be used for evaluation, we call it
the test set. Let’s refer to the design matrix of inputs as X (test) and the vector
of regression targets as y (test) .
One way of measuring the performance of the model is to compute the mean
squared error of the model on the test set. If ŷ (test) is the predictions of the model
on the test set, then the mean squared error is given by
MSEtest =

1 X (test)
− y(test) )2i .
(ŷ
m
i

Intuitively, one can see that this error measure decreases to 0 when yˆ(test) = y(test) .
We can also see that
MSE test =

1 (test)
||ŷ
− y(test) ||22,
m

so the error increases whenever the Euclidean distance between the predictions
and the targets increases.
To make a machine learning algorithm, we need to design an algorithm that
will improve the weights w in a way that reduces MSE test when the algorithm
is allowed to gain experience by observing a training set (X(train), y (train) ). One
intuitive way of doing this (which we will justify later) is just to minimize the
mean squared error on the training set, MSEtrain .
To minimize MSE train, we can simply solve for where its gradient is 0:
∇wMSEtrain = 0
1
⇒ ∇w ||yˆ(train) − y(train)||22 = 0
m
1
⇒ ∇w||X (train) w − y(train)|| 22 = 0
m
⇒ ∇w (X

(train)

(train) >

w−y

) (X

(train)

w−y

(train)

)=0

⇒ ∇ w(w> X (train)>X (train) w − 2w>X (train)>y (train) + y (train)>y (train) ) = 0
⇒ 2X (train)> X (train)w − 2X(train)> y (train) = 0
101

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.1: Consider this example linear regression problem, with a training set consisting
of 5 data points, each containing one feature. This means that the weight vector w
contains only a single parameter to learn, w1 . (Left) Observe that linear regression learns
to set w 1 such that the line y = w1x comes as close as possible to passing through all the
training points. (Right) The plotted point indicates the value of w 1 found by the normal
equations, which we can see minimizes the mean squared error on the training set.

⇒ w = (X (train)> X (train)) −1 X (train)> y(train)

(5.1)

The system of equations deﬁned by Eq. 5.1 is known as the normal equations.
Solving these equations constitutes a simple learning algorithm. For an example
of the linear regression learning algorithm in action, see Fig. 5.1.
It’s worth noting that the term linear regression is often used to refer to a
slightly more sophisticated model with one additional parameter—an intercept
term b. In this model
ŷ = w>x + b
so the mapping from parameters to predictions is still a linear function but the
mapping from features to predictions is now an aﬃne function. This extension
to aﬃne functions means that the plot of the model’s predictions still looks like
a line, but it need not pass through the origin. We will frequently use the term
“linear” when referring to aﬃne functions throughout this book.
Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work. In
the subsequent sections we will describe some of the basic principles underlying
learning algorithm design and demonstrate how these principles can be used to
build more complicated learning algorithms.

102

CHAPTER 5. MACHINE LEARNING BASICS

5.3

Generalization, Capacity, Overﬁtting and Underﬁtting

The central challenge in machine learning is that we must perform well on new,
previously unseen inputs—not just those on which our model was trained. The
ability to perform well on previously unobserved inputs is called generalization.
Typically, when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training
error, and we reduce this training error. So far, what we have described is simply
an optimization problem. What separates machine learning from optimization is
that we want the generalization error to be low as well. The generalization error
is deﬁned as the expected value of the error on a new input. Here the expectation
is taken across diﬀerent possible inputs, drawn from the distribution of inputs we
expect the system to encounter in practice.
We typically estimate the generalization error of a machine learning model by
measuring its performance on a test set of examples that were collected separate
from the training set.
In our linear regression example, we trained the model by minimizing the
training error,
1
||X(train) w − y (train)|| 22 ,
(train)
m
1
||X(test) w − y (test) ||22 .
but we actually care about the test error, m(test)
How can we aﬀect performance on the test set when we only get to observe
the training set? The ﬁeld of statistical learning theory provides some answers. If
the training and the test set are collected arbitrarily, there is indeed little we can
do. If we are allowed to make some assumptions about how the training and test
set are collected, then we can make some progress.
We typically make a set of assumptions known collectively as the i.i.d. assumptions. These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed,
drawn from the same probability distribution as each other. We call that shared
underlying distribution the data generating distribution, or data generating process (which is particularly relevant if the examples are not independent). This
probabilistic framework allows us to mathematically study the relationship between training error and test error.
One immediate connection we can observe between the training and test error
is that for a randomly selected model, the two have the same expected value.
Suppose we have a probability distribution p(x, y) and we sample from it repeatedly to generate the train set and the test set. For some ﬁxed value w, then
the expected training set error under this sampling process is exactly the same

103

CHAPTER 5. MACHINE LEARNING BASICS

as the expected test set error under this sampling process. The only diﬀerence
between the two conditions is the name we assign to the dataset we sample. From
this observation, we can see that it is natural for there to be some relationship
between training and test error under these assumptions.
Of course, when we use a machine learning algorithm, we do not ﬁx the
parameters ahead of time, then sample both datasets. We sample the training
set, then use it to choose the parameters to reduce training set error, then sample
the test set. Under this process, the expected test error is greater than or equal to
the expected value of training error. The factors determining how well a machine
learning algorithm will perform are its ability to:
1. Make the training error small.
2. Make the gap between training and test error small.
These two factors correspond to the two central challenges in machine learning:
underﬁtting and overﬁtting. Underﬁtting occurs when the model is not able to
obtain a suﬃciently low error value on the training set. Overﬁtting occurs when
the gap between the training error and test error is too large.
We can control whether a model is more likely to overﬁt or underﬁt by altering
its capacity. Informally, a model’s capacity is its ability to ﬁt a wide variety of
functions. Models with low capacity may struggle to ﬁt the training set. Models
with high capacity can overﬁt, i.e., memorize properties of the training set that
do not serve them well on the test set.
One way to control the capacity of a learning algorithm is by choosing its
hypothesis space, the set of functions that the learning algorithm is allowed to
choose as being the solution. For example, the linear regression algorithm has the
set of all linear functions of its input as its hypothesis space. We can generalize
linear regression to include polynomials, rather than just linear functions, in its
hypothesis space. Doing so increases the model’s capacity.
A polynomial of degree one gives us the linear regression model with which
we are already familiar, with prediction
ŷ = b + wx.
By introducing x 2 as another feature provided to the linear regression model, we
can learn a model that is quadratic as a function of x:
ŷ = b + w 1x + w2 x2 .
Note that this is still a linear function of the parameters, so we can still use the
normal equations to train the model in closed form. We can continue to add more
104

CHAPTER 5. MACHINE LEARNING BASICS

powers of x as additional features, for example to obtain a polynomial of degree
9:
9
X
ŷ = b +
w ixi .
i=1

Machine learning algorithms will generally perform best when their capacity
is appropriate in regard to the true complexity of the task they need to perform
and the amount of training data they are provided with. Models with insuﬃcient
capacity are unable to solve complex tasks. Model with high capacity can solve
complex tasks, but when their capacity is higher than needed to solve the present
task they may overﬁt.
Fig. 5.2 shows this principle in action. We compare a linear, quadratic and
degree-9 predictor attempting to ﬁt a problem where the true underlying function is quadratic. The linear function is unable to capture the curvature in the
true underlying problem, so it underﬁts. The degree-9 predictor is capable of
representing the correct function, but it is also capable of representing inﬁnitely
many other functions that pass exactly through the training points, because we
have more parameters than training examples. We have little chance of choosing
a solution that generalizes well when so many wildly diﬀerent solutions exist. In
this example, the quadratic model is perfectly matched the true structure of the
task so it generalizes well to new data.

Figure 5.2: We ﬁt three models to this example training set. The training data was
generated synthetically, by randomly sampling x values and choosing y deterministically
by evaluating a quadratic function. (Left) A linear function ﬁt to the data suﬀers from
underﬁtting—it cannot capture the curvature that is present in the data. (Center) A
quadratic function ﬁt to the data generalizes well to unseen points. It does not suﬀer
from a signiﬁcant amount of overﬁtting or underﬁtting. (Right) A polynomial of degree
9 ﬁt to the data suﬀers from overﬁtting. Here we used the Moore-Penrose pseudo-inverse
to solve the underdetermined normal equations. The solution passes through all of the
training points exactly, but we have not been lucky enough for it to extract the correct
structure. It now has a deep valley in between two training points that does not appear
in the true underlying function. It also increases sharply on the left side of the data,
while the true function decreases in this area.
105

CHAPTER 5. MACHINE LEARNING BASICS

Here we have only described changing a model’s capacity by changing the
number of input features it has (and simultaneously adding new parameters associated with those features). There are many other ways of controlling the capacity
of a machine learning algorithm, which we will explore in the sections ahead.
Our modern ideas about improving the generalization of machine learning
models are reﬁnements of thought dating back to philosophers at least as early
as Ptolemy. Many early scholars invoke a principle of parsimony that is now
most widely known as Occam’s razor (c. 1287-1347). This principle states that
among competing hypotheses that explain known observations equally well, one
should choose the “simplest” one. This idea was formalized and made more precise in the 20th century by the founders of statistical learning theory (Vapnik and
Chervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik, 1995). Statistical
learning theory provides various means of quantifying model capacity and showing
that the discrepancy between training error and generalization error is bounded
by a quantity that grows with the ratio of capacity to number of training examples (Vapnik and Chervonenkis, 1971; Vapnik, 1982; Blumer et al., 1989; Vapnik,
1995). These bounds provide intellectual justiﬁcation that machine learning algorithms can work, but they are rarely used in practice when working with deep
learning algorithms. This is in part because the bounds are often quite loose and
in part because it can be quite diﬃcult to determine the capacity of deep learning
algorithms.
We must remember that while simpler functions are more likely to generalize
(to have a small gap between training and test error) we must still choose a
suﬃciently complex hypothesis to achieve low training error. Typically, training
error decreases until it asymptotes to the minimum possible error value as model
capacity increases (assuming your error measure has a minimum value). Typically,
generalization error has a U-shaped curve as a function of model capacity. This
is illustrated in Figure 5.3.
To reach the most extreme case of arbitrarily high capacity, we introduce
the concept of non-parametric models. So far, we have seen only parametric
models, such as linear regression. Parametric models learn a function described
by a parameter vector whose size is ﬁnite and ﬁxed before any data is observed.
Non-parametric models have no such limitation.
Sometimes, non-parametric models are just theoretical abstractions (such an
algorithm that searches over all possible probability distributions) that cannot be
implemented in practice. However, we can also design practical non-parametric
models by making their complexity a function of the training set size. One example of such an algorithm is nearest neighbor regression. Unlikely linear regression,
which has a ﬁxed-length vector of weights, the nearest neighbor regression model
simply stores the X and y from the training set. When asked to classify a test
106

CHAPTER 5. MACHINE LEARNING BASICS

Error

Underﬁtting zone

Overﬁtting zone

generalization
error
generalization gap

training
error

optimal
capacity

Capacity

Figure 5.3: Typical relationship between capacity (horizontal axis) and both training
(bottom curve, dotted) and generalization (or test) error (top curve, bold). At the left end
of the graph, training error and generalization error are both high. This is the underﬁtting
regime. As we increase capacity, training error decreases, but the gap between training
and generalization error increases. Eventually, the size of this gap outweighs the decrease
in training error, and we enter the overﬁtting regime, where capacity is too large, above
the optimal capacity.

107

CHAPTER 5. MACHINE LEARNING BASICS

point x, the model looks up the nearest entry in the training set and returns the
associated regression target. In other words, ŷ = yi where i = arg min ||X i,:−x|| 22 .
This learning algorithm is able to achieve the minimum possible training error
(which might be greater than zero, if two identical inputs are associated with
diﬀerent outputs) on any regression dataset.
Finally, we can also create a non-parametric learning algorithm by wrapping a
parametric learning algorithm inside another algorithm that increases the number
of parameters as needed. For example, we could imagine an outer loop of learning
that changes the degree of the polynomial learned by linear regression on top of
a polynomial expansion of the input.
The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on
many problems, because there may still be some noise in the distribution. In the
case of regression, the mapping from x to y may be inherently stochastic, or x
may contain insuﬃcient information to perfectly predict y, resulting in a probability distribution over possible values for y. The error incurred by an oracle
making predictions from the true distribution p(x, y) is called the Bayes error.
Training and generalization vary as the size of the training set varies. Expected
generalization error can never decrease as the number of training examples decreases. For non-parameteric models, more data yields better generalization until
the best possible error is achieved. Any ﬁxed parametric model with less than
optimal capacity will asymptote to a higher error value. See Fig. 5.4 for an illustration. Note that it is possible for the model to have optimal capacity and yet
still have a large gap between training and generalization error. In this situation,
we may be able to reduce this gap by gathering more training examples.
It’s worth mentioning that capacity is not just determined by which model
we use. The model speciﬁes which family of functions the learning algorithm can
choose from when varying the parameters in order to reduce a training objective.
This is called the representational capacity of the model. In many cases, ﬁnding
the best function within this family is a very diﬃcult optimization problem. In
practice, the learning algorithm does not actually ﬁnd the best function, just one
that signiﬁcantly reduces the training error. These additional restrictions mean
that the model’s eﬀective capacity may be less than its representational capacity.

5.3.1

The No Free Lunch Theorem

Although learning theory, sketched above, suggests that it is possible to generalize,
one should consider a serious caveat, discussed here. Generally speaking, inductive
reasoning, or inferring general rules from a limited set of examples, is not logically
valid. To logically infer a rule describing every member of a set, one must have
information about every member of that set. One may wonder then how the
108

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.4: The eﬀect of the training dataset size on the train and test error of the model,
as well as on the optimal model capacity. We used a synthetic regression problem, generated a single test set, and then generated several diﬀerent sizes of training set. For each
size, we generated 40 diﬀerent training sets in order to plot error bars showing 95% conﬁdence intervals. Left) The mean squared error on the train and test set for two diﬀerent
models. One is a quadratic model, while the other has its degree chosen to minimize the
test error. Both are ﬁt in closed form, using the Moore-Penrose pseudoinverse to solve the
normal equations. For the quadratic model, the training error increases as the size of the
training set increases. This is because larger datasets are harder to ﬁt. Simultaneously,
the test error decreases, because fewer incorrect hypotheses are consistent with the training data. However, the quadratic model does not have enough capacity to solve the task,
so its test error asymptotes to a high value. Ultimately test error of the optimal capacity
model asymptotes to the Bayes error. The training error of the optimal capacity model
can dip below the Bayes error, due to the ability of the training algorithm to memorize
speciﬁc instances of this noise, but as the training size increases to inﬁnity the training
error of any model must rise to at least the Bayes error. Right) As the training set size
increases, the optimal capacity increases. We observe this eﬀect here by plotting the
degree of the optimal polynomial regressor. The optimal capacity plateaus after reaching
a level of suﬃcient complexity to solve the task.

109

CHAPTER 5. MACHINE LEARNING BASICS

claims that machine learning can generalize well are logically valid.
In part, machine learning avoids this problem by oﬀering only probabilistic
rules, rather than the entirely certain rules used in purely logical reasoning. Machine learning promises to ﬁnd rules that are probably correct about most members
of the set they concern.
Unfortunately, even this does not resolve the entire problem. The no free
lunch theorem for machine learning (Wolpert, 1996) states that, averaged over
all possible data generating distributions, every classiﬁcation algorithm has the
same error rate when classifying previously unobserved points. In other words,
in some sense, no machine learning algorithm is universally any better than any
other. The most sophisticated algorithm we can conceive of has the same average
performance (over all possible tasks) as merely predicting that every point belongs
to the same class.
Fortunately, these results hold only when we average over all possible data
generating distributions. If we make assumptions about the kinds of probability
distributions we encounter in real-world applications, then we can design learning
algorithms that perform well on these distributions.
This means that the goal of machine learning research is not to seek a universal
learning algorithm or the absolute best learning algorithm. Instead, our goal is
to understand what kinds of distributions are relevant to the “real world” that
an AI agent experiences, and what kinds of machine learning algorithms perform
well on data drawn from the kinds of data generating distributions we care about.

5.3.2

Regularization

The no free lunch theorem implies that we must design our machine learning
algorithms to perform well on a speciﬁc task. We do so by building a set of
preferences into the learning algorithm. When these preferences are aligned with
the learning problems we ask the algorithm to solve, it performs better.
So far, the only method of modifying a learning algorithm we have discussed
is to increase or decrease the model’s capacity by adding or removing functions
from the hypothesis space of solutions the learning algorithm is able to choose. We
gave the speciﬁc example of increasing or decreasing the degree of a polynomial
for a regression problem. The view we have described so far is oversimpliﬁed.
The behavior of our algorithm is strongly aﬀected not just by how large we
make the set of functions allowed in its hypothesis space, but by the speciﬁc
identity of those functions. The learning algorithm we have studied so far, linear
regression, has a hypothesis space consisting of the set of linear functions of its
input. These linear functions can be very useful for problems where the relationship between inputs and outputs truly is close to linear. They are less useful for
problems that behave in a very non-linear fashion. For example, linear regression
110

CHAPTER 5. MACHINE LEARNING BASICS

would not perform very well if we tried to use it to predict sin(x) from x. We can
thus control the performance of our algorithms by choosing what kind of functions
we allow them to draw solutions from, as well as by controlling the amount of
these functions.
We can also give a learning algorithm a preference for one solution in its
hypothesis space to another. This means that both functions are eligible, but one
is preferred. The unpreferred solution may only be chosen if it ﬁts the training
data signiﬁcantly better than the preferred solution.
For example, we can modify the training criterion for linear regression to
include weight decay. To perform linear regression with weight decay, we minimize
not only the mean squared error on the training set, but instead a criterion J (w)
that expresses a preference for the weights to have smaller squared L2 norm.
Speciﬁcally,
J(w) = MSEtrain + λw >w,
where λ is a value chosen ahead of time that controls the strength of our preference
for smaller weights. When λ = 0, we impose no preference, and larger λ forces the
weights to become smaller. Minimizing J(w) results in a choice of weights that
make a tradeoﬀ between ﬁtting the training data and being small. This gives
us solutions that have a smaller slope, or put weight on fewer of the features.
As an example of how we can control a model’s tendency to overﬁt or underﬁt
via weight decay, we can train a high-degree polynomial regression model with
diﬀerent values of λ. See Fig. 5.5 for the results.

111

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.5: We ﬁt a high-degree polynomial regression model to our example training set
from Fig. 5.2. The true function is quadratic, but here we use only models with degree 9.
We vary the amount of weight decay to prevent these high-degree models from overﬁtting.
(Left) With very large λ, we can force the model to learn a function with no slope at
all. This underﬁts because it can only represent a constant function. (Center) With a
medium value of λ, the learning algorithm recovers a curve with the right general shape.
Even though the model is capable of representing functions with much more complicated
shape, weight decay has encouraged it to use a simpler function described by smaller
coeﬃcients. (Right) With weight decay approaching zero (i.e., using the Moore-Penrose
pseudo-inverse to solve the underdetermined problem with minimal regularization), the
degree-9 polynomial overﬁts signiﬁcantly, as we saw in Fig. 5.2.

Expressing preferences for one function over another is a more general way
of controlling a model’s capacity than including or excluding members from the
hypothesis space. We can think of excluding a function from a hypothesis space
as expressing an inﬁnitely strong preference against that function.
In our weight decay example, we expressed our preference for linear functions
deﬁned with smaller weights explicitly, via an extra term in the criterion we
minimize. There are many other ways of expressing preferences for diﬀerent
solutions, both implicitly and explicitly. Together, these diﬀerent approaches
are known as regularization. Regularization is any modiﬁcation we make
to a learning algorithm that is intended to reduce its generalization
error but not its training error. Regularization is one of the central concerns KEY
of the ﬁeld of machine learning, rivalled in its importance only by optimization. IDEA
The no free lunch theorem has made it clear that there is no best machine
learning algorithm, and, in particular, no best form of regularization. Instead
we must choose a form of regularization that is well-suited to the particular task
we want to solve. The philosophy of deep learning in general and this book in
particular is that a very wide range of tasks (such as all of the intellectual tasks
that people can do) may all be solved eﬀectively using very general-purpose forms
of regularization.

112

CHAPTER 5. MACHINE LEARNING BASICS

5.4

Hyperparameters and Validation Sets

Most machine learning algorithms have several settings that we can use to control
the behavior of the learning algorithm. These settings are called hyperparameters.
The values of hyperparameters are not adapted by the learning algorithm itself
(though we can design a nested learning procedure where one learning algorithm
learns the best hyperparameters for another learning algorithm).
In the polynomial regression example we saw in Fig. 5.2, there is a single
hyperparameter: the degree of the polynomial, which acts as a capacity hyperparameter. The λ value used to control the strength of weight decay is another
example of a hyperparameter.
Sometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is diﬃcult to optimize. More frequently, we do
not learn the hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control
model capacity. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overﬁtting (refer
to Figure 5.3). For example, we can always ﬁt the training set better with a
higher degree polynomial and a weight decay setting of λ = 0..
To solve this problem, we need a validation set of examples that the training
algorithm does not observe.
Earlier we discussed how a held-out test set, composed of examples coming
from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. It is important
that the test examples are not used in any way to make choices about the model,
including its hyperparameters. For this reason, no example from the test set can
be used in the validation set.
For this reason, we always construct the validation set from the training data.
Speciﬁcally, we split the training data into two disjoint subsets. One of these
subsets is used to learn the parameters. The other subset is our validation set,
used to estimate the generalization error during or after training, allowing for the
hyperparameters to be updated accordingly. The subset of data used to learn
the parameters is still typically called the training set, even though this may
be confused with the larger pool of data used for the entire training process.
The subset of data used to guide the selection of hyperparameters is called the
validation set. Since the validation set is used to “train” the hyperparameters,
the validation set error will underestimate the test set error, though typically by
a smaller amount than the training error. Typically, one uses about 80% of the
data for training and 20% for validation.
In practice, when the same test set has been used repeatedly to evaluate
performance of diﬀerent algorithms over many years, and especially if we consider
113

CHAPTER 5. MACHINE LEARNING BASICS

all the attempts from the scientiﬁc community at beating the reported state-ofthe-art performance on that test set, we end up having optimistic evaluations with
the test set as well. Benchmarks can thus become stale and then do not reﬂect
the true ﬁeld performance of a trained system. Thankfully, the community tends
to move on to new (and usually more ambitious and larger) benchmark datasets.

5.4.1

Cross-Validation

One issue with the idea of splitting the dataset into train/test or train/validation/test
subsets is that only a small fraction of examples are used to evaluate generalization. The consequence is that there is a lot of statistical uncertainty around the
estimated average test error, making it diﬃcult to claim that algorithm A works
better than algorithm B on the given task.
With large datasets with hundreds of thousands of examples or more, this
is not a serious issue, but when the dataset is too small, there are alternative
procedures, which allow one to use all of the examples in the estimation of the
mean test error, at the price of increased computational cost. These procedures
are based on the idea of repeating the training / testing computation on diﬀerent
randomly chosen subsets or splits of the original dataset. The most common of
these is the k-fold cross-validation procedure, in which a partition of the dataset
is formed by splitting it in k non-overlapping subsets. Then k train/test splits
can be obtained by keeping each time the i-th subset as a test set and the rest as a
training set. The average test error across all these k training/testing experiments
can then be reported. One problem is that there exists no unbiased estimators of
the variance of such average error estimators (Bengio and Grandvalet, 2004), but
approximations are typically used.
If model selection or hyperparameter optimization is required, things get more
computationally expensive: one can recurse the k-fold cross-validation idea, inside the training set. So we can have an outer loop that estimates test error and
provides a “training set” for a hyperparameter-free learner, calling it k times to
“train”. That hyperparameter-free learner can then split its received training set
by k-fold cross-validation into internal training/validation subsets (for example,
splitting into k − 1 subsets is convenient, to reuse the same test blocks as the
outer loop), call a hyperparameter-speciﬁc learner for each choice of hyperparameter value on each of the training partition of this inner loop, and compute the
validation error by averaging across the k − 1 validation sets the errors made by
the k − 1 hyperparameter-speciﬁc learners trained on each of the internal training
subsets.

114

CHAPTER 5. MACHINE LEARNING BASICS

5.5

Estimators, Bias and Variance

The ﬁeld of statistics gives us many tools that can be used to achieve the machine
learning goal of solving a task not only on the training set but also to generalize.
Foundational concepts such as parameter estimation, bias and variance are useful
to formally characterize notions of generalization, underﬁtting and overﬁtting.

5.5.1

Point Estimation

Point estimation is the attempt to provide the single “best” prediction of some
quantity of interest. In general the quantity of interest can be a single parameter
or a vector of parameters in some parametric model, such as the weights in our
linear regression example in Section 5.2, but it can also be a whole function.
In order to distinguish estimates of parameters from their true value, our
convention will be to denote a point estimate of a parameter θ by θ̂.
Let {x (1) , . . . , x(m) } be a set of m independent and identically distributed
(i.i.d.) data points. A point estimator is any function of the data:
θ̂m = g(x(1) , . . . , x (m)).

(5.2)

In other words, any statistic1 is a point estimate. Notice that no mention is made
of any correspondence between the estimator and the parameter being estimated.
There is also no constraint that the range of g(x (1) , . . . , x(m)) should correspond
to that of the true parameter.
This deﬁnition of a point estimator is very general and allows the designer
of an estimator great ﬂexibility. What distinguishes “just any” function of the
data from most of the estimators that are in common usage is their properties.
For now, we take the frequentist perspective on statistics. That is, we assume
that the true parameter value θ is ﬁxed but unknown, while the point estimate
θˆ is a function of the data. Since the data is drawn from a random process, any
function of the data is random. Therefore θ̂ is a random variable.
Point estimation can also refer to the estimation of the relationship between
input and target variables. We refer to these types of point estimates as function
estimators.
Function Estimation As we mentioned above, sometimes we are interested in
performing function estimation (or function approximation). Here we are trying
to predict a variable (or vector) y given an input vector x (also called the covariates). We consider that there is a function f (x) that describes the relationship
1

A statistic is a function of the data, typically of the whole training set, such as the mean.

115

CHAPTER 5. MACHINE LEARNING BASICS

between y and x. For example, we may assume that y = f (x)+ , where  stands
for the part of y that is not predictable from x.
In function estimation, we are interested in approximating f with a model or
estimate fˆ. Note that we are really not adding anything new here to our notion of
a point estimator, the function estimator fˆ is simply a point estimator in function
space.
The linear regression example we discussed above in Section. 5.2 and the
polynomial regression example discussed in Section. 5.3 are both examples of
function estimation where we estimate a model fˆ of the relationship between an
input x and target y.

In the following we will review the most commonly studied properties of point
estimators and discuss what they tell us about these estimators.
As θ̂ and fˆ are random variables (or vectors, or functions), they are distributed
according to some probability distribution. We refer to this distribution as the
sampling distribution. When we discuss properties of the estimator, we are really
describing properties of the sampling distribution.

5.5.2

Bias

The bias of an estimator is deﬁned as:
bias( θˆm) = E( θ̂m ) − θ

(5.3)

where the expectation is over the data (seen as samples from a random variable)
and θ is the true underlying value of θ according to the data generating distribution. An estimator θ̂ m is said to be unbiased if bias( θˆm ) = 0, i.e., if E( θ̂m ) = θ.
An estimator θ̂m is said to be asymptotically unbiased if limm→∞ bias( θ̂m ) = 0,
i.e., if lim m→∞ E(θˆm ) = θ.
Example: Bernoulli Distribution Consider a set of samples {x(1) , . . . , x(m) }
that are independently and identically distributed according to a Bernoulli distribution, x(i) ∈ {0, 1}, where i ∈ [1, m]. The Bernoulli p.m.f. (probability mass
x(i) (1 − θ)(1−x (i)) .
function, or probability function) is given by P (x (i); θ) = θP
m
(i) is biased.
We are interested in knowing if the estimator θ̂m = m1
i=1 x

116

CHAPTER 5. MACHINE LEARNING BASICS

bias(θ̂m ) = E[ θ̂m ] − θ
"
#
m
1 X (i)
−θ
=E
x
m
i=1

m
1 X h (i) i
−θ
=
E x
m
i=1
m

1


1X X
(i)
(i)
=
x (i) θx (1 − θ) (1−x ) − θ
m
(i)

=

1
m

i=1 x =0
m
X
i=1

(θ) − θ

=θ−θ=0

Since bias( θ̂) = 0, we say that our estimator θ̂ is unbiased.
Example: Gaussian Distribution Estimator of the Mean Now, consider a
set of samples {x (1) , . . . , x (m)} that are independently and identically distributed
according to a Gaussian (Normal) distribution (x (i) ∼ Gaussian(µ, σ2 ), where
i ∈ [1, m]). The Gaussian
density function) is given by
 p.d.f. (probability

(i)

2

1
p(x (i) ; µ, σ2) = √ 2πσ
−12 (x σ−µ)
.
2
2 exp
A common estimator of the Gaussian mean parameter is known as the sample
mean:
m
1 X (i)
µ̂ m =
x
(5.4)
m
i=1

To determine the bias of the sample mean, we are again interested in calculating
its expectation:
bias(µ̂m ) = E[µ̂ m] − µ
"
#
m
X
1
=E
x (i) − µ
m
i=1
!
m
h
i
X
1
(i)
=
E x
−µ
m
i=1
!
m
X
1
=
µ −µ
m i=1
= µ−µ = 0
117

CHAPTER 5. MACHINE LEARNING BASICS

Thus we ﬁnd that the sample mean is an unbiased estimator of Gaussian mean
parameter.
Example: Gaussian Distribution Estimators of the Variance Sticking
with the Gaussian family of distributions. We consider two diﬀerent estimators
of the variance parameter σ 2. We are interested in knowing if either estimator is
biased.
The ﬁrst estimator of σ 2 we consider is known as the sample variance:
σ̂m2

m
2
1 X  (i)
=
x − µ̂m ,
m

(5.5)

i=1

where µ̂ m is the sample mean, deﬁned above. More formally, we are interested in
computing
bias(σ̂2m ) = E[σ̂2m] − σ 2

We now simplify the term E[σ̂2m ]
"
#
m 

X
2
1
E[σ̂ 2m] = E
x (i) − µ̂ m
m i=1
"
#
m
1 X (i) 2
=E
(x ) − 2x(i) µ̂m + µ̂2m
m i=1




!
m
m
m
m
h
i
X
X
X
X
1
1
1
1
=
x (j)  + E 
x(j) 
x (k) 
E (x (i) )2 − 2E x (i)
m
m
m
m
i=1
j=1
j=1
k=1


 h
m
m
i 2X h
i
1 X
2
1 X h (j) 2 i
(i) 2
(i) (j)
=
+ 2
1−
E (x ) −
E x x
E (x )
m
m
m
m
i=1
j6
=i
j=1

m
1 X X h (j) (k) i 
+ 2
E x x
m j=1
k6
=j


m
2(m − 1) 2
(m − 1) 2
1 X m−2
1 2
2
2
2
=
(
)(µ + σ ) −
(µ ) + (µ + σ ) +
(µ )
m
m
m
m
m
i=1

=

m−1 2
σ
m

So the bias of σ̂ 2m is −σ 2/m. Therefore, the sample variance is a biased estimator.

118

CHAPTER 5. MACHINE LEARNING BASICS

We now consider a modiﬁed estimator of the variance sometimes called the
unbiased sample variance:
σ̃ 2m =

m
2
1 X  (i)
x − µ̂ m
m−1

(5.6)

i=1

As the name suggests this estimator is unbiased, that is, we ﬁnd that E[σ̃m2 ] = σ2 :
"
#
m 
2
X
1
E[σ̃ 2m ] = E
x (i) − µ̂ m
m − 1 i=1
m
=
E[σ̂ 2 ]
m−1  m

m
m−1 2
=
σ
m−1
m
= σ2 .
We have two estimators: one is biased and the other is not. While unbiased
estimators are clearly desirable, they are not always the “best” estimators. As we
will see we often use biased estimators that possess other important properties.

5.5.3

Variance

Another property of the estimator that we might want to consider is how much
we expect it to vary as a function of the data sample. Just as we computed the
expectation of the estimator to determine its bias, we can compute its variance.
ˆ2
Var( θ̂) = E[θˆ2 ] − E[θ]

(5.7)

The variance of an estimator provides a measure of how we would expect the
estimate we compute from data to vary as we independently resample the dataset
from the underlying data generating process. Just as we might like an estimator
to exhibit low bias we would also like it to have relatively low variance.
We can also deﬁne the standard error (se) of the estimator as
q
ˆ
se( θ̂) = Var[θ]
(5.8)
Example: Bernoulli Distribution Let’s once again consider a set samples
({x (1), . . . , x(m)}) drawn independently and identically from a Bernoulli distri(i)
(i)
bution (recall P (x(i) ; θ) = θx (1 − θ)(1−x )). This time we are interested in

119

CHAPTER 5. MACHINE LEARNING BASICS

P
(i)
computing the variance of the estimator θˆm = 1m m
i=1 x .
!
m
 
X
1
Var θ̂m = Var
x(i)
m
i=1

m
 
1 X
= 2
Var x(i)
m
i=1

m
1 X
= 2
θ(1 − θ)
m
i=1

1
mθ(1 − θ)
m2
1
=
θ(1 − θ)
m
Note that the variance of the estimator decreases as a function of m, the number of examples in the dataset. This is a common property of popular estimators
that we will return to when we discuss consistency (see Sec. 5.5.5).
=

Example: Gaussian Distribution Estimators of the Variance We again
consider a set of samples {x (1), . . . , x (m) } independently and identically distributed
according to a Gaussian distribution (x(i) ∼ Gaussian(µ, σ 2 ), where i ∈ [1, m]).
We now consider the variance of the two estimators of the variance: the sample
variance,
m
2
1 X  (1)
2
σ̂m =
x − µ̂m ,
(5.9)
m
i=1

and the unbiased sample variance,
2
σ̃m

m
2
1 X (1)
=
x − µ̂m .
m−1

(5.10)

i=1

In order to determine the variance of these estimators we will take advantage
of a known relationship between the sample variance and the Chi Squared distribution, speciﬁcally, that m−1
σ̂2 happens to be χ 2 distributed. We can then use
σ2
this together with the fact that the variance of a χ2 random variable with m − 1
degrees of freedom is 2(m − 1).


m−1 2
Var
σ̃ = 2(m − 1)
σ2
 2
(m − 1)2
Var
σ̃ = 2(m − 1)
σ4
2σ 4
Var σ̃ 2 =
(m − 1)
120

CHAPTER 5. MACHINE LEARNING BASICS

2
2
2
By noticing that σ̂2 = m−1
m σ̃ , and using σ̃ ’s relationship to the χ distribution,
 
4
it is straightforward to show that Var σ̂ 2 = 2(m−1)σ
.
m2
   m 2
 
To derive this last relation, we used the fact that Var σ̃2 = m−1
Var σ̂2 ,
 
 
that is Var σ̃ 2 > Var σ̂ 2 . So while the bias of σ̃2 is smaller than the bias of
σ̂ 2 , the variance of σ̃2 is greater.

5.5.4

Trading oﬀ Bias and Variance and the Mean Squared Error

Bias and variance measure two diﬀerent sources of error in an estimator. Bias
measures the expected deviation from the true value of the function or parameter.
Variance on the other hand, provides a measure of the deviation from the true
value that any particular sampling of the data is likely to cause.
What happens when we are given a choice between two estimators, one with
more bias and one with more variance? How do we choose between them? For
example, let’s imagine that we are interested in approximating the function shown
in Fig. 5.2 and we are only oﬀered the choice between a model with large bias
and one that suﬀers from large variance. How do we choose between them?
In machine learning, perhaps the most common and empirically successful way
to negotiate this kind of trade-oﬀ, in general is by cross-validation, discussed in
Section 5.4.1. Alternatively, we can also compare the mean squared error (MSE)
of the estimates:
MSE = E[(θˆn − θ)2 ]
= Bias(θˆn )2 + Var( θ̂n)

(5.11)

The MSE measures the overall expected deviation—in a squared error sense—
between the estimator and the true value of the parameter θ. As is clear from Eq.
5.11, evaluating the MSE incorporates both the bias and the variance. Desirable
estimators are those with small MSE and these are estimators that manage to
keep both their bias and variance somewhat in check.
The relationship between bias and variance is tightly linked to the machine
learning concepts of capacity, underﬁtting and overﬁtting discussed in Section.
5.3. In the case where generalization error is measured by the MSE (where
bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance and decrease bias. This is illustrated in
Figure 5.6, where we see again the U-shaped curve of generalization error as a
function of of capacity, as in Section 5.3 and Figure 5.3.
Example: Gaussian Distribution Estimators of the Variance In the last
section we saw that when we compared the sample variance, σ̂2, and the unbiased
sample variance, σ̃ 2 , we see that while σ̂ 2 has higher bias, σ̃ 2 has higher variance.
121

CHAPTER 5. MACHINE LEARNING BASICS

Underﬁtting zone

bias

Overﬁtting zone

generalization
error
optimal
capacity

variance
Capacity

Figure 5.6: As capacity increases (x-axis), bias (dotted) decreases and variance (dashed)
increases, yielding another U-shaped curve for generalization error (bold curve). If we
vary capacity along one axis, there is an optimal capacity, with underﬁtting when the
capacity is below this optimum and overﬁtting when it is above.

The mean squared error oﬀers a way of balancing the tradeoﬀ between bias
and variance and suggest which estimator we might prefer. For σ̂ 2, the mean
squared error is given by:
MSE(σ̂2m) = Bias(σ̂ 2m )2 + Var(σ̂ 2m)

2
−σ2
2(m − 1)σ4
=
+
m
m2


1 + 2(m − 1) 4
=
σ
m2


2m − 1
=
σ4
m2

(5.12)
(5.13)
(5.14)
(5.15)

The mean squared error of the unbiased alternative is given by:
MSE(σ̃ 2m) = Bias(σ̃ 2m)2 + Var(σ̃ 2m )
2σ 4
=0+
(m − 1)
2
=
σ 4.
(m − 1)

(5.16)
(5.17)
(5.18)

Comparing the two, we see that the MSE of the unbiased sample variance, σ̃m2 , is
actually higher than the MSE of the (biased) sample variance, σ̂m2 . This implies
that despite incurring bias in the estimator σ̂ 2m, the resulting reduction in variance
more than makes up for the diﬀerence, at least in a mean squared sense.
122

CHAPTER 5. MACHINE LEARNING BASICS

5.5.5

Consistency

As we have already discussed, sometimes we may wish to choose an estimator
that is biased. For example, in order to minimize the variance of the estimator.
However we might still wish that, as the number of data points in our dataset
increases, our point estimates converge to the true value of the parameter. More
p
formally, we would like that limn→∞ θ̂ n → θ.2 This condition is known as consistency 3 and ensures that the bias induced by the estimator is assured to diminish
as the number of data examples grows.
Asymptotic unbiasedness is not equivalent to consistency. For example, consider estimating the mean parameter µ of a normal distribution N (µ, σ 2), with
a dataset consisting of n samples: {x 1, . . . , x n }. We could use the ﬁrst sample
x 1 of the dataset as an unbiased estimator: θ̂ = x 1, In that case, E( θ̂n) = θ so
the estimator is unbiased no matter how many data points are seen. This, of
course, implies that the estimate is asymptotically unbiased. However, this is not
a consistent estimator as it is not the case that θˆn → θ as n → ∞.

5.6

Maximum Likelihood Estimation

Previously, we have seen some deﬁnitions of common estimators and analyzed
their properties. But where did these estimators come from? Rather than guessing
that some function might make a good estimator and then analyzing its bias and
variance, we would like to have some principle from which we can derive speciﬁc
functions that are good estimators for diﬀerent models.
The most common such principle is the maximum likelihood principle.
Consider a set of m examples X = {x(1) , . . . , x (m) } drawn independently from
the true but unknown data generating distribution pdata(x).
Let pmodel (x; θ) be a parametric family of probability distributions over the
same space indexed by θ. In other words, pmodel (x; θ) maps any conﬁguration x
to a real number estimating the true probability pdata (x).
The maximum likelihood estimator for θ is then deﬁned as
θML = arg max p model(X; θ)

(5.19)

θ

= arg max
θ
2

m
Y

p model (x (i); θ)

(5.20)

i=1

p

The symbol → means that the convergence is in probability, i.e. for any  > 0, P (| θ̂n − θ| >
) → 0 as n → ∞.
3
This is sometime referred to as weak consistency, with strong consistency referring to the
almost sure convergence of θˆ to θ.

123

CHAPTER 5. MACHINE LEARNING BASICS

This product over many probabilities can be inconvenient for a variety of
reasons. For example, it is prone to numerical underﬂow. To attain a more
convenient but equivalent optimization problem, we observe that the logarithm
of the arg max is the arg max of logarithm:
θML = arg max
θ

m
X

log pmodel (x (i); θ).

(5.21)

i=1

Because the argmax does not change when we rescale the cost function, we can
divide by m to obtain a version of the criterion that is expressed as an expectation:
θML = arg max Ex∼p̂ data log pmodel (x; θ).

(5.22)

θ

One way to interpret maximium likelihood estimation is to view at as minimizing the dissimilarity between the empirical distribution deﬁned by the training
set and the model distribution, with the degree of dissimilarity between the two
measured by the KL divergence. The KL divergence is given by
DKL (p̂data kpmodel) = E x∼p̂data [log p̂data (x) − log pmodel (x)] .
Note that the term on the left is a function only of the data generating process, not
the model. This means when we train the model to minimize the KL divergence,
we need only minimize
−Ex∼p̂ data [log p̂model (x)]
which is of course the same as the maximization in Eq. 5.22. Note that this also
corresponds exactly to minimizing the cross entropy between the distributions.
We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution p̂ data . Ideally, we would like to match
the true data generating distribution pdata , but we have no direct access to this
distribution.
While the optimal θ is the same regardless of whether we are maximizing the
likelihood or minimizing the KL divergence, the values of the objective functions
are diﬀerent. In software, we often phrase both as minimizing a cost function.
Maximimum likelihood thus becomes minimization of the negative log-likelihood
(NLL), or equivalently, minimization of the cross entropy. The perspective of
maximum likelihood as minimum KL divergence becomes helpful in this case
because the KL divergence has a known minimum value of zero. The negative
log-likelihood can actually become negative when x is real-valued.

124

CHAPTER 5. MACHINE LEARNING BASICS

5.6.1

Conditional Log-Likelihood and Mean Squared Error

The maximum likelihood estimator can readily be generalized to the case where
our goal is not to estimate a probability function but rather a conditional probability, e.g., P (y | x; θ), to predict y given x. This is actually the most common
situation where we do supervised learning (Section 5.8), i.e., the examples are
pairs (x, y). If X represents all our inputs and Y all our observed targets, then
the conditional maximum likelihood estimator is
θML = arg max P (Y | X; θ).

(5.23)

θ

If the examples are assumed to be i.i.d., then this can be decomposed into
θML = arg max
θ

m
X
i=1

log P (y (i) | x(i) ; θ).

(5.24)

Example: Linear Regression Let us consider as an example the special case
of linear regression, introduced earlier in Section 5.2. In that case, the conditional
density of y, given x = x, is a Gaussian with mean µ(x) that is a learned function
of x, with unconditional variance σ 2. Since the examples are assumed to be i.i.d.,
the conditional log-likelihood (Eq. 5.23) becomes
log P (Y | X; θ) =
=

m
X
i=1
m
X
i=1

log P (y(i) | x (i) ; θ)
−1 (i)
m
||ŷ − y (i)|| 2 − m log σ − log(2π)
2
2σ
2

where ŷ(i) = µ(x(i) ) is the output of the linear regression on the i-th input x (i)
and m is the dimension of the y vectors. Comparing the above with the mean
squared error (Section 5.2) we immediately see that if σ is ﬁxed, maximizing the
above is equivalent (up to an additive and a multiplicative constant that do not
change the value of the optimal parameter) to minimizing the training set mean
squared error, i.e.,
m
1 X (i)
|| ŷ − y(i) ||2.
M SE train =
m
i=1

Note that the MSE is an average rather than a sum, which is more practical from
a numerical point of view (so you can compare MSEs of sets of diﬀerent sizes
more easily). In practice, researchers reporting log-likelihoods and conditional
log-likelihoods also tend to report the per-example average log-likelihood, for the
125

CHAPTER 5. MACHINE LEARNING BASICS

very same reason. The exponential of the average log-likelihood is also called the
perplexity and is used in language modeling applications.
Whereas in the case of linear regression we have µ(x) = w·x, the above equally
applies to other forms of regression, e.g., with a neural network predicting with
µ(x) the expected value of y given x.

5.6.2

Properties of Maximum Likelihood

The main appeal of the maximum likelihood estimator is that it can be shown
to be the best estimator asymptotically, as the number of examples m → ∞, in
terms of its rate of convergence as m increases.
The maximum likelihood estimator has the property of consistency (see Sec. 5.5.5
above), i.e., as more training are considered, the estimator converges to the best
one in some sense. There are other inductive principles besides the maximum
likelihood estimator, many of which share the property of being consistent estimators. However, there is the question of how many training examples one needs
to achieve a particular generalization error, or equivalently what estimation error
one gets for a given number of training examples, also called eﬃciency. This is
typically studied in the parametric case (like in linear regression) where our goal
is to estimate the value of a parameter (and assuming it is possible to identify
the true parameter), not the value of a function. A way to measure how close
we are to the true parameter is by the expected mean squared error, computing
the squared diﬀerence between the estimated and true parameter values, where
the expectation is over m training samples from the data generating distribution.
That parametric mean squared error decreases as m increases, and for m large,
the Cramér-Rao lower bound (Rao, 1945; Cramér, 1946) shows that no consistent
estimator has a lower mean squared error than the maximum likelihood estimator.
For these reasons (consistency and eﬃciency), the maximum likelihood induction principle is often considered the preferred one in machine learning, modulo
slight adjustments such as described in the next Section, to better deal with
the non-asymptotic case where the number of examples is small enough to yield
overﬁtting behavior.

5.7

Bayesian Statistics

So far we have discussed approaches based on estimating a single value of θ, then
making all predictions thereafter based on that one estimate. Another approach is
to consider all possible values of θ when making a prediction. Bayesian statistics
provides a natural and theoretically elegant way to carry out this approach.
Historically, statistics has become divided between two communities. One of
these communities is known as frequentist statistics or orthodox statistics. The
126

CHAPTER 5. MACHINE LEARNING BASICS

other is known as Bayesian statistics. The diﬀerence is mainly one of world view
but can have important practical implications.
As discussed in Sec. 5.5.1, the frequentist perspective is that the true parameter value θ is ﬁxed but unknown, while the point estimate θ̂ is a random variable
on account of it being a function of the data (which are seen as random).
The Bayesian perspective on statistics is quite diﬀerent and, in some sense,
more intuitive. The Bayesian uses probability to reﬂect degrees of certainty of
states of knowledge. The data is directly observed and so is not random. On the
other hand, the true parameter θ is unknown or uncertain and thus is represented
as a random variable.
Before observing the data, we represent our knowledge of θ using the prior
probability distribution, p(θ) (sometimes referred to as simply ’the prior’). Generally, the prior distribution is quite broad (i.e. with high entropy) to reﬂect a
high degree of uncertainty in the value of θ before observing any data. For example, we might assume a priori that θ lies in some ﬁnite range or volume, with
a uniform distribution. Many priors instead reﬂect a preference for “simpler”
solutions (such as smaller magnitude coeﬃcients, or a function that is closer to
being constant).
Now consider that we have a set of data samples {x(1), . . . , x (m)}. We can
recover the eﬀect of data on our belief about θ by combining the data likelihood
p(x (1), . . . , x(m) | θ) with the prior via Bayes’ rule:
(1)

(m)

p(θ | x , . . . , x

p(x (1), . . . , x(m) | θ)p(θ)
)=
p(x (1) , . . . , x(m))

(5.25)

If the data is at all informative about the value of θ, the posterior distribution
p(θ | x(1) , . . . , x (m)) will have less entropy (will be more ‘peaky’) than the prior
p(θ).
Relative to maximum likelihood estimation, Bayesian estimation oﬀers two
important diﬀerences. First, unlike the maximum likelihood point estimate of
θ, the Bayesian makes decision with respect to a full distribution over θ. For
example, after observing m examples, the predicted distribution over the next
data sample, x (m+1), is given by
Z
(m+1)
(1)
(m)
p(x
(5.26)
| x , . . . , x ) = p(x(m+1) | θ)p(θ | x(1), . . . , x (m)) dθ
Here each value of θ with positive probability density contributes to the prediction
of the next example, with the contribution weighted by the posterior density itself.
After having observed {x(1) , . . . , x (m)}, if we are still quite uncertain about the
value of θ, then this uncertainty is incorporated directly into any predictions we
might make.
127

CHAPTER 5. MACHINE LEARNING BASICS

In Sec. 5.5, we discussed how the frequentist statistics addresses the uncertainty in a given point estimator of θ by evaluating its variance. The variance of
the estimator is an assessment of how the estimate might change will alternative
samplings of the observed (or training) data. The Bayesian answer to the question of how to deal with the uncertainty in the estimator is to simply integrate
over it, which tends to protect well against overﬁtting.
The second important diﬀerence between the Bayesian approach to estimation
and the Maximum Likelihood approach is due to the contribution of the Bayesian
prior distribution. The prior has an inﬂuence by shifting probability mass density
towards regions of the parameter space that are preferred a priori. In practice,
the prior often expresses a preference for models that are simpler or more smooth.
One important eﬀect of the prior is to actually reduce the uncertainty (or entropy)
in the posterior density over θ.
We have already noted that combining the prior, p(θ), with the data likelihood
p(x (1), . . . , x(m) | θ) results in a distribution that is less entropic (more peaky)
than the prior. This is just the result of a basic property of probability distributions: Entropy(product of two densities) ≤ Entropy(either density). This implies
that the posterior density on θ is also less entropic than the data likelihood alone
(when viewed and normalized as a density over θ). The hypothesis space with the
Bayesian approach is, to some extent, more constrained than that with an ML
approach. Thus we expect a contribution of the prior to be a further reduction
in overﬁtting as compared to ML estimation.
Example: Linear Regression Here we consider the Bayesian estimation approach to learning the linear regression parameters. In linear regression, we learn
a linear mapping from an input vector x ∈ R n to predict the value of a scalar
y ∈ R. The prediction is parametrized by the vector w ∈ R n :
ŷ = w> x.
Given a set of m training samples (X (train), y(train) ), we can express the prediction
of y over the entire training set as:
yˆ(train) = X (train) w.
Expressed as a Gaussian conditional distribution on y (train) , we have
p(y(train) | X (train), w) = N (y (train); X (train)> w, I)


1 (train)
(train)
> (train)
(train)
w) (y
w) ,
∝ exp − (y
−X
−X
2
128

CHAPTER 5. MACHINE LEARNING BASICS

where we will follow the standard MSE formulation in assuming that the Gaussian
variance on y is one. In what follows, to reduce the notational burden, we refer
to (X (train), y (train)) as simply (X, y).
To determine the posterior distribution over the model parameter vector w,
we ﬁrst need to specify a prior distribution. The prior should reﬂect our naive
belief about the value of these parameters. While it is sometimes diﬃcult or
unnatural to express our prior beliefs in terms of the parameters of the model, in
practice we typically assume a fairly broad distribution expressing a high degree
of uncertainty about θ in our prior belief.
For real-valued parameters it is common to use a Gaussian as a prior distribution:


1
> −1
p(w) = N (w; µ0 , Λ0 ) ∝ exp − (w − µ0 ) Λ 0 (w − µ0 )
2
where µ0 and Λ0 are the prior distribution mean vector and covariance matrix
(inverse of covariance matrix) respectively. 4
With the prior thus speciﬁed, we can now proceed in determining the posterior
distribution over the model parameters.
p(w | X, y) ∝ p(y | X, w)p(w)




1
1
−1
>
>
∝ exp − (y − Xw) (y − Xw) exp − (w − µ0 ) Λ0 (w − µ0)
2
2



1
>
> >
> −1
> −1
∝ exp − −2y Xw + w X Xw + w Λ0 w − 2µ0 Λ0 w
2

 −1
We now make the  substitutions Λm
=
X > X + Λ −1
and
0
−1
>
µm = Λ m X y + Λ0 µ 0 into the derivation of the posterior (and complete the
square) to get:


1
1 > −1
> −1
p(w | X, y) ∝ exp − (w − µm ) Λ m (w − µ m ) + µ m Λm µm
(5.27)
2
2


1
>
−1
∝ exp − (w − µm ) Λ m (w − µ m ) .
(5.28)
2
In the above, we have dropped all terms that do not include the parameter vector
w. In Eq. 5.28, we recognize that the posterior distribution has the form of
a Gaussian distribution with mean vector µm and covariance matrix Λ m. It is
interesting to note that this justiﬁes our dropping all terms unrelated to w, since
we know that the posterior distribution must be normalized and, as a Gaussian,
4

Unless there is a reason to assume a particular covariance structure, we typically assume a
diagonal covariance matrix Λ 0 = diag(λ 0).
129

CHAPTER 5. MACHINE LEARNING BASICS

we know what that normalization constant must be (where n is the dimension of
the input):


1
1
(train)
(train)
>
−1
p(w | X
,y
)= p
exp − (w − µ m) Λ m (w − µ m ) .
2
(2π)n |Λ m|
(5.29)

5.7.1

Maximum A Posteriori (MAP) Estimation

While, in principle, we can use the full Bayesian posterior distribution over the parameter θ as our estimate of this parameter, it is still often desirable to have a single point estimate (for example, most operations involving the Bayesian posterior
for most interesting models are intractable and must be heavily approximated).
Rather than simply returning to the maximum likelihood estimate, we can still
gain some of the beneﬁt of the Bayesian approach by allowing the prior to inﬂuence the choice of the point estimate. One rational way to do this is to choose
the maximum a posteriori (MAP) point estimate. The MAP estimate chooses
the point of maximal posterior probability (or maximal probability density in the
more common case of continuous θ).
θMAP = arg max p(θ | x) = arg max log p(x | θ) + log p(θ)
θ

(5.30)

θ

We recognize, above on the right hand side, log p(x | θ), i.e. the standard loglikelihood term and log p(θ) corresponding to the prior distribution.
As discussed above the advantage brought by introducing the inﬂuence of the
prior on the MAP estimate is to leverage information other than that contained
in the training data. This additional information helps to reduce the variance in
the MAP point estimate (in comparison to the ML estimate). However, it does
so at the price of increased bias.
Example: Regularized Linear Regression We discussed above the Bayesian
approach to linear regression. Given a set of m training samples of input output pairs: (X (train), y (train)), we can express the prediction of y over the entire
training set as:
yˆ(train) = X (train) w.
where prediction is parametrized by the vector w ∈ Rn .
Recall from Sec. 5.6.1 that the maximum likelihood estimate for the model
parameters is given by:
ŵML = (X (train)>X(train) )−1 X (train)> y (train)
130

(5.31)

CHAPTER 5. MACHINE LEARNING BASICS

For the sake of comparison to the maximum likelihood solution, we will make
the simplifying assumption that the prior covariance matrix is scalar: Λ0 = λ0 I.
As mentioned previously, in practice, this is a very common form of prior distribution. We will also assume that µ 0 = 0. This is also a very common assumption
in practice and corresponds to acknowledging that a priori, we do not know if
the features of x have a positive or negative correlation with y. Adding these
assumptions, the MAP estimate of the model parameters (corresponding to the
mean of the Gaussian posterior density, in Eq. 5.28) becomes:
wˆMAP = Λm X (train)> y(train)

(5.32)

where µ0 and Λ0 are the prior mean and covariance respectively and Λ m is the
posterior covariance and is given by:

−1
Λm = X(train)> X (train) + λ−1
I
(5.33)
0

Comparing Eqs. 5.31 and 5.32, we see that the MAP estimate amounts to a
weighted combination of the prior maximum probability value, µ0 , and the ML
estimate. As the variance of the prior distribution tends to inﬁnity, the MAP
estimate reduces to the ML estimate. As the variance of the prior tends to zero,
the MAP estimate tends to zero (actually it tends to µ0 which here is assumed
to be zero).
We can make the model capacity tradeoﬀ between the ML estimate and the
MAP estimate more explicit by analyzing the bias and variance of these estimates.
It is relatively easy to show that the ML estimate is unbiased, i.e. that
E[ŵ ML] = w and that it has a variance given by:
Var(wML ) = (X (train)> X (train))−1

(5.34)

In order to derive the bias of the MAP estimate, we need to calculate the
expectation:
E[ŵMAP] = E[ΛmX (train)> y(train) ]
h

i
(train)>
(train)
= E Λm X
X
w+


= Λ m X(train)>X (train)w + Λm X(train)> E []

−1
= X(train)> X(train) + λ−1
I
X (train)>X (train) w,
0

(5.35)

We see that while the expected value of the ML estimate is the true parameter
value w (i.e. the parameters that we assume generated the data); the expected
131

CHAPTER 5. MACHINE LEARNING BASICS

value of the MAP estimate is a weighted average of w and the prior mean µ. We
compute the bias as:
Bias( ŵMAP ) = E[ ŵMAP ] − w

 −1
(train)> (train)
= − λ0 X
X
+I
w.

Since the bias is not zero, we can conclude that the MAP estimate is biased, and
as expected we can see that as the variance of the prior λ0 → ∞, the bias tends
to zero. As the variance of the prior λ 0 → 0, the bias tends to w.
h i
h i2
In order to compute the variance, we use the identity Var( θ̂) = E θ̂ 2 −E θ̂ .


So before computing the variance we need to compute E ŵ MAP ŵ>
M AP :
h
i
h
i
(train)> (train) (train)> (train)
E ŵ MAP ŵ>
=
E
Λ
X
ŷ
y
X
Λ
m
m
M AP




>
(train)>
(train)
(train)
(train)
= E Λ mX
X
w+ X
w+ X
Λm
= ΛmX (train)> X (train) ww> X (train)> X(train)Λm
h
i
+ Λ m X (train)> E > X (train) Λm
= ΛmX (train)> X (train) ww> X (train)> X(train)Λm
+ Λ m X (train)> X(train) Λ m
= E[ŵMAP ]E[ŵMAP ]> + X (train)> X(train) Λm


With E ŵMAPŵ >
M AP thus computed, the variance of the MAP estimate of
our linear regression model is given by:
h
i
h
i
>
Var(ŵMAP ) = E ŵM AP ŵ>
−
E
[
ŵ
]
E
ŵ
M AP
M AP
M AP

= E[ ŵMAP ]E[ ŵMAP] > + Λ m X(train)> X (train)Λm − E[ ŵMAP ]E[ŵ MAP] >
= Λm X (train)>X (train) Λm

−1
(train)>
(train)
−1
= X
X
+ λ0 I
X (train)> X(train)

−1
× X (train)> X (train) + λ −1
I
0

(5.36)

It is perhaps diﬃcult to compare Eqs. 5.34 and 5.36. But if we assume that
w is one-dimensional (along with x), it becomes a bit easier to see
that, as long
Pm
λ
x 2i
0
1
i=1
as λ 0 is bounded, then Var(ŵ ML) = Pm 2 > Var(ŵMAP ) =
P m 2 2.
(1+λ0 i=1xi )
i=1 xi
From the above analysis we can see that the role of the prior in the MAP
estimate is to trade increased bias for a reduction in variance. The goal, of
132

CHAPTER 5. MACHINE LEARNING BASICS

course, is to try to avoid overﬁtting. The incurred bias is a consequence of the
reduction in model capacity caused by limiting the space of hypotheses to those
with signiﬁcant probability density under the prior.
Many regularized estimation strategies, such as maximum likelihood learning
regularized with weight decay, can be interpreted as making the MAP approximation to Bayesian inference. This view applies when the regularization consists
of adding an extra term to the objective function that corresponds to log p(θ).
Not all such regularizer terms correspond to MAP Bayesian inference. For example, some regularizer terms may not be the logarithm of a probability distribution.
Other regularization terms depend on the data, which of course a prior probability
distribution is not allowed to do.

5.8

Supervised Learning Algorithms

Recall from Section 5.1.3 that supervised learning algorithms are roughly speaking, learning algorithms that learn to associate some input with some output,
given a training set of examples of inputs x and outputs y. In many cases the
outputs y may be diﬃcult to collect automatically and must be provided by a
human “supervisor,” but the term still applies even when the training set targets
were collected automatically.

5.8.1

Probabilistic Supervised Learning

Most supervised learning algorithms in this book are based on estimating a probability distribution p(y | x). We can do this simply by using maximum conditional likelihood estimation (Sec. 5.6.1, also just called maximum likelihood for
short) to ﬁnd the best parameter vector θ for a parametric family of distributions
p(y | x; θ).
We have already seen that linear regression corresponds to the family p(y |
x; θ) = N (y | θ >x, I). We can generalize linear regression to the classiﬁcation
scenario by deﬁning a diﬀerent family of probability distributions. If we have two
classes, class 0 and class 1, then we need only specify the probability of one of
these classes. The probability of class 1 determines the probability of class 0,
because these two values must add up to 1.
The normal distribution over real-valued numbers that we used for linear
regression is parameterized in terms of a mean. Any value we supply for this
mean is valid. A distribution over a binary variable is slightly more complicated,
because its mean must always be between 0 and 1. One way to solve this problem
is to use the logistic sigmoid function to squash the output of the linear function

133

CHAPTER 5. MACHINE LEARNING BASICS

into the interval (0, 1) and interpret that value as a probability:
p(y = 1 | x; θ) = σ(θ> x).
This approach is known as logistic regression (a somewhat strange name since we
use the model for classiﬁcation rather than regression).
In the case of linear regression, we were able to ﬁnd the optimal weights
by solving the normal equations. Logistic regression is somewhat more diﬃcult.
There is no closed-form solution for its optimal weights. Instead, we must search
for them by maximizing the log-likelihood. We can do this by minimizing the
negative log-likelihood (NLL) using gradient descent.
This same strategy can be applied to essentially any supervised learning problem, by writing down a parametric family of probability of conditional distributions over the right kind of input and output variables.

5.8.2

Support Vector Machines

One of the most inﬂuential approaches to supervised learning is the support vector
machine (Boser et al., 1992; Cortes and Vapnik, 1995). This model is similar to
logistic regression in that it is driven by a linear function w > x+b. Unlike logistic
regression, the support vector machine does not provide probabilities, but only
outputs a class identity.
One key innovation associated with support vector machines is the kernel trick.
The kernel trick consists of observing that many machine learning algorithms can
be written exclusively in terms of dot products between examples. For example,
it can be shown that the linear function used by the support vector machine can
be re-written as
m
X
>
w x+b = b+
αi x>x(i)
i=1

where x (i) is a training example and α is a vector of coeﬃcients. Rewriting the
learning algorithm this way allows us to replace x by the output of a given feature
function φ(x) and the dot product with a function k(x, x(i) ) = φ(x)> φ(x (i)) called
a kernel.
We can then make predictions using the function
X
f (x) = b +
α i k(x, x(i)).
(5.37)
i

This function is linear in the space that φ maps to, but non-linear as a function
of x.
The kernel trick is powerful for two reasons. First, it allows us to learn models
that are non-linear as a function of x using convex optimization techniques that
134

CHAPTER 5. MACHINE LEARNING BASICS

are guaranteed to converge eﬃciently. This is only possible because we consider φ
ﬁxed and only optimize α, i.e., the optimization algorithm can view the decision
function as being linear in a diﬀerent space. Second, the kernel function k need not
be implemented in terms of explicitly applying the φ mapping and then applying
the dot product. The dot product in φ space might be equivalent to a nonlinear but computationally less expensive operation in x space. For example, we
could design an inﬁnite-dimensional feature mapping φ(x) over the non-negative
integers. Suppose that this mapping returns a vector containing x ones followed
by inﬁnitely many zeros. Explicitly constructing this mapping, or taking the dot
product between two such vectors, costs inﬁnite time and memory. But we can
write a kernel function k(x, x(i) ) = min(x, x (i) ) that is exactly equivalent to to
this inﬁnite-dimensional dot product. The most commonly used kernel is the
Gaussian kernel
k(u, v) = N (u − v; 0, σ 2I)
(5.38)
where N (x; µ, Σ) is the standard normal density. This kernel corresponds to the
dot product k(u, v) = φ(x)>φ(x) on an inﬁnite-dimensional feature space φ and
also has an interpretation as a similarity function, acting like a kind of template
matching.
Support vector machines are not the only algorithm that can be enhanced
using the kernel trick. Many linear models can be enhanced in this way. This
category of algorithms is known as kernel machines or kernel methods.
A major drawback to kernel machines is that the cost of learning the α coeﬃcients is quadratic in the number of training examples. A related problem is that
the cost of evaluating the decision function is linear in the number of training
examples, because the i-th example contributes a term αi k(x, x (i)) to the decision function. Support vector machines are able to mitigate this by learning an α
vector that contains mostly zeros. Classifying a new example then requires evaluating the kernel function only for the training examples that have non-zero α i.
These training examples are known as support vectors. Another major drawback
of common kernel machines (such as those using the Gaussian kernel) is more
statistical and regards their diﬃculty in generalizing to complex variations far
from the training examples, as discussed in Section 5.12.
The analysis of the statistical limitations of support vector machines with
general purpose kernels like the Gaussian kernels actually motivated the rebirth
of neural networks through deep learning. Support vector machines and other
kernel machines have often been viewed as a competitor to deep learning (though
some deep networks can in fact be interpreted as support vector machines with
learned kernels). The current deep learning renaissance began when deep networks were shown to outperform support vector machines on the MNIST benchmark dataset (Hinton et al., 2006). One of the main reasons for the current
135

CHAPTER 5. MACHINE LEARNING BASICS

popularity of deep learning relative to support vector machines is the fact that
the cost of training kernel machines usually scales quadratically with the number
of examples in the training set. For a deep network of ﬁxed size, the memory cost
of training is constant with respect to training set size (except for the memory
needed to store the examples themselves) and the runtime of a single pass through
the training set is linear in training set size. These asymptotic results meant that
kernelized SVMs dominated while datasets were small, but deep models currently
dominate now that datasets are large.

5.8.3

Other Simple Supervised Learning Algorithms

We have already brieﬂy encountered another non-probabilistic supervised learning
algorithm, nearest neighbor regression. More generally, k-nearest neighbors is a
family of techniques that can be used for classiﬁcation or regression. As a nonparametric learning algorithm, there are no parameters. In fact, there is not even
really a training stage or learning process. Instead, at test time, when we want
to produce an output y for a new test input x, we ﬁnd the k nearest neighbors
to x in the training data X. We then return the average of the corresponding y
values in the training set. This works for esentially any kind of supervised learning
where we can deﬁne an average over y values. In the case of classiﬁcation, we can
average over one-hot code vectors c with cy = 1 and ci = 0 for all other values
of i. We can then interpret the average over these one-hot codes as giving a
probability distribution over classes. As a non-parametric learning algorithm, knearest neighbors has unlimited capacity and will eventually converge to the Bayes
error given a large enough training set if k is properly reduced as the number
of examples is increased. However, it may perform very badly on small, ﬁnite
training sets. One weakness of k-nearest neighbors is that it cannot learn that
one feature is more discriminative than another. For example, imagine we have
a regression task with x ∈ R1 00 drawn from an isotropic Gaussian distribution,
but only a single variable x1 is relevant to the output. Suppose further that this
feature simply encodes the output directly, i.e. that y = x1 in all cases. Nearest
neighbor regression will not be able to detect this simple pattern. The nearest
neighbor of most points x will be determined by the large number of features x2
through x1 00, not by the lone feature x1. Thus the output on small training sets
will essentially be random.

136

CHAPTER 5. MACHINE LEARNING BASICS

010

00
01
0

011

0

00

1

01

010

10

011

11

110

111

110

1110

1111

11
10

1

111
1110

1111

Figure 5.7: Decision tree (right) and how it cuts the input space into regions, with a
constant output in each region (left). Each node of the tree (circle or square) is associated
with a region (the entire space for the root node, with the empty string identiﬁer).
Internal nodes (circles) split their region in two, via an axis-aligned cut (occasionally,
some decision trees use more complicated cuts than shown in this example). Leaf nodes
(squares) are associated with a model output, typically set to the average target output
for the training examples that fall in the corresponding region. Each node is displayed
with a binary string identiﬁer corresponding to its position in the tree, obtained by
appending a bit to its parent identiﬁer (0=choose left or top, 1=choose right or bottom).
Note that the result is a piecewise-constant function, and note how the number of regions
(pieces) cannot be greater than the number of examples, hence it is not possible to learn
a function that has more local maxima than the number of training examples.

Another type of learning algorithm that also breaks the input space into regions and has separate parameters for each region is the decision tree (Breiman
et al., 1984) and its many variants. As shown in Fig. 5.7, each node of the decision tree is associated with a region in the input space, and internal nodes break
that region into one sub-region for each child of the node (typically using an
axis-aligned cut). Space is thus sub-divided into non-overlapping regions, with a
one-to-one correspondence between leaf nodes and input regions. Each leaf node
usally maps every point in its input region to the same output. Decision trees
137

CHAPTER 5. MACHINE LEARNING BASICS

are usually trained with specialized algorithms that are beyond the scope of this
book. The learning algorithm can be considered non-parametric if it is allowed to
learn a tree of arbitrary size, though decision trees are usually regularized with
size constraints that turn them into parametric models in practice. Note that
decision trees as they are typically used, with axis-aligned splits and constant
outputs within each node, struggle to solve some problems that are easy even for
logistic regression. For example, if we have a two-class problem and the positive
class occurs wherever x2 > x1, the decision boundary is not axis-aligned. The
decision tree will thus need to approximate the decision boundary with many
nodes, implementing a step function that constantly walks back and forth across
the true decision function with axis-aligned steps.
As we have seen, nearest neighbor predictors and decision trees have many
limitations. Nonetheless, they are useful learning algorithms when computational
resources are constrained. We can also build intuition for more sophisticated
learning algorithms by thinking about the similarities and diﬀerences between
sophisticated algorithms and k-NN or decision tree baselines.
See Murphy (2012); Bishop (2006); Hastie et al. (2001) or other machine learning textbooks for more material on traditional supervised learning algorithms.

5.9

Unsupervised Learning Algorithms

Recall from Section 5.1.3 that unsupervised algorithms are those that experience
only “features” but not a supervision signal. The distinction between supervised
and unsupervised algorithms is not formally and rigidly deﬁned because there is no
objective test for distinguishing whether a value is a feature or a target provided by
a supervisor. Informally, unsupervised learning refers to most attempts to extract
information from a distribution that do not require human labor to annotate
examples. The term is usually associated with density estimation, learning to
draw samples from a distribution, learning to denoise data from some distribution,
ﬁnding a manifold that the data lies near, or clustering the data into groups of
related examples.
Learning a representation of data A classic unsupervised learning task is to
ﬁnd the ‘best’ representation of the data. By ‘best’ we can mean diﬀerent things,
but generally speaking we are looking for a representation that preserves as much
information about x as possible while obeying some penalty or constraint aimed
at keeping the representation simpler or more accessible than x itself.
There are multiple ways of deﬁning a simpler representation, some of the
most common include lower dimensional representations, sparse representations
and independent representations. Low-dimensional representations attempt to
138

CHAPTER 5. MACHINE LEARNING BASICS

compress as much information about x as possible in a smaller representation.
Sparse representations generally embed the dataset into a high-dimensional representation 5 where the number of non-zero entries is small. This results in an
overall structure of the representation that tends to distribute data along the
axes of the representation space. Independent representations attempt to disentangle the sources of variation underlying the data distribution such that the
dimensions of the representation are statistically independent.
Of course these three criteria are certainly not mutually exclusive. Lowdimensional representations often yield elements that have fewer or weaker dependencies than the original high-dimensional data. This is because one way to
reduce the size of a representation is to ﬁnd and remove redundancies. Identifying
and removing more redundancy allows the dimensionality reduction algorithm to
achieve more compression while discarding less information.
The notion of representation is one of the central themes of deep learning
and therefore one of the central themes in this book. Chapter 16 discusses some
of the qualities we would like in our learned representations, along with speciﬁc
representation learning algorithms more powerful than the simple one presented
next, Principal Components Analysis.

5.9.1

Principal Components Analysis

In the remainder of this section we will consider one of the most widely used unsupervised learning methods: Principle Components Analysis (PCA). PCA is an
orthogonal, linear transformation of the data that projects it into a representation
where the elements are uncorrelated (shown in Figure 5.8).
x2

z2

x2

x1

Z =XW

z1

x1

Figure 5.8: Illustration of the data representation learned via PCA.

In section 2.12, we saw that we could learn a one-dimensional representation
5

sparse representations often use over-complete representations: the representation dimension
is greater than the original dimensionality of the data.
139

CHAPTER 5. MACHINE LEARNING BASICS

that best reconstructs the original data (in the sense of mean squared error) and
that this representation actually corresponds to the ﬁrst principal component of
the data. Thus we can use PCA as a simple and eﬀective dimensionality reduction
method that preserves as much of the information in the data as possible (again,
as measured by least-squares reconstruction error). In the following, we will
take a look at other properties of the PCA representation. Speciﬁcally, we will
study how the PCA representation can be said to decorrelate the original data
representation X.
Let us consider the n × m-dimensional design matrix X. We will assume that
the data has a mean of zero, E[x] = 0. If this is not the case, the data can easily
be centered (mean removed). The unbiased sample covariance matrix associated
with X is given by:
1
Var[x] =
X >X
(5.39)
n−1
One important aspect of PCA is that it ﬁnds a representation (through linear
transformation) z = W x where Var[z] is diagonal. To do this, we will make use
of the singular value decomposition (SVD) of X: X = U ΣW >, where Σ is an
n × m-dimensional rectangular diagonal matrix with the singular values of X on
the main diagonal, U is an n× n matrix whose columns are orthonormal (i.e. unit
length and orthogonal) and W is an m × m matrix also composed of orthonormal
column vectors.
Using the SVD of X, we can re-express the variance of X as:
1
X >X
n−1
1
=
(U ΣW >)> U ΣW >
n−1
1
=
W Σ>U >U ΣW >
n−1
1
=
W Σ2W >,
n−1

Var[x] =

(5.40)
(5.41)
(5.42)
(5.43)

where we use the orthonormality of U (U >U = I) and deﬁne Σ 2 as an m × mdimensional diagonal matrix with the squares of the singular values of X on the
diagonal, i.e. the ith diagonal elements is given by Σ2i,i. This shows that if we

140

CHAPTER 5. MACHINE LEARNING BASICS

take z = W x, we can ensure that the covariance of z is diagonal as required.
1
Z >Z
n−1
1
=
W >X >XW
n−1
1
=
W W > Σ2 W W >
n−1
1
=
Σ2
n−1

Var[z] =

(5.44)
(5.45)
(5.46)
(5.47)

Similar to our analysis of the variance of X above, we exploit the orthonormality
of W (i.e., W >W = I). Our use of SVD to solve for the PCA components of X
(i.e. elements of z) reveals an interesting connection to the eigen-decomposition
of a matrix related to X. Speciﬁcally, the columns of W are the eigenvectors of
the n × n-dimensional matrix X >X.
The above analysis shows that when we project the data x to z, via the linear
transformation W , the resulting representation has a diagonal covariance matrix
(as given by Σ2) which immediately implies that the individual elements of z are
mutually uncorrelated.
This ability of PCA to transform data into a representation where the elements are mutually uncorrelated is a very important property of PCA. It is a
simple example of a representation that attempt to disentangle the unknown factors of variation underlying the data. In the case of PCA, this disentangling takes
the form of ﬁnding a rotation of the input space (mediated via the transformation
W ) that aligns the principal axes of variance with the basis of the new representation space associated with z, as illustrated in Fig. 5.8. While correlation is
an important category of dependency between elements of the data, we are also
interested in learning representations that disentangle more complicated forms of
feature dependencies. For this, we will need more than what can be done with a
simple linear transformation. These issues are discussed below in Sec. 5.12 and
later in detail in Chapter 16.

5.10

Weakly Supervised Learning

Weakly supervised learning is another class of learning methods that stands between supervised and unsupervised learning. It refers to a setting where the
datasets consists of (x, y) pairs, as in supervised learning, but where the labels
y are either unreliably present (i.e. with missing values) or noisy (i.e. where the
label given is not the true label).
Methods for working with weakly labeled data have recently grown in importance due to the—largely untapped—potential for using large quantities of readily
141

CHAPTER 5. MACHINE LEARNING BASICS

available weakly labeled data in a transfer learning paradigm to help solve problems where large, clean datasets are hard to come-by. The Internet has become
a major source of this kind of noisy data.
For example, although we would like to train a computer vision system with
labels indicating the presence and location of every object (and which pixels
correspond to which object) in every image, such labeling is very human-labor
intensive. Instead, we want to take advantage of images for which only the main
object is identiﬁed, like the ImageNet dataset (Deng et al., 2009), or worse, of
video for which some general and high-level semantic spoken caption is approximately temporally aligned with the corresponding frames of the video, like the
DVS data (Descriptive Video service) which has recently been released (Torabi
et al., 2015).

5.11

Building a Machine Learning Algorithm

Nearly all deep learning algorithms can be described as particular instances of
a fairly simple recipe: combine a speciﬁcation of a dataset, a cost function, an
optimization procedure and a model.
For example, the linear regression algorithm combines a dataset consisting of
X and y, the cost function
J(w, b) = −E x,y∼p̂data pmodel (y | x),
and the model speciﬁcation pmodel (y | x) = N (y | x>w + b, 1). Typically we
then observe that J simpliﬁes to the mean squared error and we can choose to
optimize this in closed form by solving the normal equations with the MoorePenrose pseudo-inverse.
By realizing that we can modify any of these components, we can obtain a
very wide variety of algorithms.
The cost function typically includes at least one term that causes the learning
process to perform statistical estimation. The most common cost function is the
negative log-likelihood, so that minimizing the cost function causes maxmimum
likelihood estimation. This main term of the cost function often decomposes as a
sum over training examples of some per-example loss function. For example, the
negative conditional log-likelihood of the training data can be written as
m

1X
J(θ) = E x,y∼p̂data L(x, y, θ) =
L(x( i), y( i), θ)
m i=1
where y is the per-example loss L(x, y, θ) = log p(y | x; θ). We can design many
diﬀerent cost functions just by taking the expectation across the training set of
diﬀerent per-example loss functions.
142

CHAPTER 5. MACHINE LEARNING BASICS

The cost function may also include additional terms, such as regularization
terms. For example, we can add weight decay to the linear regression cost function
to obtain
J(w, b) = λ||w||22 − Ex,y∼p data pmodel (y | x).

This still allows closed-form optimization.
If we change the model to be non-linear, then most cost functions can no longer
be optimized in close form. This requires us to choose an iterative numerical
optimization procedure, such as gradient descent.
This recipe supports both supervised and unsupervised learning. The linear regression example shows how to support supervised learning. Unsupervised
learning can be supported by deﬁning a dataset that contains only X and providing an appropriate unsupervised cost and model. For example, we can obtain
the ﬁrst PCA vector by specifying that our loss function is
J(w) = Ex∼p data||x − r(x; w)|| 22
while our model is deﬁned to have w with norm one and reconstruction function
r(x) = w>wx.
In some cases, the cost function may be a function that we cannot actually
evaluate, for computational reasons. In these cases, we can still approximately
minimize it using iterative numerical optimization so long as we have some way
of approximating its gradients.
Most machine learning algorithms make use of this recipe, though it may
not immediately be obvious. If a machine learning algorithm seems especially
unique or hand-designed, it can usually be understood as using a special-case
optimizer. Some models such as decision trees or k-means require special-case
optimizers because their cost functions have ﬂat regions that make them inappropriate for minimization by gradient-based optimizers. Recognizing that most
machine learning algorithms can be described using this recipe helps to see the
diﬀerent algorithms as part of a taxonomy of methods for doing related tasks that
work for similar reasons, rather than as a long list of algorithms that each have
separate justiﬁcations.

5.12

The Curse of Dimensionality and Statistical Limitations of Local Generalization

The number of variable conﬁgurations grows exponentially with the number of
variables, i.e., with dimension, which brings up a statistical form of the curse
of dimensionality, introduced in the next section. Many non-parametric learning
algorithms, such as kernel machines with a Gaussian kernel, rely on a simple preference over functions which corresponds to an assumption of smoothness or local
143

CHAPTER 5. MACHINE LEARNING BASICS

constancy. As argued in Section 5.12.2 that follows, this allows these algorithms to
generalize near the training examples, but does not allow them to generalize in a
non-trivial way far from them: the number of ups and downs that can be captured
is limited by the number of training examples. This is particularly problematic
with high-dimensional data, because of the curse of dimensionality. In order to
reduce that diﬃculty, researchers have introduced the idea of dimensionality reduction and manifold learning, introduced in Section 5.12.3. This motivates the
introduction of additional knowledge, i.e., a priori information, about the task to
be learned, as well as the idea of learning to better represent the data, the topic
which constitutes the bulk of the rest of this book.

5.12.1

The Curse of Dimensionality

Many machine learning problems become exceedingly diﬃcult when the number
of dimensions in the data is high. This phenomenon is known as the curse of
dimensionality. Of particular concern is that the number of possible distinct conﬁgurations of the variables of interest increases exponentially as the dimensionality
increases.

144

CHAPTER 5. MACHINE LEARNING BASICS

Figure 5.9: As the number of relevant dimensions of the data increases (from left to right),
the number of conﬁgurations of interest may grow exponentially. In the ﬁgure we ﬁrst
consider one-dimensional data (left), i.e., one variable for which we only care to distinguish
10 regions of interest. With enough examples falling within each of these regions (cells,
in the ﬁgure), learning algorithms can easily generalize correctly, i.e., estimate the value
of the target function within each region (and possibly interpolate between neighboring
regions). With 2 dimensions (center), but still caring to distinguish 10 diﬀerent values of
each variable, we need to keep track of up to 10×10=100 regions, and we need at least
that many examples to cover all those regions. With 3 dimensions (right) this grows to
10 3 = 1000 regions and at least that many examples. For d dimensions and V values to
be distinguished along each axis, it looks like we need O(V d ) regions and examples. This
is an instance of the curse of dimensionality. However, note that if the data distribution is
concentrated on a smaller set of regions, we may actually not need to cover all the possible
regions, only those where probability is non-negligible. Figure graciously provided by, and
with authorization from, Nicolas Chapados.

The curse of dimensionality rears its ugly head in many places in computer
science, and especially so in machine learning.
One challenge posed by the curse of dimensionality is a statistical challenge.
As illustrated in Figure 5.9, a statistical challenge arises because the number of
possible conﬁgurations of the variables of interest is much larger than the number
of training examples. To understand the issue, let us consider that the input space
is organized into a grid, like in the ﬁgure. In low dimensions we can describe this
space with a low number of grid cells that are mostly occupied by the data. The
least we can assume about the data generating distribution is that our learner
should provide the same answer to two examples falling in the same grid cell.
It is a form of local constancy assumption, a notion that we develop further in
the next section. When generalizing to a new data point, we can usually tell
what to do simply by inspecting the training examples that lie in the same cell
as the new input. For example, if estimating the probability density at some
point x, we can just return the number of training examples in the same unit
volume cell as x, divided by the total number of training examples. If we wish to
145

CHAPTER 5. MACHINE LEARNING BASICS

classify an example, we can return the most common class of training examples
in the same cell. If we are doing regression we can average the target values
observed over the examples in that cell. But what about the cells for which
we have seen no example? Because in high-dimensional spaces the number of
conﬁgurations is going to be huge, much larger than our number of examples,
most conﬁgurations will have no training example associated with it. How could
we possibly say something meaningful about these new conﬁgurations? A simple
answer is to extend the local constancy assumption into a smoothness assumption,
as explained next.

5.12.2

Smoothness and Local Constancy A Priori Preference

As argued previously, and especially in high-dimensional spaces (because of the
curse of dimensionality introduced above), machine learning algorithms need priors, i.e., a preference over the space of solutions, in order to generalize to new
conﬁgurations not seen in the training set. The speciﬁcation of these preferences
includes the choice of model family, as well as any regularizer or other aspects
of the algorithm that inﬂuence the ﬁnal outcome of training. We consider here
a particular family of preferences which underlie many classical machine learning
algorithms, and which we call the smoothness prior or the local constancy prior.
We ﬁnd that when the function to be learned has many ups and downs, and this
is typically the case in high-dimensional spaces because of the curse of dimensionality (see above), then the smoothness prior is insuﬃcient to achieve good
generalization. We argue that more assumptions are needed in order to generalize better, in this setting. Deep learning algorithms typically introduce such
additional assumptions. This starts with the classical multi-layer neural networks
studied in the next chapter (Chapter 6), and in Chapter 16 we return to the advantages that representation learning, distributed representations and depth can
bring towards generalization, even in high-dimensional spaces.
Diﬀerent smoothness or local constancy priors can be expressed, but what
they basically say is that the target function or distribution of interest f ∗ is such
that
f ∗(x) ≈ f ∗(x + )
(5.48)
for most conﬁgurations x and small change . In other words, if we know a
good answer (e.g., for an example x) then that answer is probably good in the
neighborhood of x, and if we have several good answers in some neighborhood we
would combine them (e.g., by some form of averaging or interpolation) to produce
an answer that agrees with them as much as possible.
An extreme example of the local constancy approach is the k-nearest neighbors
family of learning algorithms. These predictors are literally constant over each
region R containing all the points x that have the same set of k nearest neighbors
146

CHAPTER 5. MACHINE LEARNING BASICS

in the training set. Note that for k = 1, the number of distinguishable regions
cannot be more than the number of training examples.

Figure 5.10: Illustration of interpolation and kernel-based methods, which construct a
smooth function by interpolating in various ways between the training examples (circles),
which act like knot points controlling the shape of the implicit regions that separate them
as well as the values to output within each region. Depending on the type of kernel, one
obtains a piecewise constant (histogram-like, in dotted red), a piecewise linear (dashed
black) or a smoother kernel (bold blue). The underlying assumption is that the target
function is as smooth or locally as constant as possible. This assumption allows to
generalize locally, i.e., to extend the answer known at some point x to nearby points, and
this works very well so long as, like in the ﬁgure, there are enough examples to cover
most of the ups and downs of the target function.

To obtain even more smoothness, we can interpolate between neighboring
training examples, as illustrated in Figure 5.10. For example, non-parametric kernel density estimation methods and kernel regression methods construct a learned
function f of the form of Eq. 5.37 for classiﬁcation or regression, or alternatively,
e.g., in the Parzen regression estimator, of the form
f (x) = b +

n
X

k(x, x(i) )
αi P n
.
(j))
k(x,
x
j=1
i=1

If the kernel function k is discrete (e.g. 0 or 1), then this can include the above
cases where f is piecewise constant and a discrete set of regions (no more than
one per training example) can be distinguished. However, better results can often
be obtained if k is smooth, e.g., the Gaussian kernel from Eq. 5.38. With k a local
kernel (Bengio et al., 2006b; Bengio and LeCun, 2007b; Bengio, 2009)6 , we can
6

i.e., with k(u, v) large when u = v and decreasing as they get farther apart
147

CHAPTER 5. MACHINE LEARNING BASICS

think of each x(i) as a template and the kernel function as a similarity function
that matches a template and a test example.
With the Gaussian kernel, we do not have a piecewise constant function but
instead a continuous and smooth function. In fact, the choice of k can be shown
to correspond to a particular form of smoothness. Equivalently, we can think of
many of these estimators as the result of smoothing the empirical distribution
by convolving it with a function associated with the kernel, e.g., the Gaussian
kernel density estimator is the empirical distribution convolved with the Gaussian
density.
Although in classical non-parametric estimators the α i of Eq. 5.37 are ﬁxed
(e.g. to 1/n for density estimation and to y(i) for supervised learning from examples (x(i) , y (i))), they can be optimized, and this is the basis of more modern
non-parametric kernel methods (Schölkopf and Smola, 2002) such as the Support Vector Machine (Boser et al., 1992; Cortes and Vapnik, 1995) (see also Section 5.8.2).
However, as illustrated in Figure 5.10, even though these smooth kernel methods generalize better, the main thing that has changed is that one can basically
interpolate between the neighboring examples, in some space associated with the
kernel. One can then think of the training examples as control knots which locally
specify the shape of each region and the associated output.
Decision trees also suﬀer from the limitations of exclusively smoothness-based
learning. Because each example only informs the region in which it falls about
which output to produce, one cannot have more regions than training examples.
If the target function can be well approximated by cutting the input space into
N regions (with a diﬀerent answer in each region), then at least N examples
are needed (and a multiple of N is needed to achieve some level of statistical
conﬁdence in the predicted output). All this is also true if the tree is used for
density estimation (the output is simply an estimate of the density within the
region, which can be obtained by the ratio of the number of training examples in
the region by the region volume) or whether a non-constant (e.g. linear) predictor
is associated with each leaf (then more examples are needed within each leaf node,
but the relationship between number of regions and number of examples remains
linear). We examine below how this may hurt the generalization ability of decision
trees and other learning algorithms that are based only on the smoothness or local
constancy priors, when the input is high-dimensional, i.e., because of the curse of
dimensionality.
In all cases, the smoothness assumption (Eq. 5.48) allows the learner to generalize locally. Since we assume that the target function obeys f ∗ (x) ≈ f∗(x + )
most of the time for small , we can generalize the empirical distribution (or the
(x, y) training pairs) to the neighborhood of the training examples. If (x (i), y(i) )
148

CHAPTER 5. MACHINE LEARNING BASICS

is a supervised (input,target) training example, then we expect f ∗ (x(i) ) ≈ y (i),
and therefore if x is a near neighbor of x(i) , we expect that f ∗(x) ≈ y (i). By considering more neighbors, we can obtain better generalization, by better executing
the smoothness assumption.

Figure 5.11: Illustration of how non-parametric learning algorithms that exploit only the
smoothness or local constancy priors typically break up the input space into regions, with
examples in those regions being used both to deﬁne the region boundaries and what the
output should be within each region. The ﬁgure shows the case of clustering or 1-nearestneighbor classiﬁers, for which each training example (cross of a diﬀerent color) deﬁnes a
region or a template (here, the diﬀerent regions form a Voronoi tessellation). The number
of these contiguous regions cannot grow faster than the number of training examples. In
the case of a decision tree, the regions are recursively obtained by axis-aligned cuts within
existing regions, but for these and for kernel machines with a local kernel (such as the
Gaussian kernel), the same property holds, and generalization can only be local: each
training example only informs the learner about how to generalize in some neighborhood
around it.

In general, to distinguish O(N ) regions in input space, all of these methods require O(N ) examples (and typically there are O(N ) parameters associated
with the O(N ) regions). This is illustrated in Figure 5.11 in the case of a nearestneighbor or clustering scenario, where each training example can be used to deﬁne
one region. Is there a way to represent a complex function that has many more
regions to be distinguished than the number of training examples? Clearly, assuming only smoothness of the underlying function will not allow a learner to do
that. For example, imagine that the target function is a kind of checkerboard,
149

CHAPTER 5. MACHINE LEARNING BASICS

i.e., with a lot of variations, but a simple structure to them, and imagine that
the number of training examples is substantially less than the number of black
and white regions. Based on local generalization and the smoothness or local
constancy prior, we could get the correct answer within a constant-colour region,
but we could not correctly predict the checkerboard pattern. The only thing that
an example tells us, with this prior, is that nearby points should have the same
colour, and the only way to get the checkerboard right is to cover all of its cells
with at least one example.
The smoothness assumption and the associated non-parametric learning algorithms work extremely well so long as there are enough examples to cover most of
the ups and downs of the target function. This is generally true when the function
to be learned is smooth enough, which is typically the case for low-dimensional
data. And if it is not very smooth (we want to distinguish a huge number of
regions compared to the number of examples), is there any hope to generalize
well?
Both of these questions are answered positively in Chapter 16. The key insight
is that a very large number of regions, e.g., O(2N ), can be deﬁned with O(N )
examples, so long as we introduce some dependencies between the regions via additional priors about the underlying data generating distribution. In this way, we
can actually generalize non-locally (Bengio and Monperrus, 2005; Bengio et al.,
2006c). A neural network can actually learn a checkerboard pattern. Similarly,
some recurrent neural networks can learn the n-bit parity (at least for some not
too large values of n). Of course we could also solve the checkerboard task by
making a much stronger assumption, e.g., that the target function is periodic.
However, neural networks can generalize to a much wider variety of structures,
and indeed our AI tasks have structure that is much too complex to be limited
to periodicity, so we want learning algorithms that embody more general-purpose
assumptions. The core idea in deep learning is that we assume that the data was
generated by the composition of factors or features, potentially at multiple levels
in a hierarchy. These apparently mild assumptions allow an exponential gain in
the relationship between the number of examples and the number of regions that
can be distinguished, as discussed in Chapter 16. Priors that are based on compositionality, such as arising from learning distributed representations and from
a deep composition of representations, can give an exponential advantage, which
can hopefully counter the exponential curse of dimensionality. Chapter 16 discusses these questions from the angle of representation learning and the objective
of disentangling the underlying factors of variation.

150

CHAPTER 5. MACHINE LEARNING BASICS

5.12.3

Manifold Learning and the Curse of Dimensionality

We consider here a particular type of machine learning task called manifold learning. Although they have been introduced to reduce the curse of dimensionality.
We will argue that they allow one to visualize and highlight how the smoothness
prior is not suﬃcient to generalize in high-dimensional spaces. Chapter 17 is devoted to the manifold perspective on representation learning and goes in much
greater details in this topic as well as in actual manifold learning algorithms based
on neural networks.
A manifold is a connected region, i.e., a set of points, associated with a neighborhood around each point, which makes it locally look like a Euclidean space.
The notion of neighbor implies the existence of transformations that can be applied to move on the manifold from one position to a neighboring one. Although
there is a formal mathematical meaning to this term, in machine learning it tends
to be used more loosely to talk about a connected set of points that can be well
approximated by considering only a small number of degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each dimension corresponds
to a local direction of variation, i.e., moving along the manifold in some direction.
The manifolds we talk about in machine learning are subsets of points, also called
a submanifold, of the embedding space (which is also a manifold).
Manifold learning algorithms assume that the data distribution is concentrated in a small number of dimensions, i.e., that the set of high-probability conﬁgurations can be approximated by a low-dimensional manifold. Figure 5.8 (left)
illustrates a distribution that is concentrated near a linear manifold (the manifold
is along a 1-dimensional straight line). Manifold learning was introduced in the
case of continuous-valued data and the unsupervised learning setting, although
this probability concentration idea can be generalized to both discrete data and
the supervised learning setting: the key assumption remains that probability mass
is highly concentrated.
Is this assumption reasonable? It seems to be true for almost all of the AI
tasks such as those involving images, sounds and text. To be convinced of this we
will invoke (a) the observation that probability mass is concentrated and (b) the
observed objects can generally be transformed into other plausible conﬁgurations
via some small changes (which indicates a notion of direction of variation while
staying on the “manifold”). For (a), consider that if the assumption of probability concentration was false, then sampling uniformly at random from in the set
of all conﬁgurations (e.g., uniformly in Rn ) should produce probable (data-like)
conﬁgurations reasonably often. But this is not what we observe in practice. For
example, generate pixel conﬁgurations for an image by independently picking the
grey level (or a binary 0 vs 1) for each pixel. What kind of images do you get?
You get “white noise” images, that look like the old television sets when no signal
151

CHAPTER 5. MACHINE LEARNING BASICS

is coming in, as illustrated in Figure 5.12 (left). What is the probability that
you would obtain something that looks like a natural image, with this procedure?
Almost zero, because the set of probable conﬁgurations (near the manifold of
natural images) occupies a very small volume out of the total set of pixel conﬁgurations. Similarly, if you generate a document by picking letters randomly,
what is the probability that you will get a meaningful English-language text? Almost zero, again, because most of the long sequences of letters do not correspond
to a natural language sequence: the distribution of natural language sequences
occupies a very small volume in the total space of sequences of letters.

Figure 5.12: Sampling images uniformly at random, e.g., by randomly picking each pixel
according to a uniform distribution, gives rise to white noise images such as illustrated on
the left. Although there is a non-zero probability to generate something that looks like a
natural image (like those on the right), that probability is exponentially tiny (exponential
in the number of pixels!). This suggests that natural images are very “special”, and that
they occupy a tiny volume of the space of images.

The above thought experiments, which are in agreement with the many experimental results of the manifold learning literature, e.g. (Cayton, 2005; Narayanan
and Mitter, 2010; Schölkopf et al., 1998; Roweis and Saul, 2000; Tenenbaum
et al., 2000; Brand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes, 2003;
Weinberger and Saul, 2004), clearly establish that for a large class of datasets of
interest in AI, the manifold hypothesis is true: the data generating distribution
concentrates in a small number of dimensions, as in the cartoon of Figure 17.4,
152

CHAPTER 5. MACHINE LEARNING BASICS

from Chapter 17. That chapter explores the relationships between representation
learning and manifold learning: if the data distribution concentrates on a smaller
number of dimensions, then we can think of these dimensions as natural coordinates for the data, and we can think of representation learning algorithms as ways
to map the input space to a new and often lower-dimensional space which captures
the leading dimensions of variation present in the data as axes or dimensions of
the representation.
An initial hope of early work on manifold learning (Roweis and Saul, 2000;
Tenenbaum et al., 2000) was to reduce the eﬀect of the curse of dimensionality,
by ﬁrst reducing the data to a lower dimensional representation (e.g. mapping
(x1, x 2 ) to z1 in Figure 5.8 (right)), and then applying ordinary machine learning
in that transformed space. This dimensionality reduction can be achieved by
learning a transformation (generally non-linear, unlike with PCA introduced in
Section 5.9.1) of the data that is invertible for most training examples, i.e., that
keeps the information in the input example. It is only possible to reconstruct input
examples from their low-dimensional representation because they lie on a lowerdimensional manifold, of course. This is basically how auto-encoders (Chapter 15)
are trained.
The hope was that by non-linearly projecting the data in a new space of lower
dimension, we would reduce the curse of dimensionality by only looking at relevant dimensions, i.e., a smaller set of regions of interest (cells, in Figure 5.9).
This can indeed be the case, however, as discussed in Chapter 17, the manifolds
can be highly curved and have a very large number of twists, requiring still a very
large number of regions to be distinguished (every up and down of each corner of
the manifold). And even if we were to reduce the dimensionality of an input from
10000 (e.g. 100×100 binary pixels) to 100, 2 100 is still too large to hope covering with a training set. This still rules out the use of purely local generalization
(i.e., the smoothness prior only) to model such manifolds, as discussed in Chapter 17 around Figure 17.4 and 17.5. It may also be that although the eﬀective
dimensionality of the data could be small, some examples could fall outside of the
main manifold and that we do not want to systematically lose that information.
A sparse representation then becomes a possible way to represent data that is
mostly low-dimensional, although occasionally occupying more dimensions. This
can be achieved with a high-dimensional representation whose elements are 0 most
of the time. We can see that the eﬀective dimension (the number of non-zeros)
then can change depending on where we are in input space, which can be useful.
Sparse representations are discussed in Section 15.8.
The next part of the book introduces speciﬁc deep learning algorithms that
aim at discovering representations that are useful for some task, i.e., trying to
extract the directions of variations that matter for the task of interest, often in a
153

CHAPTER 5. MACHINE LEARNING BASICS

supervised setting. The last part of the book concentrates more on unsupervised
representation learning algorithms, which attempt to capture all of the directions
of variation that are salient in the data distribution.

154

Part II

Modern Practical Deep
Networks

155

This part of the book summarizes the state of modern deep learning as it is
used to solve practical applications.
Deep learning has a long history and many aspirations. Several approaches
have been proposed that have yet to entirely bear fruit. Several ambitious goals
have yet to be realized. These less-developed branches of deep learning appear in
the ﬁnal part of the book.
This part focuses only on those approaches that are essentially working technologies that are already used heavily in industry.
Modern deep learning provides a very powerful framework for supervised learning. By adding more layers and more units within a layer, a deep network can
represent functions of increasing complexity. Most tasks that consist of mapping
an input vector to an output vector, and that are easy for a person to do rapidly,
can be accomplished via deep learning, given suﬃciently large model and dataset
of labeled training examples. Other tasks, that can not be described as associating one vector to another, or that are diﬃcult enough that to do them a person
would require time to think and reﬂect, remain beyond the scope of deep learning
for now.
This part of the book describes the core parametric function approximation
technology that is behind nearly all modern practical applications of deep learning. Our description includes details such as, how to eﬃciently model speciﬁc
kinds of inputs, how to process image inputs with convolutional networks as well
as how to process sequence inputs with recurrent and recursive networks. Moreover, we provide guidance for how to preprocess the data for various tasks and
how to choose the values of the various settings that govern the behavior of these
algorithms.
These chapters are the most important for a practitioner – someone who wants
to begin implementing and using deep learning algorithms to solve real-world
problems today.

156

Chapter 6

Feedforward Deep Networks
Feedforward deep networks, also known as multilayer perceptrons (MLPs), are the
quintessential deep networks. They are parametric functions deﬁned by composing together many parametric functions. Each of these component functions has
multiple inputs and multiple outputs. In neural network terminology, we refer to
each sub-function as a layer of the network, and each scalar output of one of these
functions as a unit or sometimes as a feature. Even though each unit implements a
relatively simple mapping or transformation of its input, the function represented
by the entire network can become arbitrarily complex.
Not every deep learning algorithm can be understood in terms of deﬁning a
single, deterministic function like feedforward deep networks, but all of them share
the property of containing many layers of many units. We can think of the number
of units in each layer as being the width of a machine learning model, and the
number of layers as its depth. Feedforward deep networks provide a conceptually
simple example of an algorithm that captures the many advantages that come
from having signiﬁcant width and depth. Feedforward deep networks are also the
key technology underlying most of the contemporary commercial applications of
deep learning to large datasets.
In Chapter 5, we encountered several diﬀerent traditional machine learning
algorithms, including linear regression, linear classiﬁers, logistic regression and
kernel machines. All of these algorithms work by applying a linear transformation
to a ﬁxed set of features. These algorithms can learn non-linear functions, but
the non-linear part is ﬁxed. In other words, the functions are non-linear in the
space of inputs x, but they are linear in some other pre-deﬁned space.
Neural networks allow us to learn new kinds of non-linearity. Another way to
view this idea is that neural networks allow us to learn the features provided to
a linear model. From this point of view, neural networks allow us to automate
the design of features—a task that until recently was performed gradually and
157

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

collectively, by the combined eﬀorts of an entire community of researchers.

6.1

Vanilla MLPs

Feedforward supervised neural networks were among the ﬁrst and most successful
non-linear learning algorithms (Rumelhart et al., 1986e,c). These networks learn
at least one function deﬁning the features, as well as a (typically linear) function
mapping from features to output. The layers of the network that correspond to
features rather than outputs are called hidden layers. This is because the correct
values of the features are unknown. The features must be created by the training
algorithm. The input and output of the network is by contrast observed or visible
in the training data. Figure 6.1 shows a vanilla MLP architecture with a single
hidden layer. A deeper version is obtained by simply having more hidden layers.

V

W

Figure 6.1: Vanilla (shallow) MLP, with one sigmoid hidden layer, computing vectorvalued hidden unit vector h = sigmoid(c + W x) with weight matrix W and oﬀset vector
c. The output vector is obtained via another learned aﬃne transformation ŷ = b + V h,
with weight matrix V and output oﬀset vector b. The vector of hidden unit values h
provides a new set of features, i.e., a new representation, derived from the raw input x.

Example 6.1.1 introduces the equations for a vanilla MLP for regression similar
to the one illustrated in Figure 6.1, which we will generalize below in the next
few sections.

158

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Example 6.1.1. Vanilla (Shallow) Multi-Layer Neural Network
for Regression

Based on the above deﬁnitions, we could pick the family of input-output
functions to be
fθ (x) = b + V sigmoid(c + W x),
illustrated in Figure 6.1, where sigmoid(a) = 1/(1 + e−a ) is applied
element-wise, the input is the vector x ∈ Rni , the hidden layer outputs
are the elements of the vector h = sigmoid(c + W x) with nh entries, the
parameters are θ = (b, c, V , W ) (with θ also viewed as the ﬂattened vectorized version of the tuple) with b ∈ Rno a vector the same dimension as
the output (n o ), c ∈ Rnh of the same dimension as h (number of hidden
units), V ∈ Rn o ×nh and W ∈ Rnh ×ni being weight matrices.
The loss function for this classical example could be the squared error
L( ŷ, y) = ||ŷ − y||2 (see Section 6.2 discussing how it makes ŷ an estimator of E[Y | x]). The regularizer could be the ordinary L 2 weight decay
P
P
||ω||2 = ( ij W2ij + ki Vki2 ), where we deﬁne the set of weights ω as the
concatenation of the elements of matrices W and V . The L2 weight decay thus penalizes the squared norm of the weights, with λ a scalar that is
larger to penalize stronger weights, thus yielding smaller weights. During
training, we minimize a cost function obtained by adding together the
squared loss and the regularization term:
n

1 X (t)
J(θ) = λ||ω|| +
||y − (b + V sigmoid(c + W x(t)))||2 .
n
2

t=1

where (x (t), y(t) ) is the t-th training example, an (input,target) pair.
TODO: so we introduce SGD for the ﬁrst time in the book practically
hidden in an example in a chapter that appears to mostly be about one
speciﬁc model family.... we should do this more gracefully, maybe put
it in numerical.tex or ml.tex Finally, the classical training procedure in
this example is stochastic gradient descent, which iteratively updates θ
according to


(t)
(t)
ω ← ω −  2λω + ∇ω L(fθ (x ), y )
β ← β − ∇ βL(f θ (x (t)), y (t) ),

where β = (b, c) contains the oﬀset 1 parameters, ω = (W , V ) the weight
matrices,  is a learning rate and159
t is incremented after each training
example, modulo n. Section 6.4 shows how gradients can be computed

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

MLPs can learn powerful non-linear transformations: in fact, with enough
hidden units they can represent arbitrarily complex but smooth functions, they
can be universal approximators, as described below in Section 6.5. This is achieved
by composing simple but non-linear learned transformations. By transforming the
data non-linearly into a new space, a classiﬁcation problem that was not linearly
separable (not solvable by a linear classiﬁer) can become separable, as illustrated
in Figures 6.2 and 6.3.

Figure 6.2: Each layer of a trained neural network non-linearly transforms its input,
distorting the space so that the task becomes easier to perform, e.g., linear classiﬁcation
in the new feature space, in the above ﬁgures. The ﬁgure shows how a vanilla neural
network with 2 inputs, 2 hidden units and one output can transform the 2-D input space
so that the examples from the two classes become linearly separable. The red and blue
solid curves are where the training examples come from (with color indicating class).
The paler red (resp. blue) region indicates the region that should be labeled as red
(resp. blue), with the blue-to-red interface corresponding to a good decision surface.
On the left, we see in a black square the good decision surface in the original input
space, and see that it is non-linear (i.e., a linear classiﬁer could not do a good job if
applied directly on the raw inputs). The raw input space is also mapped by a regular
grid in the left square, and the grid gets transformed non-linearly, i.e., warped diﬀerently
in diﬀerent parts of the space (consider how the grid was transformed), to obtain the
middle square, i.e., in the space of hidden units: every (x 1 , x2 ) point in the left block is
mapped to a point (h1, h2 ) in the middle block using the parameters of the hidden units.
We see that when the data are properly transformed non-linearly by the ﬁrst hidden
layer, the good decision surface becomes a linear one, which can be implemented by the
output layer (which is basically a linear classiﬁer). Reproduced with permission by Chris
Olah from http://colah.github.io/, where many more insightful visualizations can be
found.

160

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Figure 6.3: Like Figure 6.2, this ﬁgure shows how the hidden layer of an MLP can nonlinearly warp the input space so as to make the classiﬁcation task easier for a linear
classiﬁer (the output layer of a vanilla MLP). The MLP now has 784 inputs (pixels of
a 28 × 28 image of a digit), 100 hidden units and 10 outputs corresponding to the 10
digit classes. We cannot directly visualize with a 2-D grid the input and hidden layer
spaces, but we can visualize a 2-D approximation obtained by dimensionality reduction.
The squares below the input and hidden layer show where the training examples are
in this reduced space, with one point per example, colored according to the digit class.
Again, we see that the digits of diﬀerent classes can be more easily separated in the
feature space of the hidden layer than in the raw pixel space, with digits of the same
class tending to form better separated clusters. Reproduced with permission by Chris
Olah from http://colah.github.io/, where more detail about this experiment can be
found.

161

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

6.2

Estimating Conditional Statistics

To gently move from linear predictors to non-linear ones, let us consider the
squared error loss function studied in the previous chapter, where the learning
task is the estimation of the expected value of y given x. In the context of linear
regression, the conditional expectation of y is used as the mean of a Gaussian
distribution that we ﬁt with maximum likelihood. We can generalize linear regression to regression via any function f by deﬁning the mean squared error of
f:
E[||y − f (x)||2]
where the expectation is over the training set during training, and over the data
generating distribution to obtain generalization error.
We can generalize its interpretation beyond the case where f is linear or
aﬃne, uncovering an interesting property: minimizing it yields an estimator of
the conditional expectation of the output variable y given the input variable x,
i.e.,
(6.1)
arg min Ep(x,y) [||y − f (x)||2] = E p(x,y) [y|x].
f ∈H

provided that our set of function H contains E p(x,y)[y | x]. (If you would like to
work out the proof yourself, it is easy to do using calculus of variations, which we
describe in Chapter 19.4.2).
Similarly, we can generalize conditional maximum likelihood (introduced in
Section 5.6.1) to other distributions than the Gaussian, as discussed below when
deﬁning the objective function for MLPs.

6.3

Parametrizing a Learned Predictor

There are many ways to deﬁne the family of input-output functions, cost function
(including optional regularizers) and optimization procedure. The most common
ones are described below, while more advanced ones are left to later chapters.

6.3.1

Family of Functions

A motivation for the family of functions deﬁned by multi-layer neural networks
is to compose simple transformations in order to obtain highly non-linear ones. In
particular, MLPs compose aﬃne transformations and element-wise non-linearities.
As discussed in Section 6.5 below, with the appropriate choice of parameters,
multi-layer neural networks can in principle approximate any smooth function,
with more hidden units allowing one to achieve better approximations.

162

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

A multi-layer neural network with more than one hidden layer can be deﬁned by generalizing the above structure, e.g., as follows, where we chose to use
hyperbolic tangent 2 activation functions instead of sigmoid activation functions:
h k = tanh(bk + W k h k−1)
where h 0 = x is the input of the neural net, hk (for k > 0) is the output of the
k-th hidden layer, which has weight matrix W k and oﬀset (or bias) vector bk . If
we want the output f θ (x) to lie in some desired range, then we typically deﬁne
an output non-linearity (which we did not have in the above Example 6.1.1). The
non-linearity for the output layer is generally diﬀerent from the tanh, depending
on the type of output to be predicted and the associated loss function (see below).
There are several other non-linearities besides the sigmoid and the hyperbolic
tangent which have been successfully used with neural networks. In particular,
we introduce some piece-wise linear units below such as the the rectiﬁed linear
unit (max(0, b +w · x)) and the maxout unit (maxi (b i +W:,i ·x)) which have been
particularly successful in the case of deep feedforward or convolutional networks.
A longer discussion of these can be found in Section 6.7.
These and other non-linear neural network activation functions commonly
found in the literature are summarized below. Most of them are typically combined with an aﬃne transformation a = b + W x and applied element-wise:
h = φ(a) ⇔ hi = φ(a i) = φ(bi + W i,:x).

(6.2)

• Rectiﬁer or rectiﬁed linear unit (ReLU) or positive part: transformation of the output of the previous layer: φ(a) = max(0, a), also written
φ(a) = (a)+ .
• Hyperbolic tangent: φ(a) = tanh(a).
• Sigmoid: φ(a) = 1/(1 + e−a).
• Softmax: This is a vector-to-vector transformation φ(a) = softmax(a) =
P
P
e ai / j ea j such that i φ i(a) = 1 and φi (a) > 0, i.e., the softmax output
can be considered as a probability distribution over a ﬁnite set of outcomes.
Note that it is not applied element-wise but on a whole vector of “scores”.
It is mostly used as output non-linearity for predicting discrete probabilities
over output categories. See deﬁnition and discussion below, around Eq. 6.4.
• Radial basis function or RBF unit: this one is not applied after a general
aﬃne transformation but acts on x using a diﬀerent form that corresponds
2

which is linearly related to the sigmoid via tanh(x) = 2 × sigmoid(2x) − 1 and typically
yields easier optimization with stochastic gradient descent (Glorot and Bengio, 2010a).
163

CHAPTER 6. FEEDFORWARD DEEP NETWORKS



to a template matching, i.e., hi = exp −||wi − x||2/σ 2i (or typically with
all the σ i set to the same value). This is heavily used in kernel SVMs (Boser
et al., 1992; Schölkopf et al., 1999) and has the advantage that such units
can be easily initialized (Powell, 1987; Niranjan and Fallside, 1990) as a
random (or selected) subset of the input examples, i.e., wi = x (t) for some
assignment of examples t to hidden unit templates i.
• Softplus: φ(a) = log(1 + ea ). This is a smooth version of the rectiﬁer,
introduced in Dugas et al. (2001) for function approximation and in Nair
and Hinton (2010a) in RBMs. Glorot et al. (2011a) compared the softplus
and rectiﬁer and found better results with the latter, in spite of the very
similar shape and the diﬀerentiability and non-zero derivative of the softplus
everywhere, contrary to the rectiﬁer.
• Hard tanh: this is shaped similarly to the tanh and the rectiﬁer but unlike
the latter, it is bounded, φ(a) = max(−1, min(1, a)). It was introduced
by Collobert (2004).
• Absolute value rectiﬁcation: φ(a) = |a| (may be applied on the aﬃne
dot product or on the output of a tanh unit). It is also a rectiﬁer and has
been used for object recognition from images (Jarrett et al., 2009a), where
it makes sense to seek features that are invariant under a polarity reversal
of the input illumination.
• Maxout: this is discussed in more detail in Section 6.7. It generalizes the
rectiﬁer but introduces multiple weight vectors wi (called ﬁlters) for each
hidden unit. hi = maxi (bi + wi · x).
This is not an exhaustive list but covers most of the non-linearities and unit
computations seen in the deep learning and neural nets literature. Many variants
are possible.
As discussed in Section 6.4, the structure (also called architecture) of the
family of input-output functions can be varied in many ways, which calls for a
generic principle for eﬃciently computing gradients, described in that section.
For example, a common variation is to connect layers that are not adjacent, with
so-called skip connections, which are found in the visual cortex (where the word
“layer” should be replaced by the word “area”). Other common variations depart
from a full connectivity between adjacent layers. For example, each unit at layer
k may be connected to only a subset of units at layer k − 1. A particular case
of such form of sparse connectivity is discussed in chapter 9 with convolutional
networks. The set of connections between units of the whole network needs to
form a directed acyclic graph in order to deﬁne a meaningful computation (see
the ﬂow graph formalism below, Section 6.4). Recurrent networks, treated in 10,
164

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

are typically depicted using graphs containing cycles. Such graphs are using a
diﬀerent kind of graphical language. The cycles indicate that the value of a unit
at time step t + 1 is a function of the value of the unit at time step t. These
cyclical graphs in the recurrent network language can be unrolled into directed
acyclic graphs containing multiple time steps in order to obtain a traditional
computational graph.

6.3.2

Loss Function and Conditional Log-Likelihood

In the 80’s and 90’s the most commonly used loss function was the squared error
L(fθ (x), y) = ||f θ (x)− y||2 . As discussed in Section 6.2, if f is unrestricted (nonparametric), minimizing the expected value of the loss function over some datagenerating distribution P (x, y) yields f (x) = E[y | x = x], the true conditional
expectation of y given x. This tells us what the neural network is trying to learn.
Replacing the squared error by an absolute value makes the neural network try
to estimate not the conditional expectation but the conditional median3.
However, when y is a discrete label, i.e., for classiﬁcation problems, other loss
functions such as the Bernoulli negative log-likelihood4 have been found to be
more appropriate than the squared error. In the case where y ∈ {0, 1} is binary
this gives
L(f θ(x), y) = −y log f θ(x) − (1 − y) log(1 − f θ (x))
(6.3)
also known as cross entropy objective function. It can be shown that the optimal
(non-parametric) f minimizing this loss function is f(x) = P (y = 1 | x). In
other words, when maximizing the conditional log-likelihood objective function,
we are training the neural net output to estimate conditional probabilities as
well as possible in the sense of the KL divergence (see Section 3.9, Eq. 3.3).
Note that in order for the above expression of the criterion to make sense, f θ (x)
must be strictly between 0 and 1 (an undeﬁned or inﬁnite value would otherwise
arise). To achieve this, it is common to use the sigmoid as non-linearity for
the output layer, which matches well with the binomial negative log-likelihood
cost function 5. As explained below (Softmax subsection), the log-likelihood of
a Bernoulli variable whose mean is parameterized by a sigmoidal unit allows
gradients to pass through the output non-linearity even when the neural network
3

Showing this is another interesting exercise.
Many authors use the term “cross entropy” to identify speciﬁcally the negative log-likelihood
of a Bernoulli or softmax distribution, but that is a misnomer. Any loss consisting of a negative
log-likelihood is a cross entropy between the empirical distribution deﬁned by the training set
and the model. For example, mean squared error is the cross entropy between the empirical
distribution and a Gaussian model.
5
In reference to statistical models, this “match” between the loss function and the output
non-linearity is similar to the choice of a link function in generalized linear models (McCullagh
and Nelder, 1989).
4

165

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

produces a conﬁdently wrong answer, unlike the squared error criterion coupled
with a sigmoid or softmax non-linearity.
Learning a Conditional Probability Model
More generally, one can deﬁne a loss function as corresponding to a conditional
log-likelihood, i.e., the negative log-likelihood (NLL) cost function
L NLL(f θ (x), y) = − log P (y = y | x = x; θ).
See Section 5.6.1 (and the one before) which shows that this criterion corresponds
to minimizing the KL divergence between the model P of the conditional probability of y given x and the data generating distribution Q, approximated here
by the ﬁnite training set, i.e., the empirical distribution of pairs (x, y). Hence,
minimizing this objective, as the amount of data increases, yields an estimator of
the true conditional probability of y given x.
For example, if y is a continuous random variable and we assume that, given
x, it has a Gaussian distribution with mean f θ (x) and variance σ2, then
− log P (y | x; θ) =

1
(f θ(x) − y) 2/σ 2 + log(2πσ 2).
2

Up to an additive and multiplicative constant (which would give the same choice
of θ), minimizing this negative log-likelihood is therefore equivalent to minimizing
the squared error loss. Once we understand this principle, we can readily generalize it to other distributions, as appropriate. For example, it is straightforward to
generalize the univariate Gaussian to the multivariate case, and under appropriate
parametrization consider the variance to be a parameter or even a parametrized
function of x (for example with output units that are guaranteed to be positive,
or forming a positive deﬁnite matrix, as outlined below, Section 6.3.2).
Similarly, for discrete variables, the binomial negative log-likelihood cost function corresponds to the conditional log-likelihood associated with the Bernoulli
distribution (also known as cross entropy) with probability p = fθ(x) of generating y = 1 given x = x (and probability 1 − p of generating y = 0):
L NLL = − log P (y | x; θ) = −1y=1 log p − 1y=0 log(1 − p)
= −y log fθ(x) − (1 − y) log(1 − f θ(x)).

where 1y=1 is the usual binary indicator.
Softmax
When y is discrete and has a ﬁnite domain (say {1, . . . , n}) but is not binary,
the Bernoulli distribution is extended to the multinoulli distribution (deﬁned in
166

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Section 3.10.2). This distribution is speciﬁed by a vector of n − 1 probabilities
whose sum is less than or equal to 1, each element of which provides
the probability
Pn−1
pi = P (y = i | x). We can then recover P (y = N | x) as 1 − i=1 P (y = i | x).
Alternatively, one can specify a vector of n probabilities whose sum is exactly
1. The two options have the same representational power but diﬀerent learning
dynamics.
The softmax non-linearity (Bridle, 1990): was designed for the purpose of
specifying multinoulli distributions:
eai
p = softmax(a) ⇐⇒ p i = P a .
j
je

(6.4)

where typically a is a set of activations coming from the lower layers of the network. We often use the overparameterized a = b + W h, but we may hardcode
an to 0 in order to specify only n − 1 of the output probabilities. We can think
of a as a vector of scores whose elements a i are associated with each category i,
with larger relative scores yielding exponentially larger probabilities. The corresponding loss function is therefore L NLL(p, y) = − log p y. Note how minimizing
this loss will push ay up (increase the score ay associated with the correct label
y) while pushing down ai for i 6
= y (decreasing the score of the other labels, in
the context x). The ﬁrst eﬀect comes from the numerator of the softmax while
the second eﬀect comes from the normalizing denominator. These forces cancel
on a speciﬁc example only if py = 1 and they cancel in average over examples (say
sharing the same x) if pi equals the fraction of times that y = i for this value x.
To see this, consider the gradient with respect to the scores a:
X
∂
∂
∂
(− log py ) =
(−ay + log
LNLL(p, y) =
e aj )
∂a k
∂ak
∂a k
j

e ak

= −1 y=k + P

j

e aj

= p k − 1y=k or

∂
LNLL(p, y) = (p − e y )
∂a

(6.5)

where e y = [0, . . . , 0, 1, 0, . . . , 0] is the one-hot vector with a 1 at position y.
Examples that share the same x share the same a, so the average gradient on a
over these examples is 0 when the average of the above expression cancels out,
i.e., p = E y[e y | x] where the expectation is over these examples. Thus the
optimal pi for these examples is the average number of times that y = i among
those examples. Over an inﬁnite number of examples, we would obtain that the
gradient is 0 when pi perfectly estimates the true P (y = i | x). What the above
167

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

gradient decomposition teaches us as well is the division of the total gradient into
(1) a term due to the numerator (the ey ) and dependent on the actually observed
target y and (2) a term independent of y but which corresponds to the gradient of
the softmax denominator. The same principles and the role of the normalization
constant (or “partition function”) can be seen at play in the training of Markov
Random Fields, Boltzmann machines and RBMs, in Chapter 13.
The softmax has other interesting properties. First of all, the gradient of
logp(y = i | x) with respect to a only saturates in the case when p(y = i | x) is
already nearly maximal, i.e., approaching 1. Speciﬁcally, let us consider the case
where the correct label is i, i.e. y = i. The element of the gradient associated
with an erroneous label, say j 6
= i, is
∂
L (p, y) = p j .
∂aj NLL

(6.6)

So if the model correctly predicts a low probability that the y = j, i.e. that
pj ≈ 0, then the gradient is also close to zero. But if the model incorrectly
and conﬁdently predicts that j is the correct class, i.e., p j ≈ 1, there will be a
strong push to reduce aj . Conversely, if the model incorrectly and conﬁdently
predicts that the correct class y should have a low probability, i.e., p y ≈ 0, there
will be a strong push (a gradient of about -1) to push ay up. One way to see
these is to imagine doing gradient descent on the aj ’s themselves (that is what
backprop is really based on): the update on aj would be proportional to minus one
times the gradient on aj , so a positive gradient on a j (e.g., incorrectly conﬁdent
that pj ≈ 1) pushes aj down, while a negative gradient on aj (e.g., incorrectly
conﬁdent that py ≈ 0) pushes a y up. In fact note how ay is always pushed up
because py − 1 y=y = p y − 1 < 0, and the other scores a j (for j =
6 y) are always
pushed down, because their gradient is p j > 0.
There are other loss functions such as the squared error applied to softmax (or
sigmoid) outputs (which was popular in the 80’s and 90’s) which have vanishing
gradient when an output unit saturates (when the derivative of the non-linearity
is near 0), even if the output is completely wrong (Solla et al., 1988). This may be
a problem because it means that the parameters will basically not change, even
though the output is wrong.
To see how the squared error interacts with the softmax output, we need to
introduce a one-hot encoding of the label, y = ei = [0, . . . , 0, 1, 0, . . . , 0], i.e for
the label y = i, we have y i = 1 and y j = 0, ∀j 6
= i. We will again consider that
we have the output of the network to be p = softmax(a), where, as before, a is
the input to the softmax function ( e.g. a = b + W h with h the output of the
last hidden layer).
For the squared error loss L2 (p(a), y) = ||p(a) − y||2 , the gradient of the loss
168

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

with respect to the input vector to the softmax, a, is given by:
∂
∂L 2 (p(a), y) ∂p(a)
L2 (p(a), y) =
∂ai
∂p(a)
∂a i
X
=
2(pj (a) − yj )p j(1 i=j − p i ).

(6.7)

j

So if the model incorrectly predicts a low probability for the correct class y = i,
i.e., if py = p i ≈ 0, then the score for the correct class, ay , does not get pushed
up in spite of a large error, i.e., ∂a∂y L2 (p(a), y) ≈ 0. For this reason, practitioners
prefer to use the negative log-likelihood (cross entropy) cost function, with the
softmax non-linearity (as well as with the sigmoid non-linearity), rather than
applying the squared error criterion to these probabilities.
Another uesful property of the softmax is that its output is invariant to adding
a scalar to all of its inputs:
softmax(a) = softmax(a + b).
This property is used to implement the numerically stable variant of the softmax,
which exploits the fact that softmax(a) = softmax(a −maxi a i). This allows us to
evaluate softmax with only small numerical errors even when a contains extremely
large or extremely negative numbers.
Finally, it is interesting to think of the softmax as a way to create a form of
competition between the units (typically output units, but not necessarily) that
participate in it: the softmax outputs always sum to 1 so an increase in the value
of one unit necessarily corresponds to a decrease in the value of others. This is
analogous to the lateral inhibition that is believed to exist between nearby neurons
in cortex. Aat the extreme (when the diﬀerence between the maximal ai and the
others is large in magnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1 and the others are nearly 0). A more computationally expensive
form of competition is found with sparse coding, described in Section 19.3.
Neural Net Outputs as Parameters of a Conditional Distribution
In general, for any parametric probability distribution p(y | ω) with parameters ω, we can construct a conditional distribution p(y | x) by making ω a
parametrized function of x and learning that function:
p(y | ω = fθ (x))
where f θ(x) is the output of a predictor, x is its input, and y can be thought
of as a “target”. The use of the word “target” comes from the common cases of
169

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

classiﬁcation and regression, where fθ (x) is really a prediction associated with
random variable y, or with its expected value. However, in general ω = fθ (x)
may contain parameters of the distribution of y other than its expected value.
For example, it could contain its variance or covariance, in the case where y is
conditionally Gaussian. In the above examples, with the squared error loss, ω is
the mean of the Gaussian which captures the conditional distribution of y (which
means that the variance is considered ﬁxed, not a function of x). In the common
classiﬁcation case, ω contains the probabilities associated with the various events
of interest.
Once we view things in this way, if we apply the principle of maximum likelihood in the conditional case (Section 5.6.1), we automatically get as the natural
cost function the negative log-likelihood L(x, y) = − log p(y | ω = f θ (x)). Besides the expected value of y, there could be other parameters of the conditional
distribution of y that control the distribution of y, given x. For example, we
may wish to learn the variance of a conditional Gaussian for y, given x, and that
variance could be a function that varies with x or that is a constant with respect
to x. If the variance σ 2 of y given x is not a function of x, its maximum likelihood
value can be computed analytically because the maximum likelihood estimator of
variance is simply the empirical mean of the squared diﬀerence between observations y and their expected value (here estimated by fθ (x)). In the scalar case,
we could estimate σ as follows:
n
1 X (t)
2
σ ←
(y − fθ (x(t) ))2
(6.8)
n
i=1

where (t) indicates the t-th training example (x(t) , y (t)). In other words, the
conditional variance can simply be estimated from the mean squared error. If
y is a d-vector and the conditional covariance is σ 2 times the identity, then the
above formula should be modiﬁed as follows, again by setting the gradient of the
log-likelihood with respect to σ to zero:
n
1 X (t)
σ ←
||y − fθ (x (t))||2 .
nd i=1
2

(6.9)

In the multivariate case with a diagonal covariance matrix with entries σ2i , we
obtain
n
1 X (t)
2
σi ←
(y − fθ,i (x (t) )) 2.
(6.10)
n i=1 i
In the multivariate case with a full covariancae matrix, we have
1
Σ←
n

n
i=1

X

(t)

(t)

(y i − fθ,i(x (t) ))(yi
170

− fθ,i (x(t) ))>

(6.11)

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

If the variance Σ(x) is a function of x, there is no known method for maximizing
the likelihood in closed form, but we can compute the gradient necessary for
use with an iterative optimization procedure. If Σ(x) is diagonal or scalar, only
positivity must be enforced, e.g., using the softplus non-linearity:
σi (x) = softplus(gθ (x)).
where g θ(x) may be a neural network that takes x as input. A positive nonlinearity may also be useful in the case where σ is a function of x, if we do not
seek the maximum likelihood solution (for example we do not have immediate
observed targets associated with that Gaussian distribution, because the samples from the Gaussian are used as input for further computation). Then we can
make the free parameter ω deﬁning the variance the argument of the positive nonlinearity, e.g., σi(x) = softplus(ω i ). If the covariance is full and conditional, then
a parametrization must be chosen that guarantees positive-deﬁniteness of the predicted covariance matrix. This can be achieved by writing Σ(x) = B(x)B >(x),
where B is an unconstrained square matrix. One practical issue if the the matrix
is full is that computing the likelihood is expensive, requiring O(d 3) computation
for the determinant and inverse of Σ(x) (or equivalently, and more commonly
done, its eigendecomposition or that of B(x)).
Besides the Gaussian, a simple and common example is the case where y is
binary (i.e. Bernoulli distributed), where it is enough to specify ω = p(y = 1 | x).
In the multinoulli case (multiple discrete values), ω is generally speciﬁed by a
vector of probabilities (one per possible discrete value) summing to 1, e.g., via
the softmax non-linearity discussed above.
Another interesting and powerful example of output distribution for neural
networks is the mixture model, and in particular the Gaussian mixture model,
introduced in Section 3.10.6. Neural networks that compute the parameters of
a mixture model were introduced in Jacobs et al. (1991); Bishop (1994). In the
case of the Gaussian mixture model with n components,
p(y | x) =

n
X
i=1

p(c = i | x)N (y | µi (x), Σi (x)).

The neural network must have three outputs: p(c = i | x), µ i(x) and Σi (x).
These outputs must satisfy diﬀerent constraints:
1. Mixture components p(c = i | x): these form a multinoulli distribution over
the n diﬀerent components associated with latent 6 variable c, and can typ6

c is called latent because we do not observe it in the data: given input x and target y, it
is not 100% clear which Gaussian component was responsible for y, but we can imagine that y
was generated by picking one of them, and make that unobserved choice a random variable.
171

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

ically be obtained by a softmax over an n-dimensional vector, to guarantee
that these outputs are positive and sum to 1.
2. Means µ i(x): these indicate the center or mean associated with the i-th
Gaussian component, and are unconstrained (typically with no non-linearity
at all for these output units). If y is a d-vector, then the network must
output an n × d matrix containing all n of these d-dimensional vectors.
3. Covariances Σ i (x): these specify the covariance matrix for each component
i. For the general case of an unconditional (does not depend on x) but full
covariance matrix, see Eq. 6.11 to set it by maximum likelihood. In many
models the variance is both unconditional and diagonal (like assumed with
Eq. 6.10) or even scalar (like assumed with Eq. 6.8 or 6.9).
It has been reported that gradient-based optimization of conditional Gaussian
mixtures (on the output of neural networks) can be ﬁnicky, in part because one
gets divisions (by the variance) which can be numerically unstable (when some
variance gets to be small for a particular example, yielding very large gradients).
One solution is to clip gradients (see Section 10.8.7 and Mikolov (2012); Pascanu
and Bengio (2012); Graves (2013); Pascanu et al. (2013a)), while another is to
scale the gradients heuristically (Murray and Larochelle, 2014).
Multiple Output Variables
When y is actually a tuple formed by multiple random variables y = (y1 , y2, . . . , yk ),
then one has to choose an appropriate form for their joint distribution, conditional
on x = x. The simplest and most common choice is to assume that the yi are
conditionally independent, i.e.,
p(y 1 , y2 , . . . , y k | x) =

k
Y
i=1

p(yi | x).

This brings us back to the single variable case, especially since the log-likelihood
now decomposes into a sum of terms log p(yi | x). If each p(yi | x) is separately
parametrized (e.g. a diﬀerent neural network), then we can train these neural
networks independently. However, a more common and powerful choice assumes
that the diﬀerent variables y i share some common factors, given x, that can be
represented in some hidden layer of the network (such as the top hidden layer).
See Sections 6.6 and 7.12 for a deeper treatment of the notion of underlying factors
of variation and multi-task training: each (x, yi) pair of random variables can be
associated with a diﬀerent learning task, but it might be possible to exploit what
these tasks have in common. See also Figure 7.6 illustrating these concepts.
172

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

If the conditional independence assumption is considered too strong, what can
we do? At this point it is useful to step back and consider everything we know
about learning a joint probability distribution. Since any probability distribution
p(y; ω) parametrized by parameters ω can be turned into a conditional distribution p(y | x; θ) (by making ω a function ω = f θ(x) parametrized by θ), we
can go beyond the simple parametric distributions we have seen above (Gaussian,
Bernoulli, multinoulli), and use more complex joint distributions. If the set of
values that y can take is small enough (e.g., we have 8 binary variables yi , i.e., a
joint distribution involving 28 = 256 possible values), then we can simply model
all these joint occurences as separate values, e.g., with a softmax and multinoulli
over all these conﬁgurations. However, when the set of values that y i can take
cannot be easily enumerated and the joint distribution is not unimodal or factorized, we need other tools. The third part of this book is about the frontier of
research in deep learning, and much of it is devoted to modeling such complex
joint distributions, also called graphical models: see Chapters 13, 18, 19, 20. In
particular, Section 12.5 discusses how sophisticated joint probability models with
parameters ω can be coupled with neural networks that compute ω as a function
of inputs x, yielding structured output models conditioned with deep learning.

6.3.3

Cost Functions For Neural Networks

Typically, the training criteria for neural networks are primarily based on maximum likelihood. In the case of supervised learning, this will be the conditional
version of maximum likelihood when we perform supervised learning.
In addition to the negative log-likelihood cost, we also often add some sort
of a regularization term. Many of the regularization terms that apply to linear
models also apply to neural networks. For example, weight decay applies to neural
networks as well as to linear models.
These ideas have all been described for machine learning models in general
in Chapter 5. When designing cost functions for neural networks speciﬁcally, we
can often specialize the regularization terms for neural networks in various ways,
such as controlling the properties of the individual hidden units. These strategies
are covered in Chapter 7.
In practice, a good choice for the criterion is maximum likelihood regularized
with dropout, possibly also with weight decay.

6.3.4

Optimization Procedure

Previously, we have seen simple machine learning models which could sometimes
be ﬁt in closed form. Neural networks must essentially always be optimized with
iterative procedures. Optimization of neural networks is so diﬃcult that the choice
173

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

of optimization procedure is often tightly intertwined with the choice of model.
In other words, we often design the model to make optimization easier. Chapter 8
is devoted to the iterative optimization procedures used to train neural networks
and other deep models, including optimization strategies that involve designing
the model to be easier to optimize.
In practice, a good choice for the optimization algorithm for a feedforward
network is usually stochastic gradient descent with momentum. Typically, to
make the model easier to optimize, it is best to use piecewise linear hidden units.

6.4

Flow Graphs and Back-Propagation

The term back-propagation is often misunderstood as meaning the whole learning
algorithm for multi-layer neural networks. Actually it just means the method for
computing gradients in such networks. Furthermore, it is generally understood as
something very speciﬁc to multi-layer neural networks, but once its derivation is
understood, it can easily be generalized to arbitrary functions (for which computing a gradient is meaningful), and we describe this generalization here, focusing
on the case of interest in machine learning where the output of the function to
diﬀerentiate (e.g., the training criterion J) is a scalar and we are interested in
its derivative with respect to a set of parameters (considered to be the elements
of a vector θ), or equivalently, a set of inputs 7. The partial derivative of J with
respect to θ (called the gradient) tells us whether θ should be increased or decreased in order to decrease J, and is a crucial tool in optimizing the training
objective. It can be readily proven that the back-propagation algorithm for computing gradients has optimal computational complexity in the sense that there is
no algorithm that can compute the gradient faster (in the O(·) sense, i.e., up to
an additive and multiplicative constant).
The basic idea of the back-propagation algorithm is that the partial derivative
of the cost J with respect to parameters θ can be decomposed recursively by taking
into consideration the composition of functions that relate θ to J, via intermediate
quantities that mediate that inﬂuence, e.g., the activations of hidden units in a
deep neural network.

6.4.1

Chain Rule

The basic mathematical tool for considering derivatives through compositions of
functions is the chain rule, illustrated in Figure 6.4. The partial derivative ∂y
∂x
measures the locally linear inﬂuence of a variable x on another one y, while we
7

It is useful to know which inputs contributed most to the output or error made, and the sign
of the derivative is also interesting in that context.
174

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

denote ∇ θJ for the gradient vector of a scalar J with respect to some vector of
variables θ. If x inﬂuences y which inﬂuences z, we are interested in how a tiny
change in x propagates into a tiny change in z via a tiny change in y. In our case
of interest, the “output” is the cost, or objective function z = J(g(θ)), we want
the gradient with respect to some parameters x = θ, and there are intermediate
quantities y = g(θ) such as neural net activations. The gradient of interest can
then be decomposed, according to the chain rule, into
∂g(θ)
∇ θJ(g(θ)) = ∇g(θ) J(g(θ))
∂θ

(6.12)

which works also when J, g or θ are vectors rather than scalars (in which case
the corresponding partial derivatives are understood as Jacobian matrices of the
appropriate dimensions). In the purely scalar case we can understand the chain
rule as follows: a small change in θ will propagate into a small change in g(θ) by
∂g(θ)
getting multiplied by ∂θ . Similarly, a small change in g(θ) will propagate into
a small change in J(g(θ)) by getting multiplied by ∇ g(θ)J(g(θ)). Hence a small
change in θ ﬁrst gets multiplied by ∂g(θ)
to obtain the change in g(θ) and this
∂θ
then gets multiplied by ∇g(θ) J(g(θ)) to obtain the change in J(g(θ)). Hence the
ratio of the change in J(g(θ)) to the change in θ is the product of these partial
derivatives.

Figure 6.4: The chain rule, illustrated in the simplest possible case, with z a scalar
function of y, which is itself a scalar function of x. A small change ∆x in x gets turned
into a small change ∆y in y through the partial derivative ∂y
∂x , from the ﬁrst-order Taylor
approximation of y(x), and similarly for z(y). Plugging the equation for ∆y into the
equation for ∆z yields the chain rule.

Now, if g is a vector, we can rewrite the above as follows:
∇ θJ(g(θ)) =

X ∂J(g(θ)) ∂gi (θ)
i

∂gi (θ)

∂θ

which sums over the inﬂuences of θ on J(g(θ)) through all the intermediate
variables g i(θ). This is illustrated in Figure 6.5 with x = θ, y i = gi (θ), and
z = J(g(θ)).
175

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Figure 6.5: Top: The chain rule, when there are two intermediate variables y 1 and y2
between x and z, creating two paths for changes in x to propagate and yield changes in
z. Bottom: more general case, with n intermediate variables y1 to y n.

6.4.2

Back-Propagation in an MLP

Whereas example 6.1.1 illustrated the case of of an MLP with a single hidden
layer let us consider in this section back-propagation for an ordinary but deep
MLP, i.e., like the above vanilla MLP but with several hidden layers. For this
purpose, we will recursively apply the chain rule illustrated in Figure 6.5. The
algorithm proceeds by ﬁrst computing the gradient of the cost J with respect to
output units, and these are used to compute the gradient of J with respect to the
top hidden layer activations, which directly inﬂuence the outputs. We can then
continue computing the gradients of lower level hidden units one at a time in the
same way The gradients on hidden and output units can be used to compute the
gradient of J with respect to the parameters (e.g. weights and biases) of each
layer (i.e., that directly contribute to the output of that layer).
176

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Algorithm 6.1 Forward computation associated with input x for a deep neural
network with ordinary aﬃne layers composed with an arbitrary elementwise differentiable (almost everywhere) non-linearity f . There are M such layers, each
mapping their vector-valued input hk to a pre-activation vector a k via a weight
matrix W (k) which is then transformed via f into hk+1 . The input vector x corresponds to h 0 and the predicted outputs ŷ corresponds to h M . TODO: careful with
wording about loss functions here. Is it really important to say which things are
per-example and which are regularizer, etc? TODO: more broadly, is it necessary
to cover traditional nn-specialized backprop anymore or can we just do generic
symbolic diﬀ? nn-specialized backprop seems like a historical footnote now The
cost function L(ŷ, y) depends on the output ŷ and on a target y (see Section 6.3.2
for examples of loss functions). The loss may be added to a regularizer Ω (see
Section 6.3.3 and Chapter 7) to obtain the example-wise cost J. Algorithm 6.2
shows how to compute gradients of J with respect to parameters W and b. For
computational eﬃciency on modern computers (especially GPUs), it is important
TODO: the following line is the ﬁrst use of the word minibatch in the book. need
to introduce ahead of time. to implement these equations minibatch-wise, i.e.,
h(k) (and similary a(k) ) should really be a matrix whose second dimension is the
example index in the minibatch. Accordingly, y and ŷ should have an additional
dimension for the example index in the minibatch, while J remains a scalar, i.e.,
the average of the costs over all the minibatch examples.
h0 = x
for k = 1 . . . , M do
a (k) = b (k) + W (k)h(k−1)
h(k) = f(a(k) )
end for
ŷ = h(M )
J = L(ˆ
y, y) + λΩ

Algorithm 6.1 describes in matrix-vector form the forward propagation computation for a classical multi-layer network with M layers, where each layer computes an aﬃne transformation (deﬁned by a bias vector b(k) and a weight matrix
W (k) ) followed by a non-linearity f. In general, the non-linearity may be diﬀerent
on diﬀerent layers. Typically at least the output layer has a diﬀerent type than
(k)
the other layers (see Section 6.3.1). Each unit at layer k computes an output h i

177

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Algorithm 6.2 Backward computation for the deep neural network of Algorithm 6.1, which uses in addition to the input x a target y. This computation
yields the gradients on the activations a(k) for each layer k, starting from the
output layer and going backwards to the ﬁrst hidden layer. From these gradients, which can be interpreted as an indication of how each layer’s output should
change to reduce error, one can obtain the gradient on the parameters of each
layer. The gradients on weights and biases can be immediately used as part of a
stochastic gradient update (performing the update right after the gradients have
been computed) or used with other gradient-based optimization methods.
After the forward computation, compute the gradient on the output layer:
g ← ∇yˆJ = ∇ ŷ L(ŷ, y) + λ∇ ŷΩ
(typically Ω is only a function of parameters not activations, so the last term
would be zero)
for k = M down to 1 do
Convert the gradient on the layer’s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):
g ← ∇a (k) J = g  f0 (a(k) )
Compute gradients on weights and biases (including the regularization term,
where needed):
∇b (k)J = g + λ∇b (k) Ω
∇W (k) J = g h(k−1)> + λ∇W (k) Ω
Propagate the gradients w.r.t. the next lower-level hidden layer’s activations:
g ← ∇h (k−1) J = W (k)> g
end for
as follows:
(k)

ai

(k)

= bi +

X

(k) (k−1)

W ij hj

j

(k)
hi

= f(a(k) )v

(6.13)

where we separate the aﬃne transformation from the non-linear activation operations for ease of exposition of the back-propagation computations.
These are described in matrix-vector form by Algorithm 6.2 and proceed from
the output layer towards the ﬁrst hidden layer, as outlined above.

6.4.3

Back-Propagation in a General Flow Graph

In this section we call the intermediate quantities between inputs (parameters
θ) and output (cost J) of the graph nodes uj (indexed by j) and consider the
178

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

general case in which they form a directed acyclic graph that has J as its ﬁnal
node u N , that depends of all the other nodes uj. The back-propagation algorithm
∂J
exploits the chain rule for derivatives to compute ∂u
when ∂J
∂ui has already been
j
computed for successors u i of u j in the graph, e.g., the hidden units in the next
∂J
layer downstream. This recursion can be initialized by noting that ∂u
= ∂J
∂J = 1
N
and at each step only requires to use the partial derivatives associated with each
i
arc of the graph, ∂u
∂uj , when ui is a successor of uj .

uN
…
u i = fi (ai )

ai

fi

uj

…
u1

u2

Figure 6.6: Illustration of recursive forward computation, where at each node ui we
compute a value u i = fi (ai ), with a i being the list of values from parents u j of node
ui . Following Algorithm 6.3, the overall inputs to the graph are u 1 . . . , uM (e.g., the
parameters we may want to tune during training), and there is a single scalar output uN
(e.g., the loss which we want to minimize).

More generally than multi-layered networks, we can think about decomposing
a function J(θ) into a more complicated graph of computations. This graph is
called a ﬂow graph. Each node u i of the graph denotes a numerical quantity
that is obtained by performing a computation requiring the values uj of other
nodes, with j < i. The nodes satisfy a partial order which dictates in what order
the computation can proceed. In practical implementations of such functions
(e.g. with the criterion J(θ) or its value estimated on a minibatch), the ﬁnal
computation is obtained as the composition of simple functions taken from a
given set (such as the set of numerical operations that the numpy library can
perform on arrays of numbers).
We will deﬁne the back-propagation in a general ﬂow-graph, using the following generic notation: u i = fi (ai ), where ai is a list of arguments for the application
179

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Algorithm 6.3 Flow graph forward computation. Each node computes numerical
value ui by applying a function fi to its argument list ai that comprises the values
of previous nodes uj , j < i, with j ∈ parents(i). The input to the ﬂow graph is
the vector x, and is set into the ﬁrst M nodes u1 to uM . The output of the ﬂow
graph is read oﬀ the last (output) node u N .
for i = 1 . . . , M do
ui ← xi
end for
for i = M + 1 . . . , N do
ai ← (u j )j∈parents(i)
ui ← fi (ai )
end for
return uN

u3

u2
u1
Figure 6.7: Illustration of indirect eﬀect and direct eﬀect of variable u1 on variable u 3 in a
ﬂow graph, which means that the derivative of u3 with respect to u1 must include the sum
of two terms, one for the direct eﬀect (derivative of u 3 with respect to its ﬁrst argument)
and one for the indirect eﬀect through u2 (involving the product of the derivative of u 3
with respect to u2 times the derivative of u2 with respect to u1 ). Forward computation of
ui ’s (as in Figure 6.6) is indicated with upward full arrows, while backward computation
(of derivatives with respect to ui ’s, as in Figure 6.8) is indicated with downward dashed
arrows.

180

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

of fi to the values uj for the parents of i in the graph: ai = (u j )j∈parents(i) . This
is illustrated in Figure 6.6.
The overall computation of the function represented by the ﬂow graph can
thus be summarized by the forward computation algorithm, Algorithm 6.3.
TODO: notation is screwed up here. Is ai a vector? Should be a(i) or something like that. If ai is an element of a vector should be ai . In addition to having
some code that tells us how to compute fi (a i) for some values in the vector ai , we
∂fi (ai )
also need some code that tells us how to compute its partial derivatives, ∂a
ik
with respect to any immediate argument aik . Let k = π(i, j) denote the index of
uj in the list ai . Note that u j could inﬂuence u i through multiple paths. Whereas
∂u i
i (ai )
would denote the total gradient adding up all of these inﬂuences, ∂f∂a
only
∂u j
ik
denotes the derivative of fi with respect to its speciﬁc k-th argument, keeping the
other arguments ﬁxed, i.e., only considering the inﬂuence through the arc from uj
to ui . TODO: this next sentence is ambiguous and extremely hard to read In general, when manipulating partial derivatives, one should keep clear in one’s mind
(and implementation) the notational and semantic distinction between a partial
derivative that includes all paths and one that includes only the immediate eﬀect
of a function’s argument on the function output, with the other arguments considered ﬁxed. For example consider f3 (a3,1 , a3,2) = ea 3,1+a3,2 and f 2(a2,1) = a22,1 ,
while u3 = f 3 (u2 , u1 ) and u2 = f2 (u1 ), illustrated in Figure 6.7. The direct deriva∂f3
tive of f3 with respect to its argument a3,2 is ∂a
= ea3,1 +a 3,2 while if we consider
3,2
the variables u3 and u1 to which these correspond, there are two paths from u 1
to u 3 , and we obtain as derivative the sum of partial derivatives over these two
u1 +u2 (1 + 2u ). The results are diﬀerent because ∂u 3 involves not
3
paths, ∂u
1
∂u1 = e
∂u 1
just the direct dependency of u3 on u 1 but also the indirect dependency through
u2 .
Armed with this understanding, we can deﬁne the back-propagation algorithm
as follows, in Algorithm 6.4, which would be computed after the forward propagation (Algorithm 6.3) has been performed. Note the recursive nature of the
application of the chain rule, in Algorithm 6.4: we compute the gradient on node
j by re-using the already computed gradient for children nodes i, starting the
N = 1 that sets the gradient for the output node.
recurrence from the trivial ∂u
∂uN
This is illustrated in Figure 6.8.
This recursion is a form of eﬃcient factorization of the total gradient, i.e., it is
an application of the principles of dynamic programming 8. Indeed, the derivative
of the output node with respect to any node can also be written down in this
8

Here we refer to “dynamic programming” in the sense of table-ﬁlling algorithms that avoid
re-computing frequently used subexpressions. In the context of machine learning, “dynamic
programming” can also refer to iterating Bellman’s equations. That is not the kind of dynamic
programming we refer to here.
181

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

∂u N
=1
∂u N

…
∂u N
∂u i

∂uN
∂uj

…

Figure 6.8: Illustration of recursive backward computation, where we associate to each
node j not just the values uj computed in the forward pass (Figure 6.6, bold upward
N
arrows) but also the gradient ∂u
∂u j with respect to the output scalar node u N . These
gradients are recursively computed in exactly the opposite order, as described in AlgoN
rithm 6.4 by using the already computed ∂u
∂u i of the children i of j (dashed downward
arrows).

intractable form:
∂u N
=
∂u i

X

paths uk 1 ...,uk n: k1 =i,kn =N

n
Y
∂u kj
∂u kj−1
j=2

where the paths uk 1 . . . , ukn go from the node k1 = i to the ﬁnal node kn = N
in the ﬂow graph and

∂uk j
∂uk j−1

refers only to the immediate derivative considering

uk j−1 as the argument number π(k j , k j−1) of akj into u kj , i.e.,
∂u kj
∂f kj (akj )
=
.
∂u kj−1
∂akj ,π(k j ,kj−1 )
Computing the sum as above would be intractable because the number of possible
paths can be exponential in the depth of the graph. The back-propagation algorithm is eﬃcient because it employs a dynamic programming strategy to reuse
rather than re-compute partial sums associated with the gradients on intermediate
nodes.
Although the above was stated as if the ui’s were scalars, exactly the same
procedure can be run with u i’s being tuples of numbers (more easily represented
182

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

Algorithm 6.4 Back-propagation computation of a ﬂow graph (full, upward arrows, Figs.6.8 and 6.6), which itself produces an additional ﬂow graph (dashed,
backward arrows). See the forward propagation in a ﬂow-graph (Algorithm 6.3,
to be performed ﬁrst) and the required data structure. In addition, a quantity
∂u N
∂ui needs to be stored (and computed) at each node, for the purpose of gradient
back-propagation. Below, the notation π(i, j) is the index of uj as an argument to
∂u
fi . The back-propagation algorithm eﬃciently computes ∂uNi for all i’s (traversing the graph backwards this time), and in particular we are interested in the
derivatives of the output node u N with respect to the “inputs” u 1 . . . , u M (which
could be the parameters, in a learning setup). The cost of the overall computation is proportional to the number of arcs in the graph, assuming that the partial
derivative associated with each arc requires a constant time. This is of the same
order as the number of computations for the forward propagation.
∂u N
∂u N

←1
for j = N − 1 down to 1 do
P
∂u N
∂uN ∂fi(ai )
i:j∈parents(i) ∂u i ∂a i,π(i,j)
∂uj ←
end for
M
∂u
N
return ∂u
i

i=1

by vectors). The same equations remain valid. Earlier, we wrote these equations using scalar multiplication of scalar partial derivatives. Using vectors, these
multiplications turn into matrix-vector products. We multiply the row vector
N
of gradients ∂u
∂ui by a Jacobian matrix of partial derivatives associated with the

∂fi (ai )
j → i arc of the graph, ∂a
.
i,π(i,j)
The quintessential neural network training scenario is the case where each
U (i) is a matrix containing m examples in a minibatch and n hidden unit activation values. In this case, both forward propagation and back-propagation can be
expressed as a product between a matrix of activations or gradients and a matrix of weights. From a computational point of view, this is much more eﬃcient
than training on a single example at a time. When we do not use the minibatch
version of the algorithm, U(i) is only a vector. The main operation used in forward and back-propagation is then a matrix-vector product, between the weight
matrix and the vector of activations or gradients. Matrix-matrix products are
typically implemented with a high degree of parallel computation (e.g. in BLAS
library implementations) which is essential for obtaining good performance on
modern multicore CPUs and GPUs. Matrix-matrix products for minibatch forward and back-propagation allow parallelization across both examples and units,
while matrix-vector products for the processing of a single example allow only

183

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

parallelization across units.

6.4.4

Symbolic Back-propagation and Automatic Diﬀerentiation

The algorithm for generalized back-propagation (Alg. 6.4) was presented with the
interpretation that actual computations take place at each step of the algorithm.
This generalized form of back-propagation is just a particular way to perform
automatic diﬀerentiation (Rall, 1981) in computational ﬂow graphs deﬁned by
Algorithm 6.3. Automatic diﬀerentiation automatically obtains derivatives of a
given expression and has numerous uses in machine learning (Baydin et al., 2015).
As an alternative (and often as a debugging tool) derivatives could be obtained
by numerical methods based on measuring the eﬀects of small changes, called
numerical diﬀerentiation (Lyness and Moler, 1967). For example, a ﬁnite diﬀerence approximation of the gradient follows from the deﬁnition of derivative as a
ratio of the change in output that results in a change in input, divided by the
change in input. Methods based on random perturbations also exist which randomly jiggle all the input variables (e.g. parameters) and associate these random
input changes with the resulting overall change in the output variable in order to
estimate the gradient (Spall, 1992). However, for obtaining a gradient (i.e., with
respect to many variables, e.g., parameters of a neural network), back-propagation
has two advantages over numerical diﬀerentiation: (1) it performs exact computation (up to machine precision), and (2) it is computationally much more eﬃcient,
obtaining all the required derivatives in one go. Instead, numerical diﬀerentiation
methods either require to redo the forward propagation separately for each parameter (keeping the other ones ﬁxed) or they yield stochastic estimators (from
a random perturbation of all parameters) whose variances grows linearly with
the number of parameters. Automatic diﬀerentiation of a function with d inputs
and m outputs can be done either by carrying derivatives forward, or carrying
them backwards. The former is more eﬃcient when d < m and the latter is more
eﬃcient when d > m. In our use case, the output is a scalar (the cost), and the
backwards approach, called reverse accumulation, i.e., back-propagation, is much
more eﬃcient than the approach of propagating derivatives forward in the graph.
Although Algorithm 6.4 can be seen as a form of automatic diﬀerentiation, it
has another interpretation: each step symbolically speciﬁes how gradient computation could be done, given a symbolic speciﬁcation of the function to diﬀerentiate, i.e., it can be used to perform symbolic diﬀerentiation. Whereas automatic
diﬀerentiation manipulates and outputs numbers, given a symbolic expression
(the program specifying the function to be computed and diﬀerentiated), symbolic diﬀerentiation manipulates and outputs symbolic expressions, i.e., pieces of
program, producing a symbolic expression for computing the derivatives. The
184

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

popular Torch library 9 for deep learning, as well as most other open source deep
learning libraries are a limited form of doing automatic diﬀerentiation restricted
to the “programs” obtained by composing a predeﬁned set of operations, each corresponding to a “module”. The set of these modules is designed such that many
neural network architectures and computations can be performed by composing
the building blocks represented by each of these modules. Each module is deﬁned
by two main functions, (1) one that computes the outputs y of the module given
its inputs x, e.g., with an “fprop” function
y = module.fprop(x),
and (2), one that computes the gradient ∂J
∂x of a scalar (typically the minibatch
cost J) with respect to the inputs x, given the gradient ∂J
with respect to the
∂y
outputs, e.g., with a “bprop” function
∇x J = module.bprop (∇ y J) .
The bprop function thus implicitly knows the Jacobian of the x to y mapping,
∂y
, at x. Speciﬁcally, the bprop function speciﬁes how to multiply this Jacobian
∂x
by a vector passed to the function as an argument. During execution of the
back-propagation algorithm, bprop will be called with this argument set to the
gradient on the output, ∇ yJ. When called in this manner, bprop computes
∇ xJ =



∂y
∂x

>

∇yJ.

In practice, implementations work in parallel over a whole minibatch (transforming matrix-vector operations into matrix-matrix operations) and may operate on
objects which are not vectors (maybe higher-order tensors like those involved with
images or sequences of vectors). Furthermore, the bprop function does not have
to explicitly compute the Jacobian matrix ∂y
and perform an actual matrix mul∂x
tiplication: it can do that matrix multiplication implicitly, which is often more
eﬃcient. For example, if the true Jacobian is diagonal, then the actual number
of computations required is much less than the size of the Jacobian matrix.
To keep computations eﬃcient and avoid the overhead of the glue required to
compose modules together, neural net packages such as Torch deﬁne modules that
perform coarse-grained operations such as the cross-entropy loss, a convolution,
the aﬃne operation associated with a fully-connected neural network layer, or a
softmax. It means that if one wants to write diﬀerentiable code for some computation that is not covered by the existing set of modules, one has to write their
own code for a new module, providing both the code for fprop and the code for
9

See torch.ch.
185

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

bprop. This is in contrast with standard automatic diﬀerentiation systems, which
know how to compute derivatives through all the operations in a general-purpose
programming language such as C.
Instead interpreting of Algorithm 6.4 as a recipe for backwards automatic differentation, it can be interpreted as a recipe for backwards symbolic diﬀerentation.
This is how the Theano (Bergstra et al., 2010b; Bastien et al., 2012) library10 handles derivatives. Like Torch, it only covers a predeﬁned set of operations (i.e., a
language that is a subset of usual programming languages), but it is a much larger
and ﬁne-grained set of operations, covering most of the operations on tensors and
linear algebra deﬁned in Python’s numpy library of numerical computation. It
is thus very rare that a user would need to write a new module for Theano, except if they want to provide an alternative implementation (say, more eﬃcient or
numerically stable in some cases). An immediate advantage of symbolic diﬀerentiation is that because it maps symbolic expressions to symbolic expressions, it can
be applied multiple times and yield high-order derivatives. Another immediate
advantage is that it can take advantage of the other tools of symbolic computation (Buchberger et al., 1983), such as simpliﬁcation (to make computation faster
and more memory-eﬃcient) and transformations that make the computation more
numerically stable (Bergstra et al., 2010b). These simpliﬁcation operations make
it still very eﬃcient in terms of computation and memory usage even with a set
of ﬁne-grained operations such as individual tensor additions and multiplications.
Theano also provides a compiler of the resulting expressions into C for CPUs and
GPUs, i.e., the same high-level expression can be implemented in diﬀerent ways
depending of the underlying hardware.

6.4.5

Back-propagation Through Random Operations and Graphical Models

Whereas traditional neural networks perform deterministic computation, they
can be extended to perform stochastic computation. In this case, we can think of
the network as deﬁning a sampling process that deterministically transforms some
random values. We can then apply backpropagation as usual, with the underlying
random values as inputs to the network.
As an example, let us consider the operation consisting of drawing samples z
from a Gaussian distribution with mean µ and variance σ 2:
z ∼ N (µ, σ2 ).
Because an individual sample of z is not produced by a function, but rather by
a sampling process whose output changes every time we query it, it may seem
10

See http://deeplearning.net/software/theano/.
186

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

counterintuitive to take the derivatives of z with respect to the parameters of
its distribution, µ and σ2 . However, we can rewrite the sampling process as
transforming an underlying random value eta ∼ N (0, 1) to obtain a sample from
the desired distribution:
z = µ + ση
(6.14)
We are now able to backpropagate through the sampling operation, by regarding it as a deterministic operation with an extra input. Crucially, the extra input
is a random variable whose distribution is not a function of any of the variables
whose derivatives we want to calculate. The result tells us how an inﬁnitesimal change in µ or σ would change the output if we could repeat the sampling
operation again with the same value of η.
Being able to backpropagate through this sampling operation allows us to
incorporate it into a larger graph; e.g. we can compute the derivatives of some loss
function J(z). Moreover, we can introduce functions that shape the distribution,
e.g. µ = f (x; θ) and σ = g(x; θ) and use back-propagation through this functions
to derive ∇ θJ(z).
The principal used in this Gaussian sampling example is true in general, i.e.,
given a value z sampled from distribution p(z | ω) whose parameters ω may
depend on other quantities of interest, we can rewrite
z ∼ p(z | ω)
as
z = f(ω, η)
where η is a source of randomness that is independent of any of the variables that
inﬂuence ω. TODO– add discussion of discrete random variables, REINFORCE.
it is true that you can express the generation process this way for any variable,
but for discrete variables it can be pointless since the gradient of the thresholding
operation is zero or undeﬁned everywhere. in that case you can fall back to
REINFORCE and the expected loss
In neural network applications, we typically choose η to be drawn from some
simple distribution, such as a unit uniform or unit Gaussian distribution, and
achieve more complex distributions by allowing the deterministic portion of the
network to reshape its input. This is actually how the random generators for
parametric distributions are implemented in software, i.e., by performing operations on approximately independent sources of noise (such as random bits). So
long as the function f in the above equation is diﬀerentiable with respect to ω,
we can back-propagate through the sampling operation.
The idea of propagating gradients or optimizing through stochastic operations
is old (Price, 1958; Bonnet, 1964), ﬁrst used for machine learning in the context
187

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

of reinforcement learning (Williams, 1992), variational approximations (Opper
and Archambeau, 2009), and more recently, stochastic or generative neural networks (Bengio et al., 2013a; Kingma, 2013; Kingma and Welling, 2014b,a; Rezende
et al., 2014; Goodfellow et al., 2014c). Many networks, such as denoising autoencoders or networks regularized with dropout, are also naturally designed to take
noise as an input without requiring any special reparameterization to make the
noise independent from the model.

6.5

Universal Approximation Properties and Depth

A linear model, mapping from features to outputs via matrix multiplication, can
by deﬁnition represent only linear functions. It has the advantage of being easy
to train because many loss functions result in a convex optimization problem
when applied to linear models. Unfortunately, we often want to learn non-linear
functions.
At ﬁrst glance, we might presume that learning a non-linear function requires
designing a specialized model family for the kind of non-linearity we want to
learn. However, it turns out that feedforward networks with hidden layers provide
a universal approximation framework. Speciﬁcally, the universal approximation
theorem (Hornik et al., 1989; Cybenko, 1989) states that a feedforward network
with a linear output layer and at least one hidden layer with any “squashing”
activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one ﬁnite-dimensional space to another
with any desired non-zero amount of error, provided that the network is given
enough hidden units. The derivatives of the feedforward network can also approximate the derivatives of the function arbitrarily well (Hornik et al., 1990). The
concept of Borel measurability is beyond the scope of this book; for our purposes
it suﬃces to say that any continuous function on a closed and bounded subset of
Rn is Borel measurable and therefore may be approximated by a neural network.
A neural network may also approximate any function mapping from any ﬁnite
dimensional discrete space to another.
The universal approximation theorem means that regardless of what function
we are trying to learn, we know that a large MLP will be able to represent this
function. However, we are not guaranteed that the training algorithm will be
able to learn that function. Even if the MLP is able to represent the function,
learning can fail for two diﬀerent reasons. First, the optimization algorithm used
for training may not be able to ﬁnd the value of the parameters that corresponds
to the desired function. Second, the training algorithm might choose the wrong
function due to overﬁtting. Recall from Chapter 5.3.1 that the “no free lunch”
theorem shows that there is no universal machine learning algorithm. Even though
188

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

feedforward networks provide a universal system for representing functions, there
is no universal procedure for examining a training set and choosing the right set
of functions among the family of functions our approximator can represent: there
could be many functions within our family that ﬁt well the data and we need to
choose one (this is basically the overﬁtting scenario).
Another but related problem facing our universal approximation scheme is
the size of the model needed to represent a given function. The universal approximation theorem says that there exists a network large enough to achieve any
degree of accuracy we desire, but it does not say how large this network will be.
Barron (1993) provides some bounds on the size of a single-layer network needed
to approximate a broad class of functions. Unfortunately, in the worse case, an
exponential number of hidden units (to basically record every input conﬁguration
that needs to be distinguished) may be required. This is easiest to see in the
binary case: the number of possible binary functions on vectors v ∈ {0, 1} n is
n
22 and selecting one such function requires 2 n bits, which will in general require
O(2n ) degrees of freedom.
In summary, a feedforward network with a single layer is suﬃcient to represent
any function, but it may be infeasibly large and may fail to learn and generalize
correctly. Both of these failure modes suggest that we may want to use deeper
models.
First, we may want to choose a model with more than one hidden layer in
order to avoid needing to make the model infeasibly large. There exist families
of functions which can be approximated eﬃciently by an architecture with depth
greater than some value d, but require a much larger model if depth is restricted
to be less than or equal to d. In many cases, the number of hidden units required
by the shallow model is exponential in n. Such results have been proven for logic
gates (Håstad, 1986), linear threshold units with non-negative weights (Håstad
and Goldmann, 1991), polynomials (Delalleau and Bengio, 2011) organized as
deep sum-product networks (Poon and Domingos, 2011), and more recently, for
deep networks of rectiﬁer units (Pascanu et al., 2013b). Of course, there is no
guarantee that the kinds of functions we want to learn in applications of machine
learning (and in particular for AI) share such a property.
We may also want to choose a deep model for statistical reasons.
Any time we choose a speciﬁc machine learning algorithm, we are implicitly
stating some set of prior beliefs we have about what kind of function the algorithm
should learn. Choosing a deep model encodes a very general belief that the
function we want to learn should involve composition of several simpler functions.
This can be interpreted from a representation learning point of view as saying that
we believe the learning problem consists of discovering a set of underlying factors
of variation that can in turn be described in terms of other, simpler underlying
189

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

factors of variation. Alternately, we can interpret the use of a deep architecture
as expressing a belief that the function we want to learn is a computer program
consisting of multiple steps, where each step makes use of the previous step’s
output. These intermediate outputs are not necessarily factors of variation, but
can instead be analogous to counters or pointers that the network uses to organize
its internal processing. Empirically, greater depth does seem to result in better
generalization for a wide variety of tasks (Bengio et al., 2007b; Erhan et al., 2009;
Bengio, 2009; Mesnil et al., 2011; Goodfellow et al., 2011; Ciresan et al., 2012;
Krizhevsky et al., 2012b; Sermanet et al., 2013; Farabet et al., 2013a; Couprie
et al., 2013; Kahou et al., 2013; Goodfellow et al., 2014d; Szegedy et al., 2014a).
See Fig. 6.9 for an example of some of these empirical results. This suggests
that using deep architectures does indeed express a useful prior over the space of
functions the model learn.

Figure 6.9: Empirical results showing that deeper networks generalize better when used
to transcribe multi-digit numbers from photographs of addresses. Reproduced with permission from Goodfellow et al. (2014d). Left) The test set accuracy consistently increases
with increasing depth. Right) This eﬀect cannot be explained simply by the model being
larger; one can also increase the model size by increasing the width of each layer. The test
accuracy cannot be increased nearly as well by increasing the width, only by increasing
the depth. This suggests that using a deep model expresses a useful preference over the
space of functions the model can learn. Speciﬁcally, it expresses a belief that the function
should consist of many simpler functions composed together. This could result either
in learning a representation that is composed in turn of simpler representations (e.g.,
corners deﬁned in terms of edges) or in learning a program with sequentially dependent
steps (e.g., ﬁrst locate a set of objects, then segment them from each other, then recognize
them).

190

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

6.6

Feature / Representation Learning

Let us consider again the single layer networks such as the perceptron, linear
regression and logistic regression: such linear models are appealing because training them involves a convex optimization problem11 , i.e., an optimization problem
with some convergence guarantees towards a global optimum, irrespective of initial conditions. Simple and well-understood optimization algorithms are available
in this case. However, this limits the representational capacity too much: many
tasks, for a given choice of input representation x (the raw input features), cannot
be solved by using only a linear predictor. What are our options to avoid that
limitation?
1. One option is to use a kernel machine (Williams and Rasmussen, 1996;
Schölkopf et al., 1999), i.e., to consider a ﬁxed mapping from x to φ(x),
where φ(x) is of much higher dimension. In this case, fθ (x) = b + w · φ(x)
can be linear in the parameters (and in φ(x)) and optimization remains
convex (or even analytic). By exploiting the kernel trick, we can computationally handle a high-dimensional φ(x) (or even an inﬁnite-dimensional
one) so long as the kernel k(u, v) = φ(u) · φ(v) (where · is the appropriate
dot product for the space of φ(·)) can be computed eﬃciently. If φ(x) is
of high enough dimension, we can always have enough capacity to ﬁt the
training set, but generalization is not at all guaranteed: it will depend on
the appropriateness of the choice of φ as a feature space for our task. Kernel
machines theory clearly identiﬁes the choice of φ to the choice of a prior.
This leads to kernel engineering, which is equivalent to feature engineering, discussed next. The other type of kernel (that is very commonly used)
embodies a very broad prior,
e.g., the Gaussian (or
 such as smoothness,

2
RBF) kernel k(u, v) = exp −||u − v||/σ . Unfortunately, this prior may
be insuﬃcient, i.e., too broad and sensitive to the curse of dimensionality,
as introduced in Section 5.12.1 and developed in more detail in Chapter 16.
2. Another option is to manually engineer the representation or features φ(x).
Most industrial applications of machine learning rely on hand-crafted features and most of the research and development eﬀort (as well as a very
large fraction of the scientiﬁc literature in machine learning and its applications) goes into designing new features that are most appropriate to the task
at hand. Clearly, faced with a problem to solve and some prior knowledge
in the form of representations that are believed to be relevant, the prior
knowledge can be very useful. This approach is therefore common in practice, but is not completely satisfying because it involves a very task-speciﬁc
11

or even one for which an analytic solution can be computed, with linear regression or the
case of some Gaussian process regression models
191

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

engineering work and a laborious never-ending eﬀort to improve systems by
designing better features. If there were some more general feature learning
approaches that could be applied to a large set of related tasks (such as
those involved in AI), we would certainly like to take advantage of them.
Since humans seem to be able to learn a lot of new tasks (for which they
were not programmed by evolution), it seems that such broad priors do exist. This whole question is discussed in more detail in Bengio and LeCun
(2007a), and motivates the third option.
3. The third option is to learn the features, or learn the representation. In a
sense, it allows one to interpolate between the almost agnostic approach
of a kernel machine with a general-purpose smoothness kernel (such as
RBF SVMs and other non-parametric statistical models) and full designerprovided knowledge in the form of a ﬁxed representation that is perfectly
tailored to the task. This is equivalent to the idea of learning the kernel, except that whereas most kernel learning methods only allow very few degrees
of freedom in the learned kernel, representation learning methods such as
those discussed in this book (including multi-layer neural networks) allow
the feature function φ(·) to be very rich (with a number of parameters that
can be in the millions or more, depending on the amount of data available).
This is equivalent to learning the hidden layers, in the case of a multi-layer
neural network. Besides smoothness (which comes for example from regularizers such as weight decay), other priors can be incorporated in this
feature learning. The most celebrated of these priors is depth, discussed
above (Section 6.5). Other priors are discussed in Chapter 16.
This whole discussion is clearly not speciﬁc to neural networks and supervised
learning, and is one of the central motivations for this book.

6.7

Piecewise Linear Hidden Units

Most of the recent improvement in the performance of deep neural networks can be
attributed to increases in computational power and the size of datasets. The machine learning algorithms involved in recent state-of-the-art systems have mostly
existed since the 1980s, with a few recent conceptual advances contributing signiﬁcantly to increased performance.
One of the main algorithmic improvements that has had a signiﬁcant impact
is the use of piecewise linear units, such as absolute value rectiﬁers and rectiﬁed
linear units. Such units consist of two linear pieces and their behavior is driven
by a single weight vector. Jarrett et al. (2009b) observed that “using a rectifying
non-linearity is the single most important factor in improving the performance of a
192

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

recognition system” among several diﬀerent factors of neural network architecture
design.
For small datasets, Jarrett et al. (2009b) observed that using rectifying nonlinearities is even more important than learning the weights of the hidden layers.
Random weights are suﬃcient to propagate useful information through a rectiﬁed
linear network, allowing the classiﬁer layer at the top to learn how to map diﬀerent
feature vectors to class identities.
When more data is available, learning begins to extract enough useful knowledge to exceed the performance of randomly chosen parameters. Glorot et al.
(2011b) showed that learning is far easier in deep rectiﬁed linear networks than
in deep networks that have curvature or two-sided saturation in their activation
functions. Because the behavior of the unit is linear over half of its domain, it
is easy for an optimization algorithm to tell how to improve the behavior of a
unit, even when the unit’s activations are far from optimal. Just as piecewise
linear networks are good at propagating information forward, back-propagation
in such a network is also piecewise linear and propagates information about the
error derivatives to all of the gradients in the network. Each piecewise linear
function can be decomposed into diﬀerent regions corresponding to diﬀerent linear pieces. When we change a parameter of the network, the resulting change
in the network’s activity is linear until the point that it causes some unit to go
from one linear piece to another. Traditional units such as sigmoids are more
prone to discarding information due to saturation both in forward propagation
and in back-propagation. The response of such a network to a change in a single
parameter may be highly nonlinear even in a small neighborhood.
Glorot et al. (2011b) motivate rectiﬁed linear units from biological considerations. The half-rectifying non-linearity was intended to capture these properties
of biological neurons: 1) For some inputs, biological neurons are completely inactive. 2) For some inputs, a biological neuron’s output is proportional to its
input. 3) Most of the time, biological neurons operate in the regime where they
are inactive (e.g., they should have sparse activations).
One drawback to rectiﬁed linear units is that they cannot learn via gradientbased methods on examples for which their activation is zero. This problem
can be mitigated by initializing the biases to a small positive number, but it is
still possible for a rectiﬁed linear unit to learn to de-activate and then never be
activated again. Goodfellow et al. (2013a) introduced maxout units and showed
that maxout units can successfully learn in conditions where rectiﬁed linear units
become stuck. Maxout units are also piecewise linear, but unlike rectiﬁed linear
units, each piece of the linear function has its own weight vector, so whichever
piece is active can always learn. Due to the greater number of weight vectors,
maxout units typically need extra regularization such as dropout, though they can
193

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

work satisfactorily if the training set is large and the number of pieces per unit
is kept low (Cai et al., 2013). Maxout units have a few other beneﬁts. In some
cases, one can gain some statistical and computational advantages by requiring
fewer parameters. Speciﬁcally, if the features captured by n diﬀerent linear ﬁlters
can be summarized without losing information by taking the max over each group
of k features, then the next layer can get by with k times fewer weights. Because
each unit is driven by multiple ﬁlters, maxout units have some redundancy that
helps them to resist forgetting how to perform tasks that they were trained on in
the past. Neural networks trained with stochastic gradient descent are generally
believed to suﬀer from a phenomenon called catastrophic forgetting but maxout
units tend to exhibit only mild forgetting (Goodfellow et al., 2014a). Maxout
units can also be seen as learning the activation function itself rather than just
the relationship between units. With large enough k, a maxout unit can learn to
approximate any convex function with arbitrary ﬁdelity. In particular, maxout
with two pieces can learn to implement the rectiﬁed linear activation function or
the absolute value rectiﬁcation function.
This same general principle of using linear behavior to obtain easier optimization also applies in other contexts besides deep linear networks. Recurrent
networks can learn from sequences and produce a sequence of states and outputs.
When training them, one needs to propagate information through several time
steps, which is much easier when some linear computations (with some directional
derivatives being of magnitude near 1) are involved. One of the best-performing
recurrent network architectures, the LSTM, propagates information through time
via summation–a particular straightforward kind of such linear activation. This
is discussed further in Section 10.8.4.
In addition to helping to propagate information and making optimization
easier, piecewise linear units also have some nice properties that can make them
easier to regularize. This is discussed further in Section 7.11.
Sigmoidal non-linearities still perform well in some contexts and are required
when a hidden unit must compute a number guaranteed to be in a bounded
interval (like in the (0,1) interval), but piecewise linear units are now by far the
most popular kind of hidden units.

6.8

Historical Notes

Section 1.2 already gave an overview of the history of neural networks and deep
learning. Here we focus on historical notes regarding back-propagation and the
connectionist ideas that are still at the heart of today’s research in deep learning.
The chain rule was invented in the 17th century (Leibniz, 1676; L’Hôpital,
1696) and gradient descent in the 19th centry (Cauchy, 1847b). Eﬃcient applica194

CHAPTER 6. FEEDFORWARD DEEP NETWORKS

tions of the chain rule which exploit the dynamic programming structure described
in this chapter are found already in the 1960’s and 1970’s, mostly for control applications (Kelley, 1960; Bryson and Denham, 1961; Dreyfus, 1962; Bryson and
Ho, 1969; Dreyfus, 1973) but also for sensitivity analysis (Linnainmaa, 1976).
Bringing these ideas to the optimization of weights of artiﬁcial neural networks
with continuous-valued outputs was introduced by Werbos (1981) and rediscovered independently in diﬀerent ways as well as actually simulated successfully
by LeCun (1985); Parker (1985); Rumelhart et al. (1986a). The book Parallel
Distributed Processing (Rumelhart et al., 1986d) presented these ﬁndings in a
chapter (Rumelhart et al., 1986b) that contributed greatly to the popularization
of back-propagation and initiated a very active period of research in multi-layer
neural networks. However, the ideas put forward by the authors of that book and
in particular by Rumelhart and Hinton go much beyond back-propagation. They
include crucial ideas about the possible computational implementation of several
central aspects of cognition and learning, which came under the name of “connectionism” because of the importance given the connections between neurons as
the locus of learning and memory. In particular, these ideas include the notion of
distributed representation, introduced in Chapter 1 and developed a lot more in
part III of this book, with Chapter 16, which is at the heart of the generalization
ability of neural networks. As discussed with the historical survey in Section 1.2,
the boom of AI and machine learning research which followed on the connectionist ideas reached a peak in the early 1990’s, as far as neural networks are
concerned, while other machine learning techniques become more popular in the
late 1990’s and remained so for the ﬁrst decade of this century. Neural networks
research in the AI and machine learning community almost vanished then, only
to be reborn ten years later (starting in 2006) with a novel focus on the depth of
representation and the current wave of research on deep learning. In addition to
back-propagation and distributed representations, the connectionists brought the
idea of iterative inference (they used diﬀerent words), viewing neural computation
in the brain as a way to look for a conﬁguration of neurons that best satisfy all
the relevant pieces of knowledge implicitly captured in the weights of the neural
network. This view turns out to be central in the topics covered in part III of
this book regarding probabilistic models and inference.

195

Chapter 7

Regularization of Deep or
Distributed Models
A central problem in machine learning is how to make an algorithm that will
perform well not just on the training data, but also on new inputs. Many strategies
used in machine learning are explicitly designed to reduce the test error, possibly
at the expense of increased training error. These strategies are known collectively
as regularization.
Chapter 5 introduced the basic concepts of generalization, underﬁtting, overﬁtting, bias, variance and regularization. If you are not already familiar with
these notions, please refer to that chapter before continuing with this one.
In this chapter, we describe regularization in more detail, focusing on general
strategies for regularizing deep or distributed models. Most deep learning algorithms can be seen as diﬀerent ways of developing the speciﬁcs of these general
strategies.
Regularization is any component of the model, training process or prediction
procedure which is included to account for limitations of the training data, including its ﬁniteness. There are many regularization strategies. Some put extra
constraints on a machine learning model, such as adding restrictions on the parameter values. Some add extra terms in the objective function that one might
consider a soft constraint on the parameter values. If chosen carefully, these extra
constraints and penalties can lead to improved performance on the test set. Sometimes these constraints and penalties are designed to encode speciﬁc kinds of prior
knowledge. Other times, these constraints and penalties are designed to express
a generic preference for a simpler model class in order to promote generalization.
Sometimes penalties and constraints are necessary to make an underdetermined
problem determined. Other forms of regularization, known as ensemble methods,
combine multiple hypotheses that explain the training data.
196

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

In the context of deep learning, most regularization strategies are based on
regularizing estimators. Regularization of an estimator works by trading increased
bias for reduced variance. An eﬀective regularizer is one that makes a proﬁtable
trade, that is it reduces variance signiﬁcantly while not overly increasing the
bias. When we discussed generalization and overﬁtting in Chapter 5, we focused
on three situations, where the model family being trained either (1) excluded the
true data generating process—corresponding to underﬁtting and inducing bias, or
(2) matched to the true data generating process—the “just right” model space,
or (3) includes the generating process but also many other possible generating
processes—the regime where variance dominates the estimation error (e.g. as
measured by the MSE—see Section. 5.5).
Note that, in practice, an overly complex model family does not necessarily
include (or even come close to) the target function or the true data generating
process. We almost never have access to the true data generating process so
we can never know if the model family being estimated includes the generating
process or not. But since, in deep learning, we are often trying to work with
data such as images, audio sequences and text, we can probably safely assume
that our model family does not include the data generating process. We can
assume that—to some extent – we are always trying to ﬁt a square peg (the data
generating process) into a round hole (our model family) and using the data to
do that as best we can.
What this means is that controlling the complexity of the model is not going
to be a simple question of ﬁnding the model of the right size, i.e. the right
number of parameters. Instead, we might ﬁnd—and indeed in practical deep
learning scenarios, we almost always do ﬁnd – that the best ﬁtting model (in the
sense of minimizing generalization error) is one that possesses a large number of
parameters that are not entirely free to span their domain.
As we will see there are a great many forms of regularization available to the
deep learning practitioner. In fact, developing more eﬀective regularizers has been
one of the major research eﬀorts in the ﬁeld.
Most machine learning tasks can be viewed in terms of learning to represent
a function fˆ(x) parametrized by a vector of parameters θ. The data consists of
inputs x(t) and (for some tasks) targets y(t) for t ∈ {1, . . . , m}. In the case of
classiﬁcation, each y (t) is an integer class label in {1, . . . , k}. For regression tasks,
each y (t) is a real number or a real-valued vector (and we then denote the target
in bold, as y (t) ), while in the case of a density estimation task, there are simply
no targets (or we consider both x(t) and y(t) as the values of observed variables
(x, x) whose joint distribution is to be captured, and we may simply lump both
of them into the name x). We may group these examples into a design matrix X
and a vector of targets y (when y (t) is a scalar), or a matrix of targets Y (when
197

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

y (t) is a vector).
In deep learning, we are mainly interested in the case where the function
fˆ(x) has a large number of parameters and as a result possesses a high capacity
to ﬁt relatively complicated functions. This means that deep learning algorithms
usually require either a large training set or careful regularization (intended either
to reduce the eﬀective capacity of the model or to guide the model toward a speciﬁc
solution using prior information) or both.

7.1

Regularization from a Bayesian Perspective

The Bayesian perspective on statistical inference oﬀers a useful framework in
which to consider many common methods of regularization. As we discussed in
Sec. 5.7, Bayesian estimation theory takes a fundamentally diﬀerent approach
to model estimation than the frequentist view by considering that the model
parameters themselves are uncertain and therefore should be considered random
variables.
There are a number of immediate consequences of assuming a Bayesian world
view. The ﬁrst is that if we are using probability distributions to assess uncertainty in the model parameters then we should be able to express our uncertainty
about the model parameters before we see any data 1 . This is the role of the prior
distribution. The second consequence comes out of the marginalization equation over probability functions: when using the model to make predictions about
outcomes, one should ideally average over over the probable parameter values,
or more precisely sum over all the possible values, weighted by their posterior
distribution.
There is a deep connection between the Bayesian perspective on estimation
and the process of regularization. This is not surprising since at the root both
are concerned with making predictions relative to the true data generating distribution while taking into account the ﬁniteness of the data. What this means
is that both are open to combining information sources. that is, both are interested in combining the information that can be extracted from the training
data with other, or “prior” sources of information. As we will see, many forms of
regularization can be given a Bayesian interpretation.
If we consider a dataset {x (1) , . . . , x(m) }, we recover the posterior distribution
on the model parameter θ by combining p(θ | x(1) , . . . , x(m) ) the data likelihood
p(x (1), . . . , x(m) | θ) with the prior p(θ):
X
log p(θ | x(1) , . . . , x(m) ) = log p(θ) +
(7.1)
log p(x(i) | θ) + constant
i

1

We should have a lot of uncertainty about the parameters before we see the data, but some
conﬁgurations may a priori be impossible, while others could seem more plausible, a priori.
198

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

where the constant is − log Z, with Z the normalization constant which does
not depend on θ but does depend on the data. When maximizing over θ, this
constant does not matter. In the context of maximum likelihood learning, the
introduction of the prior distribution plays the same role as a regularizer in that
it can be seen as a term (the ﬁrst one above) added to the objective function
that is added (to the second term, the log-likelihood) in hopes of achieving better
generalization, despite of its detrimental eﬀect on the likelihood of the training
data (the optimum of which would be achieved by considering only the second
term above).
In the following section, we will detail how the addition of a prior is equivalent to certain regularization strategies. However we must be a bit careful in
establishing the relationship between the prior and a regularizer. Regularizers
are more general than priors. Priors are distributions and as such are subject to
constraints such as they must always be positive and must sum to one over their
domain. Regularizers have no such explicit constraints and can depend on the
data, not just on the parameters. Another problem in interpreting all regularizers as priors is that the equivalence implies the overly restrictive constraint that
all unregularized objective functions be interpretable as log-likelihood functions.
Nevertheless, it remains true that many of the most popular forms of regularization can be equated to a Bayesian prior.

7.2

Classical Regularization: Parameter Norm Penalty

Regularization has been used for decades prior to the advent of deep learning.
Statistical and machine learning models traditionally represented simpler functions. Because the functions themselves had less capacity, the regularization did
not need to be as sophisticated. We use the term classical regularization to refer
to the techniques used in the general machine learning and statistics literature.
Most classical regularization approaches are based on limiting the capacity
of models, such as neural networks, linear regression, or logistic regression, by
adding a parameter norm penalty Ω(θ) to the objective function J. We denote
the regularized objective function by J˜:
J˜(θ; X, y) = J (θ; X, y) + αΩ(θ)

(7.2)

where α is a hyperparameter that weights the relative contribution of the norm
penalty term, Ω, relative to the standard objective function J (x; θ). The hyperparameter α should be a non-negative real number, with α = 0 corresponding to
no regularization, and larger values of α corresponding to more regularization.
When our training algorithm minimizes the regularized objective function J˜ it
will decrease both the original objective J on the training data and some measure
199

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

of the size of the parameters θ (or some subset of the parameters). Diﬀerent
choices for the parameter norm Ω can result in diﬀerent solutions being preferred.
In this section, we discuss the eﬀects of the various norms when used as penalties
on the model parameters.
Before delving into the regularization behavior of diﬀerent norms, we note
that for neural networks, we typically choose to use a parameter norm penalty Ω
that only penalizes the interaction weights, i.e we leave the oﬀsets unregularized.
The oﬀsets typically require less data to ﬁt accurately than the weights. Each
weight speciﬁes how two variables interact, and requires observing both variables
in a variety of conditions to ﬁt well. Each oﬀset controls only a single variable.
This means that we do not induce too much variance by leaving the oﬀsets unregularized. Also, regularizing the oﬀsets can introduce a signiﬁcant amount of
underﬁtting.

7.2.1

L2 Parameter Regularization

We have already seen one the simplest and most common kinds of classical regularization: the L2 parameter norm penalty commonly known as weight decay.
This regularization strategy drives the parameters closer to the origin 2 by adding
a regularization term Ω(θ) = 12 kwk22 to the objective function, where w is the
subset of parameters considered to be weights (i.e., parameters of a linear transformation, rather than, e.g, biases parameterizing the oﬀset of an aﬃne transformation), and α is a coeﬃcient determining the strength of the regularization
penalty relative to the other terms in the objective function, such as the negative
log-likelihood.
In various contexts, L2 regularization is also known as ridge regression or
Tikhonov regularization.
In the context of neural networks, it is sometimes desirable to use a separate
weight decay penalty with a diﬀerent α coeﬃcient for each layer of the network.
Because α is a hyperparameter and it can be expensive to search for the correct
value of multiple hyperparameters, it is still reasonable to use the same weight
decay at all layers just to reduce the search space.
We can gain some insight into the behavior of weight decay regularization by
considering the gradient of the regularized objective function. To simplify the
presentation, we assume no oﬀset term, so θ is just w. Such a model has the
2

More generally, we could regularize the parameters to be near any speciﬁc point in space
and, surprisingly, still get a regularization eﬀect, but better results will be obtained for a value
closer to the true one, with zero being a default value that makes sense when we do not know
if the correct value should be positive or negative. Since it is far more common to consider
regularizing the model parameters to zero, we will focus on this special case in our exposition.

200

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

following gradient of the total objective function:
∇ wJ̃ (w; X, y) = αw + ∇w J (w; X, y).

(7.3)

To take a single gradient step to update the weights, we perform this update:
w := w −  (αw + ∇wJ (w; X, y)) .
Written another way, the update is:
w := (1 − α)w − ∇wJ (w; X, y).
We can see that the addition of the weight decay term has modiﬁed the learning rule to multiplicatively shrink the weight vector by a constant factor on each
step, towards zero, just before performing the usual gradient update. This describes what happens in a single step. But what happens over the entire course
of training?
We will further simplify the analysis by considering a quadratic approximation
to the objective function in the neighborhood of the empirically optimal value of
the weights w ∗ . (If the objective function is truly quadratic, as in the case of
ﬁtting a linear regression model with mean squared error, then the approximation
is perfect).
1
Jˆ(θ) = J (w ∗) + (w − w ∗)> H(w − w ∗)
(7.4)
2
where H is the Hessian matrix of J with respect to w evaluated at w ∗. There is
no ﬁrst order term in this quadratic approximation, because w ∗ is deﬁned to be a
minimum, where the gradient vanishes. Likewise, because w ∗ is a minimum, we
can conclude that H is positive semi-deﬁnite.
∇ wJˆ(w) = H (w − w∗ ).

(7.5)

If we replace the exact gradient in equation 7.3 with the approximate gradient
in equation 7.5, we can write an equation for the location of the minimum of the
regularized objective function:
αw + H(w − w ∗) = 0

(7.6)

∗

(7.7)

w̃ = (H + αI)−1 Hw ∗.

(7.8)

(H + αI)w = Hw

From this, we see that the presence of the regularization term moves the
optimum from w ∗ to w̃. As α approaches 0, w̃ approaches w ∗. But what happens
as α grows? Because H is real and symmetric, we can decompose it into a diagonal
201

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

w2
w∗
w̃

w1

Figure 7.1: An illustration of the eﬀect of L2 (or weight decay) regularization on the value
of the optimal w. The solid ellipses represent contours of equal value of the unregularized
objective. The dotted circles represent contours of equal value of the L 2 regularizer. At
the point w̃, these competing objectives reach an equilibrium. Note how in the ﬁrst
dimension, where the eigenvalue of the Hessian of J is small (i.e., the objective function
does not increase much when moving horizontally away from w ∗ ), the regularizer has a
strong eﬀect, pulling the parameter values close to zero, whereas in the second dimension,
the objective function is very sensitive to movements away from w ∗ , the eigenvalue is large
(high curvature), and weight decay does not aﬀect the solution much.

matrix Λ and an orthonormal basis of eigenvectors, Q, such that H = QΛQ >.
Applying the decomposition to equation 7.8, we obtain:
w̃ = (QΛQ > + αI)−1QΛQ> w ∗
h
i −1
>
= Q(Λ + αI)Q
QΛQ> w ∗
= Q(Λ + αI)−1 ΛQ>w ∗,

Q> w̃ = (Λ + αI) −1 ΛQ> w∗ .

(7.9)

If we interpret the Q> w̃ as rotating our solution parameters w̃ into the basis
deﬁned by the eigenvectors Q of H, then we see that the eﬀect of weight decay
is to rescale the coeﬃcients of eigenvectors. Speciﬁcally the ith component is
i
rescaled by a factor of λiλ+α
. (You may wish to review how this kind of scaling
works, ﬁrst explained in Fig. 2.3).
Along the directions where the eigenvalues of H are relatively large, for example, where λ i  α, the eﬀect of regularization is relatively small. However,
components with λi  α will be shrunk to have nearly zero magnitude. This
eﬀect is illustrated in Fig. 7.1.
Only directions along which the parameters contribute signiﬁcantly to reducing the objective function are preserved relatively intact. In directions that do not
contribute to reducing the objective function, a small eigenvalue of the Hessian
tell us that movement in this direction will not signiﬁcantly increase the gradient.
202

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Components of the weight vector corresponding to such unimportant directions
are decayed away through the use of the regularization throughout training. This
eﬀect of suppressing contributions to the parameter vector along these principle
directions of the Hessian H is captured in the concept of the eﬀective number of
parameters, deﬁned to be
X λi
γ=
.
(7.10)
λi + α
i

As α is increased, the eﬀective number of parameters decreases.
Another way to gain some intuition for the eﬀect of L 2 regularization is to
consider its eﬀect on linear regression. The unregularized objective function for
linear regression is the sum of squared errors:
(Xw − y)> (Xw − y).
When we add L2 regularization, the objective function changes to
(Xw − y) > (Xw − y) +

1
αw> w.
2

This changes the normal equations for the solution from
w = (X> X)−1 X> y
to

w = (X >X + αI)−1 X> y.

We can see L 2 regularization causes the learning algorithm to “perceive” the
input X as having higher variance, which makes it shrink the weights on features
whose covariance with the output target is low compared to this added variance.
TODO–make sure the chapter includes maybe a table showing relationships
between early stopping, priors, constraints, penalties, and adding noise? e.g. look
up L1 penalty and it tells you what prior it corresponds to scratch-work thinking
about how to do it:
L2 penalty L2 constraint add noise early stopping Gaussian prior L1 penalty
L1 constraint Laplace prior Max-norm penalty

7.2.2

L1 Regularization

While L2 weight decay is the most common form of weight decay, there are other
ways to penalize the size of the model parameters. Another option is to use L 1
regularization.
Formally, L1 regularization on the model parameter w is deﬁned as:
Ω(θ) = ||w||1 =
203

i

X

|wi |,

(7.11)

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

that is, as the sum of absolute values of the individual parameters. 3 We will now
consider the eﬀect of L 1 regularization on the simple linear model, with no oﬀset
term, that we considered in our analysis of L2 regularization. In particular, we are
interested in delineating the diﬀerences between L 1 and L 2 forms of regularization.
Thus, if we consider the gradient (actually the sub-gradient) on the regularized
objective function J˜(w; X, y), we have:
∇ wJ˜(w; X, y) = βsign(w) + ∇ w J (X, y; w)

(7.12)

where sign(w) is simply the sign of w applied element-wise.
By inspecting Eqn. 7.12, we can see immediately that the eﬀect of L1 regularization is quite diﬀerent from that of L2 regularization. Speciﬁcally, we can see
that the regularization contribution to the gradient no longer scales linearly with
w, instead it is a constant factor with a sign equal to sign(w). One consequence
of this form of the gradient is that we will not necessarily see clean solutions
to quadratic forms of ∇w J (X, y; w) as we did for L2 regularization. Instead,
the solutions are going to be much more aligned to the basis space in which the
problem is embedded.
To see this, and for the sake of comparison with L2 regularization, we will
again consider a simpliﬁed setting of a quadratic approximation to the objective
function in the neighborhood of the empirical optimum w∗. (Once again, if the
per-example loss is truly quadratic, as in the case of ﬁtting a linear regression
model with mean squared error, then the approximation is perfect). The gradient
of this approximation is given by
∇ wJˆ(w) = H (w − w∗ ).

(7.13)

where, again, H is the Hessian matrix of J with respect to w evaluated at w ∗ .
We will also make the further simplifying assumption that the Hessian is diagonal, H = diag([γ 1, . . . , γN ]), where each γi > 0. With this rather restrictive
assumption, the solution of the minimum of the L1 regularized objective function
decomposes into a system of equations of the form:
1
J˜(w; X, y) = γi (wi − w∗i )2 + β|wi |.
2
It admits an optimal solution (for each dimension i), with the following form:
w i = sign(w ∗i ) max(|w ∗i | −
3

β
, 0).
γi

As with L2 regularization, we could consider regularizing the parameters to a value that is
(o)
not zero, but instead to some parameter value
that case the L1 regularization would
P w . In (o)
(o)
introduce the term Ω(θ) = ||w − w ||1 = β i |wi − w i |.
204

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

w2

w2
w

∗

w∗
w̃

w̃

w1

w1

Figure 7.2: An illustration of the eﬀect of L 1 regularization (right) on the value of the
optimal W , in comparison to the eﬀect of L2 regularization (left).

Let’s consider the situation where w ∗i > 0 for all i, there are two possible outcomes. Case 1: w ∗i ≤ βγi , here the optimal value of wi under the regularized
objective is simply wi = 0, this occurs because the contribution of J (w; X, y) to
the regularized objective J˜(w; X, y) is overwhelmed—in direction i, by the L 1
regularization which pushes the value of w i to zero. Case 2: w ∗i > βγi , here the
regularization does not move the optimal value of w to zero but instead it just
shifts it in that direction by a distance equal to βγi. This is illustrated in Fig. 7.2.
A similar argument can be made when w ∗i < 0, but with the L 1 penalty making
wi less negative by βγi , or 0.
In comparison to L 2 regularization, L 1 regularization results in a solution that
is more sparse. Sparsity in this context implies that there are free parameters of
the model that—through L1 regularization—with an optimal value (under the
regularized objective) of zero. TODO: something is wrong with the sentence
above As we discussed, for each element i of the parameter vector, this happens
when |w∗i | ≤ γβi . Comparing this to the situation for L2 regularization, where
γi
(under the same assumptions of a diagonal Hessian H) we get w L2 = γ i+α
w ∗,
which is nonzero as long as w ∗ is nonzero.
In Fig. 7.2, we see that even when the optimal value of w is nonzero, L 1
regularization acts to punish small values of parameters just as harshly as larger
values, leading to optimal solutions with more parameters having value zero and
more larger valued parameters.
The sparsity property induced by L1 regularization has been used extensively
as a feature selection mechanism. In particular, the well known LASSO (Tibshirani, 1995) (least absolute shrinkage and selection operator) model integrates an
L1 penalty with a linear model and a least squares cost function.

205

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

7.2.3

Bayesian Interpretation of the Parameter Norm Penalty

Parameter norm penalties are often amenable to being interpreted as a Bayesian
prior. Recall that parameter norm penalties are eﬀected by adding a term Ω(w)
to the unregularized objective function J .
J̃ (w; X, y) = J (w; X, y) + αΩ(w)

(7.14)

where α is a hyperparameter that weights the relative contribution of the norm
penalty term.
We can view the minimization of the regularized objective function above as
equivalent to ﬁnding the maximum a posteriori (MAP) estimate of the parameters: p(w | X, y) ∝ p(y | X, w)p(w), where the unregularized J (w; X, y) is
taken as the log-likelihood and the regularization term αΩ(w) plays the role of
the parameter prior distribution. Diﬀerence choices of regularizers correspond to
diﬀerent priors.
In the case of L2 regularization, minimizing with αΩ(w) = α2 kwk22 is functionally equivalent to maximizing the log of the posterior distribution (or minimizing
the negative log posterior) where the prior is given by a Gaussian distribution.
1
1
d
log p(w; µ, Σ) = − (w − µ) >Σ −1 (w − µ) − log |Σ| − log(2π)
2
2
2
where d is the dimension of w. Ignoring terms that are not a function of w (and
therefore do not aﬀect the MAP value), we can see that the by choosing µ = 0
and Σ −1 = αI, we recover the functional form of L 2 regularization: p(w; µ, Σ) ∝
α
2
e −2 kwk2 . Thus L 2 regularization can be interpreted as assuming independent
Gaussian prior distributions over all the model parameters, each with precision
(i.e. the inverse of variance) α.
P
For L1 regularization, minimizing with αΩ(w) = α i kw ik is equivalent to
maximizing the log of the posterior distribution with an isotropic Laplace distribution over w.
log p(w; µ, η) =

X

log Laplace(wi ; µi , η i) =

i

X |wi − µ i|
−
− log (2ηi)
ηi
i

Once again we can ignore the second term here because it does not depend on the
elements of w, so L1 regularization
is equivalent to optimizing a MAP objective
P
with a log prior given by i log Laplace(w i; 0, λ −1).

206

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

7.3

Classical Regularization as Constrained Optimization

Classical regularization adds a penalty term to the training objective:
J̃ (θ; X, y) = J (θ; X, y) + αΩ(θ).
Recall from Sec. 4.4 that we can minimize a function subject to constraints by
constructing a generalized Lagrange function (see 4.4), consisting of the original
objective function plus a set of penalties. Each penalty is a product between
a coeﬃcient, called a Karush–Kuhn–Tucker (KKT) multiplier 4 , and a function
representing whether the constraint is satisﬁed. If we wanted to constrain Ω(θ) to
be less than some constant k, we could construct a generalized Lagrange function
L(θ, α; X, y) = J (θ; X, y) + α(Ω(θ) − k).
The solution to the constrained problem is given by
θ∗ = min max L(θ, α).
θ α,α≥0

Solving this problem requires modifying both θ and α. Speciﬁcally, α must
increase whenever ||θ||p > k and decrease whenever ||θ|| p < k. However, after we
have solved the problem, we can ﬁx α ∗ and view the problem as just a function
of θ:
θ ∗ = min L(θ, α∗ ) = min J (θ; X, y) + α ∗Ω(θ).
θ

θ

This is exactly the same as the regularized training problem of minimizing J˜.
Note that the value of α∗ does not directly tell us the value of k. In principle, one
can solve for k, but the relationship between k and α ∗ depends on the form of
J . We can thus think of classical regularization as imposing a constraint on the
weights, but with an unknown size of the constraint region. Larger α will result in
a smaller constraint region, and smaller α will result in a larger constraint region.
Sometimes we may wish to use explicit constraints rather than penalties. As
described in Sec. 4.4, we can modify algorithms such as stochastic gradient descent
to take a step downhill on J (θ) and then project θ back to the nearest point that
satisﬁes Ω(θ) < k. This can be useful if we have an idea of what value of k
is appropriate and do not want to spend time searching for the value of α that
corresponds to this k.
Another reason to use explicit constraints and reprojection rather than enforcing constraints with penalties is that penalties can cause non-convex optimization
4

KKT multipliers generalize Lagrange multipliers to allow for inequality constraints.
207

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

procedures to get stuck in local minima corresponding to small θ. When training
neural networks, this usually manifests as neural networks that train with several
“dead units”. These are units that do not contribute much to the behavior of the
function learned by the network because the weights going into or out of them are
all very small. When training with a penalty on the norm of the weights, these
conﬁgurations can be locally optimal, even if it is possible to signiﬁcantly reduce
J by making the weights larger. (This concern about local minima obviously does
not apply when J̃ is convex)
Finally, explicit constraints with reprojection can be useful because they impose some stability on the optimization procedure. When using high learning
rates, it is possible to encounter a positive feedback loop in which large weights
induce large gradients which then induce a large update to the weights. If these
updates consistently increase the size of the weights, then θ rapidly moves away
from the origin until numerical overﬂow occurs. Explicit constraints with reprojection allow us to terminate this feedback loop after the weights have reached a
certain magnitude. Hinton et al. (2012c) recommend using constraints combined
with a high learning rate to allow rapid exploration of parameter space while
maintaining some stability.
TODO how L2 penalty is equivalent to L2 constraint (with unknown value),
L1 penalty is equivalent to L1 constraint maybe move the earlier L2 regularization
ﬁgure to here, now that the sub-level sets will make more sense? show the shapes
induced by the diﬀerent norms separate L2 penalty on each hidden unit vector
is diﬀerent from L2 penalty on all theta; is equivalent to a penalty on the max
across columns of the column norms

7.4

Regularization and Under-Constrained Problems

In some cases, regularization is necessary for machine learning problems to be
properly deﬁned. Many linear models in machine learning, including linear regression and PCA, depend on inverting the matrix X > X. This is not possible
whenever X> X is singular. This matrix can be singular whenever the data truly
has no variance in some direction, or when there are fewer examples (rows of X)
than input features (columns of X). In this case, many forms of regularization
correspond to inverting X > X +αI instead. This regularized matrix is guaranteed
to be invertible.
These linear problems have closed form solutions when the relevant matrix is
invertible. It is also possible for a problem with no closed form solution to be
underdetermined. For example, consider logistic regression applied to a problem
where the classes are linearly separable. If a weight vector w is able to achieve
perfect classiﬁcation, then 2w will also achieve perfect classiﬁcation and higher
208

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

likelihood. An iterative optimization procedure like stochastic gradient descent
will continually increase the magnitude of w and, in theory, will never halt. In
practice, a numerical implementation of gradient descent will eventually reach
suﬃciently large weights to cause numerical overﬂow, at which point its behavior
will depend on how the programmer has decided to handle values that are not
real numbers.
Most forms of regularization are able to guarantee the convergence of iterative
methods applied to underdetermined problems. For example, weight decay will
cause gradient descent to quit increasing the magnitude of the weights when the
slope of the likelihood is equal to the weight decay coeﬃcient. Likewise, early
stopping based on the validation set classiﬁcation rate will cause the training
algorithm to terminate soon after the validation set classiﬁcation accuracy has
stopped increasing. Even if the problem is linearly separable and there is no
overﬁtting, the validation set classiﬁcation accuracy will eventually saturate to
100%, resulting in termination of the early stopping procedure.
The idea of using regularization to solve underdetermined problems extends
beyond machine learning. The same idea is useful for several basic linear algebra
problems.
As we saw in Chapter 2.9, we can solve underdetermined linear equations
using the Moore-Penrose pseudoinverse.
One deﬁnition of the pseudoinverse X + of a matrix X is to perform linear
regression with an inﬁnitesimal amount of L2 regularization:
X + = lim(X> X + αI)−1 X >.
α&0

When a true inverse for X exists, then w = X + y returns the weights that
exactly solve the regression problem. When X is not invertible because no exact solution exists, this returns the w corresponding to the least possible mean
squared error. When X is not invertible because many solutions exactly solve
the regression problem, this returns w with the minimum possible L 2 norm.
Recall that the Moore-Penrose pseudoinverse can be computed easily using
the singular value decomposition. Because the SVD is robust to underdetermined
problems resulting from too few observations or too little underlying variance,
it is useful for implementing stable variants of many closed-form linear machine
learning algorithms. The stability of these algorithms can be viewed as a result of
applying the minimum amount of regularization necessary to make the problem
become determined.

209

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

7.5

Dataset Augmentation

The best way to make a machine learning model generalize better is to train it
on more data. Of course, in practice, the amount of data we have is limited. One
way to get around this problem is to create fake data and add it to the training
set. For some machine learning tasks, it is reasonably straightforward to create
new fake data.
This approach is easiest for classiﬁcation. A classiﬁer needs to take a complicated, high dimensional input x and summarize it with a single category identity
y. This means that the main task facing a classiﬁer is to be invariant to a wide
variety of transformations. We can generate new (x, y) pairs easily just by transforming the x inputs in our training set.
This approach is not as readily applicable to many other tasks. For example,
it is diﬃcult to generate new fake data for a density estimation task unless we
have already solved the density estimation problem.
Dataset augmentation has been a particularly eﬀective technique for a speciﬁc classiﬁcation problem: object recognition. Images are high dimensional and
include an enormous variety of factors of variation, many of which can be easily
simulated. Operations like translating the training images a few pixels in each
direction can often greatly improve generalization, even if the model has already
been designed to be partially translation invariant by using convolution and pooling. Many other operations such as rotating the image or scaling the image have
also proven quite eﬀective. One must be careful not to apply transformations
that would change the correct class. For example, optical character recognition
tasks require recognizing the diﬀerence between ’b’ and ’d’ and the diﬀerence between ’6’ and ’9’, so horizontal ﬂips and 180 ◦ rotations are not appropriate ways
of augmenting datasets for these tasks. There are also transformations that we
would like our classiﬁers to be invariant to, but which are not easy to perform.
For example, out-of-plane rotation can not be implemented as a simple geometric
operation on the input pixels.
A way to “analytically” obtain an eﬀect similar to dataset augmentation is to
add a regularizer that prefers a class probability predictor that is invariant to tiny
movements in the directions associated with known desired invariances. These directions correspond to moving the training example along the subspace spanned
by the tangent vectors associated with the manifold containing the training example and those obtained by applying these nuisance variations (such as translation
or rotation of the image). See Section 17.5 for more detail on these ideas and
the original Tangent-Prop algorithm (Simard et al., 1992), which introduced this
idea.
For many classiﬁcation and even some regression tasks, the task should still
be possible to solve even if small random noise is added to the input. Neural
210

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

networks prove not to be very robust to noise, however (Tang and Eliasmith,
2010). One way to improve the robustness of neural networks is simply to train
them with random noise applied to their inputs, and this input noise injection
is part of some unsupervised learning algorithms such as the denoising autoencoder (Vincent et al., 2008). This same approach also works when the noise
is applied to the hidden units, which can be seen as doing dataset augmentation
at multiple levels of abstraction. Poole et al. (2014) recently showed that this
approach can be highly eﬀective provided that the magnitude of the noise is
carefully tuned. Dropout, a powerful regularization strategy that will be described
in Sec. 7.11, can be seen as a process of constructing new inputs by multiplying by
noise. In a multilayer network, it can often be beneﬁcial to apply transformations
such as noise to the hidden units, as well as the inputs. This can be viewed as
augmenting the dataset as seen by the deeper layers.
When reading machine learning research papers, it is important to take the
eﬀect of dataset augmentation into account. Often, hand-designed dataset augmentation schemes can dramatically reduce the generalization error of a machine
learning technique. It is important to look for controlled experiments. When
comparing machine learning algorithm A and machine learning algorithm B, it
is necessary to make sure that both algorithms were evaluated using the same
hand-designed dataset augmentation schemes. If algorithm A performs poorly
with no dataset augmentation and algorithm B performs well when combined
with numerous synthetic transformations of the input, then it is likely the synthetic transformations and not algorithm B itself that cause the improved performance. Sometimes the line is blurry, such as when a new machine learning
algorithm involves injecting noise into the inputs. In these cases, it is best to
consider how generally applicable the new algorithm is, and to make sure that
pre-existing algorithms are re-run in as similar of conditions as possible.

7.6

Classical Regularization as Noise Robustness

In the machine learning literature, there have been two ways that noise has been
used as part of a regularization strategy. The ﬁrst and most popular way is by
adding noise to the input. While this can be interpreted simply as form of dataset
augmentation (as described above in Sec. 7.5), we can also interpret it as being
equivalent to more traditional forms of regularization.
The second way that noise has been used in the service of regularizing models
is by adding it to the weights. This technique has been used primarily in the
context of recurrent neural networks (Jim et al., 1996; Graves, 2011a). This can
be interpreted as a stochastic implementation of a Bayesian inference over the
weights. The Bayesian treatment of learning would consider the model weights
211

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

to be uncertain and representable via a probability distribution that reﬂects this
uncertainty. Adding noise to the weights is a practical, stochastic way to reﬂect
this uncertainty (Graves, 2011a).
In this section, we review these two strategies and provide some insight into
how noise can act to regularize the model.

7.6.1

Injecting Noise at the Input

Some classical regularization techniques can be derived in terms of training on
noisy inputs 5 . Let us consider a regression setting, where we are interested in
learning a model ŷ(x) that maps a set of features x to a scalar. The cost function
we will use is the least-squares error between the model prediction ŷ(x) and the
true value y:


J = Ep(x,y) (ŷ(x) − y)2 ,
(7.15)

where we are given a dataset of m input / output pairs {(x(1) , y (1) ), . . . , (x(m) , y (m))}
and the training objective is to minimize the objective function, which is the empirical average of the squared error on the training data.
Now consider that with each input presentation to the model we also include
a random perturbation  ∼ (0, νI), so that the error function becomes


J˜x = Ep(x,y,) (ŷ(x + ) − y)2


= Ep(x,y,) ŷ2 (x + ) − 2yŷ(x + ) + y2


 
= Ep(x,y,) ŷ2 (x + ) − 2Ep(x,y,)[yŷ(x + )] + E p(x,y,) y 2

(7.16)

Assuming small noise, we can consider the Taylor series expansion of ŷ(x + )
around ŷ(x).
1
ŷ(x + ) = ŷ(x) +  > ∇x ŷ(x) + >∇ 2x ŷ(x) + O(3 )
2

(7.17)

Substituting this approximation for ŷ(x + ) into the objective function (Eq. 7.16)
5

The analysis in this section is mainly based on that in Bishop (1995a,b)

212

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

and using the fact that Ep() [] = 0 and that E p()[> ] = νI to simplify6 , we get:
"
2 #
1
J˜x ≈ Ep(x,y,)
ŷ(x) + >∇ x ŷ(x) + > ∇ 2x ŷ(x)
2


 
1 > 2
>
− 2Ep(x,y,) yŷ(x) + y ∇ x yŷ(x) + y ∇ xŷ(x) + Ep(x,y,) y2
2



2


2
> 2
>
3
= Ep(x,y,) (ŷ(x) − y) + Ep(x,y,) ŷ(x) ∇xŷ(x) +  ∇ x ŷ(x) + O( )


1 > 2
y ∇ xŷ(x)
− 2Ep(x,y,)
2




= J + νEp(x,y) (ŷ(x) − y)∇2xŷ(x) + νEp(x,y) k∇x ŷ(x)k2
(7.18)
If we consider minimizing this objective function, by taking the functional gradient
of ŷ(x) and setting the result to zero, we can see that
ŷ(x) = Ep(y|x) [y] + O(ν).
This implies that the expectation in the second last term in Eq. 7.18,


E p(x,y) (ŷ(x) − y)∇ 2x ŷ(x) ,

reduces to O(ν) because the expectation of the diﬀerence (ŷ(x) − y) reduces to
O(ν).
This leaves us with the objective function of the form




J˜x = Ep(x,y) (ŷ(x) − y)2 + νE p(x,y) k∇ xŷ(x)k 2 + O(ν2).

For small ν, the minimization of J with added noise on the input (with covariance
νI) is equivalent
to minimization
of J with an additional regularization term given


2
by νE p(x,y) k∇x ŷ(x)k .
Considering the behavior of this regularization term, we note that it has the
eﬀect of penalizing large gradients of the function ŷ(x). That is, it has the eﬀect
of reducing the sensitivity of the output of the network with respect to small
variations in its input x. We can interpret this as attempting to build in some
local robustness into the model and thereby promote generalization. We note also
that for linear networks, this regularization term reduces to simple weight decay
(as discussed in Sec. 7.2.1).
6

In this derivation we have used two properties of the trace operator: (1) that a scalar is
equal to its trace; (2) that, for a square matrix AB, Tr(AB) = Tr(BA). These are discussed in
Sec. 2.10.
213

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

7.6.2

Injecting Noise at the Weights

Rather than injecting noise as part of the input, one could also consider adding
noise directly to the model parameters. As we shall see, this can also be interpreted as equivalent (under some assumptions) to a more traditional form of
regularization. Adding noise to the weights has been shown to be an eﬀective regularization strategy in the context of recurrent neural networks7 (Jim et al., 1996;
Graves, 2011b). In the following, we will present an analysis of the eﬀect of weight
noise on a standard feedforward neural network (as introduced in Chapter 6).
As we did in the last section, we again consider the regression setting, where
we wish to train a function ŷ(x) that maps a set of features x to a scalar using
the least-squares cost function between the model predictions ŷ(x) and the true
values y:


J = Ep(x,y) (ŷ(x) − y)2 .
(7.19)

We again assume we are given a dataset of m input / output pairs {(x(1) , y(1)),
. . . , (x(m) , y(m) )}.
We now assume that with each input presentation we also include a random
perturbation W ∼ (0, ηI ) of the network weights. Let us imagine that we have
a standard L-layer MLP, we denote the perturbed model as ŷW (x). Despite the
injection of noise, we are still interested in minimizing the squared error of the
output of the network. The objective function thus becomes:


J˜W = Ep(x,y,W ) (ŷW ) − y)2


= Ep(x,y,W ) ŷ2 W (x) − 2yŷ  W(x) + y2
(7.20)

Assuming small noise, we can consider the Taylor series expansion of ŷW (x)
around the unperturbed function ŷ(x).
1 > 2
3
ŷW (x) = ŷ(x) +  >
W ∇ W ŷ(x) + W ∇W ŷ(x) W + O( W )
2

(7.21)

From here, we follow the same basic strategy that was laid-out in the previous
section in analyzing the eﬀect of adding noise to the input. That is, we substitute
7

Recurrent neural networks will be discussed in detail in Chapter 10

214

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

the Taylor series expansion of ŷW (x) into the objective function in Eq. 7.20.
"
2 #
1
>
2
J˜W ≈ Ep(x,y,W )
ŷ(x) + >
W ∇ W ŷ(x) + W ∇ W ŷ(x) W
2



 
1 > 2
>
Ep(x,y,W ) −2y ŷ(x) + W ∇W ŷ(x) + W ∇ W ŷ(x) W
+ Ep(x,y, W ) y2
2


h
i
1
= Ep(x,y,W ) (ŷ(x) − y) 2 − 2Ep(x,y, W ) y >
∇ 2 ŷ(x)
2 W W



2
>
2
>
3
+ E p(x,y,W ) ŷ(x) W ∇W ŷ(x)W + W∇ W ŷ(x) + O(W ) . (7.22)
(7.23)

Where we have used the fact that E  W ) W = 0 to drop terms that are linear in
 W . Incorporating the assumption that EW ) 2W = ηI, we have:




J˜W ≈ J + νEp(x,y) (ŷ(x) − y)∇2W ŷ(x) + νE p(x,y) k∇W ŷ(x)k2

(7.24)

Again, if we consider minimizing this objective function, we can see that the
optimal value of ŷ(x) is:
ŷ(x) = Ep(y|x) [y] + O(η),


implying that the expectation in the middle term in Eq. 7.24, E p(x,y) (ŷ(x) − y)∇2W ŷ(x) ,
reduces to O(η) because the expectation of the diﬀerence (ŷ(x) − y) is reduces to
O(η).
This leaves us with the objective function of the form




J̃ W = Ep(x,y) (ŷ(x) − y)2 + ηE p(x,y) k∇ W ŷ(x)k 2 + O(η2 ).
For small η, the minimization of J with added weight noise (with covariance
ηI) is equivalent
to minimization of J with an additional regularization term:

ηEp(x,y) k∇W ŷ(x)k 2 . This form of regularization encourages the parameters
to go to regions of parameter space where weights have a relatively small inﬂuence on the output. In other words, it pushes the model into regions where the
model is relatively insensitive to small variations in the weights. Regularization
strategies with this kind of behavior have been considered before (Hochreiter
and Schmidhuber, 1995). In the simpliﬁed case of linear regression (where,
 for

>
instance, ŷ(x) = w x + b), this regularization term collapses into ηEp(x) kxk2 ,
which is not a function of parameters and therefore does not contribute to the
gradient of J˜W w.r.t the model parameters.
215

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Figure 7.3: Learning curves showing how the negative log-likelihood loss changes over
time (indicated as number of training iterations over the dataset, or epochs). In this
example, we train a maxout network on MNIST, regularized with dropout. Observe that
the training objective decreases consistently over time, but the validation set average loss
eventually begins to increase again.

216

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

7.7

Early Stopping as a Form of Regularization

When training large models with large enough capacity, we often observe that
training error decreases steadily over time, but validation set error begins to rise
again. See Fig. 7.3 for an example of this behavior. This behavior occurs very
reliably.
This means we can obtain a model with better validation set error (and thus,
hopefully better test set error) by returning to the parameter setting at the point
in time with the lowest validation set error. Instead of running our optimization
algorithm until we reach a (local) minimum of validation error, we run it until the
error on the validation set has not improved for some amount of time. Every time
the error on the validation set improves, we store a copy of the model parameters.
When the training algorithm terminates, we return these parameters, rather than
the latest parameters. This procedure is speciﬁed more formally in Alg. 7.1.
This strategy is known as early stopping. It is probably the most commonly
used form of regularization in deep learning. Its popularity is due both to its
eﬀectiveness and its simplicity.
One way to think of early stopping is as a very eﬃcient hyperparameter selection algorithm. In this view, the number of training steps is just another
hyperparameter. We can see in Fig. 7.3 that this hyperparameter has a U-shaped
validation set performance curve, just like most other model capacity control parameters, albeit with the addition of irregular oscillations. In this case, we are
controlling the eﬀective capacity of the model by determining how many steps
it can take to ﬁt the training set precisely. Most of the time, setting hyperparameters requires an expensive guess and check process, where we must set a
hyperparameter at the start of training, then run training for several steps to see
its eﬀect. The “training time” hyperparameter is unique in that by deﬁnition
a single run of training tries out many values of the hyperparameter. The only
signiﬁcant cost to choosing this hyperparameter automatically via early stopping
is running the validation set evaluation periodically during training. One typically chooses a validation set size and a period for computing validation error
such that this monitoring only adds a fraction of the training time to the total
computational cost, for example by making the number of training examples seen
between validation error measurements a multiple of the validation set size. Even
better, with access to another processor, one uses it to perform the monitoring
work, allowing one to consider larger validation sets without slowing down the
training process.
An additional cost to early stopping is the need to maintain a copy of the best
parameters. This cost is generally negligible, because it is acceptable to store
these parameters in a slower and larger form of memory (for example, training
in GPU memory, but storing the optimal parameters in host memory or on a
217

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Algorithm 7.1 The early stopping meta-algorithm for determining the best
amount of time to train. This meta-algorithm is a general strategy that works
well with a variety of training algorithms and ways of quantifying error on the
validation set.
Let n be the number of steps between evaluations.
Let p be the “patience,” the number of times to observe worsening validation
set error before giving up.
Let θo be the initial parameters.
θ ← θo
i←0
j←0
v←∞
θ∗ ← θ
i∗ ← i
while j < p do
Update θ by running the training algorithm for n steps.
i ←i+n
v0 ← ValidationSetError(θ)
if v0 < v then
j←0
θ∗ ← θ
i∗ ← i
v ← v0
else
j ← j+1
end if
end while
Best parameters are θ∗ , best number of training steps is i∗
disk drive). Since the best parameters are written to infrequently and never read
during training, these occasional slow writes are have little eﬀect on the total
training time.
Early stopping is a very unobtrusive form of regularization, in that it requires
almost no change in the underlying training procedure, the objective function,
or the set of allowable parameter values. This means that it is easy to use early
stopping without damaging the learning dynamics. This is in contrast to weight
decay, where one must be careful not to use too much weight decay and trap the
network in a bad local minimum corresponding to a solution with pathologically
small weights.
Early stopping may be used either alone or in conjunction with other regu218

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

larization strategies. Even when using regularization strategies that modify the
objective function to encourage better generalization, it is rare for the best generalization to occur at a local minimum of the training objective.
Early stopping requires a validation set, which means some training data is not
fed to the model. To best exploit this extra data, one can perform extra training
after the initial training with early stopping has completed. In the second, extra
training step, all of the training data is included. There are two basic strategies
one can use for this second training procedure.
One strategy is to initialize the model again and retrain on all of the data.
In this second training pass, we train for the same number of steps as the early
stopping procedure determined was optimal in the ﬁrst pass. There are some
subtleties associated with this procedure. For example, there is not a good way
of knowing whether to retrain for the same number of parameter updates or the
same number of passes through the dataset. On the second round of training,
each pass through the dataset will require more parameter updates because the
training set is bigger. Usually, if overﬁtting is a serious concern, you will want to
retrain for the same number of epochs, rather than the same number of parameter
updates. If the primary diﬃculty is optimization rather than generalization, then
retraining for the same number of parameter updates makes more sense (but it is
also less likely that you need to use a regularization method like early stopping
in the ﬁrst place). This algorithm is described more formally in Alg. 7.2.
Algorithm 7.2 A meta-algorithm for using early stopping to determine how long
to train, then retraining on all the data.
Let X(train) and y(train) be the training set.
Split X(train) and y (train) into (X (subtrain) , X(valid) ) and (y(subtrain) , y (valid))
respectively.
Run early stopping (Alg. 7.1) starting from random θ using X(subtrain) and
y(subtrain) for training data and X(valid) and y (valid) for validation data. This
returns i∗, the optimal number of steps.
Set θ to random values again.
Train on X(train) and y (train) for i ∗ steps.
Another strategy for using all of the data is to keep the parameters obtained
from the ﬁrst round of training and then continue training but now using all of
the data. At this stage, we now no longer have a guide for when to stop in terms
of a number of steps. Instead, we can monitor the average loss function on the
validation set, and continue training until it falls below the value of the training
set objective at which the early stopping procedure halted. This strategy avoids
the high cost of retraining the model from scratch, but is not as well-behaved. For
219

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

w2

w2
w

∗

w∗
w(τ)

w̃

w1

w1

Figure 7.4: An illustration of the eﬀect of early stopping (Right) as a form of regularization
on the value of the optimal w, as compared to L2 regularization (Left) discussed in
Sec. 7.2.1.

example, there is not any guarantee that the objective on the validation set will
ever reach the target value, so this strategy is not even guaranteed to terminate.
This procedure is presented more formally in Alg. 7.3.
Algorithm 7.3 Meta-algorithm using early stopping to determine at what objective value we start to overﬁt, then continue training until that value is reached.
Let X(train) and y(train) be the training set.
Split X(train) and y (train) into (X (subtrain) , X(valid) ) and (y(subtrain) , y (valid))
respectively.
Run early stopping (Alg. 7.1) starting from random θ using X(subtrain) and
y(subtrain) for training data and X(valid) and y (valid) for validation data. This
updates θ.
 ← J (θ, X(subtrain), y (subtrain) )
while J (θ, X(valid) , y(valid)) >  do
Train on X (train) and y(train) for n steps.
end while
TODO: coordinate with optimization.tex
Early stopping and the use of surrogate loss functions: A useful property
of early stopping is that it can help to mitigate the problems caused by a mismatch
between the surrogate loss function whose gradient we follow downhill and the
underlying performance measure that we actually care about. For example, 01 classiﬁcation loss has a derivative that is zero or undeﬁned everywhere, so it
is not appropriate for gradient-based optimization. We therefore train with a
220

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

surrogate such as the log-likelihood of the correct class label. However, 0-1 loss
is inexpensive to compute, so it can easily be used as an early stopping criterion.
Often the 0-1 loss continues to decrease for long after the log-likelihood has begun
to worsen on the validation set.
Early stopping is also useful because it reduces the computational cost of the
training procedure. It is a form of regularization that does not require adding
additional terms to the surrogate loss function, so we get the beneﬁt of regularization without the cost of any additional gradient computations. It also means
that we do not spend time approaching the exact local minimum of the surrogate
loss.
How early stopping acts as a regularizer: So far we have stated that early
stopping is a regularization strategy, but we have only backed up this claim by
showing learning curves where the validation set error has a U-shaped curve.
What is the actual mechanism by which early stopping regularizes the model? 8
Early stopping has the eﬀect of restricting the optimization procedure to a
relatively small volume of parameter space in the neighborhood of the initial
parameter value θo . More speciﬁcally, imagine taking τ optimization steps (corresponding to τ training iterations) and taking η as the learning rate. We can
view the product ητ as the reciprocal of a regularization parameter. Assuming the
gradient is bounded, restricting both the number of iterations and the learning
rate limits the volume of parameter space reachable from θ o .
Indeed, we can show how—in the case of a simple linear model with a quadratic
error function and simple gradient descent—early stopping is equivalent to L2
regularization as seen in Section 7.2.1.
In order to compare with classical L 2 regularization, we again consider the
simple setting where we will take as the parameters to be optimized θ = w and
we take a quadratic approximation to the objective function J in the neighborhood
of the empirically optimal value of the weights w ∗ .
1
Jˆ(θ) = J (w ∗ ) + (w − w ∗) >H(θ − θ ∗)
2

(7.25)

where, as before, H is the Hessian matrix of J with respect to w evaluated at
w∗. Given the assumption that w ∗ is a minimum of J (w), we can consider that
H is positive semi-deﬁnite and that the gradient is given by:
∇ wĴ (w) = H (w − w∗ ).
8

(7.26)

Material for this section was taken from Bishop (1995a); Sjöberg and Ljung (1995); for
further details regarding the interpretation of early-stopping as a regularizer, please consult
these works.
221

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Let us consider some initial parameter vector, for simplicity the origin 9 i.e.
w(0) = 0. We will consider updating the parameters via gradient descent:
w (τ) = w(τ−1) − η∇ w J (w(τ−1) )

= w(τ−1) − ηH(w (τ−1) − w∗)

w(τ) − w ∗ = (I − ηH)(w(τ−1) − w∗)

(7.27)
(7.28)
(7.29)

Let us now consider this expression in the space of the eigenvectors of H, i.e.
we will again consider the eigendecomposition of H : H = QΛQ >, where Λ is a
diagonal matrix and Q is an orthonormal basis of eigenvectors.
w(τ) − w∗ = (I − ηQΛQ> )(w(τ−1) − w ∗ )

Q>(w(τ) − w∗) = (I − ηΛ)Q >(w (τ−1) − w∗)

Assuming w 0 = 0, and that |1 − ηλi| < 1, we have after τ training updates,
(TODO: derive the expression below).
Q>w (τ) = [I − (I − ηΛ)τ ]Q >w ∗ .

(7.30)

Now, the expression for Q> w̃ in Eqn. 7.9 for L 2 regularization can rearrange as:
Q>w̃ = (Λ + αI) −1ΛQ > w ∗
Q>w̃ = [I − (Λ + αI)−1 α]Q>w ∗

(7.31)

Comparing Eqs 7.30 and 7.31, we see that if
(I − ηΛ) τ = (Λ + αI)−1 α,
then L2 regularization and early stopping can be seen to be equivalent (at least
under the quadratic approximation of the objective function). Going even further,
by taking logs and using the series expansion for log(1 + x), we can conclude that
if all λ i are small (i.e. ηλi  1 and λi /α  1) then
1
,
ηα
1
α≈ .
τη
τ≈

(7.32)

That is, under these assumptions, the number of training iterations τ plays a role
inversely proportional to the L2 regularization parameter, and the inverse of τ η
plays the role of the weight decay coeﬃcient.
9

For neural networks, to obtain symmetry breaking between hidden units, we cannot initialize
all the parameters at 0, as discussed in Section 8.7.3. However, the argument holds for any other
initial value w(0) .
222

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Parameter values corresponding to directions of signiﬁcant curvature (of the
objective function) are regularized less than directions of less curvature. Of course,
in the context of early stopping, this really means that parameters that correspond
to directions of signiﬁcant curvature tend to learn early relative to parameters
corresponding to directions of less curvature.

7.8

Parameter Tying and Parameter Sharing

Thus far, in this chapter, when we have discussed adding constraints or penalties
to the parameters, we have always done so with respect to a ﬁxed region or point.
For example, L2 regularization (or weight decay) penalizes model parameters
for deviating from the ﬁxed value of zero. However, sometimes we may need
other ways to express our prior knowledge about suitable values of the model
parameters. Sometimes we might not know precisely what values the parameters
should take but we know, from knowledge of the domain and model architecture,
that there should be some dependencies between the model parameters.
A common type of dependency that we often want to express is that certain
parameters should be close to one another.
Consider the following scenario: we have two models performing the same
classiﬁcation task (i.e. the same set of classes) but with somewhat diﬀerent input
distributions. Formally, we have model a with parameters w (a) and model b with
parameters w(b) . The two models map the input to two diﬀerent, but related
outputs: ŷa = f (w(a), x) and ŷ b = g(w(b) , x).
Let us imagine that the tasks are similar enough (perhaps with similar input
and output distributions) that we believe the model parameters should be close to
(a)
(b)
each other, i.e. ∀i, w i should be close to wi . We can leverage this information
through regularization. Speciﬁcally, we can use a parameter norm penalty of the
form: Ω(w (a), w (b)) = kw (a) − w (b) k22 . Here we used an L2 penalty, but other
choices are also possible.
This kind of approach was proposed in Lasserre et al. (2006), where they
regularized the parameters of one model, trained as a classiﬁer in a supervised
paradigm, with the parameters of another model, this one trained in an unsupervised paradigm (to capture the distribution of the observed input data). The
architectures were constructed such that many of the parameters in the classiﬁer
model could be paired to corresponding parameters in the unsupervised model.
While a parameter norm penalty is one way to regularize parameters to be
close to one another, the more popular way is to use constraints: to force sets
of parameters to be equal. This method of regularization is often referred to as
parameter sharing, where we interpret the various models or model components as
sharing a unique set of parameters. A signiﬁcant advantage of parameter sharing
223

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

over regularizing the parameters to be close (via a norm penalty) is that only a
subset of the parameters (the unique set) need to be stored in memory. In certain
models—such as the convolutional neural network—this can lead to signiﬁcant
reduction in the memory footprint of the model.
Convolutional Neural Networks By far the most popular and extensive use
of parameter sharing occurs in convolutional neural networks (CNNs) applied to
computer vision.
Natural images have many statistical properties that are invariant to translation. For example, a photo of a cat remains a photo of a cat if it is translated one
pixel to the right. CNNs take this property into account by sharing parameters
across multiple image locations. The same feature (i.e., a hidden unit with the
same weights) is computed over diﬀerent locations in the input. This means that
we can ﬁnd a cat with the same cat detector whether the cat appears at column
i or column i + 1 in the image.
Parameter sharing has allowed CNNs to dramatically lower the number of
unique model parameters and have allowed them to signiﬁcantly increase network
sizes without requiring a corresponding increase in training data. It remains one
of the best examples of how to eﬀectively incorporate domain knowledge into the
network architecture.
CNNs will be discussed in more detail in Chapter 9.

7.9

Sparse Representations

The previous sections of this chapter were concerned with direct regularization of
the model parameters. In this section we will describe a diﬀerent kind of regularization strategy where the eﬀect on the model parameters is only indirect. Specifically we consider representational sparsity as a form of regularization. We have
already discussed (in sec. 7.2.2) how L 1 penalization induces a sparse parametrization – meaning that a signiﬁcant number of the parameters is zero (or close to it).
Representational sparsity, on the other hand, describes a representation where a
signiﬁcant number of elements are zero (or close to zero). A simpliﬁed view of
this distinction can be illustrated in the context of linear regression:

224

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

TODO: reformat to ﬁt, in two blocks







18
5
15
−9
−3





 
 
 =
 
 

4
0
0
1
1

0 0 −2 0
0
0 −1 0
3
0
5 0
0
0
0
0 0 −1 0 −4
0 0
0 −5 0

y ∈ Rm











2
 3 


 −2 


 −5 


 1 
4
A ∈ Rm×n



x ∈ Rn







−14
1
1
2
23





=



On the left, we have an example of an sparsely parametrized linear regression
model. On the right, we have linear regression with a sparse representation h
of the data x. That is, h is a function of x that, in some sense, represents the
information present in x, but does so with a sparse vector.
The distinction between representation sparsity and parameter sparsity is important, but more generally regularizing the representation and regularizing the
model parameters can be related to each other in a fairly natural way. Ultimately,
the goal of either strategy is to regularize the function mapping the input to the
output space (regardless of the task). Sometimes it is more natural to express
constraints or penalties in the form of prior information. However sometimes the
connection between the parameters and the function being regularized is not well
understood and it is diﬃcult to know, a priori, how to specify a penalty or constraint on the model parameters that matches our prior knowledge of the function
being learning. In these situations it may well be more natural or more eﬀective
to regularize the representations learned by the model.
Representational regularization is mediated by the same sorts of mechanisms
that we have used in parameter regularization. Speciﬁcally both soft norm penalties and hard constraints may be considered - though norm penalties have been
the more popular of the two in the deep learning community.
Norm penalty regularization of representations is performed by adding to the
loss function J a norm penalty on the representation, denoted Ω(h). As before,
we denote the regularized loss function by J˜:
J̃ (θ; X, y) = J (θ; X, y) + αΩ(h)

(7.33)

where α (α ≥ 0) weights the relative contribution of the norm penalty term, with
larger values of α corresponding to more regularization.
Just as an L 1 penalty on the parameters induces parameter sparsity, an L1
penalty on the elements
of the representation induces representational sparsity:
P
Ω(h) = |h| 1 = i |hi |. Of course, the L1 penalty is only one choice of penalty
that can result in a sparse representation. Others include the Student-t penalty
225



3 −1
 4
2

 −1 5

 3
1
−5 4

y∈R

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

(derived from a Student-t distribution prior on the elements of the representation) (Olshausen and Field, 1996; Bergstra, 2011) and KL-divergence penalties (Lee et al., 2008; Larochelle and Bengio, 2008a; Goodfellow et al., 2009) that
are especially useful for representations with elements constrained to lie on the
unit interval. In Sec. 15.8, sparsity inducing penalties are discussed in the context
of auto-encoders.

7.10

Bagging and Other Ensemble Methods

Bagging (short for bootstrap aggregating) is a technique for reducing generalization
error by combining several models (Breiman, 1994). The idea is to train several
diﬀerent models separately, then have all of the models vote on the output for
test examples. This is an example of a general strategy in machine learning
called model averaging. Techniques employing this strategy are known as ensemble
methods.
The reason that model averaging works is that diﬀerent models will usually
not make all the same errors on the test set.
Consider for example a set of k regression models. Suppose that each model
makes an error i on each example, with the errors drawn from a zero-mean multivariate normal distribution with variances E[2i ] = v and covariances E[i j ] = c.
Then the error made by the average prediction of all the ensemble models is
P
1
i i. The expected squared error of the ensemble predictor is
k
1X
E[
i
k
i

=



!2

]


1 X 2 X
E[
i +
i j ]
k2
i

j6=i

1
k−1
v+
c.
k
k
In the case where the errors are perfectly correlated and c = v, this reduces
to v, and the model averaging does not help at all. But in the case where the
errors are perfectly uncorrelated and c = 0, then the expected squared error of
the ensemble is only 1k v. This means that the expected squared error of the
ensemble decreases linearly with the ensemble size. In other words, on average,
the ensemble will perform at least as well as any of its members, and if the
members make independent errors, the ensemble will perform signiﬁcantly better
than its members.
226

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Diﬀerent ensemble methods construct the ensemble of models in diﬀerent
ways. For example, each member of the ensemble could be formed by training a
completely diﬀerent kind of model using a diﬀerent algorithm or objective function. Bagging is a method that allows the same kind of model, training algorithm
and objective function to be reused several times.
Speciﬁcally, bagging involves constructing k diﬀerent datasets. Each dataset
has the same number of examples as the original dataset, but each dataset is
constructed by sampling with replacement from the original dataset. This means
that, with high probability, each dataset is missing some of the examples from the
original dataset and also contains several duplicate examples (in average around
2/3 of the examples from the original dataset are found in the resulting training
set, if it has the same size as the original). Model i is then trained on dataset
i. The diﬀerences between which examples are included in each dataset result in
diﬀerences between the trained models. See Fig. 7.5 for an example.
Neural networks reach a wide enough variety of solution points that they can
often beneﬁt from model averaging even if all of the models are trained on the same
dataset. Diﬀerences in random initialization, random selection of minibatches,
diﬀerences in hyperparameters, or diﬀerent outcomes of non-deterministic implementations of neural networks are often enough to cause diﬀerent members of the
ensemble to make partially independent errors.
Model averaging is an extremely powerful and reliable method for reducing
generalization error. Its use is usually discouraged when benchmarking algorithms
for scientiﬁc papers, because any machine learning algorithm can beneﬁt substantially from model averaging at the price of increased computation and memory.
For this reason, benchmark comparisons are usually made using a single model.
Machine learning contests are usually won by methods using model averaging over dozens of models. A recent prominent example is the Netﬂix Grand
Prize (Koren, 2009).
Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models. For example, a technique called
boosting (Freund and Schapire, 1996b,a) constructs an ensemble with higher capacity than the individual models, and has been applied to boost either whole neural networks (Schwenk and Bengio, 1998), constructively adding neural networks
to the ensemble, or to boost hidden units (Bengio et al., 2006a), constructively
adding hidden units to a single neural network.

7.11

Dropout

Because deep models with many parameters have a high degree of expressive
power necessary to capture complex tasks such as speech or object recognition,
227

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Original dataset

First ensemble member
First resampled dataset

8
Second ensemble member

Second resampled dataset

8
Figure 7.5: A cartoon depiction of how bagging works. Suppose we train an ’8’ detector
on the dataset depicted above, containing an ’8’, a ’6’, and a ’9’. Suppose we make
two diﬀerent resampled datasets. The bagging training procedure is to construct each of
these datasets by sampling with replacement. The ﬁrst dataset omits the ’9’ and repeats
the ’8’. On this dataset, the detector learns that a loop on top of the digit corresponds
to an ’8’. On the second dataset, we repeat the ’9’ and omit the ’6’. In this case, the
detector learns that a loop on the bottom of the digit corresponds to an ’8’. Each of these
individual classiﬁcation rules is brittle, but if we average their output then the detector
is robust, achieving maximal conﬁdence only when both loops of the ’8’ are present.

228

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

they are capable of overﬁtting signiﬁcantly. While this problem can be solved
by using a very large dataset, large datasets are not always available. Dropout
(Srivastava et al., 2014) provides a computationally inexpensive but powerful
method of regularizing a broad family of models.
Dropout can be thought of as a method of making bagging practical for large
neural networks. Bagging involves training multiple models, and evaluating multiple models on each test example. This seems impractical when each model is
a large neural network, since training and evaluating such networks is costly in
terms of runtime and memory. Dropout provides an inexpensive approximation
to training and evaluating a bagged ensemble of exponentially many neural networks.
Speciﬁcally, dropout trains the ensemble consisting of all sub-networks that
can be formed by removing units from an underlying base network. In most modern neural networks, based on a series of aﬃne transformations and nonlinearities,
we can eﬀectively remove a unit from a network by multiplying its output value
by zero. This procedure requires some slight modiﬁcation for models such as radial basis function networks, which take the diﬀerence between the unit’s state
and some reference value. Here, we will present the dropout algorithm in terms
of multiplication by zero for simplicity, but it can be trivially modiﬁed to work
with other operations that remove a unit from the network.
TODO–describe training algorithm, with reference to bagging. should also reference Gal and Gahramani paper’s about Bayesian info: http://mlg.eng.cam.ac.uk/yarin/publi
TODO– include ﬁgures from IG’s job talk TODO– training doesn’t rely on the
model being probabilistic TODO– describe inference algorithm, with reference
to bagging TODO– inference does rely on the model being probabilistic. and
speciﬁcally, exponential family?
For many classes of models that do not have nonlinear hidden units, the weight
scaling inference rule is exact. For a simple example, consider a softmax regression
classiﬁer with n input variables represented by the vector v:


P (y = y | v) = softmax W> v + b .
y

We can index into the family of sub-models by element-wise multiplication of the
input with a binary vector d:


>
P (y = y | v; d) = softmax W (d  v) + b .
y

The ensemble predictor is deﬁned by re-normalizing the geometric mean over all
ensemble members’ predictions:
P̃ensemble (y = y | v)
P˜
(y = y 0 | v)
y0 ensemble

P ensemble(y = y | v) =
229

P

(7.34)

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

where

s Y

P̃ ensemble (y = y | v) =

2

n

d∈{0,1}n

P (y = y | v; d).

To see that the weight scaling rule is exact, we can simplify P̃ ensemble:
s Y
˜
P (y = y | v; d)
Pensemble (y = y | v) = 2 n
d∈{0,1}n

=

2

s Y
n

d∈{0,1} n

v
u Y
u
n
= 2t

d∈{0,1}n

qQ

softmax (W >(d  v) + b) y


> (d  v) + b
exp Wy,:


P
> (d  v) + b
exp
W
y0
y0 ,:



> (d  v) + b
exp W y,:
= r


Q
P
2n
>
d∈{0,1} n
y 0 exp W y 0 ,: (d  v) + b
2n

d∈{0,1} n

Because P̃ will be normalized, we can safely ignore multiplication by factors that
are constant with respect to y:
s Y


> (d  v) + b
P̃ensemble (y = y | v) ∝ 2 n
exp Wy,:
d∈{0,1}n



= exp 

1
2n

X

d∈{0,1} n

= exp





>
Wy,:
(d  v) + b 

1 >
W v+b
2 y,:



Substituting this back into equation 7.34 we obtain a softmax classiﬁer with
weights 12W .
The weight scaling rule is also exact in other settings, including regression
networks with conditionally normal outputs, and deep networks that have hidden
layers without nonlinearities. However, the weight scaling rule is only an approximation for deep models that have non-linearities, and this approximation has not
been theoretically characterized. Fortunately, it works well, empirically. Goodfellow et al. (2013a) found empirically that for deep networks with nonlinearities,
the weight scaling rule can work better (in terms of classiﬁcation accuracy) than
230

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

Monte Carlo approximations to the ensemble predictor, even if the Monte Carlo
approximation is allowed to sample up to 1,000 sub-networks.
Srivastava et al. (2014) showed that dropout is more eﬀective than other standard computationally inexpensive regularizers, such as weight decay, ﬁlter norm
constraints, and sparse activity regularization. Dropout may also be combined
with more expensive forms of regularization such as unsupervised pretraining to
yield an improvement.
One advantage of dropout is that it is very computationally cheap. Using
dropout during training requires only O(n) computation per example per update,
to generate n random binary numbers and multiply them by the state. Depending
on the implementation, it may also require O(n) memory to store these binary
numbers until the backpropagation stage. Running inference in the trained model
has the same cost per-example as if dropout were not used, though we must pay
the cost of dividing the weights by 2 once before beginning to run inference on
examples.
One signiﬁcant advantage of dropout is that it does not signiﬁcantly limit the
type of model or training procedure that can be used. It works well with nearly
any model that uses a distributed representation and can be trained with stochastic gradient descent. This includes feedforward neural networks, probabilistic
models such as restricted Boltzmann machines (Srivastava et al., 2014), and recurrent neural networks (Bayer and Osendorfer, 2014; Pascanu et al., 2014a). On
the other hand, if one wants to take advantage of the regularization eﬀect of
approaches that try to capture the structure of the input distribution, such as
unsupervised pre-training, it is often necessary to change the architecture of the
model.
Though the cost per-step of applying dropout to a speciﬁc model is negligible,
the cost of using dropout in a complete system can be signiﬁcant. Because dropout
is a regularization technique, it reduces the eﬀective capacity of a model. To
oﬀset this eﬀect, we must increase the size of the model. Typically the optimal
validation set error is much lower when using dropout, but this comes at the cost
of a much larger model and many more iterations of the training algorithm. For
very large datasets, regularization confers little reduction in generalization error.
In these cases, the computational cost of using dropout and larger models may
outweigh the beneﬁt of regularization. As a very rough rule of thumb, the beneﬁt
of dropout often becomes questionable when more than around 15 million labeled
training examples are available.
When extremely few labeled training examples are available, dropout is less
eﬀective. Bayesian neural networks (Neal, 1996) outperform dropout on the Alternative Splicing Dataset (Xiong et al., 2011) where fewer than 5,000 examples are
available (Srivastava et al., 2014). When additional unlabeled data is available,
231

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

unsupervised feature learning can gain an advantage over dropout.
TODO– ”Dropout Training as Adaptive Regularization” ? (Wager et al.,
2013) TODO–perspective as L2 regularization TODO–connection to adagrad?
TODO–semi-supervised variant TODO–Baldi paper (Baldi and Sadowski, 2013)
TODO–DWF paper (Warde-Farley et al., 2014) TODO–using geometric mean
is not a problem TODO–dropout boosting, it’s not just noise robustness TODO–
what was the conclusion about mixability (DWF)?
The stochasticity used while training with dropout is not a necessary part
of the model’s success. It is just a means of approximating the sum over all
sub-models. Wang and Manning (2013) derived analytical approximations to
this marginalization. Their approximation, known as fast dropout resulted in
faster convergence time due to the reduced stochasticity in the computation of
the gradient. This method can also be applied at test time, as a more principled
(but also more computationally expensive) approximation to the average over
all sub-networks than the weight scaling approximation. Fast dropout has been
used to match the performance of standard dropout on small neural network
problems, but has not yet yielded a signiﬁcant improvement or been applied to a
large problem.
Dropout has inspired other stochastic approaches to training exponentially
large ensembles of models that share weights. DropConnect is a special case of
dropout where each product between a single scalar weight and a single hidden
unit state is considered a unit that can be dropped (Wan et al., 2013). Stochastic
pooling is a form of randomized pooling (see chapter 9.3) for building ensembles
of convolutional networks with each convolutional network attending to diﬀerent
spatial locations of each feature map. So far, dropout remains the most widely
used implicit ensemble method.
TODO–improved performance with maxout units and probably ReLUs

7.12

Multi-Task Learning

Multi-task learning (Caruana, 1993) is a way to improve generalization by pooling
the examples (which can be seen as soft constraints imposed on the parameters)
arising out of several tasks. In the same way that additional training examples
put more pressure on the parameters of the model towards values that generalize
well, when part of a model is shared across tasks, that part of the model is more
constrained towards good values (assuming the sharing is justiﬁed), often yielding
better generalization.
Figure 7.6 illustrates a very common form of multi-task learning, in which
diﬀerent supervised tasks (predicting yi given x) share the same input x, as well
as some intermediate-level representation hshared capturing a common pool of
232

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

y1

h1

y2

h2

h3

hshared

x
Figure 7.6: Multi-task learning can be cast in several ways in deep learning frameworks
and this ﬁgure illustrates the common situation where the tasks share a common input but
involve diﬀerent target random variables. The lower layers of a deep network (whether it
is supervised and feedforward or includes a generative component with downward arrows)
can be shared across such tasks, while task-speciﬁc parameters (associated respectively
with the weights into and from h1 and h2 in the ﬁgure) can be learned on top of those
yielding a shared representation hshared . The underlying assumption is that there exists
a common pool of factors that explain the variations in the input x, while each task is
associated with a subset of these factors. In the ﬁgure, it is additionally assumed that
top-level hidden units h 1 and h2 are specialized to each task (respectively predicting y 1
and y 2) while some intermediate-level representation hshared is shared across all tasks.
Note that in the unsupervised learning context, it makes sense for some of the top-level
factors to be associated with none of the output tasks (h 3 ): these are the factors that
explain some of the input variations but are not relevant for predicting y 1 or y 2 .

233

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

factors. The model can generally be divided into two kinds of parts and associated
parameters:
1. Task-speciﬁc parameters (which only beneﬁt from the examples of their task
to achieve good generalization). Example: upper layers of a neural network,
in Figure 7.6.
2. Generic parameters, shared across all the tasks (which beneﬁt from the
pooled data of all the tasks). Example: lower layers of a neural network, in
Figure 7.6.
Improved generalization and generalization error bounds (Baxter, 1995) can be
achieved because of the shared parameters, for which statistical strength can be
greatly improved (in proportion with the increased number of examples for the
shared parameters, compared to the scenario of single-task models). Of course this
will happen only if some assumptions about the statistical relationship between
the diﬀerent tasks are valid, i.e., that there is something shared across some of
the tasks.
From the point of view of deep learning, the underlying prior regarding the
data is the following: among the factors that explain the variations observed in
the data associated with the diﬀerent tasks, some are shared across two or more
tasks.

7.13

Adversarial Training

In many cases, neural networks have begun to reach human performance when
evaluated on an i.i.d. test set. It is natural therefore to wonder whether these
models have obtained a true human-level understanding of these tasks. In order
to probe the level of understanding a network has of the underlying task, we
can search for examples that the model misclassiﬁes. Szegedy et al. (2014b)
found that even neural networks that perform at human level accuracy have a
nearly 100% error rate on examples that are intentionally constructed by using
an optimization procedure to search for an input x 0 near a data point x such that
the model output is very diﬀerent at x0 . In many case, x0 can be so similar to x
that a human observer cannot tell the diﬀerence between the original example and
the adversarial example, but the network can make highly diﬀerent predictions.
See Fig. 7.7 for an example.
Adversarial examples have many implications, for example, in computer security, that are beyond the scope of this chapter. However, they are interesting in
the context of regularization because one can reduce the error rate on the original
i.i.d. test set by training on adversarially perturbed examples from the training
set (Szegedy et al., 2014b).
234

CHAPTER 7. REGULARIZATION OF DEEP OR DISTRIBUTED MODELS

+ .007 ×

x
y =“panda”
w/ 57.7%
conﬁdence

=

sign(∇x J (θ, x, y))
“nematode”
w/ 8.2%
conﬁdence

x+
 sign(∇x J (θ, x, y))
“gibbon”
w/ 99.3 %
conﬁdence

Figure 7.7: A demonstration of adversarial example generation applied to GoogLeNet
(Szegedy et al., 2014a) on ImageNet. By adding an imperceptibly small vector whose
elements are equal to the sign of the elements of the gradient of the cost function with
respect to the input, we can change GoogLeNet’s classiﬁcation of the image. Reproduced
with permission from Goodfellow et al. (2014b).

Goodfellow et al. (2014b) showed that one of the primary causes of these adversarial examples is excessive linearity. Neural networks are built out of primarily
linear building blocks, and in some empirical experiments the overall function they
implement proves to be highly linear as a result. These linear functions are easy
to optimize. Unfortunately, the value of a linear function can change very rapidly
if it has numerous inputs. If we change each input by , then a linear function
with weights w can change by as much as |w|, which can be a very large amount
if w is high-dimensional. Adversarial training discourages this highly sensitive
locally linear behavior by encouraging the network to be locally constant in the
neighborhood of the training data. This can be seen as a way of introducing
explicitly the local smoothness prior into supervised neural nets.
This phenomenon helps to illustrate the power of using a large function family
in combination with aggressive regularization. Purely linear models, like logistic
regression, are not able to resist adversarial examples because they are forced to
be linear. Neural networks are able to represent functions that can range from
nearly linear to nearly locally constant and thus have the ﬂexibility to capture
linear trends in the training data while still learning to resist local perturbation.

235

Chapter 8

Optimization for Training
Deep Models
Deep learning algorithms involve optimization in many contexts. For example, we
often solve optimization problems analytically in order to prove that an algorithm
has a certain property. Inference in a probabilistic model can be cast as an
optimization problem. Of all of the many optimization problems involved in deep
learning, the most diﬃcult is neural network training. It is quite common to
invest days to months of time on hundreds on machines in order to solve even a
single instance of the neural network training problem. Because this problem is
so important and so expensive, a specialized set of optimization techniques have
been developed for solving it. This chapter presents these optimization techniques
for neural network training.
If you’re unfamiliar with the basic principles of gradient-based optimization,
we suggest reviewing Chapter 4. That chapter includes a brief overview of numerical optimization in general.
This chapter focuses on one particular case of optimization: minimizing an
objective function J(X (train) , θ) with respect to the model parameters θ.

8.1

Optimization for Model Training

Optimization algorithms used for training of deep models diﬀer from traditional
optimization algorithms in several ways. Machine learning usually acts indirectly—
we care about some performance measure P that we do not know how to directly
inﬂuence, so instead we reduce some objective function J (θ) in hope that it will
improve P . This is in contrast to pure optimization, where minimizing J is a goal
in and of itself. Optimization algorithms for training deep models also typically
include some specialization on the speciﬁc structure of machine learning objective
236

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

functions.

8.1.1

Empirical Risk Minimization

TODO: be more clear about what p is here, and also emphasize it. Suppose that
we have input feature x, targets y, and some loss function L(x, y). Our ultimate
goal is to minimize Ex,y∼p(x,y) [L(x, y)]. This quantity is known as the risk. If we
knew the true distribution p(x, y), this would be an optimization task solveable
by an optimization algorithm. However, when we do not know p(x, y) but only
have a training set of samples from it, we have a machine learning problem.
The simplest way to convert a machine learning problem back into an optimization problem is to minimize the expected loss on the training set. This means
replacing the true distribution p(x, y) with the empirical distribution p̂(x, y) deﬁned by the training set. We now minimize the empirical risk
m
1 X
L(x(i) , y (i) )
Ex,y∼p̂(x,y) [L(x, y)] =
m
i=1

where m is the number of training examples.
This process is known as empirical risk minimization. In this setting, machine
learning is still very similar to straightforward optimization. Rather than optimizing the risk directly, we optimize the empirical risk, and hope that the risk
decreases signiﬁcantly as well. A variety of theoretical results establish conditions
under which the true risk can be expected to decrease by various amounts.
However, empirical risk minimization is prone to overﬁtting. Models with
high capacity can simply memorize the training set. In many cases, empirical
risk minimization is not really feasible. The most eﬀective modern optimization
algorithms are based on gradient descent, but many useful loss functions, such
as 0-1 loss, have no useful derivatives (the derivative is either zero or undeﬁned
everywhere). These two problems mean that, in the context of deep learning, we
rarely use empirical risk minimization. Instead, we must use a slightly diﬀerent
approach, in which the quantity that we actually optimize is even more diﬀerent
from the quantity that we truly want to optimize.
TODO– make sure 0-1 loss is deﬁned and in the index

8.1.2

Surrogate Loss Functions

TODO– coordinate with regularization.tex, surrogate loss functions now appear
to be ﬁrst introduced in the context of early stopping
TODO– in some cases, surrogate loss function actually results in being able
to learn more. for example, test 0-1 loss continues to decrease for a long time
after train 0-1 loss has reached zero when training using log-likelihood surrogate
237

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

In some cases, using a surrogate loss function allows us to extract more information
A very important diﬀerence between optimization in general and optimization
as we use it for training algorithms is that training algorithms do not usually
halt at a local minimum. Instead, using a regularization method known as early
stopping (see Sec. 7.7), they halt whenever overﬁtting begins to occur. This
is often in the middle of a wide, ﬂat region, but it can also occur on a steep
part of the surrogate loss function. This is in contrast to general optimization,
where converge is usually deﬁned by arriving at a point that is very near a (local)
minimum.

8.1.3

Batch and Minibatch Algorithms

One aspect of machine learning algorithms that separates them from general
optimization algorithms is that the objective function usually decomposes as a
sum over the training examples. Optimization algorithms for machine learning
typically compute each update to the parameters based on a subset of the terms
of the objective function, not based on the complete objective function itself.
For example, maximum likelihood estimation problems, when viewed in log
space, decompose into a sum over each example:
θ ML = arg max
θ

m
X

log pmodel (x (i) ; θ).

i=1

TODO: after agreeing on probability distribution format, make this consistent
with eq:max-log-lik
Maximizing this sum is equivalent to maximizing the expectation over the
empirical distribution deﬁned by the training set:
J (θ) = Ex∼p̂ data log pmodel (x; θ).

(8.1)

Most of the properties of the objective function J used by most of our optimization algorithms are also expectations over the training set. For example, the
most commonly used property is the gradient:
∇ θJ (θ) = Ex∼p̂ data∇ θ log pmodel(x; θ).

(8.2)

Computing this expectation exactly is very expensive because it requires evaluating the model on every example in the entire dataset. In practice, we can
compute these expectations by randomly sampling a small number of examples
from the dataset, then taking the average over only those examples.
TODO: make sure standard error of the mean is covered in statistical estimators section of chap 5
238

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Recall that the standard error of the mean estimated from n samples is given
√
by σ̂/ n, where σ̂ is the estimated standard deviation. This means that there are
less than linear returns to using more examples to estimate the gradient. Compare
two hypothetical estimates of the gradient, one based on 100 examples and another
based on 10,000 examples. The latter requires 100 times more computation than
the former, but reduces the standard error of the mean only by a factor of 10. Most
optimization algorithms converge much faster (in terms of total computation, not
in terms of number of updates) if they are allowed to rapidly compute approximate
estimates of the gradient rather than slowly computing the exact gradient.
Another way to intuitively understand the appeal of statistically estimating
the gradient from a small number of samples is to consider that there may be
redundancy in the training set. In the worst case, all m samples in the training set
could be identical copies of each other. A sampling-based estimate of the gradient
could compute the correct gradient with 1 sample, using m times less computation
than the naive approach. In practice, we are unlikely to truly encounter this
worst-case situation, but we may ﬁnd large numbers of examples that all make
very functionally similar contributions to the gradient.
Optimization algorithms that use the entire training set are called batch methods, because they process all of the training examples simultaneously in a large
batch. Optimization algorithms that use only a single example at a time are sometimes called stochastic or sometimes online methods (the term online is usually
reserved for the case where the examples are drawn from a stream of continually
created examples rather than from a ﬁxed-size training set over which several
passes will be made). Most algorithms used for deep learning fall somewhere in
between, using more than one but less than all of the training examples. These
were traditionally called minibatch or minibatch stochastic methods and it is now
common to simply call them stochastic methods.
The canonical example of a stochastic method is stochastic gradient descent,
presented in detail in Section 8.3.2.
Minibatch sizes are generally driven by the following factors:
• Larger batches provide a more accurate estimate of the gradient, but with
less than linear returns.
• Multicore architectures are usually underutilized by extremely small batches.
This motivates using some absolute minimum batch size, below which there
is no reduction in the time to process a minibatch.
• If all examples in the batch are to be processed in parallel (as is typically
the case), then the amount of memory scales with the batch size. For many
hardware setups this is the limiting factor in batch size.
239

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

• Some kinds of hardware achieve better runtime with speciﬁc sizes of arrays.
Especially when using GPU, it is common for power of 2 batch sizes to oﬀer
better runtime. Typical power of 2 batch sizes range from 32 to 256, with
16 sometimes being attempted for large models.
• Small batches can oﬀer a regularizing eﬀect. Generalization error is often
best for a batch size of 1, though this might take a very long time to train
and require a small learning rate to maintain stability.
Diﬀerent kinds of algorithms use diﬀerent kinds of information in diﬀerent
ways, and some are more sensitive to sampling error than others. Methods that
compute updates based only on the gradient g are usually relatively robust and
can handle smaller batch sizes like 100. Second order methods, that use also the
Hessian matrix H and compute updates such as H −1g, typically require much
larger batch sizes like 10,000. To see this, consider that when H and its inverse
are poorly conditioned, then very small changes in the estimate of g can cause
large changes in the update H −1 g. This is further compounded by estimation
error in H itself.
Computing the expected gradient from a set of samples requires that those
samples be independent. Many datasets are most naturally arranged in a way
where successive examples are highly correlated. For example, we might have a
dataset of medical data with a long list of blood sample test results. This list
might be arranged so that ﬁrst we have ﬁve blood samples taken at diﬀerent times
from the ﬁrst patient, then we have three blood samples taken from the second
patient, then the blood samples from the third patient, and so on. If we were
to draw examples in order from this list, then each of our minibatches would
be extremely biased, because it would represent primarily one patient out of the
many patients in the dataset. In cases such as these where the order of the dataset
holds some signiﬁcance, it is necessary to shuﬄe the examples before selecting
minibatches. For very large datasets, for example datasets containing billions
of examples on a datacenter, it can be impractical to sample examples truly
uniformly at random every time we want to construct a minibatch. Fortunately,
in practice it is usually suﬃcient to shuﬄe the order of the dataset once and then
store it in shuﬄed fashion. This will impose a ﬁxed set of possible minibatches
of consecutive examples that all models trained thereafter will use, and each
individual model will be forced to re-use this ordering every time it passes through
the training data, however, this deviation from true random selection does not
seem to have a signiﬁcant detrimental eﬀect.
Many optimization problems in machine learning decompose over examples
well enough that we can compute entire separate updates over diﬀerent examples
in parallel. In other words, we can compute the update that minimizes J (x) for
240

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

one minibatch of examples x at the same time that we compute the update for
several other minibatches. This is discussed further in Chapter 12.1.3.

8.1.4

Generalization and Early Stopping

In machine learning, typically we minimize a objective function deﬁned as an
expectation of some per-example loss across the training set:
Ĵ (θ) = Ex∼p̂data L(x, θ).
However, we would usually prefer to minimize the corresponding objective function where the expectation is taken across the data generating distribution rather
than just the ﬁnite training set:
J (θ) = Ex∼pdata L(x, θ).
In other words, we care about generalization error, not training error.
Usually, we use an optimization algorithm based on minibatch estimates of
the gradient. During the ﬁrst stages of learning, this is equivalent to minimizing
the generalization error directly. After we have used up the training data and
begin to repeat minibatches, the two criteria are diﬀerent.
This is the main way in which optimization for machine learning is actually
diﬀerent from traditional optimization, rather than just a special case of optimization. Many neural network optimization algorithms are implicitly designed
in ways that are intended to yield better results in terms of generalization error,
even if they perform worse as an optimization algorithm (yield worse training
error or minimize the training error more slowly).
Some neural network algorithms actually change all of their updates to account
for the uncertainty in the true generalization loss. For example, an algorithm
called TONGA uses the covariance between gradients across diﬀerent examples
to approximately minimize the generalization loss (Le Roux et al., 2008).
Most neural network training algorithms are actually designed to be traditional optimization algorithms that minimize the training loss, apart from one
change: the convergence criterion.

8.2

Challenges in Optimization

TODO: write this section. make title more speciﬁc to neural nets?

8.2.1

Local Minima

TODO check whether this is already covered in numerical.tex
241

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

8.2.2

Ill-Conditioning

TODO this is deﬁnitely already covered in numerical.tex

8.2.3

Plateaus, Saddle Points, and Other Flat Regions

The long-held belief that neural networks are hopeless to train because they are
fraught with local minima has been one of the reasons for the “neural networks
winter” in the 1995-2005 decade. Indeed, one can show that there may be an
exponentially large number of local minima, even in the simplest neural network
optimization problems (Sontag and Sussman, 1989; Brady et al., 1989; Gori and
Tesi, 1992).
Theoretical work has shown that saddle points (and the ﬂat regions surrounding them) are important barriers to training neural networks, and may be more
important than local minima.
Explain (Dauphin et al., 2014; Choromanska et al., 2014)

8.2.4

Cliﬀs and Exploding Gradients

Whereas the issues of ill-conditioning and saddle points discussed in the previous
sections arise because of the second-order structure of the objective function (as
a function of the parameters), neural networks involve stronger non-linearities
which do not ﬁt well with this picture. In particular, the second-order Taylor
series approximation of the objective function yields a symmetric view of the
landscape around the minimum, oriented according to the axes deﬁned by the
principal eigenvectors of the Hessian matrix. (TODO: REFER TO A PLOT
FROM THE ILL-CONDITIONING SECTION WITH COUNTOURS OF VALLEY). Second-order methods and momentum or gradient-averaging methods introduced in Section 8.5 are able to reduce the diﬃculty due to ill-conditioning by
increasing the size of the steps in the low-curvature directions (the “valley”, in
Figure 8.1) and decreasing the size of the steps in the high-curvature directions
(the steep sides of the valley, in the ﬁgure).

242

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Figure 8.1: One theory about the neural network optimization is that poorly conditioned
Hessian matrices cause much of the diﬃculty in training. In this view, some directions
have a high curvature (second derivative), corresponding to the rising sides of the valley,
and other directions have a low curvature, corresponding to the smooth slope of the
valley. Most second-order methods, as well as momentum or gradient averaging methods
are meant to address that problem, by increasing the step size in the direction of the
valley (where it’s most paying in the long run to go) and decreasing it in the directions
of steep rise, which would otherwise lead to oscillations. The objective is to smoothly go
down, staying at the bottom of the valley.

However, although classical second order methods can help, as shown in Figure 8.2, due to higher order derivatives, the objective function may have a lot
more non-linearity, which often does not have the nice symmetrical shapes that
the second-order “valley” picture builds in our mind. Instead, there are cliﬀs
where the gradient rises sharply. When the parameters approach a cliﬀ region,
the gradient update step can move the learner towards a very bad conﬁguration,
ruining much of the progress made during recent training iterations.

243

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Figure 8.2: Contrary to what is shown in Figure 8.1, the objective function for highly
non-linear deep neural networks or for recurrent neural networks is typically not made of
symmetrical sides. As shown in the ﬁgure, there are sharp non-linearities that give rise
to very high derivatives in some places. When the parameters get close to such a cliﬀ
region, a gradient descent update can catapult the parameters very far, possibly ruining
a lot of the optimization work that had been done. Figure graciously provided by Razvan
Pascanu (Pascanu, 2014).

As illustrated in Figure 8.3, the cliﬀ can be dangerous whether we approach it
from above or from below, but fortunately there are some fairly straightforward
heuristics that allow one to avoid its most serious consequences. The basic idea
is to limit the size of the jumps that one would make. Indeed, one should keep
in mind that when we use the gradient to make an update of the parameters, we
are relying on the assumption of inﬁnitesimal moves. There is no guarantee that
making a ﬁnite step of the parameters θ in the direction of the gradient will yield
an improvement. The only thing that is guaranteed is that a small enough step
in that direction will be helpful. As we can see from Figure 8.3, in the presence
of a cliﬀ (and in general in the presence of very large gradients), the decrease
in the objective function expected from going in the direction of the gradient is
only valid for a very small step. In fact, because the objective function is usually
bounded in its actual value (within a ﬁnite domain), when the gradient is large at
θ, it typically only remains like this (especially, keeping its sign) in a small region
around θ. Otherwise, the value of the objective function would have to change
a lot: if the slope was consistently large in some direction as we would move in
that direction, we would be able to decrease the objective function value by a
244

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

very large amount by following it, simply because the total change is the integral
over some path of the directional derivatives along that path.

Figure 8.3: To address the presence of cliﬀs such as shown in Figure 8.2, a useful heuristic
is to clip the magnitude of the gradient, only keeping its direction if its magnitude is above
a threshold (which is a hyperparameter, although not a very critical one). This helps to
avoid the destructive big moves which would happen when approaching the cliﬀ, either
from above or from below. Figure graciously provided by Razvan Pascanu (Pascanu,
2014).

The gradient clipping heuristics are described in more detail in Section 10.8.7.
The basic idea is to bound the magnitude of the update step, i.e., not trust the
gradient too much when it is very large in magnitude. The context in which such
cliﬀs have been shown to arise in particular is that of recurrent neural networks,
when considering long sequences, as discussed in the next section.

8.2.5

Vanishing and Exploding Gradients - An Introduction to
the Issue of Learning Long-Term Dependencies

Parametrized dynamical systems such as recurrent neural networks (Chapter 10)
face a particular optimization problem which is diﬀerent but related to that of
training very deep networks. We introduce this issue here and refer to reader to
Section 10.8 for a deeper treatment along with a discussion of approaches that
have been proposed to reduce this diﬃculty.
Exploding or Vanishing Product of Jacobians
The simplest explanation of the problem, which is shared among very deep nets
and recurrent nets, is that in both cases the ﬁnal output is the composition of a
large number of non-linear transformations. Even though each of these non-linear
stages may be relatively smooth (e.g. the composition of an aﬃne transformation
with a hyperbolic tangent or sigmoid), their composition is going to be much
245

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

“more non-linear”, in the sense that derivatives through the whole composition
will tend to be either very small or very large, with more ups and downs. TODO:
the phrase ”ups and downs” has a connotation of speciﬁcally good things and bad
things happening over time, use a diﬀerent phrase. this section is also sloppily
conﬂating many diﬀerent ideas, just having both areas of large derivatives and
small derivatives does not mean there are lots of ups and downs, consider the
function (1 − x ∗ y) 2, which has small derivatives near the origin and the global
minima, much larger derivatives in between This arises simply because the Jacobian (matrix of derivatives) of a composition is the product of the Jacobians of
each stage, i.e., if
f = fT ◦ fT −1 ◦ . . . , f2 ◦ f 1
then the Jacobian matrix of derivatives of f (x) with respect to its input vector x
is the product
f0 = f 0T f 0T −1 . . . , f20 f1
(8.3)
where
f0=

∂f(x)
∂x

f 0t =

∂ft (at)
∂at

and

where a t = ft−1(f t−2(. . . , f2 (f1(x)))), i.e. composition has been replaced by
matrix multiplication. This is illustrated in Figure 8.4. TODO: the above sentence
is incredibly long, split it up and probably put the deﬁnitions in the opposite order.
It also seems strange to say composition is ”replaced” by matrix multiplication,
more like composition in forward prop implies matrix multiplication in backprop

!!

…"

!!

="

Figure 8.4: When composing many non-linearities (like the activation non-linearity in a
deep or recurrent neural network), the result is highly non-linear, typically with most
of the values associated with a tiny derivative, some values with a large derivative, and
many ups and downs (not shown here).

TODO: can we avoid using capital letters for scalars here? (T) In the scalar
case, we can imagine that multiplying many numbers together tends to be either
very large or very small. In the special case where all the numbers in the product
246

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

have the same value α, this is obvious, since α T goes to 0 if α < 1 and goes to
∞ if α > 1, as T increases. The more general case of non-identical numbers be
understood by taking the logarithm of these numbers, considering them to be
random, and computing the variance of the sum of these logarithms. Clearly,
although some cancellation can happen, the variance grows with T , and in fact
if those numbers are independent, the variance grows linearly
with T , i.e., the
√
size of the sum (which is the standard deviation) grows as T , which means that
the product grows roughly as e T (consider the variance of log-normal variate X
if log X is normal with mean 0 and variance T ).
It would be interesting to push this analysis to the case of multiplying square
matrices instead of multiplying numbers, but one might expect qualitatively similar conclusions, i.e., the size of the product somehow grows with the number of
matrices, and that it grows exponentially. In the case of matrices, one can get
a new form of cancellation due to leading eigenvectors being well aligned or not.
The product of matrices will blow up only if, among their leading eigenvectors
with eigenvalue greater than 1, there is enough “in common” (in the sense of the
appropriate dot products of leading eigenvectors of one matrix and another).
However, this analysis was for the case where these numbers are independent.
In the case of an ordinary recurrent neural network (developed in more detail in
Chapter 10), these Jacobian matrices are highly related to each other. Each layerwise Jacobian is actually the product of two matrices: (a) the recurrent matrix W
and (b) the diagonal matrix whose entries are the derivatives of the non-linearities
associated with the hidden units, which vary depending on the time step. This
makes it likely that successive Jacobians have similar eigenvectors, making the
product of these Jacobians explode or vanish even faster.
Consequence for Recurrent Networks: Diﬃculty of Learning LongTerm Dependencies
The consequence of the exponential convergence of these products of Jacobians
towards either very small or very large values is that it makes the learning of
long-term dependencies particularly diﬃcult, as we explain below and was independently introduced in Hochreiter (1991) and Bengio et al. (1993, 1994) for the
ﬁrst time.
TODO: why the capital F? Can we use lowercase instead? This also appears
in rnn.tex Consider a fairly general parametrized dynamical system (which includes classical recurrent networks as a special case, as well as all their known
variants), processing a sequence of inputs, x 1, . . . , xt , . . ., involving iterating over
the transition operator:
st = F θ (st−1 , xt )
(8.4)
where st is called the state of the system and F θ is the recurrent function that
247

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

maps the previous state and current input to the next state. The state can be
used to produce an output via an output function,
ot = gω(st ),

(8.5)

TODO: could we avoid the capital T? and a loss Lt is computed at each time step
t as a function of ot and possibly of some targets yt . Let us consider the gradient
of a loss L T at time T with respect to the parameters θ of the recurrent function
T
Fθ . One particular way to decompose the gradient ∂L
∂θ using the chain rule is
the following:
∂L T X ∂LT ∂st
=
∂θ
∂st ∂θ
t≤T

∂L T X ∂LT ∂sT ∂Fθ (s t−1, xt)
=
∂θ
∂sT ∂st
∂θ

(8.6)

t≤T

where the last Jacobian matrix only accounts for the immediate eﬀect of θ as a
parameter of F θ when computing s t = Fθ (s t−1, x t), i.e., not taking into account
the indirect eﬀect of θ via s t−1 (otherwise there would be double counting and the
result would be incorrect). To see that this decomposition is correct, please refer
to the notions of gradient computation in a ﬂow graph introduced in Section 6.4,
and note that we can construct a graph in which θ inﬂuences each st , each of
T
which inﬂuences L T via sT . Now let us note that each Jacobian matrix ∂s
∂s t can
be decomposed as follows:
∂sT
∂sT ∂sT −1
∂s t+1
=
...
∂s t
∂sT −1 ∂sT −2
∂st

(8.7)

which is of the same form as Eq. 8.3 discussed above, i.e., which tends to either
vanish or explode.
T
As a consequence, we see from Eq. 8.6 that ∂L
∂θ is a weighted sum of terms
over spans T −t, with weights that are exponentially smaller (or larger) for longerterm dependencies relating the state at t to the state at T . As shown in Bengio
et al. (1994), in order for a recurrent network to reliably store memories, the Jat
cobians ∂s∂st−1
relating each state to the next must have a determinant that is less
than 1 (i.e., yielding to the formation of attractors in the corresponding dynamical system). Hence, when the model is able to capture long-term dependencies
it is also in a situation where gradients vanish and long-term dependencies have
an exponentially smaller weight than short-term dependencies in the total gradient. It does not mean that it is impossible to learn, but that it might take a
very long time to learn long-term dependencies, because the signal about these
dependencies will tend to be hidden by the smallest ﬂuctuations arising from
248

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

short-term dependencies. In practice, the experiments in Bengio et al. (1994)
show that as we increase the span of the dependencies that need to be captured,
gradient-based optimization becomes increasingly diﬃcult, with the probability
of successful learning rapidly reaching 0 after only 10 or 20 steps in the case of
the vanilla recurrent net and stochastic gradient descent (Section 8.3.2).
For a deeper treatment of the dynamical systems view of recurrent networks,
consider Doya (1993); Bengio et al. (1994); Siegelmann and Sontag (1995), with a
review in Pascanu et al. (2013a). Section 10.8 discusses various approaches that
have been proposed to reduce the diﬃculty of learning long-term dependencies
(in some cases allowing one to reach to hundreds of steps), but it remains one of
the main challenges in deep learning.

8.2.6

Inexact Gradients

Most optimization algorithms are primarily motivated by the case where we have
exact knowledge of the gradient or Hessian matrix. In practice, we usually only
have a noisy or even biased estimate of these quantities. Nearly every deep learning algorithm relies on sampling best estimates at least insofar as using a minibatch of training examples to compute the gradient.
In other cases, the objective function we want to minimize is actually intractable, and we can only approximate its gradient. These issues mostly arise
with the more advanced models in Part III of this book. For example, persistent
contrastive divergence gives a technique for approximating the gradient of the
intractable log-likelihood of a Boltzmann machine.
Various neural network optimization algorithms are designed to account for
these imperfections in the gradient estimate. One can also avoid the problem by
choosing a surrogate loss function that is easier to approximate than the true loss.

8.2.7

Theoretical Limits of Optimization

Several theoretical results show that there are limits on the performance of any
optimization algorithm we might design for neural networks (Blum and Rivest,
1992; Judd, 1989; Wolpert and MacReady, 1997). Typically these results have
little bearing on the use of neural networks in practice.
Some theoretical results apply only to the case where the units of a neural network output discrete values. However, most neural network units output
smoothly increasing values that make optimization via local search feasible. Some
theoretical results show that there exist problem classes that are intractable, but
it can be diﬃcult to tell whether a particular problem falls into that class. Other
results show that ﬁnding a solution for a network of a given size is intractable, but
in practice we can ﬁnd a solution easily by using a larger network for which many
249

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

more parameter settings correspond to an acceptable solution. Moreover, in the
context of neural network training, we usually do not care about ﬁnding the exact
minimum of a function, but only in reducing its value suﬃciently to obtain good
generalization error. Theoretical analysis of whether an optimization algorithm
can accomplish this goal is extremely diﬃcult. Developing more realistic bounds
on the performance of optimization algorithms therefore remains an important
goal for machine learning research.

8.3

Optimization Algorithms I: Basic Algorithms

In Sec. 6.4, we discussed the backpropagation algorithm (backprop): that is,
how to eﬃciently compute the gradient of the loss with respect to the model
parameters. The backpropagation algorithm does not specify how we use this
gradient to update the weights of the model.
In this section we introduce a number of gradient-based learning algorithms
that have been proposed to optimize the parameters of deep learning models.

8.3.1

Gradient Descent

Gradient descent is the most basic gradient-based algorithm one might apply to
train a deep model. The algorithm is also sometimes called batch gradient descent
in neural network papers because it updates the parameters only after having seen
a batch of all the training examples. It involves updating the model parameters θ
(in the case of a deep neural network, these parameters would include the weights
and biases associated with each layer) with a small step in the direction of the
gradient of the objective function, i.e., for neural networks that includes the terms
for all the training examples as well as any regularization terms. For the case of
supervised learning with data pairs [x(t) , y(t) ] we have:
X
θ ← θ + ∇θ
L(f (x (t); θ), y(t) ; θ),
(8.8)
t

where  is the learning rate, an optimization hyperparameter that controls the size
of the step the the parameters take in the direction of the gradient. Of course,
following the gradient in this way is only guaranteed to reduce the loss in the
limit as  → 0.
In fact, once the algorithm has reached a convex basin of attraction, convergence is fast, with the magnitude of the diﬀerence to the minimum decreasing in
o(1/k) after k steps (Bertsekas, 2004). In addition, the number of training iteration to reach a particular error level is proportional to the ratio of the largest to
the smallest eigenvalue of the Hessian matrix (second derivatives of the objective
250

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

function with respect to the parameters). Very slow convergence can thus occur
when the Hessian is ill-conditioned.
There exists a learning rate value  max such that if  < max and the objective
function is locally convex (around the minimum to which we converge), gradient
descent is guaranteed to converge to a minimum (local or global) However, gradient descent is rarely used in machine learning because it does not exploit the
particular structure of the objective function, which is written as a sum of generally i.i.d. terms associated with each training example. Exploiting this structure
is what allows stochastic gradient descent, discussed next, to achieve much faster
convergence, as analyzed theoretically by Bottou and Bousquet (2008).

8.3.2

Stochastic Gradient Descent

Stochastic Gradient Descent (SGD) and its variants are probably the most used
optimization algorithm for machine learning in general and for deep learning in
particular. It is very similar to (batch) gradient descent except that it uses a
stochastic (i.e. noisy) estimator of the gradient to perform its update. With
machine learning, this is typically obtained by sampling one or a small subset of
m training examples and computing their gradient, as shown in Algorithm 8.1.
When the examples are i.i.d., it means that the expected value E[ĝ] of this estimated gradient (averaging over diﬀerent draws of the examples used to compute
the estimated gradient) equals the true gradient, i.e., the gradient estimator is
unbiased:
E[ĝ] = g
where g is the total gradient.
Algorithm 8.1 Stochastic gradient descent (SGD) update at training iteration
k
Require: Learning rate η.
Require: Initial parameter θ
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set ĝ = 0
for i = 1 to m do
Compute gradient estimate: gˆ ← gˆ + ∇ θL(f (x(i) ; θ), y (i) )/m
end for
Apply update: θ ← θ k − ηˆ
g
end while
When m = 1, Algorithm 8.1 is sometimes called online gradient descent. When
m > 1 but m a fraction of the number of training examples, this algorithm is
251

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

sometimes called minibatch SGD. See Section 8.1.3 about minibatch optimization
algorithms.
A crucial hyper-parameter that is introduced when one applies SGD is the
learning rate (η k in Algorithm 8.1). Whereas ordinary gradient descent can work
with a ﬁxed learning rate, it is necessary to allow SGD’s learning rate to decrease
at an appropriate rate during training, if one wants to converge to a minimum.
This is because the SGD gradient estimator introduces a source of noise (the
random sampling of m training examples) that does not become 0 even when
we arrive at a minimum (whereas the true gradient becomes small and then 0
when we approach and reach a minimum). A suﬃcient condition to guarantee
convergence is that
∞
X
k=1
∞
X
k=1

η k = ∞,

and

η 2k < ∞.

(8.9)

For a deeper treatment of SGD, see Bottou (1998), which covers the case when
the objective function is not convex in the parameters.

8.3.3

Online Gradient Descent Minimizes Generalization Error

TODO: move this subsection elsewhere (IG votes to merge it into Batch and Minibatch Algorithms and/or Generalization and Early Stopping) see e-mail thread
”Feedback on recent commits”
Let us consider the sense of the term “online” referring to case where examples
or minibatches are drawn from a stream of data. In other words, instead of a
ﬁxed-size training set, we are in the situation similar to a living being which
sees new a example at each instant, with every example (x, y) coming from the
data generating distribution p(x, y) (the same argument could be made in the
unsupervised case, where there is no y).
Consider a loss function L(f (x; θ), y) whose expected value over p(x, y) we
would like to minimize with respect to the parameters θ. In other words, the
generalization error of the current predictor f (·; θ) with parameters θ is
Z
J (θ) =
L(f (x; θ), y)dp(x, y)
and under continuity assumptions of p, its exact gradient is
g=

∂J(θ)
=
∂θ

∂L(f(x; θ), y)
dp(x, y),
∂θ
Z

252

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

similarly to what we have seen in Eqs. 8.1 and 8.2 for the log-likelihood. Hence,
we can obtain an unbiased estimator of the exact gradient of generalization error by sampling one example (x, y) (or equivalently a minibatch) from the data
generating process p, and computing the gradient of the loss with respect to the
parameters for that example (or that minibatch),
ĝ =

∂L(f(x; θ), y)
.
∂θ

It should be clear that this stochastic gradient estimator ĝ is a noisy but unbiased
estimator of the exact gradient of generalization error, g. Hence, updating θ in
the direction of ĝ performs SGD on the generalization error. Of course, this
is only possible if the examples are not repeated (the usual scenario for many
machine learning applications). With some datasets growing rapidly in size, faster
than computing power, this online scenario is actually quite plausible. In that
setting, overﬁtting is not an issue, while underﬁtting and computational eﬃciency
matter a lot. See also Bottou and Bousquet (2008) for a discussion of the eﬀect
of computational bottlenecks on generalization error, as the number of training
examples grows.

8.3.4

Momentum

While stochastic gradient descent remains a very popular optimization strategy,
learning with it can sometimes be slow. This is especially true in situations where
the gradient is small, but consistent across minibatches. When the gradient is
consistent across consecutive minibatches, we know that we can aﬀord to take
larger steps in this direction.
The method of Momentum Polyak (1964) is designed to accelerate learning,
especially in the face of small and consistent gradients. The intuition behind
momentum, as the name suggests, is derived from a physical interpretation of
the optimization process. Imagine you have a small ball (think of a marble) that
represents the current position in parameter space (for our purposes here we can
imagine a 2-D parameter space). Now consider that the ball is on a gentle slope,
while the instantaneous force pulling the ball down hill is relatively small, their
contributions combine and the downhill velocity of the ball gradually begins to
increase over time. The momentum method is designed to inject this kind of
downhill acceleration into gradient-based optimization. The eﬀect of momentum
is illustrated in Fig. 8.5.
Formally, we introduce a variable v that plays the role of velocity (or momen-

253

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Gradients
Velocity

Figure 8.5: TODO: write this caption. TODO: resolve redundancy between this and
ﬁg:valley (right now the images are esentially the same thing)

tum) that accumulates gradient. The update rule is given by:
v ← +αv + η∇θ
θ ← θ+v

!
m
1 X
L(f (x(t) ; θ), y(t) )
m t=1

 P

The velocity v accumulates the gradient elements ∇ θ n1 nt=1 L(f (x(t) ; θ), y (t) ) .
The larger α is relative to η, the more previous gradients aﬀect the current
direction. The overall learning rate, which in the case of SGD, was a simple hyperparameter, is here a relatively complicated function of α and η. The
SGD+momentum algorithm is given in Algorithm 8.2.
Algorithm 8.2 Stochastic gradient descent (SGD) with momentum
Require: Learning rate η, momentum parameter α.
Require: Initial parameter θ, initial velocity v.
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set g = 0
for i = 1 to m do
Compute gradient estimate: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
Compute velocity update: v ← αv − ηg
Apply update: θ ← θ + v
end while

254

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

8.3.5

Nesterov Momentum

Sutskever et al. (2013) introduced a variant of the momentum algorithm that was
inspired by Nesterov.
"
#
m

1 X 
v ← +αv + η∇θ
L f (x (t); θ + αv), y(t) ,
m t=1
θ ← θ + v,

where the parameters α and η play a similar role as in the standard momentum
method. The diﬀerence between Nesterov momentum and standard momentum
is where the gradient is evaluated. With Nesterov momentum the gradient is
evaluated after the current velocity is applied. Thus one can interpret Nesterov
momentum as attempting to add a correction factor to the standard method of
momentum. Figure 8.6 illustrates the diﬀerence between Nesterov momentum and
standard momentum. The complete Nesterov momentum algorithm is presented
in Algorithm 8.3.

η∇θ

η∇θ

!

1
m

m
"
i=1

#

!

%
$

m
#
$
1 "
L f (x(i) ; θ + αv), y (i)
m
i=1

%

L f (x(i) ; θ), y(i)

αv

αv + η∇ θ

!

Standard momentum

m #
$
1"
L f (x(i) ; θ + αv), y(i)
m
i=1

%

Nesterov correction term
Nesterov accumulated gradient

Previous velocity

Figure 8.6: An illustration of the diﬀerence between Nesterov momentum and standard
momemtum. Nesterov momentum incorporates the gradient after the velocity is already
applied. This ﬁgure is derived from a similar image in Geoﬀ Hinton’s Coursera lectures.

255

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Algorithm 8.3 Stochastic gradient descent (SGD) with Nesterov momentum
Require: Learning rate η, momentum parameter α.
Require: Initial parameter θ, initial velocity v.
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Apply interim update: θ ← θ + αv
Set g = 0
for i = 1 to m do
Compute gradient (at interim point): g ← g + ∇ θL(f (x(i) ; θ), y(i) )
end for
Compute velocity update: v ← αv − ηg
Apply update: θ ← θ + v
end while

8.4

Optimization Algorithms II: Adaptive Learning
Rates

Neural network researchers have long realized that the learning rate was reliably
one of the hyperparameters that is the most diﬃcult to set because it has a significant impact on model performance. In reality, as we’ve discussed in Sections 4.3
and 8.2, we often have a subset of parameters to which the cost is much more
sensitive. These directions in parameter space will limit the size of the SGD
learning rate and consequently limit the progress that can be made in the other,
less sensitive directions. While the use of momentum can go some way to alleviate these issues, it does so by introducing another hyperparameter that may
be just as diﬃcult to set as the original learning rate. In the face of this, it is
natural to ask if there is another way. Can learning rates be set automatically
and independently for each parameter?
The delta-bar-delta algorithm (Jacobs, 1988) is an early heuristic approach
to adapting individual learning rates for model parameters during training. The
approach is based on a simple idea: if the partial derivative of the loss, with
respect to a given model parameter, remains the same sign, then the learning
rate should increase, if it changes sign, then the learning rate should decrease. Of
course, this kind of rule can only be applied to full batch optimization.
More recently, a number of incremental (or mini-batch-based) methods have
been introduced that adapt the learning rates of model parameters. This section
will brieﬂy review a few of these algorithms.

256

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

8.4.1

AdaGrad

The AdaGrad algorithm, shown in Algorithm 8.4, individually adapts the learning rates of all model parameters by scaling them inversely proportional to an
accumulated sum of squared partial derivatives over all training iterations. The
parameters with the largest partial derivative of the loss have a correspondingly
rapid decrease in their learning rate, while parameters with small partial derivatives have a relatively small decrease in their learning rate. The net eﬀect is
greater progress in the more gently sloped directions of parameter space.
In the convex optimization context, the AdaGrad algorithm enjoys some desirable theoretical properties. However, empirically it has been found that — for
training deep neural network models — the accumulation of squared gradients
from the beginning of training results in a premature and excessive decrease in
the eﬀective learning rate.
Algorithm 8.4 The Adagrad algorithm
Require: Global learning rate η,
Require: Initial parameter θ
Initialize gradient accumulation variable r = 0,
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set g = 0
for i = 1 to m do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
Accumulate gradient: r ← r + g 2 (square is applied element-wise)
Compute update: ∆θ ← − √η rg. % ( √1r applied element-wise)
Apply update: θ ← θ + ∆θ t
end while

8.4.2

RMSprop

The RMSprop algorithm (Hinton, 2012) addresses the deﬁciency of AdaGrad by
changing the gradient accumulation into an exponentially weighted moving average. As we have previously discussed (especially in Sec. 8.2 ), in deep networks,
the optimization surface is far from convex. Directions in parameter space with
strong partial derivatives early in training may ﬂatten out as training progresses.
The introduction of the exponentially weighted moving average allows the eﬀective learning rates to adapt to the changing local topology of the loss surface.
RMSprop is shown in its standard form in Algorithm 8.5 and combined with
257

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Nesterov momentum in Algorithm 8.6. Note that compared to AdaGrad, the use
of the moving average does introduce a new hyperparameter, ρ, that controls the
length scale of the moving average.
Empirically RMSprop has shown to be an eﬀective and practical optimization
algorithm for deep neural networks. It is easy to implement and relatively simple
to use (i.e. there does not appear to be a great sensitivity to the algorithm’s
hyperparameters). It is currently one of the “go to” optimization methods being
employed routinely by deep learning researchers.
Algorithm 8.5 The RMSprop algorithm
Require: Global learning rate η, decay rate ρ.
Require: Initial parameter θ
Initialize accumulation variables r = 0
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set g = 0
for i = 1 to m do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
Accumulate gradient: r ← ρr + (1 − ρ)g2
η

Compute parameter update: ∆θ = − √r  g.
Apply update: θ ← θ + ∆θ
end while

8.4.3

% ( √1r applied element-wise)

Adam

Adam (Kingma and Ba, 2014) is yet another adaptive learning rate optimization
algorithm and is presented in Algorithm 8.7. In the context of the earlier algorithms, it is perhaps best seen as a variant on RMSprop+momentum with a few
important distinctions. First, in Adam, momentum is incorporated directly as an
estimate of the ﬁrst order moment (with exponential weighting) of the gradient.
The most straightforward way to add momentum to RMSprop is to apply momentum to the rescaled gradients which is not particularly well motivated. Second,
Adam includes bias corrections to the estimates of both the ﬁrst-order moments
(the momentum term) and the (uncentered) second order moments to account
for their initialization at the origin (see Algorithm 8.7) . RMSprop also incorporates an estimate of the (uncentered) second order moment, however it lacks
the correction term. Thus, unlike in Adam, the RMSprop second-order moment
estimate may have high bias early in training.
258

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Algorithm 8.6 RMSprop algorithm with Nesterov momentum
Require: Global learning rate η, decay rate ρ, momentum para α.
Require: Initial parameter θ, initial velocity v.
Initialize accumulation variable r = 0
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Compute interim update: θ ← θ + αv
Set g = 0
for i = 1 to m do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
Accumulate gradient: r ← ρr + (1 − ρ)g2
Compute velocity update: v ← αv − √ηr  g. % ( √1r applied element-wise)
Apply update: θ ← θ + v
end while

8.4.4

AdaDelta

AdaDelta is another recently introduced optimization algorithm that seeks to directly address the issues with AdaGrad. AdaDelta starts from an attempt to
incorporate some second-order gradient information (see 4.3) into the optimization algorithm. In particular, consider the Newton’s step for a single parameter
θ j on the loss for a single example {x (i), y(i) }.1
∆θ j = −
1
∂ 2 L(f (x(i); θ 0 ), y (i))
∂θ 2j

=

1

∂2
∂θ 2j

∂
L(f (x (i); θ0 ), y(i) )
(i)
0
(i)
∂θ
L(f (x ; θ ), y )
j

∆θj
∂ L(f (x(i) ; θ0 ), y (i) )
∂θj

Thus, assuming a diagonal Hessian and a Newton update (which we do not have),
its inverse could be estimated as the ratio of the increment ∆θj over the ﬁrst
partial derivative of the loss. AdaDelta separately estimates this ratio as the ratio
of RMS estimates, using the square-roots of an exponentially weighted moving
1

Recall, from Chapter 4, that Newton’s method — in the single dimension of θj —
can be motivated by looking at the Taylor series expansion of the loss around the current
point θ 0 : L(f (x(i); θ0 + ej ∆θj ), y (i)) ≈ L(f (x(i) ; θ0 ), y(i)) + e j ∂θ∂j L(f(x (i); θ 0), y (i)) ∆θ j +
ej 12

∂2
∂θ2
j

L(f(x (i); θ0 ), y(i) ) ∆θ2j . This expression reaches its extremum, with respect to ∆θ j

when its derivative (w.r.t. ∆θj ) is equal to zero. Using this, we can solve for the optimal step
∆θ j = −

∂
(i)
∂θj L(f (x ;θ
2
∂ L(f (x (i)
;θ
∂θ2
j

0

),y (i))

0),y (i))

.
259

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Algorithm 8.7 The Adam algorithm
Require: Stepsize α
Require: Decay rates ρ1 and ρ 2 , constant 
Require: Initial parameter θ
Initialize 1st and 2nd moment variables s = 0, r = 0,
Initialize timestep t = 0
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set g = 0
for i = 1 to m do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
t ←t+1
Get biased ﬁrst moment: s ← ρ 1s + (1 − ρ 1)g
Get biased second moment: r ← ρ2 r + (1 − ρ2)g 2
Compute bias-corrected ﬁrst moment: ŝ ← s t
1−ρ 1
r
Compute bias-corrected second moment: r̂ ← 1−ρ
t
2
s
Compute update: ∆θ = −α √r+g % (operations applied element-wise)
Apply update: θ ← θ + ∆θ
end while
average over squares of increments (in the numerator) and partial derivatives (in
the denominator). The complete AdaDelta algorithm is shown in Fig. 8.8.

8.4.5

Choosing the Right Optimization Algorithm

In this section, we discussed a series of related algorithms that each seek to address the challenge of optimizing deep models by adapting the learning rate for
each model parameter. At this point, a natural question is: which algorithm
should one choose? Unfortunately, there is currently no consensus on this point.
Tom Schaul (2014) presented a valuable comparison of a large number of optimization algorithms across a wide range of learning tasks. While the results
suggest that this family of algorithms (represented by RMSprop and AdaDelta)
performed fairly robustly, no single best algorithm has emerged.
Currently, the most popular optimization algorithms actively in use include
SGD, SGD+momentum, RMSprop, RMSprop+momentum, AdaDelta and Adam.
The choice of which algorithm to use, as this point, seems to depend as much on
the users familiarity with the algorithm (for ease of hyperparameter tuning) as it
does on any established notion of superior performance.
260

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Algorithm 8.8 The Adadelta algorithm
Require: Decay rate ρ, constant 
Require: Initial parameter θ
Initialize accumulation variables r = 0, s = 0,
while Stopping criterion not met do
Sample a minibatch of m examples from the training set {x(1) , . . . , x(m) }.
Set g = 0
for i = 1 to m do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end for
Accumulate gradient: r ← √
ρr + (1 − ρ)g2
Compute update: ∆θ = − √s+
g % (operations applied element-wise)
r+
Accumulate update: s ← ρs + (1 − ρ) [∆θ] 2
Apply update: θ ← θ + ∆θ
end while

8.5
8.5.1

Optimization Algorithms III: Approximate SecondOrder Methods
Newton’s Method

In section 4.3, we discussed the diﬀerence between ﬁrst-order gradient methods
and second-order gradient methods. Namely, that second-order gradient methods
use information about the partial derivatives of the partial derivatives of the loss
- i.e. second-order gradient information.
In multiple dimensions, we may need to examine all of the second derivatives
of the function. These derivatives can be collected together into a matrix called
the Hessian matrix. The Hessian matrix H(f )(x) is deﬁned such that
H(f )(x)i,j =

∂2
f (x).
∂xi ∂xj

Equivalently, the Hessian is the Jacobian of the gradient.
Newton’s method is based on using a second-order Taylor series expansion to
approximate f (x) near some point x 0, ignoring derivatives of higher order:
1
f (x) ≈ f(x0 ) + (x − x0 )>∇ x f (x 0 ) + (x − x 0)> H(f )(x0 )(x − x0).
2
If we then solve for the critical point of this function, we obtain:
x∗ = x0 − [H (f (x0))]
261

−1

∇x f (x 0 ).

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

When the function can be locally approximated as quadratic, iteratively updating the approximation and jumping to the minimum of the approximation can
reach the critical point much faster than gradient descent would. This is a useful
property near a local minimum, but it can be a harmful property near a saddle
point. As discussed in Section 8.2.3, Newton’s method is only appropriate when
the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient descent can in principle escape a saddle point, although
it may take a lot of time if the negative eigenvalues are very small in magnitude,
producing a kind of plateau around the saddle point.

8.5.2

Conjugate Gradients

Algorithm 8.9 Conjugate gradient method
Require: Initial parameters θ 0
Initialize ρ 0 = 0
while stopping criterion not met do
Initialize the gradient g = 0
for i = 1 to n % loop over the training set. do
Compute gradient: g ← g + ∇ θL(f (x(i) ; θ), y (i) )
end forbackpropagation)
>
t−1 ) g t
Compute βt = (gt −g
(Polak — Ribière)
g> g
t−1 t−1

Compute search direction: ρt = −g t + βt ρt−1
Perform line search to ﬁnd: η∗ = argmin η J (θ t + ηρ t)
Apply update: θt+1 = θ t + η∗ρt
end while

8.5.3

8.6

BFGS

Optimization Algorithms IV: Natural Gradient Methods

TODO brief descriptions of natural gradient methods.

8.7
8.7.1

Optimization Strategies and Meta-Algorithms
Batch Normalization

TODO: actually write this section
262

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Algorithm 8.10 BFGS method
Require: Initial parameters θ 0
Initialize inverse Hessian M 0 = I
while stopping criterion not met do
Compute gradient: g t = ∇J (θt ) (via batch backpropagation)
Compute φ = g t − gt−1, ∆ =θ t − θt−1
 >
 >

φ>M t−1φ
φ φ
∆φ M t−1 +M t−1φ∆>
−1
−
Approx H : M t = M t−1 + 1 + ∆ >φ
∆ >φ
∆ >φ
Compute search direction: ρt = M tg t
Perform line search to ﬁnd: η∗ = argmin η J (θ t + ηρ t)
Apply update: θt+1 = θ t + η∗ρt
end while
Batch normalization (Ioﬀe and Szegedy, 2015) is one of the most exciting
recent innovations in optimizing deep neural networks and it is actually not an
optimization algorithm at all. Instead, it is a method of adaptive reparametrization, motivated by the observed diﬃculty in training very deep models (e.g. ¿10
layers). One major reason for this is that the distribution of the inputs to each
layer changes throughout learning – as the parameters of all lower layers adapt.

8.7.2

Coordinate Descent

In some cases, it may be possible to solve an optimization problem quickly by
breaking it into separate pieces. If we minimize f (x) with respect to a single
variable xi, then minimize it with respect to another variable xj and so on, we are
guaranteed to arrive at a (local) minimum. This practice is known as coordinate
descent, because we optimize one coordinate at a time. More generally, block
coordinate descent refers to minimizing with respect to a subset of the variables
simultaneously. The term “coordinate descent” is often used to refer to block
coordinate descent as well as the strictly individual coordinate descent.
Coordinate descent makes the most sense when the diﬀerent variables in the
optimization problem can be clearly separated into groups that play relatively
isolated roles, or when optimization with respect to one group of variables is
signiﬁcantly more eﬃcient than optimization with respect to all of the variables.
For example, the objective function most commonly used for sparse coding is not
convex. However, we can divide the inputs to the training algorithm into two
sets: the dictionary parameters and the code representations. Minimizing the
objective function with respect to either one of these sets of variables is a convex
problem. Block coordinate descent thus gives us an optimization strategy that
allows us to use eﬃcient convex optimization algorithms.
Coordinate descent is not a very good strategy when the value of one variable
263

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

strongly inﬂuences
value of another variable, as in the function f (x) =
 2the optimal

2
2
(x1 − x 2) + α x1 + y1 where α is a positive constant. As α approaches 0,
coordinate descent ceases to make any progress at all, while Newton’s method
could solve the problem in a single step.

8.7.3

Initialization Strategies

Some optimization algorithms are not iterative by nature and simply solve for a
solution point. Other optimization algorithms are iterative by nature but, when
applied to the right class of optimization problems, converge to acceptable solutions in an acceptable amount of time regardless of initialization. Deep learning
training algorithms usually do not have this luxury. Training algorithms for deep
learning models are usually iterative in nature and thus require the user to specify some initial point from which to begin the iterations. Moreover, training
deep models is a suﬃciently diﬃcult task that most algorithms are strongly affected by the choice of initialization. The initial point can determine whether the
algorithm converges at all, with some initial points being so unstable that the
algorithm encounters numerical diﬃculties and fails altogether. When learning
does converge, the initial point can determine how quickly learning converges and
whether it converges to a point with high or low cost. Also, points of comparable
cost can have wildly varying generalization error, and the initial point can aﬀect
the generalization as well.
Modern initialization strategies are simple and heuristic. Designing improved
initialization strategies is a diﬃcult task because neural network optimization is
not yet well understood. Most initialization strategies are based on achieving
some nice properties when the network is initialized. However, we do not have
a good understanding of which of these properties are preserved under which
circumstances after learning begins to proceed. A further diﬃculty is that some
initial points may be beneﬁcial from the viewpoint of optimization but detrimental
from the viewpoint of generalization. Our understanding of how the initial point
aﬀects generalization is especially primitive, oﬀering little to no guidance for how
to select the initial point.
Perhaps the only property known with complete certainty is that the initial
parameters need to “break symmetry” between diferent units. If two units with
the same activation function are connected to the same inputs, then these units
must have diﬀerent initial parameters. If they have the same initial parameters,
then a deterministic learning algorithm applied to a deterministic cost and model
will constantly update both of these units in the same way. Even if the model or
training algorithm is capable of using stochasticity to compute diﬀerent updates
for diﬀerent units (for example, if one trains with dropout) , it is usually best to
initialize each unit to compute a diﬀerent function from all of the other units. This
264

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

may help to make sure that no input patterns are lost in the null space of forward
propagation and no gradient patterns are lost in the null space of backpropagation.
This goal of having each unit compute a diﬀerent function motivates random
initialization of the parameters. We could explicitly search for a large set of basis
functions that are all mutually diﬀerent from each other, but this often incurs a
noticeable computation cost. For example, if we have at most as many outputs as
inputs, we could use Gram-Schmidt orthogonalization on an initial weight matrix,
and be guaranteed that each unit computes a very diﬀerent function from each
other unit. Random initialization from a high-entropy distribution over a highdimensional space is computationally cheaper and unlikely to assign any units to
compute the same function as each other.
Typically, we set the biases for each unit to heuristically chosen constants, and
initialize only the weights randomly. Extra parameters, for example, parameters
encoding the conditional variance of a prediction, are usually set to heuristically
chosen constants much like the biases are.
We almost always initialize all the weights in the model to values drawn randomly from a Gaussian or uniform distribution. The choice of Gaussian or uniform
distribution does not seem to matter a lot, but has not been exhaustively studied. The scale of the initial distribution, however, does have a large eﬀect on both
the outcome of the optimization procedure and on the ability of the network to
generalize.
Larger initial weights will yield a stronger symmetry breaking eﬀect, helping
to avoid redundant units. They also help to avoid losing signal during forward
or backpropagation through the linear component of each layer—larger values in
the matrix result in larger outputs of matrix multiplication. Too large of initial
weights may, however, result in exploding values during forward propagation or
backpropagation. In recurrent networks, large weights can also result in chaos
(such extreme sensitivity to small perturbations of the input that the behavior
of the deterministic forward propagation procedure appears random). To some
extent, the exploding gradient problem can be mitigated by gradient clipping
(thresholding the values of the gradients before performing a gradient descent
step). Large weights may also result in extreme values that cause the activation
function to saturate, causing complete loss of gradient through saturated units.
These competing factors determine the ideal initial scale of the weights.
The perspectives of regularization and optimization can give very diﬀerent
insights into how we should initialize a network. The optimization perspective
suggests that the weights should be large enough to propagate information successfully, but some regularization concerns encourage making them smaller. The
use of an optimization algorithm such as stochastic gradient descent that makes
small incremental changes to the weights and tends to halt in areas that are nearer
265

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

to the initial parameters (whether due to getting stuck in a region of low gradient,
or due to triggering some early stopping criterion based on overﬁtting) expresses
a prior that the ﬁnal parameters should be close to the initial parameters. Recall
from Section 7.7 that gradient descent with early stopping is equivalent to weight
decay for some models. In the general case, gradient descent with early stopping
is not the same as weight decay, but does provide a loose analogy for thinking
about the eﬀect of initialization. We can think of initializing the parameters θ to
θ 0 as being similar to imposing a Gaussian prior p(θ) with mean θ 0. From this
point of view, it makes sense to choose θ0 to be near 0. This prior says that it is
more likely that units do not interact with each other than that they do interact.
Units interact only if the likelihood term of the objective function expresses a
strong preference for them to interact. On the other hand, if we initialize θ 0 to
large values, then our prior speciﬁes which units should interact with each other,
and in pre-speciﬁed ways.
Some heuristics are available for choosing the initial scale of the weights. One
heuristic is to initialize the weights of a fully connected layer with m inputs and
n outputs by sampling each weight from U (− √1m , √1n ), while Glorot and Bengio
(2010b) suggest using the normalized initialization
W i,j ∼ U(− √

6
6
,√
).
m+n
m+n

This latter heuristic is designed to compromise between the goal of initializing all
layers to have the same activation variance and the goal of initializing all layers
to have the same gradient variance. The formula is derived using the assumption
that the network consists only of a chain of matrix multiplications, with no nonlinearities.
Saxe et al. (2013) recommend initializing to random orthogonal matrices, so
that all singular values are 1. This initialization scheme is also motivated by a
model of a deep network as a sequence of matrix multiplies without non-linearities.
In order to account for the non-linearity, Saxe et al. (2013) recommend rescaling all initial weights by a gain factor g. This can oﬀset the eﬀect of the nonlinearities on the eigenvalues of the Jacobian, though the interaction between the
non-linearities and the eigenvectors of the Jacobian remains poorly characterized and presumably cannot be accounted for by adjusting the gain. Increasing
g pushes the network toward the regime where activations increase in norm as
they propagate forward through the network and gradients increase in norm as
the propagate backward. Sussillo (2014) showed that setting the gain factor
correctly is suﬃcient to train networks as deep as 1,000 layers, without needing
to use orthogonal initializations. A key insight of this approach is that in feedforward networks, activations and gradients can grow or shrink on each step of
forward or backpropagation, following a random walk behavior. This is because
266

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

feedforward networks use a diﬀerent weight matrix at each layer. If this random walk is tuned to preserve norms, then feedforward networks can avoid the
vanishing and exploding gradients problem altogether. Feedforward networks are
qualitatively diﬀerent from recurrent networks. Recurrent networks repeatedly
use the same weight matrix for forward propagation and repeatedly use its transpose for backpropagation. If we use the same simpliﬁcation to analyze recurrent
nets as is commonly used to analyze feedforward nets, that is, if we assume that
the recurrent net consists only of matrix multiplications composed together, then
both forward and back-propagation behave very much like the power method,
systematically driving the propagated values toward the principle singular vector
of the weight matrix.
Unfortunately, these optimal criteria for initial weights often do not lead to
optimal performance. This may be for three diﬀerent reasons. First, we may
be using the wrong criteria—it may not actually be beneﬁcial to preserve the
norm of a signal throughout the entire network. Second, the properties imposed
at initialization may not persist after learning has begun to proceed. Third, the
criteria might succeed at improving the speed of optimization but inadvertently
increase generalization error. In practice, we usually need to treat the scale of the
weights as a hyperparameter whose optimal value lies somewhere roughly near
but not exactly equal to the theoretical predictions.
√
One drawback to scaling rules like 1/ m is that every individual weight becomes extremely small when the layers become large. Martens (2010) introduced
an alternative initialization scheme called sparse initialization in which each unit
is initialized to have exactly k non-zero weights. The idea is to keep the total
amount of input to the unit independent from m without making the magnitude
of individual weight elements shrink with m. This helps to achieve more diversity
among the units at initialization. However, it also imposes a very strong prior
on the weights that are chosen to have large Gaussian values. Because it takes a
long time for gradient descent to shrink “incorrect” large values, this initialization
scheme can cause problems for units such as maxout units that have several ﬁlters
that must be carefully coordinated with each other.
When computational resources allow it, it is usually a good idea to treat the
initial scale of the weights for each layer as a hyperparameter, and to choose these
scales using a hyperparameter search algorithm described in Chapter 11.2.2, such
as random search. The choice of whether to use dense or sparse initialization
can also be made a hyperparameter. Alternately, one can manually search for
the best initial scales. A good rule of thumb for choosing the initial scales is
to look at the range or standard deviation of activations or gradients on a single
minibatch of data. If the weights are too small, the range of activations across the
minibatch will shrink as the activations propagate forward through the network.
267

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

By repeatedly identifying the ﬁrst layer with unacceptably small activations and
increasing its weights, it is possible to eventually obtain a network with reasonable
initial activations throughout. If learning is still too slow at this point, it can be
useful to look at the range or standard deviation of the gradients as well as the
activations. This procedure can in principle be automated and is generally less
computationally costly than hyperparameter optimization based on validation set
error because it is based on feedback from the behavior of the initial model on
a single batch of data, rather than on feedback from a trained model on the
validation set.
So far we have focused on the initialization of the weights. Fortunately, initialization of other parameters is typically easier.
The approach for setting the biases must be coordinated with the approach
for settings the weights. Setting the biases to zero is compatible with most weight
initialization schemes. There are a few situations where we may set some biases
to non-zero values:
• If a bias is for an output unit, then it is often beneﬁcial to initialize the
bias to obtain the right marginal statistics of the output. To do this, we
assume that the initial weights are small enough that the output of the
unit is determined only by the bias. This justiﬁes setting the bias to the
inverse of the activation function applied to the marginal statistics of the
output in the training set. For example, if the output is a distribution
over classes, and this distribution is a highly skewed distribution with the
marginal probability of class i given by element c i of some vector c, then
we can set the bias vector b by solving the equation softmax(b) = c. This
applies not only to classiﬁers but also to models we will encounter in Part III
of the book, such as autoencoders and Boltzmann machines. These models
have layers whose output should resemble the input data x, and it can be
very helpful to initialize the biases of such layers to match the marginal
distribution over x.
• Sometimes we may want to choose the bias to avoid causing too much saturation at initialization. For example, we may set the bias of a ReLU hidden
unit to 0.1 rather than 0 to avoid saturating the ReLU at initialization.
This approach is not compatible with weight initialization schemes that do
not expect strong input from the biases though. For example, it is not
recommended for use with random walk initialization (Sussillo, 2014).
• When one unit gates another unit (for example, the forget gate of an LSTM),
we may want to set the bias of the gating unit to 1, in order to make the
gate initially be open and avoid discarding the gradient through the unit
that it gates (Jozefowicz et al., 2015b).
268

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Another common type of parameter is a variance or precision parameter. For
example, we can perform linear regression with a conditional variance estimate
using the model
p(y | x) = N (y | w Tx + b, 1/β)
where β is a precision parameter. We can usually initialize variance or precision
parameters to 1 safely. Another approach is to assume the initial weights are zero,
set the biases to produce the correct marginal mean of the output, and set the
variance parameters to the marginal variance of the output in the training set.
Besides these simple constant or random methods of initializing model parameters, it is possible to initialize model parameters using machine learning. A
common strategy discussed in Part III of this book is to initialize a supervised
model with the parameters learned by an unsupervised model trained on the same
inputs. One can also perform supervised training on a related task. Even performing supervised training on an unrelated tasks can sometimes yield an initialization
that oﬀers faster convergence than a random initialization. Some of these initialization strategies may yield faster convergence and better generalization because
they encode information about the distribution in the initial parameters of the
model. Others apparently perform well primarily because they set the parameters
to have the right scale or set diﬀerent units to compute diﬀerent functions from
each other.

8.7.4

Greedy Supervised Pre-training

TODO: write this section on greedy supervised pretraining

8.7.5

Designing Models to Aid Optimization

Most of the model families we study for deep learning are incredibly broad. While
most of these model families result in objective functions that are non-convex any
time we include at least one hidden layer, there are many other diﬃculties for
optimization besides just non-convexity.
In principle, we could use activation functions that increase and decrease
in jagged non-monotonic patterns. However, this would make optimization extremely diﬃcult. In practice, it is more important to choose a model family
that is easy to optimize than to use a powerful optimization algorithm.
Most of the advances in neural network learning over the past 30 years have been KEY
obtained by changing the model family rather than changing the optimization IDEA
procedure. Stochastic gradient descent with momentum, which was used to train
neural networks in the 1980s, remains in use in modern state of the art neural
network applications.
269

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Speciﬁcally, modern neural networks reﬂect a design choice to use linear transformations between layers and activation functions that are diﬀerentiable and have
signiﬁcant slope. In particular, model innovations like the LSTM, rectiﬁed linear units, and maxout units have all moved toward using more linear functions
than previous models like deep networks based on sigmoidal units. These models
have nice properties that make optimization easier. The gradient ﬂows through
many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single
direction, so even if the model’s output is very far from correct, it is clear simply
from computing the gradient which direction its output should remove to reduce
the loss function. In other words, modern neural nets have been designed so that
their local gradient information corresponds reasonably well to moving toward a
distant solution.
Other model design strategies can help to make optimization easier. For example, skip connections between layers reduce the length of the shortest path
from the lower layer’s parameter to the output, and thus mitigate the vanishing
gradient problem (TODO cite a skip connection paper). A related idea to skip
connections is adding extra copies of the output that are attached to the intermediate hidden layers of the network (Szegedy et al., 2014a) TODO also cite DSNs.
These “auxiliary heads” are trained to perform the same task as the primary
output as the top of the network in order to insure that the lower layers receive a
large gradient. When training is complete the auxiliary heads may be discarded.

8.8

Hints, Global Optimization and Curriculum Learning

Most of the work on numerical optimization for machine learning and deep learning in particular is focused on local descent, i.e., on how to locally improve the
objective function eﬃciently. What the experiments with diﬀerent initialization
strategies tell us is that local descent methods can get stuck, presumably near a
local minimum or near a saddle point, i.e., where gradients are small, so that the
initial value of the parameters can matter a lot.
As an illustration of this issue, consider the experiments reported by Gülçehre
and Bengio (2013), where a learning task is setup so that if the lower half of the
deep supervised network is pre-trained with respect to an appropriate sub-task,
the whole network can learn to solve the overall task, whereas random initialization almost always fails. In these experiments, we know that the overall task can
be decomposed into two tasks (1) (identifying the presence of diﬀerent objects
in an input image and (2) verifying whether the diﬀerent objects detected are of
the same class or not. Each of these two tasks (object recognition, exclusive-or)
270

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

are known to be learnable, but when we compose them, it is much more diﬃcult
to optimize the neural network (including a large variety of architectures), while
other methods such as SVMs, boosting and decision trees also fail. This is an
instance where the optimization diﬃculty was solved by introducing prior knowledge in the form of hints, speciﬁcally hints about what the intermediate layer in
a deep net should be doing. We have already seen in Section 8.7.4 that a useful
strategy is to ask the hidden units to extract features that are useful to the supervised task at hand, with greedy supervised pre-training. In section 16.1 we will
discuss an unsupervised version of this idea, where we ask the intermediate layers
to extract features that are good explaining the variations in the input, without
reference to a speciﬁc supervised task. Another related line of work is the FitNets (Romero et al., 2015), where the middle layer of 5-layer supervised teacher
network is used as a hint to be predicted by the middle layer of a much deeper
student network (11 to 19 layers). In that case, additional parameters are introduced to regress the middle layer of the 5-layer teacher network from the middle
layer of the deeper student network. The lower layers of the student networks
thus get two objectives: help the outputs of the student network accomplish their
task, as well as predict the intermediate layer of the teacher network. Although
a deeper network is usually more diﬃcult to optimize, it can generalize better (it
has to extract these more abstract and non-linear features). Romero et al. (2015)
were motivated by the fact that a deep student network with a smaller number
of hidden units per layer can have a lot less parameters (and faster computation)
than a fatter shallower network and yet achieve the same or better generalization,
thus allowing a trade-oﬀ between better generalization (with 3 times fewer parameters) and faster test-time computation (up to 10 fold, in the paper, using a
very thin and deep network with 35 times less parameters). Without the hints on
the hidden layer, the student network performed very poorly in the experiments,
both on the training and test set.
These drastic eﬀects of initialization and hints to middle layers bring forth
the question of what is sometimes called global optimization (Horst et al., 2000),
the main subject of this section. The objective of global optimization methods is
to ﬁnd better solutions than local descent minimizers, i.e., ideally ﬁnd a global
minimum of the objective function and not simply a local minimum. If one could
restart a local optimization method from a very large number of initial conditions,
one could imagine that the global minimum could be found, but there are more
eﬃcient approaches.
Two fairly general approaches to global optimization are continuation methods (Wu, 1997), a deterministic approach, and simulated annealing (Kirkpatrick
et al., 1983), a stochastic approach. They both proceed from the intuition that
if we suﬃciently blur a non-convex objective function (e.g. convolve it with a
271

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

Final solution

Track local minima

Easy to ﬁnd minimum

Figure 8.7: Optimization based on continuation methods: start by optimizing a smoothed
out version of the target objective function (possibly convex), then gradually reduce
the amount of smoothing while tracking the local optimum. This approach tends to
ﬁnd better local minima than a straight local descent approach on the target objective
function. Curriculum learning (starting from easy examples and gradually introducing
with higher probability more diﬃcult examples) can be justiﬁed under that light (Bengio
et al., 2009).

Gaussian) whose global minima arae not at inﬁnite values, then it becomes convex, and ﬁnding the global optimum of that blurred objective function should be
much easier. As illustrated in Figure 8.7, by gradually changing the objective
function from a very blurred easy to optimize version to the original crisp and
diﬃcult objective function, we are actually likely to ﬁnd better local minima. In
the case of simulated annealing, the blurring occurs because of injecting noise.
With injected noise, the state of the system can sometimes go uphill, and thus
does not necessarily get stuck in a local minimum. With a lot of noise, the effective objective function (averaged over the noise) is ﬂatter and convex, and if
the amount of noise is reduced suﬃciently slowly, then one can show convergence
to the global minimum. However, the annealing schedule (the rate at which the
noise level is decreased, or equivalently the temperature is decreased when you
think of the physical annealing analogy) might need to be extremely slow, so an
NP-hard optimization problem remains NP-hard.
Continuation methods have been extremely successful in recent years: see a
recent overview of recent literature, especially for AI applications in Mobahi and
Fisher III (2015). Continuation methods deﬁne a family of objective functions,
indexed by a single scalar index λ, with an easy to optimize objective function at
one end (usually convex, say λ = 1) and the target objective at the other end (say
272

CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS

λ = 0). The idea is to ﬁrst ﬁnd the solution for the easy problem (λ = 1) and
then gradually decrease λ towards the more diﬃcult objectives, while tracking the
minimum.
Curriculum learning (Bengio et al., 2009) was introduced as a general strategy for machine learning that is inspired by how humans learn, starting by learning to solve simple tasks, and then exploiting what has been learned to learn
slightly more diﬃcult and abstract tasks, etc. It was justiﬁed as a continuation method (Bengio et al., 2009) in the context of deep learning, where it was
previously observed that the optimization problem can be challenging. Experiments showed that better results could be obtained by following a curriculum, in
particular on a large-scale neural language modeling task. One view on curriculum learning introduced in that paper is that a particular intermediate objective
function corresponds to a reweighing on the examples: initially the easy to learn
examples are given more weights or a higher probability, and harder examples
see their weight or probability gradually increased as the learner gets suﬃciently
ready to learn them. The idea of curriculum learning to help train diﬃcult to
optimize models has been taken up successfully not only in natural language
tasks (Spitkovsky et al., 2010; Collobert et al., 2011a; Mikolov et al., 2011b; Tu
and Honavar, 2011) but also in computer vision (Kumar et al., 2010; Lee and
Grauman, 2011; Supancic and Ramanan, 2013). It was also found to be consistent with the way in which humans teach (Khan et al., 2011): they start by
showing easier and more prototypical examples and then help the learner reﬁne
the decision surface with the less obvious cases. In agreement with this, it was
found that such strategies are more eﬀective when teaching to humans (Basu and
Christensen, 2013).
Another important contribution to research on curriculum learning arose in
the context of training recurrent neural networks to capture long-term dependencies (Zaremba and Sutskever, 2014): it was found that much better results were
obtained with a stochastic curriculum, in which a random mix of easy and diﬃcult
examples is always presented to the learner, but where the average proportion of
the more diﬃcult examples (here, those with longer-term dependencies) is gradually increased. Instead, with a deterministic curriculum, no improvement over
the baseline (ordinary training from the fully training set) was observed.

273

Chapter 9

Convolutional Networks
Convolutional networks (also known as convolutional neural networks or CNNs)
are a specialized kind of neural network for processing data that has a known,
grid-like topology. Examples include time-series data, which can be thought of as
a 1D grid taking samples at regular time intervals, and image data, which can be
thought of as a 2D grid of pixels. Convolutional networks have been tremendously
successful in practical applications (the speciﬁcs of several of these applications
will be explained in Chapter 12.2.2). The name “convolutional neural network”
indicates that the network employs a mathematical operation called convolution.
Convolution is a specialized kind of linear operation. Convolutional networks
are simply neural networks that use convolution in place of general
matrix multiplication in at least one of their layers.
KEY
In this chapter, we will ﬁrst describe what convolution is. Next, we will ex- IDEA
plain the motivation behind using convolution in a neural network. We will then
describe an operation called pooling, which almost all convolutional networks employ. Usually, the operation used in a convolutional neural network does not
correspond precisely to the deﬁnition of convolution as used in other ﬁelds such
as engineering or pure mathematics. We will describe several variants on the convolution function that are widely used in practice for neural networks. We will
also show how convolution may be applied to many kinds of data, with diﬀerent numbers of dimensions. We then discuss means of making convolution more
eﬃcient. Convolutional networks standard out as an example of neuroscientiﬁc
principles inﬂuencing deep learning. We will discuss these neuroscientiﬁc principles, then conclude with comments about the role convolutional networks have
played in the history of deep learning.

274

CHAPTER 9. CONVOLUTIONAL NETWORKS

9.1

The Convolution Operation

In its most general form, convolution is an operation on two functions of a realvalued argument. To motivate the deﬁnition of convolution, let’s start with examples of two functions we might use.
Suppose we are tracking the location of a spaceship with a laser sensor. Our
laser sensor provides a single output x(t), the position of the spaceship at time
t. Both x and t are real-valued, i.e., we can get a diﬀerent reading from the laser
sensor at any instant in time.
Now suppose that our laser sensor is somewhat noisy. To obtain a less noisy
estimate of the spaceship’s position, we would like to average together several
measurements. Of course, more recent measurements are more relevant, so we
will want this to be a weighted average that gives more weight to recent measurements. We can do this with a weighting function w(a), where a is the age of a
measurement. If we apply such a weighted average operation at every moment,
we obtain a new function s providing a smoothed estimate of the position of the
spaceship:
Z
s(t) =

x(a)w(t − a)da

This operation is called convolution. The convolution operation is typically
denoted with an asterisk:
s(t) = (x ∗ w)(t)

In our example, w needs to be a valid probability density function, or the
output is not a weighted average. Also, w needs to be 0 for all negative arguments, or it will look into the future, which is presumably beyond our capabilities.
These limitations are particular to our example though. In general, convolution
is deﬁned for any functions for which the above integral is deﬁned, and may be
used for other purposes besides taking weighted averages.
In convolutional network terminology, the ﬁrst argument (in this example,
the function x) to the convolution is often referred to as the input and the second
argument (in this example, the function w) as the kernel. The output is sometimes
referred to as the feature map.
In our example, the idea of a laser sensor that can provide measurements at
every instant in time is not realistic. Usually, when we work with data on a
computer, time will be discretized, and our sensor will provide data at regular
intervals. In our example, it might be more realistic to assume that our laser
provides one measurement once per second. t can then take on only integer
values. If we now assume that x and w are deﬁned only on integer t, we can

275

CHAPTER 9. CONVOLUTIONAL NETWORKS

deﬁne the discrete convolution:
s[t] = (x ∗ w)(t) =

∞
X

a=−∞

x[a]w[t − a]

TODO: synch w/Yoshua and Aaron about how to handle this kind of indexing.
Add an integer-domain function line to notation.tex?
In machine learning applications, the input is usually a multidimensional array
of data and the kernel is usually a multidimensional array of learn-able parameters. We will refer to these multidimensional arrays as tensors. Because each
element of the input and kernel must be explicitly stored separately, we usually
assume that these functions are zero everywhere but the ﬁnite set of points for
which we store the values. This means that in practice we can implement the
inﬁnite summation as a summation over a ﬁnite number of array elements.
Finally, we often use convolutions over more than one axis at a time. For
example, if we use a two-dimensional image I as our input, we probably also
want to use a two-dimensional kernel K:
XX
s[i, j] = (I ∗ K)[i, j] =
I[m, n]K[i − m, j − n]
m

n

Note that convolution is commutative, meaning we can equivalently write:
XX
s[i, j] = (I ∗ K)[i, j] =
I[i − m, j − n]K[m, n]
m

n

Usually the latter view is more straightforward to implement in a machine
learning library, because there is less variation in the range of valid values of m
and n.
While the commutative property is useful for writing proofs, it is not usually
an important property of a neural network implementation. Instead, many neural
network libraries implement a related function called the cross-correlation, which
is the same as convolution but without ﬂipping the kernel:
XX
s[i, j] = (I ∗ K)[i, j] =
I[i + m, j + n]K[m, n]
m

n

Many machine learning libraries implement cross-correlation but call it convolution. In this text we will follow this convention of calling both operations
convolution, and specify whether we mean to ﬂip the kernel or not in contexts
where kernel ﬂipping is relevant.
See Fig. 9.1 for an example of convolution (without kernel ﬂipping) applied
to a 2-D tensor.
276

CHAPTER 9. CONVOLUTIONAL NETWORKS

Input
a

e

i

b

f

j

c

d

g

Kernel
w

x

y

z

h

k

l

Output
aw + bx
+ ey + fz

bw + cx
+ fy + gz

cw + dx
+ gy +
hz

ew + fx +
iy + jz

fw + gx
+ jy + kz

gw + hx
+ ky + lz

Figure 9.1: An example of 2-D convolution without kernel-ﬂipping. In this case we restrict
the output to only positions where the kernel lies entirely within the image, called “valid”
convolution in some contexts. We draw boxes with arrows to indicate how the upperleft element of the output tensor is formed by applying the kernel to the corresponding
upper-left region of the input tensor.

277

CHAPTER 9. CONVOLUTIONAL NETWORKS

Discrete convolution can be viewed as multiplication by a matrix. However,
the matrix has several entries constrained to be equal to other entries. For example, for univariate discrete convolution, each row of the matrix is constrained to be
equal to the row above shifted by one element. This is known as a Toeplitz matrix.
In two dimensions, a doubly block circulant matrix corresponds to convolution. In
addition to these constraints that several elements be equal to each other, convolution usually corresponds to a very sparse matrix (a matrix whose entries are
mostly equal to zero). This is because the kernel is usually much smaller than
the input image. Viewing convolution as matrix multiplication usually does not
help to implement convolution operations, but it is useful for understanding and
designing neural networks. Any neural network algorithm that works with matrix
multiplication and does not depend on speciﬁc properties of the matrix structure
should work with convolution, without requiring any further changes to the neural
network. Typical convolutional neural networks do make use of further specializations in order to deal with large inputs eﬃciently, but these are not strictly
necessary from a theoretical perspective.

9.2

Motivation

Convolution leverages three important ideas that can help improve a machine
learning system: sparse interactions, parameter sharing, and equivariant representations. Moreover, convolution provides a means for working with inputs of
variable size. We now describe each of these ideas in turn.
Traditional neural network layers use a matrix multiplication to describe the
interaction between each input unit and each output unit. This means every
output unit interacts with every input unit. Convolutional networks, however,
typically have sparse interactions (also referred to as sparse connectivity or sparse
weights). This is accomplished by making the kernel smaller than the input. For
example, when processing an image, the input image might have thousands or
millions of pixels, but we can detect small, meaningful features such as edges
with kernels that occupy only tens or hundreds of pixels. This means that we
need to store fewer parameters, which both reduces the memory requirements of
the model and improves its statistical eﬃciency. It also means that computing the
output requires fewer operations. These improvements in eﬃciency are usually
quite large. If there are m inputs and n outputs, then matrix multiplication
requires m × n parameters and the algorithms used in practice have O(m × n)
runtime (per example). If we limit the number of connections each output may
have to k, then the sparsely connected approach requires only k × n parameters
and O(k × n) runtime. For many practical applications, it is possible to obtain
good performance on the machine learning task while keeping k several orders of
278

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.2: Sparse connectivity, viewed from below: We highlight one input unit, X3 , and
also highlight the output units in S that are aﬀected by this unit. (Left) When S is
formed by convolution with a kernel of width 3, only three outputs are aﬀected by X3 .
(Right) When S is formed by matrix multiplication, connectivity is no longer sparse, so
all of the outputs are aﬀected by X 3 . TODO: make sure ﬁg uses latest notation

Figure 9.3: Sparse connectivity, viewed from above: We highlight one output unit, S3 ,
and also highlight the input units in X that aﬀect this unit. These units are known
as the receptive ﬁeld of S3 . (Left) When S is formed by convolution with a kernel of
width 3, only three inputs aﬀect S3 . (Right) When S is formed by matrix multiplication,
connectivity is no longer sparse, so all of the inputs aﬀect S 3 . TODO: make sure ﬁg uses
latest notation

magnitude smaller than m. For graphical demonstrations of sparse connectivity,
see Fig. 9.2 and Fig. 9.3. In a deep convolutional network, units in the deeper
layers may indirectly interact with a larger portion of the input, as shown in
Fig. 9.4. This allows the network to eﬃciently describe complicated interactions
between many variables by constructing such interactions from simple building
blocks that each describe only sparse interactions.
Parameter sharing refers to using the same parameter for more than one function in a model. In a traditional neural net, each element of the weight matrix
is used exactly once when computing the output of a layer. It is multiplied by
one element of the input, and then never revisited. As a synonym for parameter
sharing, one can say that a network has tied weights, because the value of the
weight applied to one input is tied to the value of a weight applied elsewhere. In
279

CHAPTER 9. CONVOLUTIONAL NETWORKS

g1

g2

g3

g4

g5

h1

h2

h3

h4

h5

x1

x2

x3

x4

x5

Figure 9.4: The receptive ﬁeld of the units in the deeper layers of a convolutional network
is larger than the receptive ﬁeld of the units in the shallow layers. This eﬀect increases
if the network includes architectural features like strided convolution or pooling. This
means that even though direct connections in a convolutional net are very sparse, units
in the deeper layers can be indirectly connected to all or most of the input image.
280

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.5: Parameter sharing: We highlight the connections that use a particular parameter in two diﬀerent models. (Left) We highlight uses of the central element of a
3-element kernel in a convolutional model. Due to parameter sharing, this single parameter is used at all input locations. (Right) We highlight the use of the central element of
the weight matrix in a fully connected model. This model has no parameter sharing so
the parameter is used only once.

a convolutional neural net, each member of the kernel is used at every position of
the input (except perhaps some of the boundary pixels, depending on the design
decisions regarding the boundary). The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for
every location, we learn only one set. This does not aﬀect the runtime of forward
propagation–it is still O(k×n)–but it does further reduce the storage requirements
of the model to k parameters. Recall that k is usual several orders of magnitude
less than m. Since m and n are usually roughly the same size, k is practically
insigniﬁcant compared to m × n. Convolution is thus dramatically more eﬃcient
than dense matrix multiplication in terms of the memory requirements and statistical eﬃciency. For a graphical depiction of how parameter sharing works, see
Fig. 9.5.
As an example of both of these ﬁrst two principles in action, Fig. 9.6 shows
how sparse connectivity and parameter sharing can dramatically improve the
eﬃciency of a linear function for detecting edges in an image.
In the case of convolution, the particular form of parameter sharing causes the
layer to have a property called equivariance to translation. To say a function is
equivariant means that if the input changes, the output changes in the same way.
Speciﬁcally, a function f (x) is equivariant to a function g if f(g(x)) = g(f (x)). In
the case of convolution, if we let g be any function that translate the input, i.e.,
shifts it, then the convolution function is equivariant to g. For example, deﬁne
g(x) such that for all i, g(x)[i] = x[i − 1]. This shifts every element of x one
unit to the right. If we apply this transformation to x, then apply convolution,
281

CHAPTER 9. CONVOLUTIONAL NETWORKS

the result will be the same as if we applied convolution to x, then applied the
transformation to the output. When processing time series data, this means that
convolution produces a sort of timeline that shows when diﬀerent features appear
in the input. If we move an event later in time in the input, the exact same
representation of it will appear in the output, just later in time. Similarly with
images, convolution creates a 2-D map of where certain features appear in the
input. If we move the object in the input, its representation will move the same
amount in the output. This is useful for when we know that same local function
is useful everywhere in the input. For example, when processing images, it is
useful to detect edges in the ﬁrst layer of a convolutional network, and an edge
looks the same regardless of where it appears in the image. This property is not
always useful. For example, if we want to recognize a face, some portion of the
network needs to vary with spatial location, because the top of a face does not
look the same as the bottom of a face–the part of the network processing the top
of the face needs to look for eyebrows, while the part of the network processing
the bottom of the face needs to look for a chin.
Note that convolution is not equivariant to some other transformations, such
as changes in the scale or rotation of an image. Other mechanisms are necessary
for handling these kinds of transformations.
Finally, some kinds of data cannot be processed by neural networks deﬁned by
matrix multiplication with a ﬁxed-shape matrix. Convolution enables processing
of some of these kinds of data. We discuss this further in section 9.8.

9.3

Pooling

A typical layer of a convolutional network consists of three stages (see Fig. 9.7).
In the ﬁrst stage, the layer performs several convolutions in parallel to produce a
set of presynaptic activations. In the second stage, each presynaptic activation is
run through a nonlinear activation function, such as the rectiﬁed linear activation
function. This stage is sometimes called the detector stage. In the third stage,
we use a pooling function to modify the output of the layer further.
A pooling function replaces the output of the net at a certain location with a
summary statistic of the nearby outputs. For example, the max pooling operation
reports the maximum output within a rectangular neighborhood. Other popular
pooling functions include the average of a rectangular neighborhood, the L2 norm
of a rectangular neighborhood, or a weighted average based on the distance from
the central pixel.
In all cases, pooling helps to make the representation become invariant to
small translations of the input. This means that if we translate the input by
a small amount, the values of most of the pooled outputs do not change. See
282

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.6: Eﬃciency of edge detection. The image on the right was formed by taking
each pixel in the original image and subtracting the value of its neighboring pixel on
the left. This shows the strength of all of the vertically oriented edges in the input
image, which can be a useful operation for object detection. Both images are 280 pixels
tall. The input image is 320 pixels wide while the output image is 319 pixels wide.
This transformation can be described by a convolution kernel containing 2 elements, and
requires 319 × 280 × 3 = 267, 960 ﬂoating point operations (two multiplications and one
addition per output pixel) to compute. To describe the same transformation with a matrix
multiplication would take 320 × 280 × 319 × 280, or over 8 billion, entries in the matrix,
making convolution 4 billion times more eﬃcient for representing this transformation. The
straightforward matrix multiplication algorithm performs over 16 billion ﬂoating point
operations, making convolution roughly 60,000 times more eﬃcient computationally. Of
course, most of the entries of the matrix would be zero. If we stored only the nonzero
entries of the matrix, then both matrix multiplication and convolution would require the
same number of ﬂoating point operations to compute. The matrix would still need to
contain 2 × 319 × 280 = 178, 640 entries. Convolution is an extremely eﬃcient way of
describing transformations that apply the same linear transformation of a small, local
region across the entire input. (Photo credit: Paula Goodfellow)

283

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.7: The components of a typical convolutional neural network layer. There are two
commonly used sets of terminology for describing these layers. Left) In this terminology,
the convolutional net is viewed as a small number of relatively complex layers, with each
layer having many “stages.” In this terminology, there is a one-to-one mapping between
kernel tensors and network layers. In this book we generally use this terminology. Right)
In this terminology, the convolutional net is viewed as a larger number of simple layers;
every step of processing is regarded as a layer in its own right. This means that not every
“layer” has parameters.

284

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.8: Max pooling introduces invariance. Left: A view of the middle of the output
of a convolutional layer. The bottom row shows outputs of the nonlinearity. The top
row shows the outputs of max pooling, with a stride of 1 between pooling regions and a
pooling region width of 3. Right: A view of the same network, after the input has been
shifted to the right by 1 pixel. Every value in the bottom row has changed, but only
half of the values in the top row have changed, because the max pooling units are only
sensitive to the maximum value in the neighborhood, not its exact location.

Fig. 9.8 for an example of how this works. Invariance to local translation
can be a very useful property if we care more about whether some
feature is present than exactly where it is. For example, when determining KEY
whether an image contains a face, we need not know the location of the eyes with IDEA
pixel-perfect accuracy, we just need to know that there is an eye on the left side
of the face and an eye on the right side of the face. In other contexts, it is more
important to preserve the location of a feature. For example, if we want to ﬁnd a
corner deﬁned by two edges meeting at a speciﬁc orientation, we need to preserve
the location of the edges well enough to test whether they meet.
The use of pooling can be viewed as adding an inﬁnitely strong prior that
the function the layer learns must be invariant to small translations. When this
assumption is correct, it can greatly improve the statistical eﬃciency of the network.
Pooling over spatial regions produces invariance to translation, but if we pool
over the outputs of separately parametrized convolutions, the features can learn
which transformations to become invariant to (see Fig. 9.9).
Because pooling summarizes the responses over a whole neighborhood, it is
possible to use fewer pooling units than detector units, by reporting summary
statistics for pooling regions spaced k pixels apart rather than 1 pixel apart.
See Fig. 9.10 for an example. This improves the computational eﬃciency of the
network because the next layer has roughly k times fewer inputs to process. When
the number of parameters in the next layer is a function of its input size (such as
when the next layer is fully connected and based on matrix multiplication) this
reduction in the input size can also result in improved statistical eﬃciency and
reduced memory requirements for storing the parameters.
TODO: ﬁgure resembling http://deeplearning.net/tutorial/lenet.html#the-full-m
285

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.9: Example of learned invariances: If each of these ﬁlters drive units that appear
in the same max-pooling region, then the pooling unit will detect “5”s in any rotation.
By learning to have each ﬁlter be a diﬀerent rotation of the “5” template, this pooling
unit has learned to be invariant to rotation. This is in contrast to translation invariance,
which is usually achieved by hard-coding the net to pool over shifted versions of a single
learned ﬁlter.

1.

0.1

1.

0.2

0.2

0.1

0.1

0.0

0.1

Figure 9.10: Pooling with downsampling. Here we use max-pooling with a pool width of
3 and a stride between pools of 2. This reduces the representation size by a factor of 2,
which reduces the computational and statistical burden on the next layer. Note that the
ﬁnal pool has a smaller size, but must be included if we do not want to ignore some of
the detector units.

286

CHAPTER 9. CONVOLUTIONAL NETWORKS

e.g. show a representative example of a net with multiple layers, diﬀerent numbers
of ﬁlters at each layer, diﬀerent spatial sizes as you go deeper
For many tasks, pooling is essential for handling inputs of varying size. For
example, if we want to classify images of variable size, the input to the classiﬁcation layer must have a ﬁxed size. This is usually accomplished by varying the
size of and oﬀset between pooling regions so that the classiﬁcation layer always
receives the same number of summary statistics regardless of the input size. For
example, the ﬁnal pooling layer of the network may be deﬁned to output four sets
of summary statistics, one for each quadrant of an image, regardless of the image
size. TODO: add ﬁgure showing a classiﬁer network with a fully connected layer,
and then one with global average pooling.
Some theoretical work gives guidance as to which kinds of pooling one should
use in various situations (Boureau et al., 2010). It is also possible to dynamically
pool features together, for example, by running a clustering algorithm on the
locations of interesting features (Boureau et al., 2011). This approach yields a
diﬀerent set of pooling regions for each image. Another approach is to learn a
single pooling structure that is then applied to all images (Jia et al., 2012).
Pooling can complicate some kinds of neural network architectures that use
top-down information, such as Boltzmann machines and autoencoders. These
issues will be discussed further when we present these types of networks. Pooling
in convolutional Boltzmann machines is presented in Chapter 20.7. The inverselike operations on pooling units needed in some diﬀerentiable networks will be
covered in Chapter 20.9.6.

9.4

Convolution and Pooling as an Inﬁnitely Strong
Prior

Recall the concept of a prior probability distribution from Chapter 5.3. This is a
probability distribution over the parameters of a model that encodes our beliefs
about what models are reasonable, before we have seen any data.
Priors can be considered weak or strong depending on how concentrated the
probability density in the prior is. A weak prior is a prior distribution with high
entropy, such a Gaussian distribution with high variance. Such a prior allows the
data to move the parameters more or less freely. A strong prior has very low
entropy, such as a Gaussian distribution with low variance. Such a prior plays a
more active role in determining where the parameters end up.
An inﬁnitely strong prior places zero probability on some parameters and says
that these parameter values are completely forbidden, regardless of how much
support the data gives to those values.
We can imagine a convolutional net as being similar to a fully connected net,
287

CHAPTER 9. CONVOLUTIONAL NETWORKS

but with an inﬁnitely strong prior over its weights. This inﬁnitely strong prior
says that the weights for one hidden unit must be identical to the weights of its
neighbor, but shifted in space. The prior also says that the weights must be zero,
except for in the small, spatially contiguous receptive ﬁeld assigned to that hidden
unit. Overall, we can think of the use of convolution as introducing an inﬁnitely
strong prior probability distribution over the parameters of a layer. This prior
says that the function the layer should learn contains only local interactions and
is equivariant to translation. Likewise, the use of pooling is in inﬁnitely strong
prior that each unit should be invariant to small translations.
Of course, implementing a convolutional net as a fully connected net with an
inﬁnitely strong prior would be extremely computationally wasteful. But thinking
of a convolutional net as a fully connected net with an inﬁnitely strong prior can
give us some insights into how convolutional nets work.
One key insight is that convolution and pooling can cause underﬁtting. Like
any prior, convolution and pooling are only useful when the assumptions made
by the prior are reasonably accurate. If a task relies on preserving precision
spatial information, then using pooling on all features can cause underﬁtting.
(Some convolution network architectures (Szegedy et al., 2014a) are designed to
use pooling on some channels but not on other channels, in order to get both
highly invariant features and features that will not underﬁt when the translation
invariance prior is incorrect) When a task involves incorporating information from
very distant locations in the input, then the prior imposed by convolution may
be inappropriate.
Another key insight from this view is that we should only compare convolutional models to other convolutional models in benchmarks of statistical learning
performance. Models that do not use convolution would be able to learn even
if we permuted all of the pixels in the image. For many image datasets, there
are separate benchmarks for models that are permutation invariant and must discover the concept of topology via learning, and models that have the knowledge
of spatial relationships hard-coded into them by their designer.

9.5

Variants of the Basic Convolution Function

When discussing convolution in the context of neural networks, we usually do
not refer exactly to the standard discrete convolution operation as it is usually
understood in the mathematical literature. The functions used in practice diﬀer
slightly. Here we describe these diﬀerences in detail, and highlight some useful
properties of the functions used in neural networks.
First, when we refer to convolution in the context of neural networks, we usually actually mean an operation that consists of many applications of convolution
288

CHAPTER 9. CONVOLUTIONAL NETWORKS

in parallel. This is because convolution with a single kernel can only extract one
kind of feature, albeit at many spatial locations. Usually we want each layer of
our network to extract many kinds of features, at many locations.
Additionally, the input is usually not just a grid of real values. Rather, it is a
grid of vector-valued observations. For example, a color image has a red, green,
and blue intensity at each pixel. In a multilayer convolutional network, the input
to the second layer is the output of the ﬁrst layer, which usually has the output
of many diﬀerent convolutions at each position. When working with images, we
usually think of the input and output of the convolution as being 3-D tensors, with
one index into the diﬀerent channels and two indices into the spatial coordinates
of each channel. (Software implementations usually work in batch mode, so they
will actually use 4-D tensors, with the fourth axis indexing diﬀerent examples in
the batch)
Note that because convolutional networks usually use multi-channel convolution, the linear operations they are based on are not guaranteed to be commutative, even if kernel-ﬂipping is used. These multi-channel operations are only
commutative if each operation has the same number of output channels as input
channels.
Assume we have a 4-D kernel tensor K with element K i,j,k,l giving the connection strength between a unit in channel i of the output and a unit in channel
j of the input, with an oﬀset of k rows and l columns between the output unit
and the input unit. Assume our input consists of observed data V with element
V i,j,k giving the value of the input unit within channel i at row j and column k.
Assume our output consists of Z with the same format as V. If Z is produced by
convolving K across V without ﬂipping K, then
X
Zi,j,k =
Vl,j+m,k+n Ki,l,m,n
l,m,n

where the summation over l, m, and n is over all values for which the tensor
indexing operations inside the summation is valid.
We may also want to skip over some positions of the kernel in order to reduce
the computational cost (at the expense of not extracting our features as ﬁnely).
We can think of this as downsampling the output of the full convolution function.
If we want to sample only every s pixels in each direction in the output, then we
can deﬁned a downsampled convolution function c such that:
X
Zi,j,k = c(K, V, s)i,j,k =
[Vl,j×s+m,k×s+n Ki,l,m,n ] .
(9.1)
l,m,n

We refer to s as the stride of this downsampled convolution. It is also possible
to deﬁne a separate stride for each direction of motion. TODO add a ﬁgure for
289

CHAPTER 9. CONVOLUTIONAL NETWORKS

this
One essential feature of any convolutional network implementation is the ability to implicitly zero-pad the input V in order to make it wider. Without this
feature, the width of the representation shrinks by the kernel width - 1 at each
layer. Zero padding the input allows us to control the kernel width and the size of
the output independently. Without zero padding, we are forced to choose between
shrinking the spatial extent of the network rapidly and using small kernels–both
scenarios that signiﬁcantly limit the expressive power of the network. See Fig. 9.11
for an example.
Three special cases of the zero-padding setting are worth mentioning. One is
the extreme case in which no zero-padding is used whatsoever, and the convolution
kernel is only allowed to visit positions where the entire kernel is contained entirely
within the image. In MATLAB terminology, this is called valid convolution. In
this case, all pixels in the output are a function of the same number of pixels in
the input, so the behavior of an output pixel is somewhat more regular. However,
the size of the output shrinks at each layer. If the input image is of size m×m and
the kernel is of size k × k, the output will be of size m− k + 1 × m − k + 1. The rate
of this shrinkage can be dramatic if the kernels used are large. Since the shrinkage
is greater than 0, it limits the number of convolutional layers that can be included
in the network. As layers are added, the spatial dimension of the network will
eventually drop to 1 × 1, at which point additional layers cannot meaningfully
be considered convolutional. Another special case of the zero-padding setting
is when just enough zero-padding is added to keep the size of the output equal
to the size of the input. MATLAB calls this same convolution. In this case,
the network can contain as many convolutional layers as the available hardware
can support, since the operation of convolution does not modify the architectural
possibilities available to the next layer. However, the input pixels near the border
inﬂuence fewer output pixels than the input pixels near the center. This can
make the border pixels somewhat underrepresented in the model. This motivates
the other extreme case, which MATLAB refers to as full convolution, in which
enough zeroes are added for every pixel to be visited k times in each direction,
resulting in an output image of size m + k − 1 × m + k − 1. In this case, the output
pixels near the border are a function of fewer pixels than the output pixels near
the center. This can make it diﬃcult to learn a single kernel that performs well
at all positions in the convolutional feature map. Usually the optimal amount of
zero padding (in terms of test set classiﬁcation accuracy) lies somewhere between
“valid” and “same” convolution.
In some cases, we do not actually want to use convolution, but rather locally
connected layers. In this case, the adjacency matrix in the graph of our MLP is
the same, but every connection has its own weight, speciﬁed by a 6-D tensor W.
290

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.11: The eﬀect of zero padding on network size: Consider a convolutional network
with a kernel of width six at every layer. In this example, do not use any pooling, so
only the convolution operation itself shrinks the network size. Top) In this convolutional
network, we do not use any implicit zero padding. This causes the representation to
shrink by ﬁve pixels at each layer. Starting from an input of sixteen pixels, we are only
able to have three convolutional layers, and the last layer does not ever move the kernel,
so arguably only two of the layers are truly convolutional. The rate of shrinking can
be mitigated by using smaller kernels, but smaller kernels are less expressive and some
shrinking is inevitable in this kind of architecture. Bottom) By adding ﬁve implicit zeroes
to each layer, we prevent the representation from shrinking with depth. This allows us
to make an arbitrarily deep convolutional network.

291

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.12: TODO

The indices into W are respectively: i, the output channel, j, the output row, k,
the output column, l, the input channel, m, the row oﬀset within the input, and
n, the column oﬀset within the input. The linear part of a locally connected layer
is then given by
X
Zi,j,k =
[V l,j+m,k+n wi,j,k,l,m,n] .
l,m,n

This is sometimes also called unshared convolution, because it is a similar operation to discrete convolution with a small kernel, but without sharing parameters
across locations.
TODO: Mehdi asks for a local convolution ﬁgure, showing layers in 1D topology and comparing it to fully connected layer
Locally connected layers are useful when we know that each feature should
be a function of a small part of space, but there is no reason to think that the
same feature should occur across all of space. For example, if we want to tell if
an image is a picture of a face, we only need to look for the mouth in the bottom
half of the image.
It can also be useful to make versions of convolution or locally connected layers
in which the connectivity is further restricted, for example to constraint that each
output channel i be a function of only a subset of the input channels l. TODO:
explain more, this paragraph just kind of dies. include a ﬁgure
Tiled convolution (Gregor and LeCun, 2010; Le et al., 2010) oﬀers a compromise between a convolutional layer and a locally connected layer. Rather than
learning a separate set of weights at every spatial location, we learn a set of kernels
that we rotate through as we move through space. This means that immediately
neighboring locations will have diﬀerent ﬁlters, like in a locally connected layer,
but the memory requirements for storing the parameters will increase only by a
factor of the size of this set of kernels, rather than the size of the entire output
feature map. See Fig. 9.12 for a graphical depiction of tiled convolution.
To deﬁne tiled convolution algebraically, let k be a 6-D tensor, where two of
the dimensions correspond to diﬀerent locations in the output map. Rather than
having a separate index for each location in the output map, output locations
cycle through a set of t diﬀerent choices of kernel stack in each direction. If t is
equal to the output width, this is the same as a locally connected layer.
Z i,j,k =

X

Vl,j+m,k+nKi,l,m,n,j%t,k%t

l,m,n

It is straightforward to generalize this equation to use a diﬀerent tiling range
292

CHAPTER 9. CONVOLUTIONAL NETWORKS

for each dimension.
Both locally connected layers and tiled convolutional layers have an interesting
interaction with max-pooling: the detector units of these layers are driven by
diﬀerent ﬁlters. If these ﬁlters learn to detect diﬀerent transformed versions of
the same underlying features, then the max-pooled units become invariant to the
learned transformation (see Fig. 9.9). Convolutional layers are hard-coded to be
invariant speciﬁcally to translation.
Other operations besides convolution are usually necessary to implement a
convolutional network. To perform learning, one must be able to compute the
gradient with respect to the kernel, given the gradient with respect to the outputs.
In some simple cases, this operation can be performed using the convolution
operation, but many cases of interest, including the case of stride greater than 1,
do not have this property.
Recall that convolution is a linear operation and can thus be described as a
matrix multiplication (if we ﬁrst reshape the input tensor into a ﬂat vector). The
matrix involved is a function of the convolution kernel. The matrix is sparse and
each element of the kernel is copied to several elements of the matrix. It is not
usually practical to implement convolution in this way, but this view helps us to
derive some of the other operations needed to implement a convolutional network.
Multiplication by the transpose of the matrix deﬁned by convolution is one
such operation. This is the operation needed to backpropagate error derivatives
through a convolutional layer, so it is needed to train convolutional networks
that have more than one hidden layer. This same operation is also needed to
compute the reconstruction in a convolutional autoencoder (or to perform the
analogous role in a convolutional RBM, sparse coding model, etc.). Like the
kernel gradient operation, this input gradient operation can be implemented using
a convolution in some cases, but in the general case requires a third operation to be
implemented. Care must be taken to coordinate this transpose operation with the
forward propagation. The size of the output that the transpose operation should
return depends on the zero padding policy and stride of the forward propagation
operation, as well as the size of the forward propagation’s output map. In some
cases, multiple sizes of input to forward propagation can result in the same size
of output map, so the transpose operation must be explicitly told what the size
of the original input was.
It turns out that these three operations–convolution, backprop from output
to weights, and backprop from output to inputs–are suﬃcient to compute all of
the gradients needed to train any depth of feedforward convolutional network,
as well as to train convolutional networks with reconstruction functions based
on the transpose of convolution. See (Goodfellow, 2010) for a full derivation
of the equations in the fully general multi-dimensional, multi-example case. To
293

CHAPTER 9. CONVOLUTIONAL NETWORKS

give a sense of how these equations work, we present the two dimensional, single
example version here.
Suppose we want to train a convolutional network that incorporates strided
convolution of kernel stack K applied to multi-channel image V with stride s is
deﬁned by c(K, V, s) as in equation 9.1. Suppose we want to minimize some loss
function J(V, K). During forward propagation, we will need to use c itself to
output Z, which is then propagated through the rest of the network and used
to compute J. . During backpropagation, we will receive a tensor G such that
Gi,j,k = ∂Z∂ J(V, K).
i,j,k
To train the network, we need to compute the derivatives with respect to the
weights in the kernel. To do so, we can use a function
g(G, V, s) i,j,k,l =

X
∂
J(V, K) =
Gi,m,n Vj,m×s+k,n×s+l .
∂Ki,j,k,l
m,n

If this layer is not the bottom layer of the network, we’ll need to compute the
gradient with respect to V in order to backpropagate the error farther down. To
do so, we can use a function
h(K, G, s)i,j,k

∂
=
J(V, K) =
∂V i,j,k

X

X

X

Kq,i,m,p Gi,l,n.

l,m|s×l+m=j n,p|s×n+p=k q

We could also use h to deﬁne the reconstruction of a convolutional autoencoder, or the probability distribution over visible given hidden units in a convolutional RBM or sparse coding model. Suppose we have hidden units H in the
same format as Z and we deﬁne a reconstruction
R = h(K, H, s).
In order to train the autoencoder, we will receive the gradient with respect
to R as a tensor E. To train the decoder, we need to obtain the gradient with
respect to K. This is given by g(H, E, s). To train the encoder, we need to obtain
the gradient with respect to H. This is given by c(K, E, s). It is also possible to
diﬀerentiate through g using c and h, but these operations are not needed for the
backpropagation algorithm on any standard network architectures.
Generally, we do not use only a linear operation in order to transform from the
inputs to the outputs in a convolutional layer. We generally also add some bias
term to each output before applying the nonlinearity. This raises the question
of how to share parameters among the biases. For locally connected layers it is
natural to give each unit its own bias, and for tiled convolution, it is natural to
share the biases with the same tiling pattern as the kernels. For convolutional
layers, it is typical to have one bias per channel of the output and share it across
294

CHAPTER 9. CONVOLUTIONAL NETWORKS

all locations within each convolution map. However, if the input is of known,
ﬁxed size, it is also possible to learn a separate bias at each location of the output
map. Separating the biases may slightly reduce the statistical eﬃciency of the
model, but also allows the model to correct for diﬀerences in the image statistics
at diﬀerent locations. For example, when using implicit zero padding, detector
units at the edge of the image receive less total input and may need larger biases.

9.6

Structured Outputs

TODO show diagram of an exclusively convolutional net, like for image inpainting
or segmentation (Farabet? Collobert?)

9.7

Convolutional Modules

TODO history of ReLU -¿ maxout -¿ NIN -¿ inception

9.8

Data Types

The data used with a convolutional network usually consists of several channels,
each channel being the observation of a diﬀerent quantity at some point in space
or time. See Table 9.1 for examples of data types with diﬀerent dimensionalities
and number of channels.
So far we have discussed only the case where every example in the train
and test data has the same spatial dimensions. One advantage to convolutional
networks is that they can also process inputs with varying spatial extents. These
kinds of input simply cannot be represented by traditional, matrix multiplicationbased neural networks. This provides a compelling reason to use convolutional
networks even when computational cost and overﬁtting are not signiﬁcant issues.
For example, consider a collection of images, where each image has a diﬀerent
width and height. It is unclear how to apply matrix multiplication. Convolution
is straightforward to apply; the kernel is simply applied a diﬀerent number of
times depending on the size of the input, and the output of the convolution
operation scales accordingly. Sometimes the output of the network is allowed to
have variable size as well as the input, for example if we want to assign a class
label to each pixel of the input. In this case, no further design work is necessary.
In other cases, the network must produce some ﬁxed-size output, for example if
we want to assign a single class label to the entire image. In this case we must
make some additional design steps, like inserting a pooling layer whose pooling
regions scale in size proportional to the size of the input, in order to maintain a
ﬁxed number of pooled outputs.
295

CHAPTER 9. CONVOLUTIONAL NETWORKS

1-D

2-D

3-D

Single channel
Audio waveform: The axis we
convolve over corresponds to
time. We discretize time and
measure the amplitude of the
waveform once per time step.

Audio data that has been preprocessed with a Fourier transform: We can transform the audio waveform into a 2D tensor
with diﬀerent rows corresponding to diﬀerent frequencies and
diﬀerent columns corresponding
to diﬀerent points in time. Using
convolution in the time makes
the model equivariant to shifts in
time. Using convolution across
the frequency axis makes the
model equivariant to frequency,
so that the same melody played
in a diﬀerent octave produces
the same representation but at a
diﬀerent height in the network’s
output.
Volumetric data: A common
source of this kind of data
is medical imaging technology,
such as CT scans.

Multi-channel
Skeleton
animation
data:
Animations of 3-D computerrendered characters are generated by altering the pose of
a “skeleton” over time.
At
each point in time, the pose
of the character is described
by a speciﬁcation of the angles
of each of the joints in the
character’s skeleton. Each channel in the data we feed to the
convolutional model represents
the angle about one axis of one
joint.
Color image data: One channel
contains the red pixels, one the
green pixels, and one the blue
pixels. The convolution kernel
moves over both the horizontal
and vertical axes of the image,
conferring translation equivariance in both directions.

Color video data: One axis corresponds to time, one to the
height of the video frame, and
one to the width of the video
frame.

Table 9.1: Examples of diﬀerent formats of data that can be used with convolutional
networks.
296

CHAPTER 9. CONVOLUTIONAL NETWORKS

Note that the use of convolution for processing variable sized inputs only makes
sense for inputs that have variable size because they contain varying amounts of
observation of the same kind of thing–diﬀerent lengths of recordings over time,
diﬀerent widths of observations over space, etc. Convolution does not make sense
if the input has variable size because it can optionally include diﬀerent kinds
of observations. For example, if we are processing college applications, and our
features consist of both grades and standardized test scores, but not every applicant took the standardized test, then it does not make sense to convolve the
same weights over both the features corresponding to the grades and the features
corresponding to the test scores.

9.9

Eﬃcient Convolution Algorithms

Modern convolutional network applications often involve networks containing
more than one million units. Powerful implementations exploiting parallel computation resources, as discussed in Chapter 12.1 are essential. However, in many
cases it is also possible to speed up convolution by selecting an appropriate convolution algorithm.
Convolution is equivalent to converting both the input and the kernel to the
frequency domain using a Fourier transform, performing point-wise multiplication
of the two signals, and converting back to the time domain using an inverse
Fourier transform. For some problem sizes, this can be faster than the naive
implementation of discrete convolution.
When a d-dimensional kernel can be expressed as the outer product of d
vectors, one vector per dimension, the kernel is called separable. When the kernel
is separable, naive convolution is ineﬃcient. It is equivalent to compose d onedimensional convolutions with each of these vectors. The composed approach
is signiﬁcantly faster than performing one k-dimensional convolution with their
outer product. The kernel also takes fewer parameters to represent as vectors.
If the kernel is w elements wide in each dimension, then naive multidimensional
convolution requires O(w d ) runtime and parameter storage space, while separable
convolution requires O(w × d) runtime and parameter storage space. Of course,
not every convolution can be represented in this way.
Devising faster ways of performing convolution or approximate convolution
without harming the accuracy of the model is an active area of research. Even
techniques that improve the eﬃciency of only forward propagation are useful
because in the commercial setting, it is typical to devote many more resources to
deployment of a network than to its training.

297

CHAPTER 9. CONVOLUTIONAL NETWORKS

9.10

Random or Unsupervised Features

Typically, the most expensive part of convolutional network training is learning
the features. The output layer is usually relatively inexpensive due to the small
number of features provided as input to this layer after passing through several
layers of pooling. When performing supervised training with gradient descent,
every gradient step requires a complete run of forward propagation and backward
propagation through the entire network. One way to reduce the cost of convolutional network training is to use features that are not trained in a supervised
fashion.
There are two basic strategies for obtaining convolution kernels without supervised training. One is to simply initialize them randomly. The other is to
learn them with an unsupervised criterion. This approach allows the features to
be determined separately from the classiﬁer layer at the top of the architecture.
One can then extract the features for the entire training set just once, essentially
constructing a new training set for the last layer. Learning the last layer is then
typically a convex optimization problem, assuming the last layer is something like
logistic regression or an SVM.
Random ﬁlters often work surprisingly well in convolutional networks (Jarrett et al., 2009b; Saxe et al., 2011; Pinto et al., 2011; Cox and Pinto, 2011).
Saxe et al. (2011) showed that layers consisting of convolution following by pooling naturally become frequency selective and translation invariant when assigned
random weights. They argue that this provides an inexpensive way to choose the
architecture of a convolutional network: ﬁrst evaluate the performance of several
convolutional network architectures by training only the last layer, then take the
best of these architectures and train the entire architecture using a more expensive
approach.
An intermediate approach is to learn the features, but using methods that
do not require full forward and back-propagation at every gradient step. As
with multilayer perceptrons, we use greedy layer-wise unsupervised pretraining,
to train the ﬁrst layer in isolation, then extract all features from the ﬁrst layer
only once, then train the second layer in isolation given those features, and so on.
The canonical example of this is the convolutional deep belief network (Lee et al.,
2009). Convolutional networks oﬀer us the opportunity to take this strategy one
step further than is possible with multilayer perceptrons. Instead of training an
entire convolutional layer at a time, we can actually train a small but denselyconnected unsupervised model (such as PSD, described in Chapter 15.8.2, or
k-means) of a single image patch. We can then use the weight matrices from this
patch-based model to deﬁne the kernels of a convolutional layer. This means that
it is possible to use unsupervised learning to train a convolutional network without
ever using convolution during the training process. Using this approach, we can
298

CHAPTER 9. CONVOLUTIONAL NETWORKS

train very large models and incur a high computational cost only at inference
time (Ranzato et al., 2007b; Jarrett et al., 2009b; Kavukcuoglu et al., 2010a;
Coates et al., 2013).
As with other approaches to unsupervised pretraining, it remains diﬃcult to
tease apart the cause of some of the beneﬁts seen with this approach. Unsupervised pretraining may oﬀer some regularization relative to supervised training,
or it may simply allow us to train much larger architectures due to the reduced
computational cost of the learning rule.

9.11

The Neuroscientiﬁc Basis for Convolutional Networks

Convolutional networks are perhaps the greatest success story of biologically inspired artiﬁcial intelligence. Though convolutional networks have been guided
by many other ﬁelds, some of the key design principles of neural networks were
drawn from neuroscience.
The history of convolutional networks begins with neuroscientiﬁc experiments
long before the relevant computational models were developed. Neurophysiologists David Hubel and Torsten Wiesel collaborated for several years to determine many of the most basic facts about how the mammalian vision system
works (Hubel and Wiesel, 1959, 1962, 1968). Their accomplishements were eventually recognized with a Nobel Prize. Their ﬁndings that have had the greatest
inﬂuence on contemporary deep learning models were based on recording the
activity of individual neurons in cats. By anesthetizing the cat, they could immobilize the cat’s eye and observe how neurons in the cat’s brain responded to
images projected in precise locations on a screen in front of the cat.
Their worked helped to characterize many aspects of brain function that are
beyond the scope of this book. From the point of view of deep learning, we can
focus on a simpliﬁed, cartoon view of brain function.
In this simpliﬁed view, we focus on a part of the brain called V1, also known
as the primary visual cortex. V1 is the ﬁrst area of the brain that begins to
perform signiﬁcantly advanced processing of visual input. In this cartoon view,
images are formed by light arriving in the eye and stimulating the retina, the
light-sensitive tissue in the back of the eye. The neurons in the retina perform
some simple preprocessing of the image but do not substantially alter the way it
is represented. The image then passes through the optic nerve and a brain region
called the lateral geniculate nucleus. The main role, as far as we are concerned
here, of both of these anatomical regions is primarily just to carry the signal from
the eye to V1, which is located at the back of the head.
A convolutional network layer is designed to capture three properties of V1:
299

CHAPTER 9. CONVOLUTIONAL NETWORKS

1. V1 is arranged in a spatial map. It actually has a two-dimensional structure mirroring the structure of the image in the retina. For example, light
arriving at the lower half of the retina aﬀects only the corresponding half of
V1. Convolutional networks capture this property by having their features
deﬁned in terms of two dimensional maps.
2. V1 contains many simple cells. A simple cell’s activity can to some extent be
characterized by a linear function of the image in a small, spatially localized
receptive ﬁeld. The detector units of a convolutional network are designed
to emulate these properties of simple cells. V1 also contains many complex
cells. These cells respond to features that are similar to those detected by
simple cells, but complex cells are invariant to small shifts in the position
of the feature. This inspires the pooling units of convolutional networks.
Complex cells are also invariant to some changes in lighting that cannot
be captured simply by pooling over spatial locations. These invariances
have inspired some of the cross-channel pooling strategies in convolutional
networks, such as maxout units (Goodfellow et al., 2013a).
Though we know the most about V1, it is generally believed that the same
basic principles apply to other brain regions. In our cartoon view of the visual
system, the basic strategy of detection followed by pooling is repeatedly applied
as we move deeper into the brain. As we pass through multiple anatomical layers
of the brain, we eventually ﬁnd cells that respond to some speciﬁc concept and are
invariant to many transformations of the input. These cells have been nicknamed
“grandmother cells”— the idea is that a person could have a neuron that activates
when seeing an image of their grandmother, regardless of whether she appears in
the left or right side of the image, whether the image is a close-up of her face or
zoomed out shot of her entire body, whether she is brightly lit, or in shadow, etc.
These grandmother cells have been shown to actually exist in the human brain,
in a region called the medial temporal lobe (Quiroga et al., 2005). Researchers
tested whether individual neurons would respond to photos of famous individuals,
and found what has come to be called the “Halle Berry neuron”: an individual
neuron that is activated by the concept of Halle Berry. This neuron ﬁres when
a person sees a photo of Halle Berry, a drawing of Halle Berry, or even text
containing the words “Halle Berry.” Of course, this has nothing to do with Halle
Berry herself; other neurons responded to the presence of Bill Clinton, Jennifer
Aniston, etc.
These medial temporal lobe neurons are somewhat more general than modern
convolutional networks, which would not automatically generalize to identifying
a person or object when reading its name. The closest analog to a convolutional
network’s last layer of features is a brain area called the inferotemporal cortex
(IT). When viewing an object, information ﬂows from the retina, through the
300

CHAPTER 9. CONVOLUTIONAL NETWORKS

LGN, to V1, then onward to V2, then V4, then IT. This happens within the ﬁrst
100ms of glimpsing an object. If a person is allowed to continue looking at the
object for more time, then information will begin to ﬂow backwards as the brain
uses top-down feedback to update the activations in the lower level brain areas.
However, if we interrupt the person’s gaze, and observe only the ﬁring rates that
result from the ﬁrst 100ms of mostly feed-forward activation, then IT proves to be
very similar to a convolutional network. Convolutional networks can predict IT
ﬁring rates, and also perform very similarly to (time limited) humans on object
recognition tasks (DiCarlo, 2013).
That being said, there are many diﬀerences between convolutional networks
and the mammalian vision system. Some of these diﬀerences are well known
to computational neuroscientists, but outside the scope of this book. Some of
these diﬀerences are not yet known, because many basic questions about how the
mamalian vision system works. As a brief list:
• The human eye is mostly very low resolution, except for a tiny patch called
the fovea. The fovea only observes an area about the size of a thumbnail
held at arms length. Though we feel as if we can see an entire scene in high
resolution, this is an illusion created by the subconscious part of our brain,
as it stitches together several glimpses of small areas. Most convolutional
networks actual receive large full resolution photographs as input.
• The human visual system is integrated with many other senses, such as
hearing, and factors like our moods and thoughts. Convolutional networks
so far are purely visual.
• The human visual system does much more than just recognize objects. It is
able to understand entire scenes including many objects and relationships
between objects, and processes rich 3-D geometric information needed for
our bodies to interface with the world. Convolutional networks have been
applied to some of these problems but these applications are in their infancy.
• Even simple brain areas like V1 are heavily impacted by feedback from
higher levels. Feedback has been explored extensively in neural network
models but has not yet been shown to oﬀer a compelling improvement.
• While feed-forward IT ﬁring rates capture much of the same information as
convolutional network features, it’s not clear how similar the intermediate
computations are. The brain probably uses very diﬀerent activation and
pooling functions. An individual neuron’s activation probably is not wellcharacterized by a single linear ﬁlter response. A recent model of V1 involves
multiple quadratic ﬁlters for each neuron (Rust et al., 2005). Indeed our
301

CHAPTER 9. CONVOLUTIONAL NETWORKS

cartoon picture of “simple cells” and “complex cells” might create a nonexistent distinction; simple cells and complex cells might both be the same
kind of cell but with their “parameters” enabling a continuum of behaviors
ranging from what we call “simple” to what we call “complex.”
It’s also worth mentioning that neuroscience has told us relatively little about
how to train convolutional networks. Model structures with parameter sharing across multiple spatial locations date back to early connectionist models
of vision (Marr and Poggio, 1976), but these models did not use the modern
backpropagation algorithm and gradient descent. For example, the Neocognitron (Fukushima, 1980) incorporated most of the model architecture design elements of the modern convolutional network but relied on a layerwise unsupervised
clustering algorithm.
Lang and Hinton (1988) introduced the use of backpropagation to train timedelay neural networks (TDNNs). To use contemporary terminology, TDNNs are
one-dimensional convolutional networks applied to time series. Back-propagation
applied to these models was not inspired by any neuroscientiﬁc observation and
is considered by some to be biologically implausible. Following the success of
backpropagation-based training of TDNNs, (LeCun et al., 1989) developed the
modern convolutional network by applying the same training algorithm to 2-D
convolution applied to images.
So far we have described how simple cells are roughly linear and selective for
certain features, complex cells are more non-linear and become invariant to some
transformations of these simple cell features, and stacks of layers that alternative
between selectivity and invariance can yield grandmother cells for very speciﬁc
phenomena. We have not yet described precisely what these individual cells
detect. In a deep, nonlinear network, it can be diﬃcult to understand the function
of individual cells. Simple cells in the ﬁrst layer are easier to analyze, because
their responses are driven by a linear function. In an artiﬁcial neural network,
we can just display an image of the kernel to see what the corresponding channel
of a convolutional layer responds to. In a biological neural network, we do not
have access to the weights themselves. Instead, we put an electrode in the neuron
itself, display several samples of white noise images in front of the animal’s retina,
and record how each of these samples causes the neuron to activate. We can then
ﬁt a linear model to these responses in order to obtain an approximation of the
neuron’s weights. This approach is known as reverse correlation (Ringach and
Shapley, 2004).
Reverse correlation shows us that most V1 cells have weights that are described
by Gabor functions. The Gabor function describes the weight at a 2-D point in
the image. We can think of an image as being a function of 2-D coordinates,
I(x, y). Likewise, we can think of a simple cell as sampling the image at a set of
302

CHAPTER 9. CONVOLUTIONAL NETWORKS

locations, deﬁned by a set of x coordinates X and a set of y coordinates, Y, and
applying weights that are also a function of the location, w(x, y). From this point
of view, the response of a simple cell to an image is given by
XX
s(I) =
w(x, y)I(x, y).
x∈X y∈Y

Speciﬁcally, w(x, y) takes the form of a Gabor function:


w(x, y; α, βx , β y , f, φ, x0 , y0 , τ) = α exp −β xx 02 − β y y 02 cos(f x0 + φ),
where

x 0 = (x − x 0 ) cos(τ) + (y − y0) sin(τ)
and
y0 = −(x − y0 ) sin(τ) + (y − y0 ) cos(τ).
Here, α, βx , β y , f, φ, x 0, y 0, and τ are parameters that control the properties
of the Gabor function. Fig. 9.13 shows some examples of Gabor functions with
diﬀerent settings of these parameters.
The parameters x 0, y 0, and τ deﬁne a coordinate system. We translate and
rotate x and y to form x0 and y0 . Speciﬁcally, the simple cell will respond to
image features centered at the point (x 0, y0 ), and it will respond to changes in
brightness as we move along a line rotated τ radians from the horizontal.
Viewed as a function of x0 and y0 , the function w then responds to changes in
brightness as we move along the x0 axis. It has two important factors: one is a
Gaussian function and the other
 is a 02cosine function.

The Gaussian factor α exp −βxx − βy y02 can be seen as a gating term that
ensures the simple cell will only respond to values near where x0 and y0 are both
zero, in other words, near the center of the cell’s receptive ﬁeld. The scaling
factor α adjusts the total magnitude of the simple cell’s response, while β x and
β y control how quickly its receptive ﬁeld falls oﬀ.
The cosine factor cos(f x0 +φ) controls how the simple cell responds to changing
brightness along the x0 axis. The parameter f controls the frequency of the cosine
and φ controls its phase oﬀset.
Altogether, this cartoon view of simple cells means that a simple cell responds
to a speciﬁc spatial frequency of brightness in a speciﬁc direction at a speciﬁc
location. They are most excited when the wave of brightness in the image has the
same phase as the weights (i.e., when the image is bright where the weights are
positive and dark where the weights are negative) and are most inhibited when
the wave of brightness is fully out of phase with the weights (i.e., when the image
is dark where the weights are positive and bright where the weights are negative).
303

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.13: Gabor functions with a variety of parameter settings. White indicates
large positive weight, black indicates large negative weight, and the background gray
corresponds to zero weight. Left) Gabor functions with diﬀerent values of the parameters
that control the coordinate system: x 0 , y0 , and τ . Each gabor function in this grid is
assigned a value of x0 and y0 proportional to its position in its grid, and τ is chosen so that
each Gabor is sensitive to the direction radiating out from the center of the grid. For the
other two plots, x0 , y0 , and τ are ﬁxed to zero. Center) Gabor functions with diﬀerent
Gaussian scale parameters betax and β y . Gabor functions are arranged in increasing
width (decreasing β x ) as we move left to right through the grid, and increasing height
(decreasing βy ) as we move top to bottom. For the other two plots, the β values are ﬁxed
to 1.5× the image width. Right) Gabor functions with diﬀerent sinusoid parameters f
and φ. As we move top to bottom, f increases, and as we move left to right, φ increases.
For the other two plots, φ is ﬁxed to 0 and f is ﬁxed to 5× the image width.

304

CHAPTER 9. CONVOLUTIONAL NETWORKS

2
The cartoon view of a complex cell is that it computes
p the L norm of the
2-D vector containing two simple cell’s responses: c(I) = s0 (I)2 + s 1 (I)2. An
important special case occurs when s1 has all of the same parameters as s0 except
for φ, and φ is set such that s 1 is one quarter cycle out of phase with s0 . In this
case, s0 and s1 form a quadrature pair. A complex cell deﬁned in this way responds
when the Gaussian reweighted image I(x, y) exp(−β xx02 − βy y02) contains a high
amplitude sinusoidal wave with frequency f in direction τ near (x 0, y0 ), regardless
of the phase oﬀset of this wave. In other words, the complex cell is invariant to
small translations of the image in direction τ, or to negating the image (replacing
black with white and vice versa).
Some of the most striking correspondences between neuroscience and machine
learning come from visually comparing the features learned by machine learning
models with those employed by V1. Olshausen and Field (1996) showed that
a simple unsupervised learning algorithm, sparse coding, learns features with receptive ﬁelds similar to those of simple cells. Since then, we have found that
an extremely wide variety of statistical learning algorithms learn features with
Gabor-like functions when applied to natural images. This includes most deep
learning algorithms, which learn these features in their ﬁrst layer. Fig. 9.14 shows
some examples. Because so many diﬀerent learning algorithms learn edge detectors, it is diﬃcult to conclude that any speciﬁc learning algorithm is the “right”
model of the brain just based on the features that it learns (though it can certainly be a bad sign if an algorithm does not learn some sort of edge detector
when applied to natural images). These features are an important part of the
statistical structure of natural images and can be recovered by many diﬀerent
approaches to statistical modeling. See Hyvärinen et al. (2009) for a review of
the ﬁeld of natural image statistics.

9.12

Convolutional Networks and the History of Deep
Learning

Convolutional networks have played an important role in the history of deep learning. They are a key example of a successful application of insights obtained by
studying the brain to machine learning applications. They were also some of the
ﬁrst deep models to perform well, long before arbitrary deep models were considered viable. Convolutional networks were also some of the ﬁrst neural networks to
solve important commercial applications and remain at the forefront of commercial applications of deep learning today. For example, in the 1990’s, the neural
network research group at AT&T developed a convolutional network for reading
checks (LeCun et al., 1998d). By the end of the 1990’s, this system deployed by
NEC was reading over 10% of all the checks in the US. A little bit later, several
305

CHAPTER 9. CONVOLUTIONAL NETWORKS

Figure 9.14: Many machine learning algorithms learn features that detect edges or speciﬁc
colors of edges when applied to natural images. These feature detectors are reminiscent
of the Gabor functions known to be present in primary visual cortex. Left) Weights
learned by an unsupervised learning algorithm (spike and slab sparse coding) applied to
small image patches. Right) Convolution kernels learned by the ﬁrst layer of a fully
supervised convolutional maxout network. Neighboring pairs of ﬁlters drive the same
maxout unit.

OCR and handwriting recognitions systems based on convolutional nets were deployed by Microsoft (Simard et al., 2003). See Chapter 12 for more details on
such applications and more modern applications of convolutional networks. See
LeCun et al. (2010) for a more in-depth history of convolutional networks up to
2010.
Convolutional networks were also used to win many contests. The current
intensity of commercial interest in deep learning began when Krizhevsky et al.
(2012a) won the ImageNet object recognition challenge, but convolutional networks had been used to win other machine learning and computer vision contests
with less impact for years earlier.
Convolutional nets were some of ﬁrst working deep networks trained with
back-propagation. It is not entirely clear why convolutional networks succeeded
when general backpropagation networks were considered to have failed. It may
simply be that convolutional networks were more computationally eﬃcient than
fully connected networks, so it was easier to run multiple experiments with them
and tune their implementation and hyperparameters. Larger networks also seem
to be easier to train. With modern hardware, fully connected networks appear to
perform reasonably on many tasks, even when using datasets that were available
and activation functions that were popular during the times when fully connected
306

CHAPTER 9. CONVOLUTIONAL NETWORKS

networks were believed not to work well. It may be that the primary barriers to
the success of neural networks were psychological, e.g., having trained for much
longer (rather than concluding that training had stalled out of discouragement)
might have yielded much better results. Whatever the case, it is fortunate that
convolutional networks performed well decades ago. In many ways, they “carried
the torch” for the rest of deep learning and paved the way to the acceptance of
neural networks in general.

307

Chapter 10

Sequence Modeling: Recurrent
and Recursive Nets
One of the early ideas found in machine learning and statistical models of the 80’s
is that of sharing parameters1 across diﬀerent parts of a model, allowing to extend
and apply the model to examples of diﬀerent forms and generalize across them,
e.g. with examples of diﬀerent lengths, in the case of sequential data. This can be
found in hidden Markov models (HMMs) (Rabiner and Juang, 1986), which were
the dominant technique for speech recognition for about 30 years. These models
of sequences are described a bit more in Section 10.9.3 and involve parameters,
such as the state-to-state transition matrix P (st | st−1 ), which are re-used for
every time step t, i.e., the above probability depends only on the value of s t and
s t−1 but not on t as such. This allows one to model variable length sequences,
whereas if we had speciﬁc parameters for each value of t, we could not generalize
to sequence lengths not seen during training, nor share statistical strength across
diﬀerent sequence lengths and across diﬀerent positions in time. Such sharing is
particularly important when, like in speech, the input sequence can be stretched
non-linearly, i.e., some parts (like vowels) may last longer in diﬀerent examples. It
means that the absolute time step at which an event occurs is meaningless: it only
makes sense to consider the event in some context that somehow captures what
has happened before. This sharing across time can also be found in a recurrent
neural network (Rumelhart et al., 1986c) or RNN 2 : the same weights are used for
diﬀerent instances of the artiﬁcial neurons at diﬀerent time steps, allowing us to
apply the network to input sequences of diﬀerent lengths. This idea is made more
1

see Section 7.8 for an introduction to the concept of parameter sharing
Unfortunately, the RNN acronym is sometimes also used for denoting Recursive Neural
Networks. However, since the RNN acronym has been around for much longer, we suggest
keeping this acronym for Recurrent Neural Networks.
2

308

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

explicit in the early work on time-delay neural networks (Lang and Hinton, 1988;
Waibel et al., 1989), where a fully connected network is replaced by one with local
connections that are shared across diﬀerent temporal instances of the hidden units.
Such networks are among the ancestors of convolutional neural networks, covered
in more detail in Section 9. Recurrent nets are covered below in Section 10.2. As
shown in Section 10.1 below, the ﬂow graph (a notion introduced in Section 6.4
in the case of MLPs) associated with a recurrent network is structured like a
chain, as explained next. Recurrent neural networks have been generalized into
recursive neural networks, in which the structure can be more general, i.e., and
it is typically viewed as a tree. Recursive neural networks are discussed in more
detail in Section 10.6. For a good textbook on RNNs, see Graves (2012).

10.1

Unfolding Flow Graphs and Sharing Parameters

A ﬂow graph is a way to formalize the structure of a set of computations, such
as those involved in mapping inputs and parameters to outputs and loss. Please
refer to Section 6.4 for a general introduction. In this section we explain the idea
of unfolding a recursive or recurrent computation into a ﬂow graph that has a
repetitive structure, typically corresponding to a chain of events.
For example, consider the classical form of a dynamical system:
st = fθ (st−1)

(10.1)

where s t is called the state of the system. The unfolded ﬂow graph of such a
system looks like in Figure 10.1.

st1
f✓

st+1

st

f✓

f✓

f✓

Figure 10.1: Classical dynamical system equation 10.1 illustrated as an unfolded ﬂow
graph. Each node represents the state at some time t and function f θ maps the state at
t to the state at t + 1. The same parameters (the same function fθ ) is used for all time
steps.

As another example, let us consider a dynamical system driven by an external
signal x t,
st = fθ(st−1, xt )
(10.2)
illustrated in Figure 10.2, where we see that the state now contains information about the whole past sequence, i.e., the above equation implicitly deﬁnes a
309

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

function
st = gt(x t, xt−1, xt−2, . . . , x2 , x1 )

(10.3)

which maps the whole past sequence (xt , x t−1, xt−2 , . . . , x2 , x1 ) to the current
state. Equation 10.2 is actually part of the deﬁnition of a recurrent net. We
can think of st as a kind of summary of the past sequence of inputs up to t.
Note that this summary is in general necessarily lossy, since it maps an arbitrary
length sequence (xt , xt−1 , xt−2 , . . . , x2, x 1) to a ﬁxed length vector s t . Depending
on the training criterion, this summary might selectively keep some aspects of
the past sequence with more precision than other aspects. For example, if the
RNN is used in statistical language modeling, typically to predict the next word
given previous words, it may not be necessary to distinctly keep track of all
the bits of information, only those required to predict the rest of the sentence.
The most demanding situation is when we ask st to be rich enough to allow
one to approximately recover the input sequence, as in auto-encoder frameworks
(Chapter 15).
If we had to deﬁne a diﬀerent function g t for each possible sequence length
(imagine a separate neural network, each with a diﬀerent input size), each with
its own parameters, we would not get any generalization to sequences of a size not
seen in the training set. Furthermore, one would need to see a lot more training
examples, because a separate model would have to be trained for each sequence
length, and it would need a lot more parameters (proportionally to the size of the
input sequence). It could not generalize what it learns from what happens at a
position t to what could happen at a position t0 6
= t. By instead deﬁning the state
through a recurrent formulation as in Eq. 10.2, the same parameters are used for
any sequence length, allowing much better generalization properties.

st1

s
f✓
x

unfold

f✓
x t1

st
f✓
xt

st+1
f✓
x t+1

Figure 10.2: Left: input processing part of a recurrent neural network, seen as a circuit.
The black square indicates a delay of 1 time step. Right: the same seen as an unfolded
ﬂow graph, where each node is now associated with one particular time instance.

Equation 10.2 can be drawn in two diﬀerent ways. One is in a way that is
inspired by how a physical implementation (such as a real neural network) might
look like, i.e., like a circuit which operates in real time, as in the left of Figure 10.2.
The other is as a ﬂow graph, in which the computations occurring at diﬀerent
time steps in the circuit are unfolded as diﬀerent nodes of the ﬂow graph, as in
310

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

the right of Figure 10.2. What we call unfolding is the operation that maps a
circuit as in the left side of the ﬁgure to a ﬂow graph with repeated pieces as
in the right side. Note how the unfolded graph now has a size that depends on
the sequence length. The black square indicates a delay of 1 time step on the
recurrent connection, from the state at time t to the state at time t + 1.
The other important observation to make from Figure 10.2 is that the same
parameters (θ) are shared over diﬀerent parts of the graph, corresponding here to
diﬀerent time steps.

10.2

Recurrent Neural Networks

Armed with the ideas introduced in the previous section, we can design a wide
variety of recurrent circuits, which are compact and simple to understand visually. As we will explain, we can automatically obtain their equivalent unfolded
graph, which are useful computationally and also help focus on the idea of information ﬂow forward in time (computing outputs and losses) and backward in
time (computing gradients).

o
V
s
U
x

ot1
V

W

W
unfold

st1
W
U
x t1

ot
V

st

ot+1
V

st+1
W
W
U
U
xt
x t+1

Figure 10.3: Left: vanilla recurrent network circuit with hidden-to-hidden recurrence,
seen as a circuit, with weight matrices U, V , W for the three diﬀerent kinds of connections (input-to-hidden, hidden-to-output, and hidden-to-hidden, respectively). Each
circle indicates a whole vector of activations. Right: the same seen as an time-unfolded
ﬂow graph, where each node is now associated with one particular time instance.

Some of the early circuit designs for recurrent neural networks are illustrated
in Figures 10.3, 10.4 and 10.6. Figure 10.3 shows the vanilla recurrent network
whose equations are laid down below in Eq. 10.4, and which has been shown to
be a universal approximation machine for sequences, i.e., able to implement a
Turing machine (Siegelmann and Sontag, 1991; Siegelmann, 1995; Hyotyniemi,
1996). On the other hand, the network with output recurrence shown in TODO
ﬁnish the above sentence
The vanilla recurrent network of Figure 10.3 corresponds to the following forward propagation equations, if we assume that hyperbolic tangent non-linearities
311

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

L t−1

L
y

U
x

L t+1
y t+1
o t+1

y t−1 y t
o t−1
ot

o
V
h

Lt

W

V
W

unfold

W V

ht−1

U
xt−1

W
ht

U
xt

V

W
ht+1

U
xt+1

Figure 10.4: Left: RNN circuit whose recurrence is only through the output. Right:
computational ﬂow graph unfolded in time. At each t, the input is x t , the hidden layer
activations ht , the output ot , the target y t and the loss Lt. Such an RNN is less powerful
(can express a smaller set of functions) than those in the family represented by Figure 10.3
but may be easier to train because they can exploit “teacher forcing”, i.e., constraining
some of the units involved in the recurrent loop (here the output units) to take some
target values during training. This architecture is less powerful because the only state
information (carrying the information about the past) is the previous prediction. Unless the prediction is very high-dimensional and rich, this will usually miss important
information from the past.

are used in the hidden units and softmax is used in output (for classiﬁcation
problems):
at

= b + W st−1 + Uxt

st
ot

= tanh(at )
= c +V st

pt

= softmax(ot)

(10.4)

where the parameters are the bias vectors b and c along with the weight matrices
U , V and W , respectively for input-to-hidden, hidden-to-output, and hiddento-hidden connections. This is an example of a recurrent network that maps an
input sequence to an output sequence of the same length. The total loss for a
given input/target sequence pair (x, y) would then be just the sum of the losses
over all the time steps, e.g.,
X
X
− log p yt
L(x, y) =
Lt =
(10.5)
t

t

where y t is the category that should be associated with time step t in the output
sequence.
Figure 10.4 has a more limited memory or state, which is its output, i.e., the
prediction of the previous target, which potentially limits its expressive power, but
312

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

also makes it easier to train. Indeed, the “intermediate state” of the corresponding
unfolded deep network is not hidden anymore: targets are available to guide this
intermediate representation, which should make it easier to train. In general,
the state of the RNN must be suﬃciently rich to store a summary of the past
sequence that is enough to properly predict the future target values. Constraining
the state to be the visible variable y t itself is therefore in general not enough to
learn most tasks of interest, unless, given the sequence of inputs xt , y t contains all
the required information about the past y’s that is required to predict the future
y’s.
ŷ t ⇠ P(yt | ht)

yt

P (yt | ht)

ht
xt

(xt , yt) :
next input/output training pair

Figure 10.5: Illustration of teacher forcing for RNNs, which comes out naturally from the
log-likelihood training objective (such as in Eq. 10.5). There are two ways in which the
output variable can be fed back as input to update the next state ht : what is fed back
is either the sample ŷ t generated from the RNN model’s output distribution P (yt | ht )
(dashed arrow) or the actual “correct” output y t coming from the training data (dotted
arrow) (x t, y t ). The former is what is done when one generates a sequence from the
model, and the latter is teacher forcing and what is done during training.

Teacher forcing is the training process in which the fed back inputs are not the
predicted outputs but the targets themselves, as illustrated in Figure 10.5. The
disadvantage of strict teacher forcing is that if the network is going to be later
used in an open-loop mode, i.e., with the network outputs (or samples from the
output distribution) fed back as input, then the kind of inputs that the network
will have seen during training could be quite diﬀerent from the kind of inputs that
it will see at test time when the network is run in generative mode, potentially
yielding very poor generalizations. One way to mitigate this problem is to train
with both teacher-forced inputs and with free-running inputs, e.g., predicting the
correct target a number of steps in the future through the unfolded recurrent
output-to-input paths. In this way, the network can learn to take into account
313

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

input conditions (such as those it generates itself in the free-running mode) not
seen during training and how to map the state back towards one that will make
the network generate proper outputs after a few steps. Another approach (Bengio
et al., 2015) to mitigate the gap between the generative mode of RNNs and how
they are trained (with teacher forcing, i.e., maximum likelihood) randomly chooses
to use generated values or actual data values as input, and exploits a curriculum
learning strategy to gradually use more of the generated values as input.

LT
yT
oT

W

ht−1
W
U
x t−1

ht
W
U
xt

ht+1

V

hT

…" W
U
x t+1

U
xT

Figure 10.6: Time-unfolded recurrent neural network with a single output at the end
of the sequence. Such a network can be used to summarize a sequence and produce a
ﬁxed-size representation used as input for further processing. There might be a target
right at the end (like in the ﬁgure) or the gradient on the output o t can be obtained by
back-propagating from further downstream modules.

10.2.1

Computing the Gradient in a Recurrent Neural Network

Using the generalized back-propagation algorithm (for arbitrary ﬂow graphs) introduced in Section 6.4, one can obtain the so-called Back-Propagation Through
Time (BPTT) algorithm. Once we know how to compute gradients, we can in
principle apply any of the general-purpose gradient-based techniques to train an
RNN. These general-purpose techniques were introduced in Section 4.3 and developed in greater depth in Chapter 8.
Let us thus work out how to compute gradients by BPTT for the RNN equations above (Eqs. 10.4 and 10.5). The nodes of our ﬂow graph will be the sequence
of xt ’s, st’s, o t’s, L t’s, and the parameters U , V , W , b, c. For each node a we
314

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

need to compute the gradient ∇ aL recursively, based on the gradient computed
at nodes that follow it in the graph. We start the recursion with the nodes
immediately preceding the ﬁnal loss
∂L
=1
∂Lt
and the gradient ∇ ot L on the outputs at time step t, for all i, t, is as follows:
(∇ o tL) i =

∂L
∂L ∂Lt
=
= p t,i − 1i,yt
∂o ti
∂Lt ∂oti

and work our way backwards, starting from the end of the sequence, say T , at
which point s T only has oT as descendent:
∇sT L = ∇ oT L

∂oT
= ∇o T L V .
∂sT

P
Note how the above equation is vector-wise and corresponds to ∂s∂LT j = i ∂o∂LT iV ij ,
scalar-wise. We can then iterate backwards in time to back-propagate gradients
through time, from t = T − 1 down to t = 1, noting that s t (for t < T ) has as
descendents both ot and st+1 :
∇st L = ∇st+1L

∂st+1
∂ot
+ ∇ot L
= ∇st+1L diag(1 − s 2t+1 )W + ∇o t L V
∂st
∂st

where diag(1 − s2t+1 ) indicates the diagonal matrix containing the elements 1 −
s 2t+1,i, i.e., the derivative of the hyperbolic tangent associated with the hidden
unit i at time t + 1.
Once the gradients on the internal nodes of the ﬂow graph are obtained, we
can obtain the gradients on the parameter nodes, which have descendents at all
the time steps:
∂ot X
=
∇o tL
∂c
t
t
X
∂s t X
=
∇b L =
∇ st L
∇ st L diag(1 − s2t )
∂b
t
t
X
X
∂ot
∇V L =
∇ ot L
=
∇o tL s >
t
∂V
t
t
X
X
∂s t
∇W L =
∇ st
∇ st L diag(1 − s 2t )s>
=
t−1
∂W
t
t
∇c L =

X

∇ ot L

315

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Note in the above (and elsewhere) that whereas ∇s tL refers to the full inﬂuence
t
∂s t
of s t through all paths from st to L, ∂s
∂W or ∂b refers to the immediate eﬀect
of the denominator on the numerator, i.e., when we consider the denominator
as a parent of the numerator and only that direct dependency is accounted for.
Otherwise, we would get “double counting” of derivatives.

10.2.2

Recurrent Networks as Generative Directed Acyclic Models

Up to here, we have not clearly stated what the losses L t associated with the
outputs ot of a recurrent net should be. It is because there are many possible ways
in which RNNs can be used. In this section, we consider the most common case
where the RNN models a probability distribution over a sequence of observations.
When we consider a predictive log-likelihood training objective such as Eq. 10.5,
we are training the RNN to estimate the conditional distribution of the next sequence element yt given the past inputs xs and targets y s (for s < t). As we
show below, this corresponds to viewing the RNN as a directed graphical model,
a notion introduced in Section 3.14. In this case, the set of random variables of
interest is the sequence of yt’s (given the sequence of x t ’s), and we are modeling
the joint probability of the yt ’s given the xt ’s.
To keep things simple for starters, let us assume that there is no conditioning
input sequence in addition to the output sequence, i.e., that the target output
at the next time step is also the next input. The random variable of interest is
thus the sequence of vectors X = (x 1, x2 , . . . , xT ), and we parametrize the joint
distribution of these vectors via
P (X) = P (x1 , . . . , xT ) =

T
Y
t=1

P (x t | xt−1 , x t−2, . . . , x1 )

(10.6)

using the chain rule of conditional probabilities (Section 3.6), and where the righthand side of the bar is empty for t = 1, of course. Hence the negative log-likelihood
of X according to such a model is
X
L=
Lt
t

where
Lt = − log P (xt | xt−1, xt−2, . . . , x 1).
In general directed graphical models, x t can be predicted using only a subset of its
predecessors (x1 , . . . , xt−1). However, for RNNs, the graphical model is generally
fully connected, not removing any dependency a priori. This can be achieved
eﬃciently through the recurrent parametrization, such as in Eq. 10.2, since st
316

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

is trained to summarize whatever is required from the whole previous sequence
(Eq. 10.3).
Hence, instead of cutting statistical complexity by removing arcs in the directed graphical model for (x 1, . . . , xT ), as is done in most of the work on directed graphical models, the core idea of recurrent networks is that we introduce a state variable which decouples all the past and future observations, but
we make that state variable a function of the past, through the recurrence,
Eq. 10.2. Consequently, the number of parameters required to parametrize P (xt |
xt−1 , xt−2 , . . . , x1) does not grow exponentially with t (as it would if we parametrized
that probability by a straight probability table, when the data is discrete) but
remains constant with t. It only grows with the dimension of the state st . The
price to be paid for that great advantage is that optimizing the parameters may
be more diﬃcult, as discussed below in Section 10.8. The decomposition of the
likelihood thus becomes:
P (x) =

T
Y
t=1

P (x t | g t (xt−1 , xt−2, . . . , x 1))

where
st = gt(xt , xt−1 , xt−2 , . . . , x2 , x 1) = fθ (st−1 , xt).
Note that if the self-recurrence function fθ is learned, it can discard some of the
information in some of the past values x t−k that are not needed for predicting the
future data. In fact, because the state generally has a ﬁxed dimension smaller than
the length of the sequences (times the dimension of the input), it has to discard
some information. However, we leave it to the learning procedure to choose what
information to keep and what information to throw away, so as minimize the
objective function (e.g., predict future values correctly).
The above decomposition of the joint probability of a sequence of variables
into ordered conditionals precisely corresponds to the sequence of computations
performed by an RNN. The target to be predicted at each time step t is the next
element in the sequence, while the input at each time step is the previous element
in the sequence (with all previous inputs summarized in the state), and the output
is interpreted as parametrizing the probability distribution of the target given the
state. This is illustrated in Figure 10.7.
If the RNN is actually going to be used to generate sequences, one must also
incorporate in the output information allowing to stochastically decide when to
stop generating new output elements. This can be achieved in various ways. In the
case when the output is a symbol taken from a vocabulary, one can add a special
symbol corresponding to the end of a sequence. When that symbol is generated, a
complete sequence has been generated. The target for that special symbol occurs
exactly once per sequence, as the last target after the last output element xT . In
317

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

L t−1
ot−1
V
W

s t−1
W
U
xt−1

L t+1

Lt
ot

V

st

ot+1
V

s t+1
W
W
U
U
xt
xt+1 xt+2

Figure 10.7: A generative recurrent neural network modeling P (x 1 , . . . , xT ), able to
generate sequences from this distribution. Each element xt of the observed sequence
serves both as input (for computing the state st at the current time step) and as target
(for the prediction made at the previous time step). The output o t encodes the parameters
of a conditional distribution P (xt+1 | x 1, . . . , xt ) = P (xt+1 | o t ) for xt+1 , given the past
sequence x1 . . . , x t. The loss L t is the negative log-likelihood associated with the output
prediction (or more generally, distribution parameters) o t, when the actual observed
target is xt+1 . In training mode, one measures and minimizes the sum of the losses
over observed sequence(s) x. In generative mode, x t is sampled from the conditional
distribution P (xt+1 | x1, . . . , x t) = P (x t+1 | ot ) (dashed arrows) and then that generated
sample xt+1 is fed back as input for computing the next state st+1 , the next output ot+1 ,
and generating the next sample x t+2, etc.

318

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

general, one may train a binomial output associated with that stopping variable,
for example using a sigmoid output non-linearity and the cross entropy loss, i.e.,
again negative log-likelihood for the event “end of the sequence”. Another kind
of solution is to model the integer T itself, through any reasonable parametric
distribution, and use the number of time steps left (and possibly the number of
time steps since the beginning of the sequence) as extra inputs at each time step.
Thus we would have decomposed P (x1 . . . , xT ) into P (T ) and P (x1 . . . , xT | T ).
In general, one must therefore keep in mind that in order to fully generate a
sequence we must not only generate the xt ’s, but also the sequence length T ,
either implicitly through a series of continue/stop decisions (or a special “end-ofsequence” symbol), or explicitly through modeling the distribution of T itself as
an integer random variable.
If we take the RNN equations of the previous section (Eq. 10.4 and 10.5),
they could correspond to a generative RNN if we simply make the target yt equal
to the next input xt+1 (and because the outputs are the result of a softmax, it
must be that the input sequence is a sequence of symbols, i.e., xt is a symbol or
bounded integer).
Other types of data can clearly be modeled in a similar way, following the
discussions about the encoding of outputs and the probabilistic interpretation of
losses as negative log-likelihoods, in Sections 5.6 and 6.3.2.

10.2.3

RNNs to Represent Conditional Probability Distributions

In general, as discussed in Section 6.3.2 (see especially the end of that section, in
Subsection 6.3.2 ), when we can represent a parametric probability distribution
P (y | ω), we can make it conditional by making ω a function of the appropriate
conditioning variable:
P (y | ω = f (x)).
In the case of an RNN, this can be achieved in diﬀerent ways, and we review here
the most common and obvious choices.
If x is a ﬁxed-size vector, then we can simply make it an extra input of the
RNN that generates the y sequence. Some common ways of providing an extra
input to an RNN are:
1. as an extra input at each time step, or
2. as the initial state s 0, or
3. both.
In general, one may need to add extra parameters (and parametrization) to map
x = x into the “extra bias” going either into only s 0, into the other s t (t > 0),
319

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

or into both. The ﬁrst (and most commonly used) approach is illustrated in
Figure 10.8.

L t−1
o t−1
V
W

L t+1

Lt

o t+1

ot
V

V

s t−1
st
s t+1
W
W
W
U
U
U
yt+2
yt−1
yR y
R t R t+1
xt

Figure 10.8: A conditional generative recurrent neural network maps a ﬁxed-length vector
x into a distribution over sequences Y. Each element yt of the observed output sequence
serves both as input (for the current time step) and, during training, as target (for the
previous time step). The generative semantics are the same as in the unconditional case
(Fig. 10.7). The only diﬀerence is that the state is now conditioned on the input x,
and the same parameters (weight matrix R in the ﬁgure) is used at every time step
to parametrize that dependency. Although this was not discussed in Fig. 10.7, in both
ﬁgures one should note that the length of the sequence must also be generated (unless
known in advance). This could be done by a special sigmoidal output unit that predicts
a binary target (with associated cross-entropy loss) that encodes the fact that the next
output is the last.

As an example, we could imagine that x is encoding the identity of a phoneme
and the identity of a speaker, and that y represents an acoustic sequence corresponding to that phoneme, as pronounced by that speaker.
Consider the case where the input x is a sequence of the same length as the
output sequence y, and the y t’s are independent of each other when the past
input sequence is given, i.e., P (yt | yt−1, . . . , y 1, x) = P (yt | xt, x t−1, . . . , x1). We
therefore have a causal relationship between the xt ’s and the predictions of the
yt ’s, in addition to the independence of the yt ’s, given x. Under these (pretty
strong) assumptions, we can return to Fig. 10.3 and interpret the t-th output ot
as parameters for a conditional distribution for y t , given xt, x t−1, . . . , x1 .
If we want to remove the conditional independence assumption, we can do so
by making the past y t’s inputs into the state as well. That situation is illustrated
320

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Lt−1
ot−1
V

s t−1
W
W
U
R yt−1R
xt−1

Lt+1

Lt
ot

V

ot+1
V

st+1
W
W
U
U
R
yt+1 R yt+2
yt
st

xt

xt+1

xt+2

Figure 10.9: A conditional generative recurrent neural network mapping a variable-length
sequence x into a distribution over sequences y of the same length. This architecture
assumes that the predictions of yt ’s are causally related to the x t’s, i.e., that we want
to predict the yt ’s only using the past x t’s. Note how the prediction of y t+1 is based on
both the past x’s and the past y’s. The dashed arrows indicate that yt can be generated
by sampling from the output distribution o t−1 . When yt is clamped (known), it is used
as a target in the loss L t−1 which measures the log-probability that yt would be sampled
from the distribution o t−1.

321

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

in Fig. 10.9.

Lt−1

Lt

yt−1

L t+1
yt

y t+1

ot−1

ot

ot+1

gt−1

gt

gt+1

ht−1

ht

ht+1

x t−1

xt

x t+1

Figure 10.10: Computation of a typical bidirectional recurrent neural network, meant
to learn to map input sequences x to target sequences y, with loss L t at each step t.
The h recurrence propagates information forward in time (towards the right) while the
g recurrence propagates information backward in time (towards the left). Thus at each
point t, the output units ot can beneﬁt from a relevant summary of the past in its ht
input and from a relevant summary of the future in its gt input.

10.3

Bidirectional RNNs

All of the recurrent networks we have considered up to now have a “causal”
structure, meaning that the state at time t only captures information from the
past, x1, . . . , xt . However, in many applications we want to output at time t a
prediction regarding an output which may depend on the whole input sequence.
For example, in speech recognition, the correct interpretation of the current sound
as a phoneme may depend on the next few phonemes because of co-articulation
and potentially may even depend on the next few words because of the linguistic
dependencies between nearby words: if there are two interpretations of the current
322

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

word that are both acoustically plausible, we may have to look far into the future
(and the past) to disambiguate them. This is also true of handwriting recognition
and many other sequence-to-sequence learning tasks, described in the next section.
Bidirectional recurrent neural networks (or bidirectional RNNs) were invented
to address that need (Schuster and Paliwal, 1997). They have been extremely
successful (Graves, 2012) in applications where that need arises, such as handwriting recognition (Graves et al., 2008; Graves and Schmidhuber, 2009), speech
recognition (Graves and Schmidhuber, 2005; Graves et al., 2013) and bioinformatics (Baldi et al., 1999).
As the name suggests, the basic idea behind bidirectional RNNs is to combine
a forward-going RNN and a backward-going RNN. Figure 10.10 illustrates the
typical bidirectional RNN, with h t standing for the state of the forward-going
RNN and g t standing for the state of the backward-going RNN. This allows the
units ot to compute a representation that depends on both the past and the future
but is most sensitive to the input values around time t, without having to specify
a ﬁxed-size window around t (as one would have to do with a feedforward network,
a convolutional network, or a regular RNN with a ﬁxed-size look-ahead buﬀer).
This idea can be naturally extended to 2-dimensional input, such as images,
by having four RNNs, each one going in one of the four directions: up, down,
left, right. At each point (i, j) of a 2-D grid, an output o i,j could then compute a
representation that would capture mostly local information but could also depend
on long-range inputs, if the RNN are able to learn to carry that information.

10.4

Encoder-Decoder Sequence-to-Sequence Architectures

We have seen in Figure 10.6 how an RNN can map an input sequence to a ﬁxed-size
prediction. We have seen in Figure 10.7 how an RNN can model a distribution over
sequences and generate new ones from the estimated distribution. We have seen
in Figure 10.8 how one can condition on an input vector to learn to generate such
sequences. We have seen in Figures 10.9 and 10.10 how an RNN (unidirectional
or bidirectional) can map an input sequence to an output sequence of the same
length.
Here we discuss how an RNN can be trained to map an input sequence to an
output sequence which is not necessarily of the same length. This comes up in
many applications, such as speech recognition, machine translation or question
answering, where the input and output sequences in the training set are generally
not of the same length (although their lengths might be related).
The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was ﬁrst proposed in Cho et al. (2014) and shortly
323

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Decoder

yn y

y2

y1

c

x1

x2

xnx

Encoder

Figure 10.11: Example of encoder-decoder or sequence-to-sequence RNN architecture,
for learning to generate an output sequence (y 1, . . . , y ny) given an input sequence
(x1 , x2 , . . . x n x). It is composed of an encoder RNN that reads the input sequence and
a decoder RNN that generates the output sequence (or computes the probability of a
given output sequence). The ﬁnal hidden state of the encoder RNN is used to compute a
generally ﬁxed-size context variable C which represents a semantic summary of the input
sequence and conditions computations in the decoder RNN.

after in Sutskever et al. (2014b). These authors respectively called this architecture, illustrated in Figure 10.11, the encoder-decoder or sequence-to-sequence
architecture. The idea is very simple: (1) an encoder or reader or input RNN processes the input sequence, producing from its last hidden state a representation C
of the input sequence X = (x 1 , . . . , xnx ); (2) a decoder or writer or output RNN is
conditioned on that ﬁxed-length vector (just like in Figure 10.8) to generate the
output sequence Y = (y 1, . . . , yn y ), where the lengths n x and ny can vary from
training pair to training pair. The two RNNs are trained jointly to maximize the
average of log P (Y = Y |X = X) over all the training pairs (X, Y ). The last
state s n x of the input RNN is typically used as a representation C of the input
sequence that conditions the output RNN. The output RNN can be conditioned
in at least two ways, which can be combined, as we have seen earlier, i.e., either
by producing a starting state for the output RNN or by producing an extra input
at each time step of the output RNN. In any case, one inserts an extra set of
parameters to map C into a bias or initial state. Hence the two RNNs do not
have to have the same hidden layer dimensionality, and sometimes it makes sense
to make that mapping more complex and non-linear, e.g., an MLP could be used
to map the output of the encoder RNN into an input for the decoder RNN.
One clear limitation of this architecture is when the output of the encoder
RNN has a dimension that is too small to properly summarize a long sequence.
324

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

This phenomenon was observed by Bahdanau et al. (2014) in the context of machine translation, and they proposed to make C a variable length sequence rather
than a ﬁxed-size vector, introducing a particular attention mechanism that learns
to associate elements of the sequence C to elements of the output RNN sequence.
See Section 12.4.6 for more details.

10.5

Deep Recurrent Networks

The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:
1. from input to hidden state,
2. from previous hidden state to next hidden state, and
3. from hidden state to output,
where the ﬁrst two are actually brought together to map the input and previous
state into the next state. With the vanilla RNN architecture (such as in Figure 10.3), each of these three blocks is associated with a single weight matrix. In
other words, when the network is unfolded, each of these corresponds to a shallow
transformation, i.e., a single layer within a deep MLP.
yt

z t-1

ht-1

yt

yt

zt

ht

ht

ht-1

xt

xt

h t-1

ht
xt

Figure 10.12: A recurrent neural network can be made deep in many ways. First, the
hidden recurrent state can be broken down into groups organized hierarchically (left).
Second, deeper computation (e.g., an MLP in the ﬁgure) can be introduced in the inputto-hidden, hidden-to-hidden, and hidden-to-output parts (Middle). However, this may
lengthen the shortest path linking diﬀerent time steps, but this can be mitigated by
introduced skip connections (Right). Figures from Pascanu et al. (2014a) with permission.

Would it be advantageous to introduce depth in each of these operations? Experimental evidence (Graves et al., 2013; Pascanu et al., 2014a) strongly suggests
325

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

so, and this is in agreement with the idea that we need enough depth in order to
perform the required mappings. See also Schmidhuber (1992); El Hihi and Bengio
(1996); Jaeger (2007a) for earlier work on deep RNNs.
El Hihi and Bengio (1996) ﬁrst introduced the idea of decomposing the hidden
state of an RNN into multiple groups of units that would operate at diﬀerent
time scales. Graves et al. (2013) were the ﬁrst to show a signiﬁcant beneﬁt of
decomposing the state of an RNN into groups of hidden units, with a restricted
connectivity between the groups, e.g., as in Figure 10.12 (left). Indeed, if there
were no restriction at all and no pressure for some units to represent a slower time
scale, then having N groups of M hidden units would be equivalent to having a
single group of N M hidden units. Koutnik et al. (2014) showed how the multiple
time scales idea from El Hihi and Bengio (1996) can be advantageous on several
sequential learning tasks: each group of hidden unit is updated at a diﬀerent
multiple of the time step index e.g., at every time step, at every 2nd step, at
every 4th step, etc.
We can also think of the lower layers in this hierarchy as playing a role in
transforming the raw input into a representation that is more appropriate, at
the higher levels of the hidden state. Pascanu et al. (2014a) go a step further
and propose to have a separate MLP (possibly deep) for each of the three blocks
enumerated above, as illustrated in Figure 10.12 (middle). It makes sense to
allocate enough capacity in each of these three steps, but having a deep state-tostate transition may also hurt: it makes the shortest path from an event at time
t to an event at time t0 > t substantially longer, which make it more diﬃcult to
learn long-term dependencies (see Sections 8.2.5 and 10.8). For example if a onehidden-layer MLP is used for the state-to-state transition, we have doubled the
length of that path, compared with a vanilla RNN. However, as argued by Pascanu
et al. (2014a), this can be mitigated by introducing skip connections in the hiddento-hidden path, as illustrated in Figure 10.12 (right).

10.6

Recursive Neural Networks

Recursive networks represent yet another generalization of recurrent networks,
with a diﬀerent kind of computational graph, which is structured as a deep tree,
rather than the chain-like structure of RNNs. The typical computational graph for
a recursive network is illustrated in Figure 10.13. Recursive neural networks were
introduced by Pollack (1990) and their potential use for learning to reason were
nicely laid down by Bottou (2011). Recursive networks have been successfully
applied in processing data structures as input to neural nets (Frasconi et al.,
1997, 1998), in natural language processing (Socher et al., 2011a,c, 2013) as well
as in computer vision (Socher et al., 2011b).
326

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

L"

y"

o"

U
V"
x 1"

U

W

W

U

V"
x2"

V"

x 3"

W
V"
x4"

Figure 10.13: A recursive network has a computational graph that generalizes that of
the recurrent network from a chain to a tree. In the ﬁgure, a variable-size sequence
x1, x 2 , . . . can be mapped to a ﬁxed-size representation (the output o), with a ﬁxed
number of parameters (e.g. the weight matrices U , V , W ). The ﬁgure illustrates a
supervised learning case in which some target y is provided which is associated with the
whole sequence.

327

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

One clear advantage of recursive nets over recurrent nets is that for a sequence
of the same length N , depth3 can be drastically reduced from N to O(log N ),
which might help deal with long-term dependencies. An open question is how
to best structure the tree, though. One option is to have a tree structure which
does not depend on the data, e.g., a balanced binary tree. Another is to use an
external method, such as a natural language parser (Socher et al., 2011a, 2013).
Ideally, one would like the learner itself to discover and infer the tree structure
that is appropriate for any given input, as suggested in Bottou (2011).
Many variants of the recursive net idea are possible. For example, in Frasconi
et al. (1997, 1998), the data is associated with a tree structure in the ﬁrst place,
and inputs and/or targets with each node of the tree. The computation performed
by each node does not have to be the traditional artiﬁcial neuron computation
(aﬃne transformation of all inputs followed by a monotone non-linearity). For
example, Socher et al. (2013) propose using tensor operations and bilinear forms,
which have previously been found useful to model relationships between concepts (Weston et al., 2010; Bordes et al., 2012) when the concepts are represented
by continuous vectors (embeddings).

10.7

Auto-Regressive Networks

One of the basic ideas behind recurrent networks is that of directed graphical
models with a twist: we decompose a probability distribution as a product of conditionals without explicitly cutting any arc in the graphical model4 , but instead
reducing complexity by parametrizing the transition probability in a recursive
way that requires a ﬁxed (and not exponential) number of parameters, due to a
form of parameter sharing (see Section 7.8 for an introduction to the concept).
Instead of reducing P (xt | xt−1 , . . . , x1 ) to something like P (xt | xt−1 , . . . , x t−k)
(assuming the k previous ones as the parents), we keep the full dependency but
we parametrize the conditional eﬃciently in a way that does not grow with t,
exploiting parameter sharing. When the above conditional probability distribution is in some sense stationary, i.e., the relation between the past and the next
observation does not depend on t, only on the values of the past observations,
then this form of parameter sharing makes a lot of sense, and for recurrent nets
it allows one to use the same model for sequences of diﬀerent lengths.
Auto-regressive networks are similar to recurrent networks in the sense that
we also decompose a joint probability over the observed variables as a product of
3

i.e., the number of compositions of non-linear operations
i.e., every element of the sequence depends on all the previous ones; however it is possible
to obtain a more compact graphical model from an RNN if one introduces deterministic nodes
in the graph to represent the hidden units at diﬀerent time steps, but if we restrict ourselves to
non-deterministic nodes, the graphical model is fully-connected.
4

328

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

conditionals of the form P (x t | xt−1, . . . , x1) but we drop the form of parameter
sharing that makes these conditionals all share the same parametrization. This
makes sense when the variables are not elements of a translation-equivariant sequence (see Section 9.2 for more on equivariance), but instead form an arbitrary
tuple without any particular ordering that would correspond to a translationequivariant form of relationship between variables at position k and variables
at position k 0. Such models have been called fully-visible Bayes networks (Frey
et al., 1996) and used successfully in many forms, ﬁrst with logistic regression for
each conditional distribution (Frey, 1998) and then with neural networks (Bengio and Bengio, 2000b; Larochelle and Murray, 2011). In some forms of autoregressive networks, such as NADE (Larochelle and Murray, 2011), described in
Section 10.7.3 below, we can re-introduce a form of parameter sharing that is different from the one found in recurrent networks, but that brings both a statistical
advantage (less parameters) and a computational advantage (less computation).
Although we drop the sharing over time, as we see below in Section 10.7.2, using
a deep learning concept of reuse of features, we can share features that have been
computed for predicting x t−k with the sub-network that predicts xt .

P(x1 )# P(x |x ) P(x3 |x2#,x 1 )#
P(x4 |x3#,#x2# ,x1 )#
2 1#
#

x 1#

x2#

x3#

x4#

x1#

x2#

x 3#

x4#

Figure 10.14: An auto-regressive network predicts the i-th variable from the i−1 previous
ones. Left: corresponding graphical model (which is the same as that of a recurrent
network). Right: corresponding computational graph, in the case of the logistic autoregressive network, where each prediction has the form of a logistic regression, i.e., with i
free parameters (for the i−1 weights associated with i−1 inputs, and an oﬀset parameter).

10.7.1

Logistic Auto-Regressive Networks

Let us ﬁrst consider the simplest auto-regressive network, without hidden units,
and hence no sharing at all. Each P (xt | x t−1 , . . . , x1) is parametrized as a linear
model, e.g., a logistic regression. This model was introduced by Frey (1998) and
329

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

has O(T 2) parameters when there are T variables to be modeled. It is illustrated
in Figure 10.14, showing both the graphical model (left) and the computational
graph (right).
A clear disadvantage of the logistic auto-regressive network is that one cannot
easily increase its capacity in order to capture more complex data distributions. It
deﬁnes a parametric family of ﬁxed capacity, like the linear regression, the logistic
regression, or the Gaussian distribution. In fact, if the variables are continuous,
one gets a linear auto-regressive model, which is thus another way to formulate
a Gaussian distribution, i.e., only capturing pairwise interactions between the
observed variables.
P(x1 )# P(x2 |x1)# P(x3 |x 2#,x1 )# P(x |x ,#x ,x )
4 3#
2# 1 #
#

h2#

h1#

x 1#

h3#

x2#

x3#

x4#

Figure 10.15: A neural auto-regressive network predicts the i-th variable x i from the i− 1
previous ones, but is parametrized so that features (groups of hidden units denoted hi )
that are functions of x 1, . . . , x i can be reused in predicting all of the subsequent variables
xi+1 , xi+2, . . ..

10.7.2

Neural Auto-Regressive Networks

Neural Auto-Regressive Networks have the same left-to-right graphical model as
logistic auto-regressive networks (Figure 10.14, left) but a diﬀerent parametrization that is at once more powerful (allowing to extend the capacity as needed and
approximate any joint distribution) and can improve generalization by introducing a parameter sharing and feature sharing principle common to deep learning in
general. The ﬁrst paper on neural auto-regressive networks by Bengio and Bengio
(2000b) (see also Bengio and Bengio (2000a) for the more extensive journal version) were motivated by the objective to avoid the curse of dimensionality arising
330

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

out of traditional non-parametric graphical models, sharing the same structure
as Figure 10.14 (left). In the non-parametric discrete distribution models, each
conditional distribution is represented by a table of probabilities, with one entry
and one parameter for each possible conﬁguration of the variables involved. By
using a neural network instead, two advantages are obtained:
1. The parametrization of each P (x t | xt−1, . . . , x 1) by a neural network with
(t − 1) × k inputs and k outputs (if the variables are discrete and take k
values, encoded one-hot) allows one to estimate the conditional probability
without requiring an exponential number of parameters (and examples),
yet still allowing to capture high-order dependencies between the random
variables.
2. Instead of having a diﬀerent neural network for the prediction of each x t ,
a left-to-right connectivity illustrated in Figure 10.15 allows one to merge
all the neural networks into one. Equivalently, it means that the hidden
layer features computed for predicting xt can be reused for predicting xt+k
(k > 0). The hidden units are thus organized in groups that have the
particularity that all the units in the t-th group only depend on the input
values x1 , . . . , xt . In fact the parameters used to compute these hidden units
are jointly optimized to help the prediction of all the variables xt+1 , xt+2 , . . ..
This is an instance of the reuse principle that makes multi-task learning
and transfer learning successful with neural networks and deep learning in
general (See Sections 7.12 and 16.2).
Each P (x t | x t−1 , . . . , x1) can represent a conditional distribution by having
outputs of the neural network predict parameters of the conditional distribution
of xt , as discussed in Section 6.3.2. Although the original neural auto-regressive
networks were initially evaluated in the context of purely discrete multivariate
data (e.g., with a sigmoid output - Bernoulli case - or softmax output - multinoulli case) it is natural to extend such models to continuous variables or joint
distributions involving both discrete and continuous variables, as for example with
RNADE introduced below (Uria et al., 2013).

331

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

P(x 1)# P(x |x ) P(x 3|x2# ,x1)#
2 1 #
P(x4|x3#,#x2# ,x1) #
#

h 2#

h1#
W 1#

W1#

h3#
W 2#
W3#

W1#

x 1#

W2#

x2#

x3#

x 4#

Figure 10.16: NADE (Neural Auto-regressive Density Estimator) is a neural autoregressive network, i.e., the hidden units are organized in groups h j so that only the
inputs x 1, . . . , x i participate in computing hi and predicting P (xj | x j−1 , . . . , x1 ), for
j > i. The particularity of NADE is the use of a particular weight sharing pattern: the
same W 0jki = W ki is shared (same color and line pattern in the ﬁgure) for all the weights
outgoing from x i to the k-th unit of any group j ≥ i. The vector (W1i, W 2i, . . .) is denoted
W:,i here.

10.7.3

NADE

A very successful recent form of neural auto-regressive network was proposed
by Larochelle and Murray (2011). The architecture is basically the same as for
the original neural auto-regressive network of Bengio and Bengio (2000b) except
for the introduction of a weight-sharing scheme: as illustrated in Figure 10.16.
The parameters of the hidden units of diﬀerent groups j are shared, i.e., the
0
weights W jki
from the i-th input xi to the k-th element of the j-th group of
hidden unit h jk (j ≥ i) are shared:
W 0jki = Wki
with (W1i, W 2i, . . .) denoted W:,i in Figure 10.16.
This particular sharing pattern is motivated in Larochelle and Murray (2011)
by the computations performed in the mean-ﬁeld inference5 of an RBM, when only
5

Here, unlike in Section 13.5, the inference is over some of the input variables that are missing,
given the observed ones.
332

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

the ﬁrst i inputs are given and one tries to infer the subsequent ones. This meanﬁeld inference corresponds to running a recurrent network with shared weights
and the ﬁrst step of that inference is the same as in NADE. The only diﬀerence
is that with the proposed NADE, the output weights are not forced to be simply
transpose values of the input weights (they are not tied). One could imagine
actually extending this procedure to not just one time step of the mean-ﬁeld
recurrent inference but to k steps, as in Raiko et al. (2014).
Although the neural auto-regressive networks and NADE were originally proposed to deal with discrete distributions, they can in principle be generalized to
continuous ones by replacing the conditional discrete probability distributions (for
P (xj | x j−1 , . . . , x1 )) by continuous ones and following general practice to predict continuous random variables with neural networks (see Section 6.3.2) using
the log-likelihood framework. A fairly generic way of parametrizing a continuous
density is as a Gaussian mixture, and this avenue has been successfully evaluated
for the neural auto-regressive architecture with RNADE (Uria et al., 2013). One
interesting point to note is that stochastic gradient descent can be numerically
ill-behaved due to the interactions between the conditional means and the conditional variances (especially when the variances become small). Uria et al. (2013)
have used a heuristic to rescale the gradient on the component means by the
associated standard deviation which seems to have helped optimizing RNADE.
Another very interesting extension of the neural auto-regressive architectures
gets rid of the need to choose an arbitrary order for the observed variables (Murray and Larochelle, 2014). In auto-regressive networks, the idea is to train the
network to be able to cope with any order by randomly sampling orders and
providing the information to hidden units specifying which of the inputs are observed (on the right side of the conditioning bar) and which are to be predicted
and are thus considered missing (on the left side of the conditioning bar). This
is nice because it allows one to use a trained auto-regressive network to perform
any inference (i.e. predict or sample from the probability distribution over any
subset of variables given any subset) extremely eﬃciently. Finally, since many
orders are possible, the joint probability of some set of variables can be computed in many ways (n! for n variables), and this can be exploited to obtain a
more robust probability estimation and better log-likelihood, by simply averaging the log-probabilities predicted by diﬀerent randomly chosen orders. In the
same paper, the authors propose to consider deep versions of the architecture,
but unfortunately that immediately makes computation as expensive as in the
original neural auto-regressive neural network (Bengio and Bengio, 2000b). The
ﬁrst layer and the output layer can still be computed in O(nh) multiply-add operations, as in the regular NADE, where h is the number of hidden units (the
size of the groups hi , in Figures 10.16 and 10.15), whereas it is O(n2 h) in Bengio
333

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

and Bengio (2000b). However, for the other hidden layers, the computation is
O(n 2h 2) if every “previous” group at layer l participates in predicting the “next”
group at layer l + 1, assuming n groups of h hidden units at each layer. Making
the i-th group at layer l + 1 only depend on the i-th group, as in Murray and
Larochelle (2014) at layer l reduces it to O(nh2 ), which is still h times worse than
the regular NADE.

10.8

Facing the Challenge of Long-Term Dependencies

The mathematical challenge of learning long-term dependencies in recurrent networks was introduced in Section 8.2.5. The basic problem is that gradients propagated over many stages tend to either vanish (most of the time) or explode
(rarely, but with much damage to the optimization). Even if we assume that the
parameters are such that the recurrent network is stable (can store memories,
with gradients not exploding), the diﬃculty with long-term dependencies arises
from the exponentially smaller weights given to long-term interactions (involving
the multiplication of many Jacobians) compared short-term ones. See Hochreiter
(1991); Doya (1993); Bengio et al. (1994); Pascanu et al. (2013a) for a deeper
treatment.
In this section we discuss various approaches that have been proposed to
alleviate this diﬃculty with learning long-term dependencies.

10.8.1

Echo State Networks: Choosing Weights to Make Dynamics Barely Contractive

The recurrent weights and input weights of a recurrent network are those that
deﬁne the state representation captured by the model, i.e., how the state st (hidden units vector) at time t (Eq. 10.2) captures and summarizes information from
the previous inputs x1, x 2, . . . , xt . Since learning the recurrent and input weights
is diﬃcult, one option that has been proposed (Jaeger and Haas, 2004; Jaeger,
2007b; Maass et al., 2002) is to set those weights such that the recurrent hidden units do a good job of capturing the history of past inputs, and only learn
the output weights. This is the idea that was independently proposed for Echo
State Networks or ESNs (Jaeger and Haas, 2004; Jaeger, 2007b) and Liquid State
Machines (Maass et al., 2002). The latter is similar, except that it uses spiking neurons (with binary outputs) instead of the continuous-valued hidden units
used for ESNs. Both ESNs and liquid state machines are termed reservoir computing (Lukoševičius and Jaeger, 2009) to denote the fact that the hidden units
form of reservoir of temporal features which may capture diﬀerent aspects of the
334

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

history of inputs.
One way to think about these reservoir computing recurrent networks is that
they are similar to kernel machines: they map an arbitrary length sequence (the
history of inputs up to time t) into a ﬁxed-length vector (the recurrent state
s t ), on which a linear predictor (typically a linear regression) can be applied to
solve the problem of interest. The training criterion is therefore convex in the
parameters (which are just the output weights) and can actually be solved online
in the linear regression case (using online updates for linear regression (Jaeger,
2003)).
The important question is therefore: how do we set the input and recurrent
weights so that a rich set of histories can be represented in the recurrent neural
network state? The answer proposed in the reservoir computing literature is to
make the dynamical system associated with the recurrent net nearly be on the
edge of stability, i.e., more precisely with values around 1 for the leading singular
value of the Jacobian of the state-to-state transition function. As alluded to
in 8.2.5, an important characteristic of a recurrent network is the eigenvalue
t
spectrum of the Jacobians J (t) = ∂s∂st−1
, and in particular the spectral radius
of J (t) , i.e., its largest eigenvalue. If it is greater than 1, the dynamics can
diverge, meaning that small diﬀerences in the state value at t can yield a very
large diﬀerence at T later in the sequence. To see this, consider the simpler case
where the Jacobian matrix J does not change with t. If a change ∆s in the state
at t is aligned with an eigenvector v of J with eigenvalue λ > 1, then the small
change ∆s becomes λ∆s after one time step, and λ N ∆s after N time steps. If
λ > 1 this makes the change exponentially large. More generally, ∆s can be the
component of the state change vector in the direction v. With a non-linear map,
the Jacobian keeps changing and the dynamics is more complicated but what
remains is that a small initial variation can turn into a large variation after a few
steps. Note that, in general, the recurrent dynamics are bounded (for example,
if the hidden units use a bounded non-linearity such as the hyperbolic tangent)
so that the change after N steps must also be bounded. Instead when the largest
eigenvalue λ < 1, we say that the map from t to t + 1 is contractive: a small
change gets contracted, becoming smaller after each time step. This necessarily
makes the network forgetting information about the long-term past, but it also
makes its dynamics stable and easier to use.
What has been proposed to set the weights of reservoir computing machines
is to make the Jacobians slightly contractive. This is achieved by making the
spectral radius of the weight matrix large but slightly less than 1. However,
in practice, good results are often found with a spectral radius of the recurrent
weight matrix being slightly larger than 1, e.g., 1.2 (Sutskever, 2012; Sutskever
et al., 2013). Keep in mind that with hyperbolic tangent units, the maximum
335

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

derivative is 1, so that in order to guarantee a Jacobian spectral radius less than
1, the weight matrix should have spectral radius less than 1 as well. However,
most derivatives of the hyperbolic tangent will be less than 1, which may explain
Sutskever’s empirical observation.
More recently, it has been shown that the techniques used to set the weights in
ESNs could be used to initialize the weights in a fully trainable recurrent network
(e.g., trained using back-propagation through time), helping to learn long-term
dependencies (Sutskever, 2012; Sutskever et al., 2013). In addition to setting the
spectral radius to 1.2, Sutskever sets the recurrent weight matrix to be initially
sparse, with only 15 non-zero input weights per hidden unit.
Note that when some eigenvalues of the Jacobian are exactly 1, information
can be kept in a stable way, and back-propagated gradients neither vanish nor
explode. The next two sections show methods to make some paths in the unfolded
graph correspond to “multiplying by 1” at each step, i.e., keeping information for
a very long time.

10.8.2

Combining Short and Long Paths in the Unfolded Flow
Graph

An old idea that has been proposed to deal with the diﬃculty of learning longterm dependencies is to use recurrent connections with long delays (Lin et al.,
1996). Whereas the ordinary recurrent connections are associated with a delay
of 1 (relating the state at t with the state at t + 1), it is possible to construct
recurrent networks with longer delays (Bengio, 1991), following the idea of incorporating delays in feedforward neural networks (Lang and Hinton, 1988) in order
to capture temporal structure (with Time-Delay Neural Networks, which are the
1-D predecessors of Convolutional Neural Networks, discussed in Chapter 9).

336

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

o

ot−1
W3

W1

st−2

s
W3

x

unfold

ot
W3

W3

st−1
xt−1

W1

xt

W3

st+1

st

W1

W1

ot+1

W1

xt+1

Figure 10.17: A recurrent neural networks with delays, in which some of the connections
reach back in time to more than one time step. Left: connectivity of the recurrent net,
with square boxes indicating the number of time delays associated with a connection.
Right: unfolded recurrent network. In the ﬁgure there are regular recurrent connections
with a delay of 1 time step (W 1) and recurrent connections with a delay of 3 time steps
(W3 ). The advantage of these longer-delay connections is that they allow to connect past
states to future states through shorter paths (3 times shorter, here), going through these
longer delay connections (in red).

As we have seen in Section 8.2.5, gradients may vanish or explode exponentially with respect to the number of time steps. If we have recurrent connections
with a time-delay of d, then instead of the vanishing or explosion going as O(λ T )
t
over T time steps (where λ is the largest eigenvalue of the Jacobians ∂s∂st−1
), the unfolded recurrent network now has paths through which gradients grow as O(λ T/d)
because the number of eﬀective steps is T /d. This allows the learning algorithm
to capture longer dependencies although not all long-term dependencies may be
well represented in this way. This idea was ﬁrst explored in Lin et al. (1996) and
is illustrated in Figure 10.17.

10.8.3

Leaky Units and a Hierarchy of Diﬀerent Time Scales

A related idea in order to obtain paths on which the product of derivatives is close
to 1 is to have units with linear self-connections and a weight near 1 on these
connections. The strength of that linear self-connection corresponds to a time
scale and thus we can have diﬀerent hidden units which operate at diﬀerent time
scales (Mozer, 1992). Depending on how close to 1 these self-connection weights
are, information can travel forward and gradients backward with a diﬀerent rate
of “forgetting” or contraction to 0, i.e., a diﬀerent time scale. One can view this
idea as a smooth variant of the idea of having diﬀerent delays in the connections
presented in the previous section. Such ideas were proposed in Mozer (1992);
ElHihi and Bengio (1996), before a closely related idea discussed in the next
337

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

section of gating these self-connections in order to let the network control at what
rate each unit should be contracting.
The idea of leaky units with a self-connection actually arises naturally when
considering a continuous-time recurrent neural network such as
ṡ iτ i = −si + σ(b i + W s + Ux)
where σ is the neural non-linearity (e.g., sigmoid or tanh), τi > 0 is a time constant
and ṡi indicates the temporal derivative of unit s i . A related equation is
ṡi τi = −si + (b i + W σ(s) + U x)
where the state vector s (with elements si ) now represents the pre-activation of
the hidden units.
When discretizing in time such equations (which changes the meaning of τ ),
one gets
st,i
1
+ σ(bi + W st + Uxt )
τi
τi
1
1
= (1 − )st,i + σ(b i + W st + Ux t).
τi
τi

st+1,i − s t,i = −
st+1,i

(10.7)

We see that the new value of the state is a convex linear combination of the old
value and of the value computed based on current inputs and recurrent weights,
if 1 ≤ τi < ∞. When τi = 1, there is no linear self-recurrence, only the nonlinear update which we ﬁnd in ordinary recurrent networks. When τ i > 1, this
linear recurrence allows gradients to propagate more easily. When τ i is large, the
state changes very slowly, integrating the past values associated with the input
sequence.
By associating diﬀerent time scales τi with diﬀerent units, one obtains diﬀerent
paths corresponding to diﬀerent forgetting rates. Those time constants can be
ﬁxed manually (e.g., by sampling from a distribution of time scales) or can be
learned as free parameters, and having such leaky units at diﬀerent time scales
appears to help with long-term dependencies (Mozer, 1992; Pascanu et al., 2013a).
Note that the time constant τ thus corresponds to a self-weight of (1 − 1τ), but
without any non-linearity involved in the self-recurrence.
Consider the extreme case where τ → ∞: because the leaky unit just averages
contributions from the past, the contribution of each time step is equivalent and
there is no associated vanishing or exploding eﬀect. An alternative is to avoid the
weight of τ1i in front of σ(bi + W st + U xt ), thus making the state sum all the past
values when τi is large, instead of averaging them.
338

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.8.4

The Long-Short-Term-Memory Architecture and Other
Gated RNNs

Whereas in the previous section we consider creating paths where derivatives
neither vanish nor explode too quickly by introducing self-loops, leaky units have
self-weights that are not context-dependent: they are ﬁxed, or learned, but remain
constant during a whole test sequence.

output
X

output gate
self-loop
+

X

state
forget gate
X

input gate
input

Figure 10.18: Block diagram of the LSTM recurrent network “cell”. Cells are connected
recurrently to each other, replacing the usual hidden units of ordinary recurrent networks.
An input feature is computed with a regular artiﬁcial neuron unit, and its value can be
accumulated into the state if the sigmoidal input gate allows it. The state unit has a
linear self-loop whose weight is controlled by the forget gate. The output of the cell can
be shut oﬀ by the output gate. All the gating units have a sigmoid non-linearity, while
the input unit can have any squashing non-linearity. The state unit can also be used as
extra input to the gating units. The black square indicates a delay of 1 time unit.

It is worthwhile to consider the role played by leaky units: they allow the
network to accumulate information (e.g. evidence for a particular feature or category) over a long duration. However, once that information gets used, it might be
useful for the neural network to forget the old state. For example, if a sequence
is made of subsequences and we want a leaky unit to accumulate evidence inside
each sub-subsequence, we need a mechanism to forget the old state by setting it
to zero and starting to count from fresh. Instead of manually deciding when to
clear the state, we want the neural network to learn to decide when to do it.
339

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

LSTM
This clever idea of conditioning the forgetting on the context is a core contribution
of the Long-Short-Term-Memory (LSTM) algorithm (Hochreiter and Schmidhuber, 1997), described below. Several variants of the LSTM are found in the
literature (Hochreiter and Schmidhuber, 1997; Graves, 2012; Graves et al., 2013;
Graves, 2013; Sutskever et al., 2014a) but the principle is always to have a linear
self-loop through which gradients can ﬂow for long durations. By making the
weight of this self-loop gated (controlled by another hidden unit), the time scale
of integration can be changed dynamically (even for ﬁxed parameters, but based
on the input sequence). The LSTM has been found extremely successful in a number of applications, such as unconstrained handwriting recognition (Graves et al.,
2009), speech recognition (Graves et al., 2013; Graves and Jaitly, 2014), handwriting generation (Graves, 2013), machine translation (Sutskever et al., 2014a),
image to text conversion (captioning) (Kiros et al., 2014b; Vinyals et al., 2014b;
Xu et al., 2015b) and parsing (Vinyals et al., 2014a).
The LSTM block diagram is illustrated in Figure 10.18. The corresponding
forward (state update) equations are given below, in the case of a shallow recurrent
network architecture. Deeper architectures have been successfully used in Graves
et al. (2013); Pascanu et al. (2014a). Instead of a unit that simply applies a
squashing function on the aﬃne transformation of inputs and recurrent units,
LSTM networks have “LSTM cells”. Each cell has the same inputs and outputs
as a vanilla recurrent network, but has more parameters and a system of gating
units that controls the ﬂow of information. The most important component is the
state unit st that has a linear self-loop similar to the leaky units described in the
previous section, but where the self-loop weight (or the associated time constant)
f
is controlled by a forget gate unit h t,i (for time step t and cell i), that sets this
weight to a value between 0 and 1 via a sigmoid unit:
X f
X f
f
f
h t,i = sigmoid(bi +
U ijxt,j +
Wijh t,j ).
(10.8)
j

j

where xt is the current input vector and h t is the current hidden layer vector,
containing the outputs of all the LSTM cells, and b f ,Uf , W f are respectively
biases, input weights and recurrent weights for the forget gates. The LSTM cell
internal state is thus updated as follows, following the pattern of Eq. 10.7, but
with a conditional self-loop weight hft,i :
X
X
st+1,i = hft,is t,i + h et,iσ(bi +
Uij x t,j +
W ij h t,j).
(10.9)
j

j

b, U and W respectively the biases, input weights and recurrent weights into
the LSTM cell, and the external input gate unit h et,i is computed similarly to the
340

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

forget gate (i.e., with a sigmoid unit to obtain a gating value between 0 and 1),
but with its own parameters:
X
X
h et,i = sigmoid(bei +
U eijxt,j +
Weijh t,j).
(10.10)
j

j

The output h t+1,i of the LSTM cell can also be shut oﬀ, via the output gate h ot,i,
which also uses a sigmoid unit for gating:
h t+1,i = tanh(s t+1,i )hot,i
X o
X o
o
o
h t,i = sigmoid(bi +
U ijx t,j +
W ij ht,j )
j

(10.11)

j

which has parameters bo , U o , Wo for its biases, input weights and recurrent
weights, respectively. Among the variants, one can choose to use the cell state s t,i
as an extra input (with its weight) into the three gates of the i-th unit, as shown
in Figure 10.18. This would require three additional parameters.
LSTM networks have been shown to learn long-term dependencies more easily
than vanilla recurrent architectures, ﬁrst on artiﬁcial data sets designed for testing the ability to learn long-term dependencies Bengio et al. (1994); Hochreiter
and Schmidhuber (1997); Hochreiter et al. (2000), then on challenging sequence
processing tasks where state-of-the-art performance was obtained (Graves, 2012;
Graves et al., 2013; Sutskever et al., 2014a).
Other Gated RNNs
Which pieces of the LSTM architecture are actually necessary? What other successful architectures could be designed that allow the network to dynamically
control the time scale and forgetting behavior of diﬀerent units?
Some answers to these questions are given with the recent work on gated
RNNs, whose units are also known as Gated Recurrent Units (GRU) (Chung
et al., 2014), which were successfully used in reaching the MOSES state-of-the-art
for English-to-French machine translation (Cho et al., 2014). The main diﬀerence
with the LSTM is that a single gating unit simultaneously controls the forgetting
factor and the decision to update the state unit, which is natural if we consider the
continuous-time interpretation of the self-weight of the state, as in the equation
for leaky units, Eq. 10.7. The update equations are the following:
X
X
ht+1,i = h ut,ih t,i + (1 − hut,i )σ(b i +
Uij xt,j +
Wij h rt,j h t,j).
(10.12)
j

j

where g u stands for “update” gate and g r for “reset” gate. Their value is deﬁned
as usual:
hut,i = sigmoid(bui +
U iju xt,j +
Wijuht,j )
(10.13)
j

j

341
X

X

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

and
h rt,i = sigmoid(bri +

X

U rijxt,j +

j

X

Wrijh t,j).

(10.14)

j

Many more variants around this theme can be designed. For example the reset
gate (or forget gate) output could be shared across a number of hidden units. Or
the product of a global gate (covering a whole group of units, e.g., a layer) and
a local gate (per unit) could be used to combine global control and local control.
However, several investigations over architectural variations of the LSTM and
GRU found no variant that would clearly beat both of these across a wide range
of tasks (Greﬀ et al., 2015; Jozefowicz et al., 2015a). Greﬀ et al. (2015) found
that a crucial ingredient is the forget gate, while Jozefowicz et al. (2015a) found
that adding a bias of 1 to the LSTM forget gate, a practice advocated by Gers
et al. (2000), makes the LSTM as strong as the best of the explored architectural
variants.
reading mechanism

me
mo

ry
cel
ls

writing
mechanism

Task network,
controlling the
memory

Figure 10.19: A schematic example of memory network architecture, in which we explicitly distinguish the “representation” part of the model (the “task network”, here a
recurrent net in the bottom) from the “memory” part of the model (the set of cells).
From this representation, one learns to “control” the memory, decided from where to
read and write (through the reading and writing mechanims, indicated by circles with
arrows pointing at the reading and writing addresses).

342

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.8.5

Memory Networks

One way to extend gated RNNs (such as the LSTM and the GRU) is to make
each cell store not just a single number but a whole vector, and make the diﬀerent
cells compete for the privilege of being updated by external inputs (writing into
the memory) or read out. We can now think of the state of these recurrent units
as the content of a memory, the input/forget gates as mechanisms to decide when
and what to write into the memory, and the output gate as a mechanism to decide
from which cells to read. If the content of a cell is copied (not forgotten) at most
time steps, then the information it contains can be propagated forward in time
and the gradients backward in time without either vanishing or exploding. This
is illustrated in Figure 10.19, where we see that a neural network (recurrent in
the ﬁgure) is coupled with a memory, and can choose to read from it or write to
it at speciﬁc addresses. If one wants to backprop through the choice of reading or
writing address, though, choosing a single discrete address is problematic, as the
derivative through a discrete decision is essentially zero. To avoid this problem,
it was proposed to consider multiple addresses at once, linearly weighting the
reading or writing action over many (or all) the addresses, with learned weights
through which we can back-propagate (Weston et al., 2014; Graves et al., 2014b).
For example, when reading from the memory, we can take a weighted average
of the contents of all the memory addresses, and those weights can be forced
to sum to 1 by using a softmax. Another option is to consider these weights as
probabilities for choosing to read or write at the given address, and a variant of the
REINFORCE algorithm (Williams, 1992) can be used to estimate a noisy gradient
on the addressing weights (Zaremba and Sutskever, 2015). These architectures
with an explicit memory have been called memory networks (Weston et al., 2014)
or neural Turing machines (Graves et al., 2014b) and seem to allow models to
learn tasks that ordinary RNNs or LSTM RNNs cannot learn. One reason for
this advantage may be because information and gradients can be propagated
(forward in time or backwards in time, respectively) for very long durations.

10.8.6

Better Optimization

A central optimization diﬃculty with RNNs regards the learning of long-term
dependencies (Hochreiter, 1991; Bengio et al., 1993, 1994). This diﬃculty has
been explained in detail in Section 8.2.5. The gist of the problem is that the
composition of the non-linear recurrence with itself over many many time steps
yields a highly non-linear function whose derivatives (e.g. of the state at T w.r.t.
∂s
the state at t < T , i.e. the Jacobian matrix ∂sTt ) tend to either vanish or explode
as T −t increases, because it is equal to the product of the state transition Jacobian
t+1
matrices ∂s∂s
)
t
343

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

If it explodes, the parameter gradient ∇ θ L also explodes, yielding gradientbased parameter updates that are poor. A simple heuristic but practical solution
to this problem is discussed in the next section (Sec. 10.8.7). However, as discussed
in Bengio et al. (1994), if the state transition Jacobian matrix has eigenvalues that
are larger than 1 in magnitude, then it can yield to “unstable” dynamics, in the
sense that a bit of information cannot be stored reliably for a long time in the
presence of input “noise”. Indeed, the state transition Jacobian matrix eigenvalues
indicate how a small change in some direction (the corresponding eigenvector) will
be expanded (if the eigenvalue is greater than 1) or contracted (if it is less than
1).
An interesting idea proposed in Martens and Sutskever (2011) is that at the
same time as ﬁrst derivatives are becoming smaller in directions associated with
long-term eﬀects, so may the higher derivatives. In particular, if we use a secondorder optimization method (such as the Hessian-free method of Martens and
Sutskever (2011)), then we could diﬀerentially treat diﬀerent directions: divide
the small ﬁrst derivative (gradient) by a small second derivative, while not scaling
up in the directions where the second derivative is large (and hopefully, the ﬁrst
derivative as well). Whereas in the scalar case, if we add a large number and a
small number, the small number is “lost”, in the vector case, if we add a large
vector with a small vector, it is still possible to recover the information about
the direction of the small vector if we have access to information (such as in the
second derivative matrix) that tells us how to rescale appropriately each direction.
One disadvantage of many second-order methods, including the Hessian-free
method, is that they tend to be geared towards “batch” training (or fairly large
minibatches) rather than “stochastic” updates (where only one example or a
small minibatch of examples are examined before a parameter update is made).
Although the experiments on recurrent networks applied to problems with longterm dependencies showed very encouraging results in Martens and Sutskever
(2011), it was later shown that similar results could be obtained by much simpler
methods (Sutskever, 2012; Sutskever et al., 2013) involving better initialization,
a cheap surrogate to second-order optimization (a variant on the momentum
technique, Section 8.5), and the clipping trick described below.

10.8.7

Clipping Gradients

As discussed in Section 8.2.4, strongly non-linear functions such as those computed by a recurrent net over many time steps tend to have derivatives that can
be either very large or very small in magnitude. This is illustrated in Figures 8.2
and 8.3, in which we see that the objective function (as a function of the parameters) has a “landscape” in which one ﬁnds “cliﬀs”: wide and rather ﬂat regions
separated by tiny regions where the objective function changes quickly, forming
344

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

a kind of cliﬀ.
The diﬃculty that arises is that when the parameter gradient is very large,
a gradient descent parameter update could throw the parameters very far, into
a region where the objective function is larger, wasting a lot of the work that
had been done to reach the current solution. This is because gradient descent is
hinged on the assumption of small enough steps, and this assumption can easily
be violated when the same learning rate is used for both the ﬂatter parts and the
steeper parts of the landscape.

Figure 10.20: Example of the eﬀect of gradient clipping in a recurrent network with two
parameters w and b. Vertical axis is the objective function to minimize. Note the cliﬀ
where the gradient explodes and from where gradient descent can get pushed very far.
Clipping the gradient when its norm is above a threshold (Pascanu et al., 2013a) prevents
this catastrophic outcome and helps training recurrent nets with long-term dependencies
to be captured.

A simple type of solution has been in used by practitioners for many years:
clipping the gradient. There are diﬀerent instances of this idea (Mikolov, 2012;
Pascanu et al., 2013a). One option is to clip the parameter gradient from a minibatch element-wise (Mikolov, 2012) just before the parameter update. Another
is to clip the norm ||g|| of the gradient g (Pascanu et al., 2013a) just before the
parameter update:
if ||g|| > v
g←

gv
||g||

(10.15)

where v is the norm threshold and g is used to update parameters. The latter
method has the advantage that it guarantees that each step is still in the gradient
345

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

direction, but experiments suggest that both forms work similarly. Althought the
parameter update has the same direction as the true gradient, with gradient norm
clipping, the parameter update vector norm is now bounded, typically avoiding to
perform a detrimental step when gradient explosion occurs. In fact, even simply
taking a random step when the gradient magnitude is above a threshold tends
to work almost as well. Note that even though the per-minibatch direction does
not change, the importance given to diﬀerent minibatches may vary when using
norm-clipping. With element-wise clipping, the direction of the update is not
anymore aligned with the true gradient, but at least it is still a descent direction.

10.8.8

Regularizing to Encourage Information Flow

Whereas clipping helps dealing with exploding gradients, it does not help with
vanishing gradients. To address vanishing gradients and better capture longterm dependencies, we discussed the idea of creating paths in the computational
graph of the unfolded recurrent architecture along which the product of gradients
associated with arcs is near 1. One approach to achieve this is with LSTMs
and other self-loops and gating mechanisms, described above in Section 10.8.4.
Another idea is to regularize or constrain the parameters so as to encourage
“information ﬂow”. In particular, we would like the gradient vector ∇ st L being
back-propagated to maintain its magnitude (even if there is only a loss at the end
of the sequence), i.e., we want
∇ s tL

∂st
∂s t−1

to be as large as
∇st L.

With this objective, Pascanu et al. (2013a) propose the following regularizer:

 
2
∂st 
X |∇ s tL ∂s t−1  |

Ω=
(10.16)
− 1 .
||∇
L|
|
s
t
t

It looks like computing the gradient of this regularizer is diﬃcult, but Pascanu
et al. (2013a) propose an approximation in which we consider the back-propagated
vectors ∇ stL as if they were constants (for the purpose of this regularizer, i.e., no
need to back-prop through them). The experiments with this regularizer suggest
that, if combined with the norm clipping heuristic (which handles gradient explosion), it can considerably increase the span of the dependencies that an RNN can
learn. Because it keeps the RNN dynamics on the edge of explosive gradients, the
gradient clipping is particularly important: otherwise gradient explosion prevents
learning to succeed.
346

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.8.9

Organizing the State at Multiple Time Scales

Another promising approach to handle long-term dependencies is the old idea
of organizing the state of the RNN at multiple time-scales (El Hihi and Bengio,
1996), with information ﬂowing more easily through long distances at the slower
time scales. This is illustrated in Figure 10.21.

Figure 10.21: Example of a multi-scale recurrent net architecture (unfolded in time), with
higher levels operating at a slower time scale. Information can ﬂow unhampered (either
forward or backward in time) over longer durations at the higher levels, thus creating
long-paths (such as the dotted path) through which long-term dependencies between
elements of the input/output sequence can be captured.

There are diﬀerent ways in which a group of recurrent units can be forced to
operate at diﬀerent time scales. One option is to make the recurrent units leaky
(as in Eq. 10.7), but to have diﬀerent groups of units associated with diﬀerent
ﬁxed time scales. This was the proposal in Mozer (1992) and has been successfully
used in Pascanu et al. (2013a). Another option is to have explicit and discrete
updates taking place at diﬀerent times, with a diﬀerent frequency for diﬀerent
groups of units, as in Figure 10.21. This is the approach of El Hihi and Bengio
(1996); Koutnik et al. (2014) and it also worked well on a number of benchmark
datasets.

10.9

Handling Temporal Dependencies with n-grams,
HMMs, CRFs and Other Graphical Models

This section regards probabilistic approaches to sequential data modeling which
have often been viewed as in competition with RNNs, although RNNs can be seen
as a particular form of dynamic Bayesian networks6 , as directed graphical models
with deterministic latent variables7 .
6

Dynamic Bayesian networks or dynamic probabilistic networks are directed graphical models
for sequential data, with shared parameters across time (Dean and Kanazawa, 1989; Kanazawa
et al., 1995; ?)
7
Latent variables are random variables that are not directly observed, although they can
depend on some that are, and here the dependency is so strong that the latent variables are
functions of observed variables
347

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.9.1

n-grams

n-grams are non-parametric estimators of conditional probabilities based on counting relative frequencies of occurrence, and they have been the core building block
of statistical language modeling for many decades (Jelinek and Mercer, 1980;
Katz, 1987; Chen and Goodman, 1999). Like RNNs, they are based on the product rule (or chain rule) decomposition of the joint probability into conditionals,
Eq. 10.6, which relies on estimates P (xt | xt−1 , . . . , x1) to compute P (x1 , . . . , xT ).
What is particular of n-grams is that
1. they estimate these conditional probabilities based only on the last n − 1
values (to predict the next one)
2. they assume that the data is symbolic, i.e., x t is a symbol taken from a
ﬁnite alphabet V (for vocabulary), and
3. the conditional probability estimates are obtained from frequency counts
of all the observed length-n subsequences, hence the names unigram (for
n=1), bigram (for n=2), trigram (for n=3), and n-gram in general.
The maximum likelihood estimator for these conditional probabilities is simply
the relative frequency of occurrence of the left hand symbol in the context of the
right hand symbols, compared to all the other possible symbols in V:
#{x t, x t−1, . . . , xt−n+1 }
0
x 0 ∈V #{x t, x t−1 , . . . , xt−n+1 }

P (xt | xt−1, . . . , xt−n+1) = P

(10.17)

t

where #{a, b, c} denotes the cardinality of the set of tuples (a, b, c) in the training
set, and where the denominator is also a count (if border eﬀects are handled
properly).
A fundamental limitation of the above estimator is that it is very likely to be
zero in many cases, even though the tuple (x t, xt−1 , . . . , xt−n+1) may show up in
the test set. In that case, the test log-likelihood would be inﬁnitely bad (−∞). To
avoid that catastrophic outcome, n-grams employ some form of smoothing, i.e.,
techniques to shift probability mass from the observed tuples to unobserved ones
that are similar, a central idea behind most non-parametric statistical methods.
See Chen and Goodman (1999) for a review and empirical comparisons. One
basic technique consists in assigning a non-zero probability mass to any of the
possible next symbol values. Another very popular idea consists in backing oﬀ,
or mixing (as in mixture model), the higher-order n-gram predictor with all the
lower-order ones (with smaller n). Back-oﬀ methods look-up the lower-order ngrams if the frequency of the context xt−1 , . . . , xt−n+1 is too small, i.e., considering
the contexts xt−1 , . . . , xt−n+k , for increasing k, until a suﬃciently reliable estimate
is found.
348

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Another interesting idea that is related to neural language models (Section 12.4)
is to break up the symbols into classes (by some form of clustering) and back-up
to, or mix with, less precise models that only consider the classes of the words
in the context (i.e. aggregating statistics from a larger portion of the training
set). One can view the word classes as a very impoverished learned representation of words which help to generalize across words of the same class. What
distributed representations (e.g. neural word embeddings) bring is a richer notion
of similarity by which individual words keep their own identity (instead of being
indistinguishable from the other words in the same class) and yet share learned
attributes with other words with which they have some elements in common (but
not all). This kind of richer notion of similarity makes generalization more speciﬁc
and the representation not necessarily lossy, unlike with word classes.

10.9.2

Eﬃcient Marginalization and Inference for Temporally Structured Outputs by Dynamic Programming

Many temporal modeling approaches can be cast in the following framework,
which also includes hybrids of neural networks with HMMs and conditional random ﬁelds (CRFs), ﬁrst introduced in Bottou et al. (1997); LeCun et al. (1998c)
and later developed and applied with great success in Graves et al. (2006); Graves
(2012) with the Connectionist Temporal Classiﬁcation (CTC) approach, as well
as in Do and Artières (2010) and other more recent work (Farabet et al., 2013b;
Deng et al., 2014).
These ideas have been rediscovered in a simpliﬁed form
(limiting the input-output relationship to a linear one) as CRFs (Laﬀerty et al.,
2001), i.e., undirected graphical models whose parameters are linear functions of
input variables. In section 10.10 we consider in more detail the neural network
hybrids and the “graph transformer” generalizations of the ideas presented below.
All these approaches (with or without neural nets in the middle) concern the
case where we have an input sequence (discrete or continuous-valued) {x t} and
a symbolic output sequence {y t} (typically of the same length, although shorter
output sequences can be handled by introducing “empty string” values in the
output). Generalizations to non-sequential output structure have been introduced
more recently (e.g. to condition the Markov Random Fields sometimes used to
model structural dependencies in images (Stewart et al., 2007)), at the loss of
exact inference (the dynamic programming methods described below).
Optionally, one also considers a latent variable sequence {s t} that is also discrete and inference needs to be done over {st}, either via marginalization (summing over all possible values of the state sequence) or maximization, i.e., picking
exactly or approximately the so-called MAP sequence, the one with the largest
probability, given the input. If the state variables s t and the target variables yt
have a 1-D Markov structure to their dependency, then computing likelihood, par349

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

tition function and MAP values can all be done eﬃciently by exploiting dynamic
programming to factorize the computation. On the other hand, if the state or
output sequence dependencies are captured by an RNN, then there is no ﬁniteorder Markov property and no eﬃcient and exact inference is generally possible.
However, many reasonable approximations have been used in the past, such as
with variants of the beam search algorithm (Lowerre, 1976). The idea of beam
search is that one maintains a set of promising candidate paths that end at some
time step t. For each additional time step, one considers extensions to t + 1 of
each of these paths and then prunes those with the worse overall cumulative score
(up to t + 1). The beam size is the number of candidates that are kept. See
Section 10.10.1 for more details on beam search.
The application of the principle of dynamic programming in these setups is the
same as what is used in the Forward-Backward algorithm (detailed more around
Eq. 10.21), for graphical models and HMMs (detailed more in Section 10.9.3) and
the Viterbi algorithm detailed below (Eq. 10.23). For both of these algorithms, we
are trying to sum (Forward-Backward algorithm) or maximize (Viterbi algorithm)
over paths the probability or score associated with each path.

…

Figure 10.22: Example of a temporally structured output graph, as can be found in CRFs,
HMMs, and neural net hybrids. Each node corresponds to a particular value of an output
random variable at a particular point in the output sequence (contrast with a graphical
model representation, where each node corresponds to a random variable). A path from
the source node to the sink node (e.g. red bold arrows) corresponds to an interpretation of
the input as a sequence of output labels. The dynamic programming recursions that are
used for computing likelihood (or conditional likelihood) or performing MAP inference
(ﬁnding the best path) involve sums or maximizations over sub-paths ending at one of
the particular interior nodes.
350

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Let G be a directed acyclic graph whose paths correspond to the sequences
that can be selected (for MAP) or summed over (marginalized for computing a
likelihood), as illustrated in Figure 10.22. In the above example, let z t represent
the choice variable (e.g., s t and yt in the above example), and each arc with score
a corresponds to a particular value of z t in its Markov context. In the language
of undirected graphical models, if a is the score associated with an arc from the
node for z t−1 = j to the one for z t = i, then a is minus the energy of a term
of the energy function associated with the event 1 zt−1 =j,zt =i and the associated
information from the input x (e.g. some value of xt ).
Hidden Markov models are based on the notion of Markov chain, which is
covered in much more detail in Section 14.1. A Markov chain is a sequence of
random variables z1 , . . . zT , and for our purposes the main property of a Markov
chain chain of order 1 is that the current value of zt contains enough information
about the previous values z1 , . . . z t−1 in order to predict the distribution of the next
random variable, zt+1 . In our context, we can make the z’s conditioned on some
x, the order 1 Markov property then means that P (zt | z t−1 , z t−2, . . . , z 1, x) =
P (zt | z t−1 , x), where x is conditioning information (the input sequence). When
we consider a path in that space, i.e. a sequence of values, we draw a graph with a
node for each discrete value of zt, and if it is possible to transition from z t−1 = j to
z t = i we draw an arc between these two nodes. Hence, the total number of nodes
in the graph would be equal to the length of the sequence, T , times the number of
values of z t, n, and the number of arcs of the graph would be up to T n2 (if every
value of z t can follow every value of zt−1, although in practice the connectivity is
often much smaller because not all transitions are typically feasible). A score a
is computed for each arc (which may include some component that only depends
on the source or only on the destination node), as a function of the conditioning
information x. The inference or marginalization problems involve performing the
following computations.
For the marginalization task, we want to compute the sum over all complete
paths (e.g. from source to sink) of the product along the path of the exponentiated
scores associated with the arcs on that path:
X Y
m(G) =
ea
(10.18)
path∈G a∈path

where the product is over all the arcs on a path (with score a), and the sum is
over all the paths associated with complete sequences (from beginning to end of
a sequence). m(G) may correspond to a likelihood, numerator or denominator of
a probability. For example,
P ({z t } ∈ Y | x) =
351

m(GY )
m(G)

(10.19)

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

where G Y is the subgraph of G which is restricted to sequences that are compatible
with some target answer Y.
For the inference task, we want to compute
Y
X
π(G) = arg max
ea = arg max
a
path∈G a∈path

v(G) =

max

path∈G

X

path∈G a∈path

a

a∈path

where π(G) is the most probable path and v(G) is its log-score or value, and again
the set of paths considered includes all of those starting at the beginning and
ending at the end the sequence.
The principle of dynamic programming is to recursively compute intermediate
quantities that can be reused eﬃciently so as to avoid actually going through an
exponential number of computations, e.g., though the exponential number of
paths to consider in the above sums or maxima. Note how it is already at play
in the underlying eﬃciency of back-propagation (or back-propagation through
time), where gradients w.r.t. intermediate layers or time steps or nodes in a ﬂow
graph can be computed based on previously computed gradients (for later layers,
time steps or nodes). Here it can be achieved by considering to restrictions of the
graph to those paths that end at a node n, which we denote Gn . GnY indicates
the additional restriction to subsequences that are compatible with the target
sequence Y , i.e., with the beginning of the sequence Y .

352

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

…

Figure 10.23: Illustration of the recursive computation taking place for inference or
marginalization by dynamic programming. See Figure 10.22. These recursions involve
sums or maximizations over sub-paths ending at one of the particular interior nodes (red
in the ﬁgure), each time only requiring to look up previously computed values at the
predecessor nodes (green).

We can thus perform marginalization eﬃciently as follows, and illustrated in
Figure 10.23. This is a generalization of the so-called Forward-Backward algorithm for HMMs
X
m(G) =
m(G n)
(10.20)
n∈ﬁnal(G)

where ﬁnal(G) is the set of ﬁnal nodes in the graph G, and we can recursively
compute the node-restricted sum via
X
0
m(Gn ) =
m(Gn )e an0 ,n
(10.21)
n 0 ∈pred(n)

where pred(n) is the set of predecessors of node n in the graph and a m,n is the
log-score associated with the arc from m to n. It is easy to see that expanding
the above recursion recovers the result of Eq. 10.18.
Similarly, we can perform eﬃcient MAP inference (also known as Viterbi
decoding) as follows.
v(G) =

max

n∈ﬁnal(G)

v(Gn )

(10.22)

and
v(G n ) =

max v(G m ) + am,n.

m∈pred(n)

353

(10.23)

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

To obtain the corresponding path, it is enough to keep track of the argmax associated with each of the above maximizations and trace back π(G) starting from
the nodes in ﬁnal(G). For example, the last element of π(G) is
n∗ ← arg max v(G n)
n∈ﬁnal(G)

and (recursively) the argmax node before n ∗ along the selected path is a new n ∗,
n∗ ← arg max v(Gm ) + am,n ∗,
m∈pred(n ∗ )

etc. Keeping track of these n ∗ along the way gives the selected path. Proving
that these recursive computations yield the desired results is straightforward and
left as an exercise.

10.9.3

HMMs

Hidden Markov Models (HMMs) are probabilistic models of sequences that were
introduced in the 60’s (Baum and Petrie, 1966) along with the E-M algorithm
(Section 19.2). They are very commonly used to model sequential structure, in
particular having been since the mid 80’s and until recently the technological
core of speech recognition systems (Rabiner and Juang, 1986; Rabiner, 1989).
Just like RNNs, HMMs are dynamic Bayes nets (Koller and Friedman, 2009),
i.e., the same parameters and graphical model structure are used for every time
step. Compared to RNNs, what is particular to HMMs is that the latent variable
associated with each time step (called the state) is discrete, with a separate set of
parameters associated with each state value. We consider here the most common
form of HMM, in which the Markov chain is of order 1, i.e., the state s t at time
t, given the previous states, only depends on the previous state st−1 :
P (st | st−1 , s t−2, . . . , s 1) = P (st | st−1 ),
which we call the transition or state-to-state distribution. Generalizing to higherorder Markov chains is straightforward: for example, order-2 Markov chains can
be mapped to order-1 Markov chains by considering as order-1 “states” all the
pairs (s t = i, s t−1 = j).
Given the state value, a generative probabilistic model of the visible variable
xt is deﬁned, that speciﬁes how each observation xt in a sequence (x 1, x2 , . . . , xT )
can be generated, via a model P (x t | st). Two kinds of parameters are distinguished: those that deﬁne the transition distribution, which can be given by a
matrix
Aij = P (s t = i | s t−1 = j),
354

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

and those that deﬁne the output model P (xt | s t). For example, if the data are
discrete and x t is a symbol x t, then another matrix can be used to deﬁne the
output (or emission) model:
Bki = P (xt = k | st = i).
Another common parametrization for P (xt | s t = i), in the case of continuous
vector-valued x t, is the Gaussian mixture model, where we have a diﬀerent mixture (with its own means, covariances and component probabilities) for each state
s t = i. Alternatively, the means and covariances (or just variances) can be shared
across states, and only the component probabilities are state-speciﬁc.
The overall likelihood of an observed sequence is thus
X Y
P (x1 , x2 , . . . , xT ) =
P (x t | st )P (st | st−1 ).
(10.24)
s1 ,s2,...,sT

t

In the language established earlier in Section 10.9.2, we have a graph G with
one node n per time step t and state value i, i.e., for s t = i, and one arc between
each node n (for 1s t=i) and its predecessors m for 1 s t−1 =j (when the transition
= 0). Following Eq. 10.24, the
probability is non-zero, i.e., P (st = i | s t−1 = j) 6
log-score a m,n for the transition between m and n would then be
a m,n = log P (xt | st = i) + log P (s t = i | s t−1 = j).
As explained in Section 10.9.2, this view gives us a dynamic programming
algorithm for computing the likelihood (or the conditional likelihood given some
constraints on the set of allowed paths), called the forward-backward or sumproduct algorithm, in time O(kN T ) where T is the sequence length, N is the
number of states and k the average in-degree of each node.
Although the likelihood is tractable and could be maximized by a gradientbased optimization method, HMMs are typically trained by the E-M algorithm
(Section 19.2), which has been shown to converge rapidly (approaching the rate of
Newton-like methods) in some conditions (if we view the HMM as a big mixture,
then the condition is for the ﬁnal mixture components to be well-separated, i.e.,
have little overlap) (Xu and Jordan, 1996).
At test time, the sequence of states that maximizes the joint likelihood
P (x 1, x2 , . . . , xT , s1, s2 , . . . , sT )
can also be obtained using a dynamic programming algorithm (called the Viterbi
algorithm). This is a form of inference (see Section 13.5) that is called MAP
(Maximum A Posteriori) inference because we want to ﬁnd the most probable
value of the unobserved state variables given the observed inputs. Using the same
355

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

deﬁnitions as above (from Section 10.9.2) for the nodes and log-score of the graph
G in which we search for the optimal path, the Viterbi algorithm corresponds to
the recursion deﬁned by Eq. 10.23.
If the HMM is structured in such a way that states have a meaning associated
with labels of interest, then from the MAP sequence one can read oﬀ the associated
labels. When the number of states is very large (which happens for example with
large-vocabulary speech recognition based on n-gram language models), even the
eﬃcient Viterbi algorithm becomes too expensive, and approximate search must
be performed. A common family of search algorithms for HMMs is the beam
search algorithm (Lowerre, 1976) (Section 10.10.1).
More details about speech recognition are given in Section 12.3. An HMM
can be used to associate a sequence of labels (y 1 , y2, . . . , yN ) with the input
(x 1, x2, . . . , x T), where the output sequence is typically shorter than the input
sequence, i.e., N < T . Knowledge of (y 1, y 2 , . . . , yN ) constrains the set of compatible state sequences (s1, s2, . . . , sT ), and the generative conditional likelihood
X
Y
P (x1, x2 , . . . , xT | y 1, y2 , . . . , yN ) =
P (xt | st)P (s t | s t−1 ).
s1 ,s2 ,...,s T∈S(y 1 ,y2 ,...,y N ) t

(10.25)
can be computed using the same forward-backward technique, and its logarithm
maximized during training, as discussed above.
Various discriminative alternatives to the generative likelihood of Eq. 10.25
have been proposed (Brown, 1987; Bahl et al., 1987; Nadas et al., 1988; Juang and
Katagiri, 1992; Bengio et al., 1992a; Bengio, 1993; Leprieur and Haﬀner, 1995;
Bengio, 1999a), the simplest of which is simply P (y1 , y2, . . . , yN | x1 , x2 , . . . , xT ),
which is obtained from Eq. 10.25 by Bayes rule, i.e., involving a normalization
over all sequences, i.e., the unconstrained likelihood of Eq. 10.24:
P (y1 , y2 , . . . , yN | x1 , x2 , . . . , x T) =

P (x 1 , x2 , . . . , xT | y1, y2 , . . . , yN )P (y 1, y2, . . . , yN )
.
P (x1 , x2 , . . . , xT )

Both the numerator and denominator can be formulated in the framework of the
previous section (Eqs. 10.19-10.21), where for the numerator we merge (add) the
log-scores coming from the structured output output model P (y1 , y2 , . . . , y N ) and
from the input likelihood model P (x1 , x 2, . . . , x T | y 1 , y2, . . . , yN ). Again, each
node of the graph corresponds to a state of the HMM at a particular time step
t (which may or may not emit the next output symbol y i), associated with an
input vector x t. Instead of making the relationship to the input the result of a
simple parametric form (Gaussian or multinomial, typically), the scores can be
computed by a neural network (or any other parametrized diﬀerential function).
This gives rise to discriminative hybrids of search or graphical models with neural
networks, discussed below, Section 10.10.
356

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

10.9.4

CRFs

Whereas HMMs are typically trained to maximize the probability of an input
sequence x given a target sequence y and correspond to a directed graphical
model, Conditional Random Fields (CRFs) (Laﬀerty et al., 2001) are undirected
graphical models that are trained to maximize the joint probability of the target
variables, given input variables, P (y | x). CRFs are special cases of the graph
transformer model introduced in Bottou et al. (1997); LeCun et al. (1998c), where
neural nets are replaced by aﬃne transformations and there is a single graph
involved. A graph transformer is an computational module that transforms a
weighted (with a scalar on each arc) directed acyclic graph into another one.
Examples of graph transformers are illustrated in Figure 10.24. In this context,
graph transformers can be seen as transforming the set of weights in their input
graph into a set of weights on their output graph, typically so that we can compute
derivatives of the output graph weights with respect to input graph weights, i.e.,
we can back-propagated costs through the graph transformer. Section 10.10 below
provides more discussion and examples.
Many applications of CRFs involve sequences and the discussion here will be
focused on this type of application, although applications to images (e.g. for
image segmentation) are also common. Compared to other graphical models,
another characteristic of CRFs is that there are no latent variables. The general
equation for the probability distribution modeled by a CRF is basically the same
as for fully visible (not latent variable) undirected graphical models, also known as
Markov Random Fields (MRFs, see Section 13.2.2), except that the “potentials”
(terms of the energy function) are parametrized functions of the input variables,
and the likelihood of interest is the posterior probability P (y | x).
As in many other MRFs, CRFs often have a particular connectivity structure
in their graph, which allows one to perform learning or inference more eﬃciently.
In particular, when dealing with sequences, the energy function typically only has
terms that relate neighboring elements of the sequence of target variables. For
example, the target variables could form a homogeneous8 Markov chain of order
k (given the input variables). A typical linear CRF example with binary outputs
would have the following structure:


k
X
X
X
X
1
P (y = y | x) = exp 
yt (b +
wi xtj ) +
yt yt−i (ui +
v ij x tj )
Z
t

j

i=1

j

(10.26)
where Z is the normalization constant, which is the sum over all y sequences of the
numerator. In that case, the score marginalization framework of Section 10.9.2
8

meaning that the same parameters are used for every time step
357

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

and coming from Bottou et al. (1997); LeCun et al. (1998c) can be applied by
making terms in the above exponential correspond to scores associated with nodes
t of a graph G. If there were more than two output classes, more nodes per time
step would be required but the principle would remain the same. A more general
formulation for Markov chains of order d is the following:
!
d
XX
1
P (y = y | x) =
exp
fd0 (yt , yt−1, . . . , y t−d0 , xt )
(10.27)
Z
0
t

d =0

where f d0 computes a potential of the energy function, a parametrized function
of both the past target values (up to y t−d0 ) and of the current input value xt . For
example, as discussed below fd 0 could be the output of an arbitrary parametrized
computation, such as a neural network.
Although Z looks intractable, because of the Markov property of the model
(order 1, in the example), it is again possible to exploit dynamic programming to
compute Z eﬃciently, as per Eqs. 10.19-10.21). Again, the idea is to compute the
sub-sum for sequences of length t ≤ T (where T is the length of a target sequence
y), ending in each of the possible state values at t, e.g., y t = 1 and yt = 0 in the
above example. For higher order Markov chains (say order d instead of 1) and a
larger number of state values (say N instead of 2), the required sub-sums to keep
track of are for each element in the cross-product of d − 1 state values, i.e., N d−1.
For each of these elements, the new sub-sums for sequences of length t + 1 (for
each of the N values at t + 1 and corresponding N max(0,d−2) past values for the
past d − 2 time steps) can be obtained by only considering the sub-sums for the
N d−1 joint state values for the last d − 1 time steps before t + 1.
Following Eq. 10.23, the same kind of decomposition can be performed to
eﬃciently ﬁnd the MAP conﬁguration of y’s given x, where instead of products
(sums inside the exponential) and sums (for the outer sum of these exponentials,
over diﬀerent paths) we respectively have sums (corresponding to adding the sums
inside the exponential) and maxima (across the diﬀerent competing “previousstate” choices).

10.10

Combining Neural Networks and Search

The idea of combining neural networks with HMMs or related search or alignmentbased components (such as graph transformers) for speech and handwriting recognition dates from the early days of research on multi-layer neural networks (Bourlard
and Wellekens, 1990; Bottou et al., 1990; Bengio, 1991; Bottou, 1991; Haﬀner
et al., 1991; Bengio et al., 1992a; Matan et al., 1992; Bourlard and Morgan, 1993;
Bengio et al., 1995; Bengio and Frasconi, 1996; Baldi and Brunak, 1998) – and
358

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Figure 10.24: Illustration of the stacking of graph transformers (right, c) as a generalization of the stacking of convolutional layers (middle, b) or of regular feedforward layers
that transform ﬁxed-size vectors (left, a). Figure reproduced with permission from the authors of Bottou et al. (1997). Quoting from that paper, (c) shows that “multilayer graph
transformer networks are composed of trainable modules that operate on and produce
graphs whose arcs carry numerical information”.

see more references in Bengio (1999b). See also 12.5 for combining recurrent and
other deep learners with generative models such as CRFs, GSNs or RBMs.
The principle of eﬃcient marginalization and inference for temporally structured outputs by exploiting dynamic programming (Sec. 10.9.2) can be applied
not just when the log-scores of Eqs. 10.18 and 10.20 are parameters or linear
functions of the input, but also when they are learned non-linear functions of the
input, e.g., via a neural network transformation, as was ﬁrst done in Bottou et al.
(1997); LeCun et al. (1998c). These papers additionally introduced the powerful
idea of learned graph transformers, illustrated in Figure 10.24. In this context,
a graph transformer is a machine that can map a directed acyclic graph G in to
another graph G out . Both input and output graphs have paths that represent
hypotheses about the observed data.
For example, in the above papers, and as illustrated in Figure 10.25, a segmentation graph transformer takes a singleton input graph (the image x) and outputs
a graph representing segmentation hypotheses (regarding sequences of segments
that could each contain a character in the image). Such a graph transformer could
be used as one layer of a graph transformer network for handwriting recognition
or document analysis for reading amounts on checks, as illustrated respectively
in Figures 10.26 and 10.27.
359

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Figure 10.25: Illustration of the input and output of a simple graph transformer that maps
a singleton graph corresponding to an input image to a graph representing hypothesized
segmentation hypotheses. Reproduced with permission from the authors of Bottou et al.
(1997).

Figure 10.26: Illustration of the graph transformer network that has been used for ﬁnding
the best segmentation of a handwritten word, for handwriting recognition. Reproduced
with permission from Bottou et al. (1997).

360

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

Figure 10.27: Illustration of the graph transformer network that has been used for reading
amounts on checks, starting from the single graph containing the image of the graph
to the recognized sequences of characters corresponding to the amount on the graph,
with currency and other recognized marks. Note how the grammar graph transformer
composes the grammar graph (allowed sequences of characters) and the recognition graph
(with character hypotheses associated with speciﬁc input segments, on the arcs) into an
interpretation graph that only contains the recognition graph paths that are compatible
with the grammar. Reproduced with permission from Bottou et al. (1997).

361

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

For example, after the segmentation graph transformer, a recognition graph
transformer could expand each node of the segmentation graph into a subgraph
whose arcs correspond to diﬀerent interpretations of the segment (which character is present in the segment?). Then, a dictionary graph transformer takes
the recognition graph and expands it further by considering only the sequences
of characters that are compatible with sequences of words in the language of interest. Finally, a language-model graph transformer expands sequences of word
hypotheses so as to include multiple words in the state (context) and weigh the
arcs according to the language model next-word log-probabilities.
Each of these transformations is parametrized and takes real-valued scores
on the arcs of the input graph into real-valued scores on the arcs of the output
graph. These transformations can be parametrized and learned by gradient-based
optimization over the whole series of graph transformers.

10.10.1

Approximate Search

Unfortunately, as in the above example, when the number of nodes of the graph
becomes very large (e.g., considering all previous n words to condition the logprobability of the next one, for n large), even dynamic programming (whose
computation scales with the number of arcs) is too slow for practical applications
such as speech recognition or machine translation. A common example is when a
recurrent neural network is used to compute the arcs log-score, e.g., as in neural
language models (Section 12.4). Since the prediction at step t depends on all
t − 1 previous choices, the number of states (nodes of the search graph G) grows
exponentially with the length of the sequence. In that case, one has to resort to
approximate search.
Beam Search
In the case of sequential structures as discussed in this chapter, a common family
of approximate search algorithms is the beam search (Lowerre, 1976).
• Break the nodes of the graph into g groups containing only “comparable
nodes”, e.g., the group of nodes n for which the maximum length of the
paths ending at n is exactly t.
• Process these groups of nodes sequentially, keeping only at each step t a
selected subset St of the nodes (the “beam”), chosen based on the subset
St−1 . Each node in St is associated with a score v̂(G n) that represents an
approximation (a lower bound) on the maximum total log-score of the path
ending at the node, v(Gn ) (deﬁned in Eq. 10.23, Viterbi decoding).
362

CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS

• St is obtained by following all the arcs from the nodes in St−1, and sorting
all the resulting group t nodes n according to their estimated (lower bound)
score
n
n0
v̂(G ) =
max
v̂(G ) + an 0,n ,
n 0 ∈S t−1andn 0∈pred(n)

while keeping track of the argmax in order to trace back the estimated best
path. Only the k nodes with the highest log-score are kept and stored in
St, and k is called the beam width.
• The estimated best ﬁnal node can be read oﬀ from max n∈ST v̂(G n ) and the
estimated best path from the associated argmax choices made along the
way, just like in the Viterbi algorithm.
One problem with beam search is that the beam often ends up lacking in
diversity, making the approximation poor. For example, imagine that we have
two “types” of solutions, but that each type has exponentially many variants (as
a function of t), due, e.g., to small independent variations in ways in which the
type can be expressed at each time step t. Then, even though the two types may
have close best log-score up to time t, the beam could be dominated by the one
that wins slightly, eliminating the other type from the search, although later time
steps might reveal that the second type was actually the best one.

363

Chapter 12

Applications
In this chapter, we describe how to put deep learning models to practical use. We
begin by discussing the large scale neural network implementations required for
most serious AI applications. Next, we review several speciﬁc application areas
that deep learning has been used to solve. While one goal of deep learning is to
design algorithms that are capable of solving a broad variety of tasks, so far some
degree of specialization is needed. For example, vision tasks require processing
a large number of input features (pixels) per example. Language tasks require
modeling a large number of possible values (words in the vocabulary) per input
feature.

12.1

Large Scale Deep Learning

Deep learning is based on the philosophy of connectionism: while an individual
biological neuron or an individual feature in a machine learning model is not
intelligent, a large population of these neurons or features acting together can
exhibit intelligent behavior. It truly is important to emphasize the fact that
the number of neurons must be large. One of the key factors responsible for the
improvement in neural network’s accuracy and the improvement of the complexity
of tasks they can solve between the 1980s and today is the dramatic increase in
the size of the networks we use. As we saw in Chapter 1.2.3, network sizes have
grown exponentially for the past three decades, yet artiﬁcial neural networks are
only as large as the nervous systems of insects.
Because the size of neural networks is of paramount importance, deep learning
requires high performance hardware and software infrastructure.

376

CHAPTER 12. APPLICATIONS

12.1.1

Fast CPU Implementations

Traditionally, neural networks were trained using the CPU of a single machine.
Today, this approach is generally considered insuﬃcient. We now mostly use GPU
computing or the CPUs of many machines networked together. Before moving to
these expensive setups, researchers worked hard to demonstrate that CPUs could
not manage the high computational workload required by neural networks.
A description of how to implement eﬃcient numerical CPU code is beyond
the scope of this book, but we emphasize here that careful implementation for
speciﬁc CPU families can yield large improvements. For example, in 2011, the best
CPUs available could run neural network workloads faster when using ﬁxed-point
arithmetic rather than ﬂoating-point arithmetic. By creating a carefully tuned
ﬁxed-point implementation, Vanhoucke et al. (2011) obtained a 3× speedup over a
strong ﬂoating-point system. Each new model of CPU has diﬀerent performance
characteristics, so sometimes ﬂoating-point implementations can be faster too.
The important principle is that careful specialization of numerical computation
routines can yield a large payoﬀ. Other strategies, besides choosing whether to
use ﬁxed or ﬂoating point, including optimizing data structures to avoid cache
misses and using vector instructions. Many machine learning researchers neglect
these implementation details, but when they restrict the size of the network one
can train, they in turn restrict the machine learning capabilities of the network.

12.1.2

GPU Implementations

Most modern neural network implementations are based on graphics processing
units. Graphics processing units (GPUs) are specialized hardware components
that were originally developed for graphics applications. The consumer market
for video gaming systems spurred development of graphics processing hardware.
The performance characteristics needed for good video gaming systems turn out
to be beneﬁcial for neural networks as well.
Video game rendering requires performing many operations in parallel quickly.
Models of characters and environments are speciﬁed in terms of lists of 3-D coordinates of vertices. Graphics cards must perform matrix multiplication and
division on many vertices in parallel to convert these 3-D coordinates into 2-D
on-screen coordinates. The graphics card must then perform many computations
at each pixel in parallel to determine the color of each pixel. In both cases, the
computations are fairly simple and do not involving much branching compared to
the computational workload that a CPU usually encounters. For example, each
vertex in the same rigid object will be multiplied by the same matrix; there is no
need to evaluate an if statement per-vertex to determine which matrix to multiply
by. The computations are also entirely independent of each other, and thus may
377

CHAPTER 12. APPLICATIONS

be parallelized easily. The computations also involve processing massive buﬀers
of memory, containing bitmaps describing the texture (color pattern) of each object to be rendered. Together, this results in graphics cards having been designed
to have a high degree of parallelism and high memory bandwidth, at the cost of
having a lower clock speed and less branching capability relative to traditional
CPUs.
Neural networks also beneﬁt from the same performance characteristics. Neural networks usually involve large and numerous buﬀers of parameters, activation
values, and gradient values, each of which must be completely updated during
every step of training. These buﬀers are large enough to fall outside the cache
of a traditional desktop computer so the memory bandwidth of the system often becomes the rate limiting factor. GPUs oﬀer a compelling advantage over
CPUs due to their high memory bandwidth. Neural network training algorithms
typically do not involve much branching or sophisticated control, so they are appropriate for neural network hardware. Since neural networks can be divided into
multiple individual “neurons” that can be processed independently from the other
neurons in the same layer, neural networks easily beneﬁt from the parallelism of
GPU computing.
GPU hardware was originally so specialized that it could only be used for
graphics tasks. Over time, GPU hardware became more ﬂexible, allowing custom
subroutines to be used to transform the coordinates of vertices or assign colors to
pixels. In principle, there was no requirement that these pixel values actually be
based on a rendering task. These GPUs could be used for scientiﬁc computing by
writing the output of a computation to a buﬀer of pixel values. Steinkrau et al.
(2005) implemented a two-layer fully connected neural network on an early GPU
and reported a 3X speedup over their CPU-based baseline. Shortly thereafter,
Chellapilla et al. (2006) demonstrated that the same technique could be used to
accelerate supervised convolutional networks.
The popularity of graphics cards for neural network training exploded after
the advent of General Purpose GPUs. These GP-GPUs could execute arbitrary
code, not just rendering subroutines. NVIDIA’s CUDA programming language
provided a way to write this arbitrary code in a C-like language. With their
relatively convenient programming model, massive parallelism, and high memory
bandwidth, GP-GPUs now oﬀer an ideal platform for neural network programming. This platform was rapidly adopted by deep learning researchers soon after
it became available (Raina et al., 2009; Ciresan et al., 2010).
Writing eﬃcient code for GP-GPUs remains a diﬃcult task best left to specialists. The techniques required to obtain good performance on GPU are very
diﬀerent from those used on CPU. For example, good CPU-based code is usually
designed to read information from the cache as much as possible. On GPU, most
378

CHAPTER 12. APPLICATIONS

writable memory locations are not cached, so it can actually be faster to compute the same value twice, rather than compute it once and read it back from
memory. GPU code is also inherently multi-threaded and the diﬀerent threads
must be coordinated with each other carefully. For example, memory operations
are faster if they can be coalesced. Coalesced reads or writes occur when several
threads can each read or write a value that they need simultaneously, as part
of a single memory transaction. Diﬀerent models of GPUs are able to coalesce
diﬀerent kinds of read or write patterns. Typically, memory operations are easier
to coalesce if among n threads, thread i accesses byte i + j of memory, and j
is a multiple of some power of 2. The exact speciﬁcations diﬀer between models of GPU. Another common consideration for GPUs is making sure that each
thread in a group executes the same instruction simultaneously. This means that
branching can be diﬃcult on GPU. Threads are divided into small groups called
warps. Each thread in a warp executes the same instruction during each cycle,
so if diﬀerent threads within the same warp need to execute diﬀerent code paths,
these diﬀerent code paths must be traversed sequentially rather than in parallel.
Due to the diﬃculty of writing high performance GPU code, researchers should
structure their workﬂow to avoid needing to write new GPU code in order to test
new models or algorithms. Typically, one can do this by building a software library
of high performance operations like convolution and matrix multiplication, then
specifying models in terms of calls to this library of operations. For example,
the machine learning library Pylearn2 (Warde-Farley et al., 2011) speciﬁes all of
its machine learning algorithms in terms of calls to the Theano (Bergstra et al.,
2010a; Bastien et al., 2012) and cuda-convnet (Krizhevsky, 2010), which provide
these high-performance operations. This factored approach can also ease support
for multiple kinds of hardware. For example, the same Theano program can run
on either CPU or GPU, without needing to change any of the calls to Theano
itself. Other libraries like Torch (Collobert et al., 2011b) provide similar features.

12.1.3

Large Scale Distributed Implementations

In many cases, the computational resources available on a single machine are
insuﬃcient. We therefore want to distribute the workload of training and inference
across many machines.
Distributing inference is simple, because each input example we want to process can be run by a separate machine. This is known as data parallelism.
It is also possible to get model parallelism, where multiple machines work
together on a single datapoint, with each machine running a diﬀerent part of the
model. This is feasible for both inference and training.
Data parallelism during training is somewhat harder. We can increase the
size of the minibatch used for a single SGD, but usually we get less than linear
379

CHAPTER 12. APPLICATIONS

returns in terms of optimization performance. It would be better to allow multiple
machines to compute multiple gradient descent steps in parallel. Unfortunately,
the standard deﬁntion of gradient descent is as a completely sequential algorithm:
the gradient at step t is a function of the parameters produced by step t − 1.
This can be solved using asynchronous stochastic gradient descent (Bengio
et al., 2001a; Recht et al., 2011). In this approach, several processor cores share
the memory representing the parameters. Each core reads parameters without
a lock, then computes a gradient, then increments the parameters without a
lock. This reduces the average amount of improvement that each gradient descent
step yields, because some of the cores overwrite each other’s progress, but the
increased rate of production of steps causes the learning process to be faster
overall. Dean et al. (2012) pioneered the multi-machine implementation of this
lock-free approach to gradient descent, where the parameters are managed by a
parameter server rather than stored in shared memory. Distributed asynchronous
gradient descent remains the primary strategy for training large deep networks
and is used by most major deep learning groups in industry (Chilimbi et al., 2014;
Wu et al., 2015). Academic deep learning researchers typically cannot aﬀord the
same scale of distributed learning systems but some research has focused on how
to build distributed networks with relatively low-cost hardware available in the
university setting (Coates et al., 2013).

12.1.4

Model Compression

In many commercial applications, it is much more important that the time and
memory cost of running inference in a machine learning model be low than that
the time and memory cost of training be low. For applications that do not require
personalization, it is possible to train a model once, then deploy it to be used by
billions of users. In many cases, the end user is more resource-constrained than
the developer. For example, one might train a speech recognition network with a
powerful computer cluster, then deploy it on mobile phones.
A key strategy for reducing the cost of inference is model compression (Buciluǎ
et al., 2006). The basic idea of model compression is to replace the original,
expensive model with a smaller model that requires less resources to store and
evaluate.
Model compression is applicable when the size of the original model is driven
primarily by a need to prevent overﬁtting. In most cases, the model with the
lowest generalization error is an ensemble of several independently trained models.
Evaluating all n ensemble members is expensive. Sometimes, even a single model
generalizes better if it is large (for example, if it is regularized with dropout).
These large models learn some function f (x), but do so using many more
parameters than are necessary for the task. Their size is necessary only due to
380

CHAPTER 12. APPLICATIONS

the limited number of training examples. As soon as we have ﬁt this function
f (x), we can generate a training set containing inﬁnitely many examples, simply
by applying f to randomly sampled points x. We then train the new, smaller,
model to match f (x) on these points. In order to most eﬃciently use the capacity
of the new, small model, it is best to sample the new x points from a distribution
resembling the actual test inputs that will be supplied to the model later. This can
be done by corrupting training examples or by drawing points from a generative
model trained on the original training set.
Alternatively, one can train the smaller model only on the original training
points, but train it to copy other features of the model, such as its posterior
distribution over the incorrect classes (Hinton et al., 2014).

12.1.5

Dynamic Structure

One strategy for accelerating data processing systems in general is to build systems that have dynamic structure in the graph describing the computation needed
to process an input. Data processing systems can dynamically determine which
subset of many neural networks should be run on a given input. Individual neural networks can also exhibit dynamic structure internally by determining which
subset of features to compute given information from the input. This form of
dynamic structure inside neural networks is sometimes called conditional computation (Bengio, 2013a; Bengio et al., 2013a) , though many kinds of dynamic
structure predate this term. Since many components of the architecture may be
relevant only for a small amount of possible inputs, the system can run faster by
computing these features only when they are needed.
Dynamic structure of computations is a basic computer science principle applied generally throughout the software engineering discipline. The simplest versions of dynamic structure applied to neural networks are based on determining
which subset of some group of neural networks (or other machine learning models)
should be applied to a particular input.
A venerable strategy for accelerating inference in a classiﬁer is to use a cascade
of classiﬁers. The basic idea is that we are trying to detect the presence of a rare
object (or event). To know for sure that the object is present, we must use a
sophisticated classiﬁer with high capacity, that is expensive to run. However,
because the object is rare, we can usually reject inputs as not containing the
object with much less computation. In these situations, we can train a sequence
of classiﬁers. The ﬁrst classiﬁers in the sequence have low capacity, and are
trained to have high recall. In other words, they are trained to make sure we
do not wrongly reject an input when the object is present. The ﬁnal classiﬁer
is trained to have high precision. At test time, we run inference by running the
classiﬁers in a sequence, abandoning any example as soon as any one element in
381

CHAPTER 12. APPLICATIONS

the cascade rejects it. Overall, this allows us to verify the presence of objects
with high conﬁdence, using a high capacity model, but does not force us to pay
the cost of inference in a high capacity model for every example. The system as a
whole has somewhat high capacity just from the use of many models, even if all
of the individual models have the same capacity. It is also possible to design the
cascade so that models that come later have higher capacity. Viola and Jones
(2001) used a cascade of boosted decision trees to implement a fast and robust
face detector suitable for use in handheld digital cameras. Their classiﬁer localizes
a face using essentially a sliding window approach in which many windows are
examined and rejected if they do not contain faces. Another version of cascades
uses the earlier models to implement a sort of hard attention mechanism, e.g.
with early members of the cascade localizing an object and later members of the
cascade performing further processing on it. For example, Google transcribes
address numbers from Street View imagery using a two-step cascade that ﬁrst
locates the address number with one machine learning model and then transcribes
it with another (Goodfellow et al., 2014d).
Decision trees themselves are an example of dynamic structure, because each
node in the tree determines which of its subtrees should be evaluated for each
input. A simple way to accomplish the union of deep learning and dynamic
structure is to train a decision tree in which each node uses a neural network to
make the splitting decision (Guo and Gelfand, 1992), though this has typically
not been done with the primary goal of accelerating inference computations.
TODO–cite work on hard mixtures of experts TODO–work on attention mechanisms, Olshausen’s dynamic routing
One major obstacle to using dynamically structured systems is the decreased
degree of parallelism that results from the system following diﬀerent code branches
for diﬀerent inputs. This means that few operations in the network can be described as matrix multiplication or batch convolution on a minibatch of examples.
We can write more specialized sub-routines that convolve each example with different kernels or multiply each row of a design matrix by a diﬀerent set of columns
of weights. Unfortunately, these more specialized subroutines are diﬃcult to implement eﬃciently. CPU implementations will be slow due to the lack of cache
coherence and GPU implementations will be slow due to the lack of coalesced
memory transactions and the need to serialize warps when members of a warp
take diﬀerent branches. In some cases, these issues can be mitigated by partitioning the examples into groups that all take the same branch, and processing
these groups of examples simultaneously. This can be an acceptable strategy for
minimizing the time required to process a ﬁxed amount of examples in an oﬄine
setting. In a real-time setting where examples must be processed continuously,
partitioning the workload can result in load-balancing issues. For example, if we
382

CHAPTER 12. APPLICATIONS

assign one machine to process the ﬁrst step in a cascade and another machine to
process the last step in a cascade, then the ﬁrst will tend to be overloaded and the
last will tend to be underloaded. Similar issues arise if each machine is assigned
to implement diﬀerent nodes of a neural decision tree.

12.1.6

Specialized Hardward Implementations of Deep Networks

Since the early days of neural networks research, hardware designers have worked
on specialized hardware implementations that could speed up training and/or
inference of neural network algorithms. See early and more recent reviews of
specialized hardware for deep networks (Lindsey and Lindblad, 1994; Beiu et al.,
2003; Misra and Saha, 2010).
Diﬀerent forms of specialized hardware (Graf and Jackel, 1989; Mead and
Ismail, 2012; Kim et al., 2009; ?; Chen et al., 2014a,b) have been considered over
the last decades, starting with ASICs (application-speciﬁc integrated circuit),
either with digital (based on binary representations of numbers), analog (Graf
and Jackel, 1989; Mead and Ismail, 2012) (based on physical implementations of
continuous values as voltages or currents) or hybrid implementations (combining
digital and analog components), and in recent years with the more ﬂexible FPGA
(ﬁeld programmable gated array) implementations (where the particulars of the
circuit can be written on the chip after it has been built).
Whereas software implementations on general-purpose processing units (CPUs
and GPUs) typically use 32 or 64 bits precision ﬂoating point representations of
numbers, it has long been known that it was possible to use less precision, at
least at inference time (Holt and Baker, 1991; Holi and Hwang, 1993; Presley and
Haggard, 1994; Simard and Graf, 1994; Wawrzynek et al., 1996; Savich et al.,
2007). This has become a more pressing issue in recent years as deep learning
has gained in popularity in industrial products, and as the great impact of faster
hardware was demonstrated with GPUs. Another factor that motivates current
research on specialized hardware for deep networks is that the rate of progress of
a single CPU or GPU core has slowed down, and most recent improvements in
computing speed have come from parallelization across cores (either in CPUs or
GPUs). This is very diﬀerent from the situation of the 1990’s (the previous neural network era) where the hardware implementations of neural networks (which
might take two years from inception to availability of a chip) could not keep up
with the rapid progress and low prices of general-purpose CPUs. Building specialized hardware is thus a way to push the envelope further, at a time when new
hardware designs are being developed for low-power devices such as phones, aiming for general-public applications of deep learning (e.g., with speech, computer
vision or natural language).
Recent work on low-precision implementations of backprop-based neural nets
383

CHAPTER 12. APPLICATIONS

(Vanhoucke et al., 2011; Courbariaux et al., 2015; Gupta et al., 2015) suggests
that between 8 and 16 bits of precision can suﬃce for using or training deep
neural networks with back-propagation. What is clear is that more precision is
required during training than at inference time, and that some forms of dynamic
ﬁxed point representation of numbers can be used to reduce how many bits are
required per number. Whereas ﬁxed point numbers are restricted to a ﬁxed
range (which corresponds to a given exponent in a ﬂoating point representation),
dynamic ﬁxed point representations share that range among a set of numbers
(such as all the weights in one layer). Using ﬁxed point rather than ﬂoating
point representations and using less bits per number reduces the surface area,
power requirements and computing time needed for performing multiplications,
and multiplications are the most demanding of the operations needed to use or
train a modern deep network with backprop.

12.2

Computer Vision

Computer vision has traditionally been one of the most active research areas for
deep learning applications. Many of the most popular standard benchmark tasks
for deep learning algorithms are forms of object recognition or optical character
recognition.
Computer vision is a very broad ﬁeld encompassing a wide variety of ways of
processing images, and an amazing diversity of applications such as recovering
sound waves from the vibrations they induce in objects visible in a video (Davis
et al., 2014). Most deep learning research on computer vision has not focused on
such exotic applications that expand the realm of what is possible with imagery
but rather a small core of AI goals aimed at replicating human abilities. Most
deep learning for computer vision is used for object recognition or detection of
some form, whether this means reporting which object is present in an image,
annotating an image with bounding boxes around each object, transcribing a
sequence of symbols from an image, or labeling each pixel in an image with the
identity of the object it belongs to. Because generative modeling has been a
guiding principle of deep learning research, there is also a large body of work on
image synthesis using deep models. While image synthesis ex nihilo is usually
not considered a computer vision endeavor, models capable of image synthesis
are usally useful for image restoration, a computer vision task involving repairing
defects in images or removing objects from images.

12.2.1

Preprocessing

Many application areas require sophisticated preprocessing because the original
input comes in a form that is diﬃcult for many deep learning architectures to
384

CHAPTER 12. APPLICATIONS

represent. Computer vision usually requires relatively little of this kind of preprocessing. The images should be standardized so that their pixels all lie in the
same, reasonable range, like [0,1] or [-1, 1]. Mixing images that lie in [0,1] with
images that lie in [0, 255] will usually result in failure. This is the only kind
of preprocessing that is strictly necessary. Many computer vision architectures
require images of a standard size, so images must be cropped or scaled to ﬁt that
size. However, even this rescaling is not always strictly necessary. some convolutional models are able to process variable size input if their output varies in size
with the input or if they use Some convolutional models accept variably-sized
inputs and dynamically adjust the size of their pooling regions to keep the output
size constant (Waibel et al., 1989). Other convolutional models have variablesized output that automatically scales in size with the input, such as models that
denoise or label each pixel in an image (Hadsell et al., 2007).
Many other kinds of preprocessing are less necessary but help to reduce the
size of the model required to obtain good accuracy on the training set, the amount
of time required for training, or the size of the training set required to obtain good
accuracy on the test set.
Any form of preprocessing that removes some of the complexity from the
vision task will accomplish both of these goals. Simpler tasks do not require as
large of models to solve, and simpler solutions are more likely to generalize well.
Preprocessing of this kind is usually designed to remove some kind of variability
in the input data that is easy for a human designer to describe and that the
human designer is conﬁdent has no relevance to the task. When training with
large datasets and large models, this kind of preprocessing is often unnecessary,
and it is best to just let the model learn which kinds of variability it should
become invariant to. For example, the AlexNet system for classifying ImageNet
only has one preprocessing step: subtracting the mean across training examples
of each pixel (Krizhevsky et al., 2012a).
Another approach to preprocessing is to artiﬁcially introduce more variation
into the training set. Dataset augmentation gives the model more training data
without requiring the collection of as much real data. This kind of preprocessing
usually increases the optimal model size and the amount of time required for
training.
Contrast Normalization
One of the most obvious sources of variation that can be safely removed for
many tasks is the amount of contrast in the image. Contrast simply refers to the
magnitude of the diﬀerence between the bright and the dark pixels in an image.
There are many ways of quantifying the contrast of an image. In the context of
deep learning, contrast usually refers to the standard deviation of the pixels in an
385

CHAPTER 12. APPLICATIONS

image or region of an image. Suupose we have an image represented by a tensor
X ∈ Rr×c×3 , with Xi,j,0 being the red intensity at row i and column j, Xi,j,1
giving the green intensity and Xi,j,2 giving the green intensity. Then the contrast
of the entire image is given by
v
u
r X
c X
3
u 1 X

2
t
Xi,j,k − X̄
3rc
i=1 j=1 k=1

where X̄ is the mean intensity of the entire image:
r
c
3
1 X XX
X̄ =
X i,j,k.
3rc
i=1 j=1 k=1

Global contrast normalization (GCN) aims to prevent images from having
varying amounts of contrast by subtracting the mean from each image, then
rescaling it so that the standard deviation across its pixels is equal to some constant s. This approach is complicated by the fact that no scaling factor can change
the contrast of a zero-contrast image (one whose pixels all have equal intensity).
Images with very low but non-zero contrast often have little information content.
Dividing by the true stand deviation usually accomplishes nothing more than
amplifying sensor noise or compression artifacts in such cases. This motivates
introducing a small, positive regularization parameter λ to bias the estimate of
the standard deviation. Alternately, one can constrain the denominator to be at
least . Given an input image X, GCN produces an output image X0 , deﬁned such
that
X 0i,j,k = s

 q
max , λ +

1
3rc

X i,j,k − X̄
P r Pc P3
i=1

j=1


2
k=1 x i,j,k − X̄

.

(12.1)

Datasets consisting of large images cropped to interesting objects are unlikely
to contain any images with nearly constant intensity. In these cases, it is safe
to practically ignore the small denominator problem by setting λ = 0 and avoid
division by 0 in extremely rare cases by setting  to an extremely low value like
10−8 . This is the approach used by Goodfellow et al. (2013a) on the CIFAR-10
dataset. Small images cropped randomly are more likely to have nearly constant
intensity, making aggressive regularization more useful. Coates et al. (2011) used
 = 0 and λ = 10 on small, randomly selected patches drawn from CIFAR-10.
The scale parameter s can usually be set to 1, as done by Coates et al. (2011),
or chosen to make each individual pixel have standard deviation across examples
close to 1, as done by Goodfellow et al. (2013a).
386

CHAPTER 12. APPLICATIONS

Figure 12.1: GCN maps examples onto a sphere. Left) Raw input data may have any
norm. Center) GCN with λ = 0 maps all non-zero examples perfectly onto a sphere.
Right) Regularized GCN, with λ > 0, draws examples toward the sphere but does not
completely discard the variation in their norm.

Note that the standard deviation in equation 12.1 is just a rescaling of the
norm of the image. It is preferable to deﬁne GCN in terms of standard
deviation so that the same s may be used regardless of image size. However,
this observation can be useful because it helps to understand GCN as mapping
examples to a spherical shell. See Fig. 12.1 for an illustration. This can be a useful
property because neural networks are often better at responding to directions
in space rather than exact locations. Responding to multiple distances in the
same direction requires hidden units with collinear weight vectors but diﬀerent
biases. Such coordination can be diﬃcult for the learning algorithm to discover.
Additionally, many shallow graphical models have problems with representing
multiple separated modes along the same line. GCN avoids these problems by
reducing each example to a direction rather than a direction and a distance.
Counterintuitively, there is a preprocessing operation known as sphering and
it is not the same operation as GCN. Sphering does not refer to making the
data lie on a spherical shell, but rather to rescaling the principal components to
have equal variance, so that the multivariate normal distribution used by PCA
has spherical contours. Sphering is more commonly known as whitening and is
described in section 12.2.1.
Global contrast normalization will often fail to highlight image features we
would like to stand out, such as edges and corners. If we have a scene with a
large dark area and a large bright area (such as a city square with half the image
in the shadow of a building) then global contrast normalization will ensure there
is a large diﬀerence between the brightness of the dark area and the brightness
of the light area. It will not, however, ensure that edges within the dark region
stand out.
This motivates local contrast normalization. Local contrast normalization
ensures that the contrast is normalized across each small window, rather than
over the image as a whole. See Fig. 12.2 for a comparison of global and local
L2

387

CHAPTER 12. APPLICATIONS

Input image

GCN

LCN

Figure 12.2: A comparison of global and local contrast normalization. Visually, the eﬀects
of global contrast normalization are subtle. It places all images on roughly the same scale,
which reduces the burden on the learning algorithm to handle multiple scales. Local
contrast normalization modiﬁes the image much more, discarding all regions of constant
intensity. This allows the model to focus on just the edges. Regions of ﬁne texture,
such as the houses in the second row, may lose some detail due to the bandwidth of the
normalization kernel being too high.

388

CHAPTER 12. APPLICATIONS

contrast normalization.
Various deﬁnitions of local contrast normalization are possible. In all cases,
one modiﬁes each pixel by subtracting a mean of nearby pixels and dividing by
a standard deviation of nearby pixels. In some cases, this is literally the mean
and standard deviation of all pixels in a rectangular window centered on the
pixel to be modiﬁed (Pinto et al., 2008). In other cases, this is a weighted mean
and weighted standard deviation using Gaussian weights centered on the pixel to
be modiﬁed. In the case of color images, some strategies process diﬀerent color
channels separately while others combine information from diﬀerent channels to
normalize each pixel (Sermanet et al., 2012).
Local contrast normalization can usually be implemented eﬃciently by using
separable convolution (see Sec. 9.9) to compute feature maps of local means and
local standard deviations, then using elementwise subtraction and elementwise
division on diﬀerent feature maps.
Local contrast normalization is a diﬀerentiable operation and can also be
used as a nonlinearity applied to the hidden layers of a network, as well as a
preprocessing operation applied to the input.
As with global contrast normalization, we typically need to regularize local
contrast normalization to avoid division by zero. In fact, because local contrast
normalization typically acts on smaller windows, it is even more important to
regularize. Smaller windows are more likely to contain values that are all nearly
the same as each other, and thus more likely to have zero standard deviation.
Whitening
TODO– ZCA and PCA whitening
TODO– refer to Fig. 12.3
TODO–show whitened images: ﬁnish ﬁgure below
We often regularize ZCA or PCA by adding a small constant to all of the estimated eigenvalues. This is equivalent to adding the same constant to the diagonal
of the estimated covariance, i.e., it is equivalent to ﬁtting the covariance matrix
to a larger dataset formed by adding isotropric noise to the training examples.
It is also common to simply cut out some of the highest frequency components,
because these usually correspond to artifacts of the imaging process (Olshausen
and Field, 1997).
TODO– convolutional version
Whitening may also be motivated from a biological point of view. Retinal
ganglion cells seem to implement a kind of whitening ﬁlter. This suggests that
when we want to build a model of the lower levels of visual processing in the
brain, we should preprocess the input to this model by whitening it, in order to
389

CHAPTER 12. APPLICATIONS

Figure 12.3: The ﬁlters learned by PCA and ZCA whitening applied to 8 × 8 pixel patches
of the CIFAR-10 dataset. Left) PCA ﬁlters, arranged left to right, top to bottom. The ﬁrst
ﬁlters correspond to the directions of greatest variance. The earliest ﬁlters are thus low
frequency and the last ﬁlters are high frequency. When we apply the PCA transformation,
the result is no longer an image with any spatial relationships between the features. Right)
ZCA ﬁlters. Each ZCA ﬁlter extracts a center-surround sharpened red, green, or blue
pixel. Because the features are all localized in the same way, transforming by the ZCA
ﬁlters preserves the concept of spatial locality.

390

CHAPTER 12. APPLICATIONS

most closely mimic the input provided to the brain from the retina (Olshausen
and Field, 1997).
Dataset Augmentation
TODO–random translations, horizontal ﬂips, distortions, color changes (coordinate with regularization chapter)

12.2.2

Convolutional Nets

TODO–Describe how conv nets are usually resource intensive, need good GPU
implementation or distributed implementation Cuda-Convnet Pylearn2 + Theano
Torch Decaf / Caﬀe OverFeat DistBelief TODO– Schmidhuber convolutional
nets for contests, MNIST TODO– ImageNet TODO– Christian’s detection net
TODO– Ouais’s text transcription HMM TODO– Street Number transcriber
TODO– pixel labeling TODO– image restoration

12.3

Speech Recognition

The task of speech recognition consists in mapping an acoustic signal corresponding to a spoken natural language utterance into the corresponding sequence of
words intended by the speaker. If we denote by X = (x1, x 2, . . . xT ) the input
sequence of acoustic vectors (describing the recorded sounds in discrete time units
such as the traditional 20ms frames), and by y = (y1 , y2, . . . y N ) the target output
or linguistic sequence (e.g., whose elements are words or characters from a natural language), the Automatic Speech Recognition (ASR) task can be described
∗
as looking for a function fASR ≈ f ∗ASR, where fASR
ﬁnds the most likely linguistic
sequence y given the acoustic sequence X :
f ∗ASR(X) = arg max P ∗(y|X = X)

(12.2)

y

where P ∗ is the true conditional distribution relating the inputs X to the targets
y.

12.3.1

Historical Perspective

Speech recognition was one of the ﬁrst areas to which neural networks were applied in the 80’s and 90’s (Bourlard and Wellekens, 1989; Waibel et al., 1989;
Robinson and Fallside, 1991; Bengio et al., 1991, 1992b; Konig et al., 1996) and
it reached the state-of-the-art that was then (and until recently) held by systems
based on Hidden Markov Models (HMMs), which were brieﬂy described in Section 10.9.3 (Rabiner, 1989), along with Gaussian Mixture Models (GMMs) for
391

CHAPTER 12. APPLICATIONS

learning the acoustic density associated with each HMM state. It turned out that
with much larger models, deeper models (more hidden layers), and training with
much larget datasets, neural nets could very advantageously replace the GMMs,
i.e., basically to associate acoustic features to phonemes (or sub-phonemic states).
Starting in 2009, unsupervised pretraining was used to build stacks of RBMs
taking spectral acoustic representations in a ﬁxed-size input window (around a
center frame) and predicting the conditional probabilities of HMM states for
that center frame. Early results on the TIMIT dataset (the MNIST of speech)
suggested that training such deep networks actually helped to signiﬁcantly improve the HMM recognition rate on TIMIT (Mohamed et al., 2012). This was
quickly followed up by work to expand the architecture from phoneme recognition (which is what TIMIT is basically focused on) to large-vocabulary speech
recognition (Dahl et al., 2012). By that time, several of the major speech groups
in industry had started exploring deep learning (in collaboration with some of
the authors of the above papers) for speech recogniton and they collaborated to
report on the breakthroughs they were all getting (Hinton et al., 2012a) and these
systems started being deployed in products such as Android phones.
As it turned out later, as these groups explored larger and larger labeled
datasets and incorporated some of the methods for initializing, training, and
setting up the architecture of deep nets, they realized that the unsupervised pretraining phase was either unnecessary or did not bring any signiﬁcant improvement.
These breakthroughs in recognition performance for speech recognition were
unprecedented (around 30% improvement) and were following a long period of
about ten years during which error rates did not improve much with the traditional
GMM+HMM technology, in spite of the continuously growing size of training sets
(see Figure 2.4 of Deng and Yu (2014)). This created a rapid shift in the speech
recognition community towards deep learning, at conferences such as ICASSP.
In a matter of two years, most of the industrial products for speech recognition
incorporated that innovation and this interest spurred a new wave of explorations
for deep learning algorithms and architectures, which is still ongoing.
One of these innovations was the use of convolutional networks (Chapter 9)
instead of fully-connected feedforward networks (Sainath et al., 2013). In that
case the input spectrogram is seen not as one long vector but as an image, with
one axis corresponding to time and the other to frequency of spectral components.
Another important push, still ongoing, has been towards end-to-end deep
learning speech recognition systems, without the need for the HMM. The ﬁrst
major breakthrough in this direction came from Graves et al. (2013) which trained
a deep LSTM RNN (see Section 10.8.4), using MAP inference over the frame-tophoneme alignment, i.e., as in LeCun et al. (1998c) and in the the CTC frame392

CHAPTER 12. APPLICATIONS

work (Graves et al., 2006; Graves, 2012), described in more detail in Section 10.9.2.
A deep RNN (Graves et al., 2013) has several layers state variables at each time
step, making the unfolded graph deep in two ways, ordinary depth due to a stack
of layers, and depth due to time unfolding. See Pascanu et al. (2014a); Chung
et al. (2014) for other variants of deep RNNs.
Following that push, the idea was introduced of using an attention mechanism
to let the system learn how to “align” the acoustic-level information with the
phonetic-level information Chorowski et al. (2014).

output

R(w 1 ) R(w2 ) R(w3 ) R(w4 ) R(w5 ) R(w6 )

w1

w2

w3

w4

w5

w6

input sequence
Figure 12.4: Neural language models and their extensions can always be decomposed
into two components: (1) the word embeddings, i.e., a mapping from any word index (a
symbol) to a learned vector and (2) other parameters dedicated to the task at hand (such
as predicting the next word, or translating one sentence into another), based on those
representations. The training objective is deﬁned in terms of the output of the second
component. It is the second component that drives the learning of the word embeddings in
such a way as to make similar words (according to the task) share attributes or dimensions
in their embeddings.

12.4

Natural Language Processing and Neural Language Models

Natural language processing includes applications such as language modeling and
machine translation. As with the other applications discussed in this chapter,
393

CHAPTER 12. APPLICATIONS

very generic neural network techniques can be successfully applied to natural
language processing. However, to achieve excellent performance and scale well
to large applications, some domain-speciﬁc strategies become important. Natural
language modeling usually forces us to use some of the many techniques that are
specialized for processing sequential data. In many case, we choose to regard
natural language as a sequence of words, rather than a sequence of individual
characters. In this case, because the total number of possible words is so large,
we are modeling an extremely high-dimensional and sparse discrete space. Several
strategies have been developed to make models of such a space eﬃcient, both in
a computational and in a statistical sense.

12.4.1

Historical Perspective

The idea of distributed representations for symbols was introduced by Rumelhart
et al. (1986a) in one of the ﬁrst explorations of back-propagation, with symbols
corresponding to the identity of family members and the neural network capturing
the family relationships between family members, e.g., with examples of the form
(Colin, Mother, Victoria). It turned out that the ﬁrst layer of the neural network
learned a representation of each family member, with learned features, e.g. for
Colin, representing which family tree Colin was in, what branch of that tree he
was in, what generation he was from, etc. One can think of these learned features
as a set of attributes and the rest of the neural network computing micro-rules
relating these attributes together in order to obtain the desired predictions, e.g.,
who is the mother of Colin? A similar idea was the basis of the research on
neural language model started by Bengio et al. (2001b), where this time each
symbol represented a word in a natural languge vocabulary, and the task was to
predict the next word given a few previous ones. Instead of having a small set
of symbols, we have a vocabulary with tens or hundreds of thousands of words
(and nowadays it goes up to the million, when considering proper names and
misspellings). This raises serious computational challenges, discussed below in
Section 12.4.4. The basic idea of a neural language models and their extensions,
e.g., for machine translation, is illustrated in Figure 12.4 and a speciﬁc instance
(which was used by Bengio et al. (2001b)) is illustrated in Figure 12.4. Figure 12.4
explains the basic of idea of splitting the model into two parts, one for the word
embeddings (mapping symbols to vectors) and one for the task to be performed.
Sometimes, diﬀerent maps can be used, e.g., for input words and output words,
as in Figure 12.4, or in neural machine translation models (Section 12.4.6).
Earlier work had looked at modeling sequences of characters in text using neural networks (Miikkulainen and Dyer, 1991; Schmidhuber, 1996), but it turned
out that working with word symbols worked better as a language model and, more
importantly, immediately yielded word embeddings, i.e., interpretable representa394

CHAPTER 12. APPLICATIONS

tions of words, as illustrated in Figures 12.6 and 12.7. Actually, these compelling
2-dimensional visualization arose thanks to the the development of the t-SNE algorithm (van der Maaten and Hinton, 2008a) in 2008, and the ﬁrst visualization
of word embeddings was made by Joseph Turian in 2009: these t-SNE visualizations of word embedding quickly became a standard tool to understand the
learned word representations, ﬁrst in talks and then in papers.
The early eﬀorts at training language models typically yielded neural nets that
did not beat an n-gram model by themselves, but when adding the probability
prediction coming from the neural net and from the n-gram model, one would
typically get substantial gains in log-likelihood (Bengio et al., 2003a).
An important development after the demonstration that neural language models could be used to improve upon classical n-gram models in terms of negative
log-likelihood (also called perplexity in the language modeling literature) has been
the demonstration that these improved language models could yield an improvement in word error rate for state-of-the-art speech recognition systems (Schwenk
and Gauvain, 2002, 2005; Schwenk, 2007). The same technique (replacing the
n-gram by a combination of n-gram and neural language model) was then used
to improve classical statistical machine translation systems (Schwenk et al., 2006;
Schwenk, 2010).
More developments of the original model are described in the sections below.

12.4.2

The Problem With n-grams

The basic motivation for neural language models discussed by Bengio et al.
(2001b, 2003a) is that this approach has the potential to bypass the curse-ofdimensionality issue arising with more classical n-gram approaches. To get a
clear understanding of the curse of dimensionality issue in general for machine
learning, please refer to Sections 5.12.1 and 16.6. In the case of n-grams, the
issue amounts to the exponential growth in the number of discrete contexts to
be considered. n-gram models store at least one coeﬃcient (a frequency count,
typically) for each one of a set of contexts corresponding to a short subsequence of
words. Please refer to Section 10.9.1 for a description of language models based
on n-grams. As discussed in that section, in order to properly consider longer
contexts (of length n − 1), n-gram models require exponentially growing memory
and training data. In practice, this is mitigated by only considering the most
frequent sets of longer contexts, but this also means that for less frequent longer
contexts, the only form of generalization that is possible comes from ignoring the
older part of the context.
There is actually another form of generalization that was introduced to ngrams and that can on the surface approach some of the ideas behind distributed
representations. So-called class-based language models (Brown et al., 1992; Ney
395

CHAPTER 12. APPLICATIONS

i−th output = P(w(t) = i | context)
softmax

...

...
most computation here

tanh
...

C(w(t−n+1))
...

...

Table
look−up
in C
index for w(t−n+1)

C(w(t−2))
...

...

C(w(t−1))
...

Matrix C
shared parameters
across words
index for w(t−2)

index for w(t−1)

Figure 12.5: This is the original architecture for a neural language model that was developed by Bengio et al. (2001b) and is a special case of the general architecture for neural
language-related models illustrated in Figure 12.4. Here the “second component” is an
ordinary MLP with a single hidden layer and a very large softmax output layer predicting
the probability of the next word (given the previous words seen in input).

396

CHAPTER 12. APPLICATIONS

and Kneser, 1993; Niesler et al., 1998) introduce the notion of word categories in
order to share statistical strength between words that are semantically close. The
idea is to partition the set of words into clusters or classes (e.g., based on their cooccurence frequencies with other words), and to condition the n-gram probability
on a rougher representation than the original tuple of words: the corresponding
tuple of word classes. Although this clearly gives a way to generalize between
sequences in which some word is replaced by another of the same class, much
information is lost in this representation. We can think of the word class as
an attribute, but one where the only allowed values are mutually exclusive. If
one had multiple attributes (e.g., diﬀerent ways of partitioning the set of words),
then with enough attributes one would get both the beneﬁt of generalization to
semantically similar words and the beneﬁt of keeping all the information about
the speciﬁc choice of word. This is essentially what distributed representations of
words (word embeddings) aim to achieve.

12.4.3

How Neural Language Models can Generalize Better

The fundamental reason why neural language models can break the barrier encountered with models based on n-grams is that neural language models can
learn an intermediate representation for words, such that there can be a generalization occuring (sharing of statistical strength) between words (and the contexts
in which they appear) that have some common attributes. For example, if the
word dog and the word cat share many attributes (except maybe some indicator
of being feline or not, for example), then sentences that contain the word cat can
inform the predictions that will be made by the model for sentences that contain
the word dog, and vice-versa. And because there are many such attributes, there
are many ways in which generalization can happen, transfering information from
each training sentence to an exponentially large number of semantically related
sentences (e.g., in which some words are replaced by semantically similar ones).
Basically, the exponential arising in the curse of dimensionality (growing with the
sentence size) is addressed with another exponential, arising out of the exponential number of small variations that each element in the sequence can have (and
now the exponential grows with the number of dimensions of the representation).
We sometimes call these word representations “word embeddings” because we
can think of these representations as points in a semantic space of lower dimension
than the raw symbols (which live in a space of dimension equal to the vocabulary
size). Whereas in the original space every pair of words is at the same distance
as any other pair of words, in the embedding space, semantically similar words
(or any pair of words sharing some “features” learned by the model) end up
close to each other, and one can also observe diﬀerent types of words clustering
semantically, like shown in Figure 12.6.
397

CHAPTER 12. APPLICATIONS

Figure 12.6: Word embeddings obtained from neural language models tend to cluster by
semantic categories, as visualized here via t-SNE dimensionality reduction and coloring
of words by such categories. Reproduced with permission by Chris Olah from http:
//colah.github.io/, where many more insightful visualizations can be found.

Figure 12.7 zooms in on speciﬁc areas of such a picture of word embeddings,
and we see more clearly how semantically similar words end up with representations that are close to each other. However, keep in mind that these were
obtained using a strong dimensionality reduction (from hundreds to 2), which
means that the actual embeddings are much richer than what can be seen via
those visualizations.

Figure 12.7: Two-dimensional visualizations of word embeddings obtained from a neural
machine translation model (Bahdanau et al., 2014), zooming in on speciﬁc areas where
we see semantically nearby words, e.g., years on the left, countries in the middle, and
numbers on the right. Consider Fig. 12.6 for a sense of the big picture.

398

CHAPTER 12. APPLICATIONS

12.4.4

High-Dimensional Outputs

One of the nagging questions that early work with neural language models raised
is the computational cost of the aﬃne/softmax output layer (with one output per
word in the vocabulary), both at training time (to compute the likelihood and
its gradient) and at test time (to compute probabilities for all or selected words).
Indeed, the aﬃne/softmax output layer performs the following computation (see
also Section 6.3.1):
X
(12.3)
a i = bi +
W ij h j ∀i ∈ {1, . . . , |V|}
j

pi = P

e ai

a i0
i 0∈{1,...,|V|} e

(12.4)

where V is the vocabulary and |V| its size, h is the top hidden layer used to predict
the output probabilities p, and b, W are learned parameters. If h contains nh
elements then the above operation is O(|V|n h). With n h in the thousand(s) and
|V| in the hundreds of thousands, this computation dominates the computation
of most neural language models.
Use of a Short List
The early work dealt with this problem ﬁrst (Bengio et al., 2003a) by limiting the
vocabulary size (e.g. to 10,000 or 20,000 words), then by splitting the vocabulary
V into a short list L of most frequent words (handled by the neural net) and a
tail T = V\L of more rare words (handled by an n-gram model) (Schwenk, 2007).
To be able to combine the two predictions, the neural net also has to predict the
probability for a word to belong to none of those in the short list. This is achieved
by decomposing probabilities as follows:
P (y = i | C) =1 i∈L P (y = i | C, i ∈ L)P (i ∈ L | C)
+ 1 i∈TP (y = i | C, i ∈ T)(1 − P (i ∈ L | C))

(12.5)

where C stands for the generic context in which we want to predict the next word
y, P (y = i | C, i ∈ L) is the usual softmax output of the neural language model,
P (i ∈ L | C) is an extra output of the neural language model (either computed by
adding a sigmoid output or simply an extra output to the softmax for the category
of words outside of L), and P (y = i | C, i ∈ T) is computed by the n-gram model,
normalizing the frequencies only over the output words y that belong to T.
An obvious disadvantage of the short list approach is that the potential generalization advantage of the neural language models is limited to the most frequent
words, and this has stimulated the exploration of alternative methods to deal with
high-dimensional outputs, described below.
399

CHAPTER 12. APPLICATIONS

Hierarchical Softmax
When trying to parametrize and compute a multinoulli probability distribution
over a large set (e.g. hundreds of thousands of words) of dimension |V| there is
a classical approach (Goodman, 2001) to decompose probabilities hierarchically.
Instead of having a number of computations proportional to |V| (and also proportional to the number of hidden units n h, in our case), |V| factor can be reduced
to as low as log |V|. This idea of a hierarchical decomposition of words has been
introduced to neural language models by applied by Bengio (2002); Morin and
Bengio (2005) and is described below.

0

00

1

10

01

11

000 001 010 011 100 101 110 111
w 0 w1 w2 w3 w 4 w 5 w6 w7

Figure 12.8: Illustration of a hierarchy of word categories, with actual words at the leaves
and groups of words at the internal nodes. Any node can be indexed by the sequence
of binary decisions (0=left, 1=right) to reach the node from the root. If the tree is
suﬃciently balanced, the maximum depth (number of binary decisions) is on the order
of the logarithm of the number of words |V|: the choice of one out of |V| words can be
obtained by doing O(log |V|) operations (one for each of the nodes on the path from the
root).

One can think of this hierarchy as corresponding to clusters of categories and
cluster of clusters of categories, etc. For example, consider in Figure 12.8 the
simple case where we have 8 words w 0 , . . . , w7 organized into a 3-level hierarchy:
super-class 0 contains the classes 00 and 01, which respectively contain the words
(w0 , w1) and (w 2, w 3), and similarly super-class 1 contains the classes 10 and 11,
which respectively contain the words (w 4, w5 ) and (w 6, w7 ). Hence, computing the
probability of a word y can be done by computing three binomial probabilities,
associated with the left or right binary decisions associated with the nodes from
400

CHAPTER 12. APPLICATIONS

the root to a node y. Let bi be the i-th binary decision when traversing the
tree towards the value y. Thus, the probability of sampling and output y can be
decomposed into a product of conditional probabilities, using the chain rule for
conditional probabilities, with each node indexed by the preﬁx of these bits, e.g.,
node 10 in Figure 12.8 corresponds to the preﬁx (b0 (w4) = 1, b 1(w 4) = 0), and
the probability of w 4 can be decomposed as follows:
P (y = w4) = P (b0 = 1, b 1 = 0, b2 = 0)
(12.6)
= P (b0 = 1)P (b1 = 0 | b0 = 1)P (b2 = 0 | b0 = 1, b 1 = 0).
Each of these conditional probabilities can be computed at one node, starting
from the root, and associated with the arc going from a parent node to one of its
children nodes.
For neural language models, these probabilities are typically conditioned on
some context, so in general we decompose the log-likelihood of the next word y
given its context as follows:
X
log P (y | C) =
log P (bi | b1, b 2, . . . , b i−1 , C)
i

where the sum runs over all k bits of y. This can be obtained by computing the
sigmoid of k dot products, each with a weight vector indexed by the identiﬁer
n = b 1 , b2 , . . . , b i−1 associated with some internal node n on the path to y:
pn = sigmoid(c n + v n · hC)

(12.7)

logP (bi | b1, b 2 , . . . , b i−1, C) = bi log p n + (1 − bi ) log(1 − pn )
which corresponds to the usual Bernoulli cross-entropy for logistic regression and
probabilistic binary classiﬁcation in neural networks (Sections 6.3.2 and 6.3.2).
Since the output log-likelihood can be computed eﬃciently (as low as log |V|
rather than |V|), so can its gradient with respect to the output parameters (the
v n and cn above) as well as with respect to the hidden layer activations hC .
Note that in principle we could optimize the tree structure to minimize the
expected number of computations, following Shannon’s theorem, i.e., by structuring the tree so that the number of bits associated with a word be approximately
equal to the logarithm of the frequency of that word. However, in practice, this
is typically not worth it because the computation of the output probabilities is
only one part of the total computation. For example, if there are several hidden
layers of width nh, then the associated computations grow as O(n 2h ) while the
output computations grow as O(n h L) where L is the average number of bits of
output words (weighted by their frequency). Hence, there is not much advantage
in making L much less than n h. Consider that n h is typically large (e.g., around
401

CHAPTER 12. APPLICATIONS

a thousand or more), and the vocabulary sizes are typically not more than the
order of a million. Since log2 (10 6) is about 20, we could get L on the order of 20
for such a large vocabulary, but in fact it would not make much of a diﬀerence
to take L on the order of 1000 (the same asp
n h ), which means that a 2-level tree
(which has average depth L on the order of |V|) is suﬃcient to reap most of the
beneﬁt of a hierarchical softmax, with the typical vocabulary sizes and hidden
layer sizes that are currently used. A 2-level tree corresponds to simply deﬁning
a set of mutually exclusive words classes.
One question that remains somewhat open is how to best deﬁne these word
classes, or how to deﬁne the word hierarchy in general. Early work used existing hierarchies (Morin and Bengio, 2005) but it can also be learned, ideally
jointly with the neural language model, although an exact optimization of the
log-likelihood appears intractable because the choice of a word hierarchy is a discrete one, not amenable to gradient-based optimization. However, one could use
discrete optimization to approximately optimize the partition of words into word
classes.
An important advantage of the hierarchical softmax is that it brings computational beneﬁts both at training time and at test time, if at test time we want to
compute the probability of speciﬁc words. Of course computing the probability of
all |V| words will remain expensive. Another interesting question is how to pick
the most likely word, in a given context, and unfortunately the tree structure does
not provide an eﬃcient and exact answer. However, in practice (e.g., for translation or speech recognition), we want to pick the best sequence of words, and this
typically requires a heuristic search such as the beam search (Section 10.10.1).
A disadvantage is that in practice the hierarchical softmax tends to give worse
test results than sampling-based methods such as described below, although this
may be due to a poor choice of word classes.
Importance Sampling
An idea that is almost as old as the hierarchical softmax, for speeding up training
of neural language models, is the use of a sampling technique to approximate
the “negative phase” contribution of the gradient, i.e., the “counter-examples”
on which the model should give a low score (or high energy), compared to the
observed word. See Section 18.1 for the decomposition of the log-likelihood into
a “positive phase” term (pushing the score of the correct word up, or pushing
down its energy, which is easy here because we only have to consider one word)
and a “negative phase” term (pushing down the score of all the other words, in
proportion to the probability that the model gives them). Using the notation

402

CHAPTER 12. APPLICATIONS

introduced in Eq. 12.3, the gradient can be written as follows:
∂ log softmaxy (a)
∂ log P (y | C)
=
∂θ
∂θ
∂
ea y
=
log P a
i
∂θ
i e
X
∂
=
(a y − log
e a i)
∂θ
i
X
∂a y
∂a i
=
−
P (i | C)
)
∂θ
∂θ

(12.8)

i

where a is the vector of pre-softmax activations (or scores), with one element
per word. The ﬁrst term is the “positive phase” term (pushing ay up) while the
second term is the “negative phase” term (pushing ai down for all i, with weight
P (i | C). Since the negative phase term is an expectation, we can estimated by
a Monte-Carlo sample. However, that would require sampling from the model
itself, i.e., computing P (i | C) for all i in the vocabulary, which is precisely what
we are trying to avoid.
The solution proposed by Bengio and Sénécal (2003); Bengio and Sénécal
(2008) is to sample from another distribution, called the proposal distribution
(denoted q), and use appropriate weights to correct for that. This is called importance sampling and is introduced in Section 14.1.2. But even exact importance
sampling is not appropriate because it requires computing weights pi /qi , where
p i = P (i | C), which can only be computed if all the scores ai are computed.
The solution adopted is called biased importance sampling, where the importance
weights are normalized to sum to 1, i.e., when negative word n i is sampled, the
associated gradient is weighted by
p /q
w i = PN n i n i .
j=1 pnj /q n j

These weights are used to give the appropriate importance to the N negative
samples from q used to form the estimated negative phase contribution to the
gradient:
|V|
N
X
∂a i
1 X ∂a ni
wi
.
P (i|C)
)≈
∂θ
N
∂θ
i=1

i=1

In these papers, the authors used a unigram or a bigram distribution as proposal
distribution q, because can be easily estimated from the data as well as sampled
from very eﬃciently.
A related application of importance sampling to speed-up training of a larger
class of model was introduced by Dauphin et al. (2011). These are models where
403

CHAPTER 12. APPLICATIONS

the output is not necessarily a 1-of-n choice (which one can think of as an integer,
or as a one-hot vector), but more generally a sparse vector, where only a few
of the entries are non-zero. This occurs for example when the output is a bagof-words, i.e., a sparse vector where the non-zeros indicate the presence (0 or
1) or the frequency (a small count) of each word of a document. In the paper,
the authors study the case of denoising auto-encoders with a bag-of-words as
input. Whereas the sparsity of the input can be easily exploited (by ignoring the
zeros in the computation), it is not so clear for the output (reconstruction) units.
Because an auto-encoder also predicts its input, the target for the reconstruction is
sparse but the prediction (probabilities that any particular word is present) is not.
The algorithm ends up minimizing reconstruction error (minus log-likelihood)
for the “positive words” (those that are non-zero in the target) and an equal
number of “negative words” chosen randomly, but with their gradients reweighted
appropriately according to importance sampling.
In all of these cases, the computational complexity of gradient estimation for
the output layer is reduced to be proportional to the number of negative samples
rather than proportional to the size of the output vector.
Noise-Contrastive Estimation and Ranking Loss
Other approaches based on sampling have been proposed to reduce the computational cost of training neural language models with large vocabularies.
An early one is the ranking loss proposed by Collobert and Weston (2008), in
which we view the output of the neural language model for each word as a score
and ask that the score of the correct word a y be ranked high in comparison to
the other scores ai . The ranking loss proposed then is
X
L=
max(0, 1 − a y + a i ).
(12.9)
i

Note that the gradient is zero for the i-th term if the score of the observed word,
a y is greater than the score of negative word ai by a margin of 1. One issue
with this criterion is that it does not provide estimated conditional probabilities,
which are useful in some applications, e.g., speech recognition, or (conditional)
text generation.
A more recently used training objective for neural language model is noisecontrastive estimation, which is introduced in Section 18.6. The idea is to turn
the training task into a probabilistic classiﬁcation problem where the learner tries
to identify whether a given value is sampled from the data generating distribution (under the probability estimated by the trained model) or from a ﬁxed
“noise” model. This approach has been successfully applied to neural language
models (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013). That probabilistic
404

CHAPTER 12. APPLICATIONS

classiﬁer output (when the output is the observed word y) can be obtained by
combining the score of the observed word a y with a learned parameter that estimates the log of the normalizing constant of the softmax. The training objective
also requires that one samples a word from the “noise” distribution, which acts
like a proposal distribution for importance sampling.

12.4.5

Combining Neural Language Models with n-grams to Increase Capacity without Greatly Increasing Computation

A major advantage of n-grams over neural networks is that the computationcapacity trade-oﬀ allows n-grams to have a huge capacity (storing all of the frequent tuples frequency) but fast prediction (only looking up a few of those tuples,
that match the current context), which almost does not grow with capacity (using hash tables or trees to access the counts). In comparison, doubling a neural
network’s capacity typically also roughly doubles its computation time1 .
To easily add capacity, it has thus been proposed to combine both approaches,
i.e. to use an ensemble consisting of a neural language model and an n-gram language model (Bengio et al., 2001b, 2003a). As with any ensemble, this technique can also reduce test error, and many ways of combining the ensemble
members’ predictions are possible (uniform weighting, weights chosen on a validation set, etc.) Later, this ensemble idea was extended to include not just
two models but a large array of models (Mikolov et al., 2011a) as well as training the neural net jointly with the n-gram model (or rather maximum entropy
model) (Mikolov et al., 2011b). Basically, the latter amounts to introducing a
very high-dimensional and sparse extra input vector, composed of indicators for
the presence of particular n-grams in the input context. In addition, these extra
inputs were only connected and directly connected to the outputs. Because these
extra inputs are very sparse, the extra computation is minimal, while the added
capacity is huge (one parameter for each n-gram tuple considered, i.e., for up to
n − 1 words of context with one output next word).

12.4.6

Neural Machine Translation

The early use of neural networks for machine translation (Schwenk et al., 2006;
Schwenk, 2010) took advantage of the good performance of neural language models in order to replace one component of a machine translation system, the statistical language model, which was traditionally done by an n-gram-based model.
1

it is not true for the word embeddings used in the input layer, and the techniques described
in Section 12.4.4 allow to reduce computation for the output embeddings, but the capacity added
in the intermediate layers still comes with linearly growing computation time.

405

CHAPTER 12. APPLICATIONS

These n-gram based model include not just traditional back-oﬀ n-gram models (Jelinek and Mercer, 1980; Katz, 1987; Chen and Goodman, 1999) but also
so-called maximum entropy language models (Berger et al., 1996), in which an
aﬃne / softmax layer predicts the next word given the presence of frequent ngrams in the context (as outlined in the previous section).
output object, e.g. English sentence

Decoder

intermediate representation
= semantic representation

Encoder

source object, e.g. French sentence or image

Figure 12.9: The encoder-decoder architecture to map back and forth between a surface representation (e.g., sequence of words, image) and a semantic representation. By
coupling the encoder for one modality (e.g. French to “meaning”) with the decoder for
another modality (e.g. “meaning” to English), we can train systems that translate from
one modality to another. This idea has been applied successfully not just to machine
translation but also to caption generation from images.

However, once we have an architecture for learning neural language model
that captures the joint probability P (w1, w 2, . . . , wT ) of a sequence of words, we
can in principle make the distribution conditional on some generic context C by
making some of its parameters a function of C, as explained in Section 6.3.2.
For example, Devlin et al. (2014) beat the state-of-the-art in some statistical machine translation benchmarks by using an MLP to score a phrase t 1, t 2, . . . , t k in
the target language given a phrase s1 , s 2 , . . . , s n in the source language, i.e., estimate P (t1 , t2, . . . , t k | s1 , s2 , . . . , sn ) and use it to replace the traditional phrase
table (that estimates the same quantity) based on n-grams. To make this trans406

CHAPTER 12. APPLICATIONS

lation more ﬂexible, we would like to use a model that can accommodate variable length inputs and variable length outputs. If an RNN is used to capture
P (w 1, w 2, . . . , wT ), we can make the initial state of the RNN or the biases used
at each time step for the hidden units be a function of some generic context C.
If C is obtained from a source sentence in another language, we can thus train
our neural language to translate from one language to another. If we think of C
as a semantic summary of the source sentence, it can for example be obtained
as the ﬁnal state of another RNN (the encoder RNN or “reader”), as in Cho
et al. (2014); Sutskever et al. (2014b); Jean et al. (2014), or as the top layer of
a convolutional network, as in (Kalchbrenner and Blunsom, 2013). This general
idea of an encoder-decoder framework for machine translation is illustrated in
Figure 12.9.
This raises the question of representing not just words but sequences of words.
The idea of learning a semantic representation of phrases and even sentences so
that the representation of the source and target sentences are close to each other
and can be mapped from one to the other has been explored (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014b; Jean et al., 2014), ﬁrst
using a combination of convolutions and RNNs (Kalchbrenner and Blunsom, 2013)
and then using both RNNs for encoding the source sentence and for generating
the output target-language sentence (Cho et al., 2014; Sutskever et al., 2014b;
Jean et al., 2014).

Figure 12.10: Illustration of the attention mechanism used in a neural machine translation
system introduced in Bahdanau et al. (2014).

Using an Attention Mechanism
Using a ﬁxed-size representation to capture all the semantic details of a very
long sentence of say 60 words is very diﬃcult. It can be achieved by training a
suﬃciently large RNN well enough and for long enough, as demonstrated by Cho
et al. (2014); Sutskever et al. (2014b). However, this is not how humans translate
long sequences of words. What they usually do, after having read the whole
407

CHAPTER 12. APPLICATIONS

sentence or paragraph (to get the context and the jist of what is being expressed),
they produce the translated words one at a time, each time focusing on a diﬀerent
part of the input sentence in order to gather the semantic details that are required
to produce the next output word. That is exactly the idea that Bahdanau et al.
(2014) ﬁrst introduced and that is illustrated in Figure 12.10. Earlier work showed
that one could learn a kind of translation matrix relating the word embeddings in
one language with the word embeddings in another (Kočiský et al., 2014), yielding
lower alignmnent error rates than traditional approaches based on the frequency
counts in the phrase table. There is even earlier work on learning crosslingual
word vectors (Klementiev et al., 2012) and several papers following up on this
approach, e.g., to ﬁnd ways to make crosslingual alignment more eﬃcient (Gouws
et al., 2014) to be able to train on larger scale datasets.

12.5

Structured Outputs

In principle, if we have good models P (Y|ω) of the joint distribution of random
variables Y = (y1 , . . . , yk ), with parameters ω we can use them to build conditional models P (Y | X) conditioned on some input variables X by making ω
a parametrized function of the input X. In Chapter 10, in particular with Section 10.4, we saw how an RNN can represent a joint distribution over elements of
a sequence that can be conditioned, e.g., on another sequence, such as for machine
translation (in the previous section, 12.4.6). Conditional joint distribution models are sometimes called “structured output models”, to distinguish them from
the more usual supervised learning tasks where the outputs represent a single
random variable (like a class) or conditionally independent random variables (like
diﬀerent attributes, discussed in Section 6.3.2).
In the third part of this book, we will go beyond RNNs as means of capturing the joint distribution between output variables, conditioned on input variables. For example Restricted Boltzmann Machines (RBMs) were made conditional by Taylor et al. (2007); Taylor and Hinton (2009) in the context of modeling motion style and by Boulanger-Lewandowski et al. (2012) in the context of
symbolic sequences describing polyphonic music. In both of these examples, we
actually use an RBM as a “output model” for an RNN, i.e., at each time step in
the sequence, we want to output a distribution over a group of random variables
(articulators for Taylor and Hinton (2009), and musical notes for BoulangerLewandowski et al. (2012)), given the current state of the RNN. With the RNNRBM (Boulanger-Lewandowski et al., 2012), a generative model is setup to model
a sequence of frames xt , with the following structure. An RNN captures the past
context through a state variable h t that follows a deterministic recurrence
ht = f (h t−1, xt ).
408

CHAPTER 12. APPLICATIONS

In addition, at each time step, a joint distribution over the elements of the next
frame x t+1 is formed using an RBM with parameters ω:
P (xt+1 = x t+1 | ωt).
The RBM parameters ωt = (ω RBM, ω0t ) are composed of two subsets of parameters, the parameters ω RBM that are ordinary parameters (namely the weights of
the RBM and the visible units biases) and the parameters ω 0t that are conditioned
on the RNN state h t (namely, the hidden units biases):
ωt0 = g(ht).
where both f and g have free parameters that are going to be updated by SGD,
along with ωRBM . Since ωt is a function of the past frames, we are modeling the
joint distribution of the sequence of frames:
Y
P (x 1, . . . , xT ) =
P (xt+1 |xt, xt−1 , . . . , x1 )
t

=

Y
t

P (xt+1 | ωt ) =

Y
t

P (x t+1 | h t).

(12.10)

The contrastive divergence algorithm can be used to obtain an estimated loglikelihood gradient on ω t in a conditional RBM (Taylor et al., 2007):
δω ≈

∂ log P (xt+1 = xt+1 | ω)
.
∂ω

Let us decompose δ like ωt into δ = (δRBM, δ0 ) for the part that corresponds to
ωRBM and the part that corresponds to ω0t . The estimated gradient δRBM can
be used to update ωRBM directly (e.g., by SGD) while δ0 can be back-propagated
through the RNN (as if it was the true gradient of log P (xt+1 = xt+1 |ωt ) with
respect to ω0t ), thereby providing the estimated gradient on the parameters of f
and g.

12.6

Other Applications

TODO– dimensionality reduction (nature paper)
TODO– clustering
TODO– collaborative ﬁltering (including Netﬂix), the explore-exploit issues
arising there YOSHUA
TODO– knowledge graphs YOSHUA

409

Part III

Deep Learning Research

410

This part of the book describes the more ambitious and advanced approaches
to deep learning, currently pursued by the research community.
In the previous parts of the book, we have shown how to solve supervised
learning problems—how to learn to map one vector to another, given enough
examples of the mapping.
Not all problems we might want to solve fall into this category. We may wish to
generate new examples, or determine how likely some point is, or handle missing
values and take advantage of a large set of unlabeled examples or examples from
related tasks. Many deep learning algorithms have been designed to tackle such
unsupervised learning problems, but none have truly solved the problem in the
same way that deep learning has largely solved the supervised learning problem
for a wide variety of tasks. In this part of the book, we describe the existing
approaches to unsupervised learning and some of the popular thought about how
we can make progress in this ﬁeld.
Another shortcoming of the current state of the art for industrial applications
is that our learning algorithms require large amounts of supervised data to achieve
good accuracy. In this part of the book, we discuss some of the speculative
approaches to reducing the amount of labeled data necessary for existing models
to work well.
This section is the most important for a researcher—someone who wants to
understand the breadth of perspectives that have been brought to the ﬁeld of
deep learning, and push the ﬁeld forward towards true artiﬁcial intelligence.

411

Chapter 13

Structured Probabilistic
Models for Deep Learning
Deep learning draws upon many modeling formalisms that researchers can use to
guide their design eﬀorts and describe their algorithms. One of these formalisms is
the idea of structured probabilistic models. We have already discussed structured
probabilistic models brieﬂy in Chapter 3.14. That brief presentation was suﬃcient
to understand how to use structured probabilistic models as a language to describe
some of the algorithms in part II of this book. Now, in part III, structured
probabilistic models are a key ingredient of many of the most important research
topics in deep learning. In order to prepare to discuss these research ideas, this
chapter describes structured probabilistic models in much greater detail. This
chapter is intended to be self-contained; the reader does not need to review the
earlier introduction before continuing with this chapter.
A structured probabilistic model is a way of describing a probability distribution, using a graph to describe which random variables in the probability distribution interact with each other directly. Here we use “graph” in the graph theory
sense–a set of vertices connected to one another by a set of edges. Because the
structure of the model is deﬁned by a graph, these models are often also referred
to as graphical models.
The graphical models research community is large and has developed many
diﬀerent models, training algorithms, and inference algorithms. In this chapter, we provide basic background on some of the most central ideas of graphical
models, with an emphasis on the concepts that have proven most useful to the
deep learning research community. If you already have a strong background in
graphical models, you may wish to skip most of this chapter. However, even a
graphical model expert may beneﬁt from reading the ﬁnal section of this chapter, section 13.6, in which we highlight some of the unique ways that graphical
412

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

models are used for deep learning algorithms. Deep learning practitioners tend to
use very diﬀerent model structures, learning algorithms, and inference procedures
than are commonly used by the rest of the graphical models research community.
In this chapter, we identify these diﬀerences in preferences and explain the reasons
for them.
In this chapter we ﬁrst describe the challenges of building large-scale probabilistic models in section 13.1. Next, we describe how to use a graph to describe
the structure of a probability distribution in section 13.2. We then revisit the
challenges we described in section 13.1 and show how the structured approach to
probabilistic modeling can overcome these challenges in section 13.3. One of the
major diﬃculties in graphical modeling is understanding which variables need to
be able to interact directly, i.e., which graph structures are most suitable for a
given problem. We outline two approaches to resolving this diﬃculty by learning
about the dependencies in section 13.4. Finally, we close with a discussion of the
unique emphasis that deep learning practitioners place on speciﬁc approaches to
graphical modeling in section 13.6.

13.1

The Challenge of Unstructured Modeling

The goal of deep learning is to scale machine learning to the kinds of challenges
needed to solve artiﬁcial intelligence. This means being able to understand highdimensional data with rich structure. For example, we would like AI algorithms
to be able to understand natural images1 , audio waveforms representing speech,
and documents containing multiple words and punctuation characters.
Classiﬁcation algortihms can take such a rich high-dimensional input and summarize it with a categorical label—what object is in a photo, what word is spoken
in a recording, what topic a document is about. The process of classiﬁcation discards most of the information in the input and produces on a single output (or
a probability distribution over values of that single output). The classiﬁer is also
often able to ignore many parts of the input. For example, when recognizing an
object in a photo, it is usually possible to ignore the background of the photo.
It is possible to ask probabilistic models to do many other tasks. These tasks
are often more expensive than classiﬁcation. Some of them require producing
multiple output values. Most require a complete understanding of the entire
structure of the input, with no option to ignore sections of it. These tasks include
• Density estimation: given an input x, the machine learning system returns
an estimate of p(x). This requires only a single output, but it does require
1

A natural image is an image that might captured by a camera in a reasonably ordinary
environment, as opposed to synthetically rendered images, screenshots of web pages, etc.
413

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a complete understanding of the entire input. If even one element of the
vector is unusual, the system must assign it a low probability.
• Denoising: given a damaged or incorrectly observed input x̃, the machine
learning system returns an estimate of the original or correct x. For example, the machine learning system might be asked to remove dust or scratches
from an old photograph. This requires multiple outputs (every element of
the estimated clean example x) and an understanding of the entire input
(since even one damaged area will still reveal the ﬁnal estimate as being
damaged).
• Missing value imputation: given the observations of some elements of x,
the model is asked to return estimates of or a probability distribution over
some or all of the unobserved elements of x. This requires multiple outputs,
and because the model could be asked to restore any of the elements of x,
it must understand the entire input.
• Sampling: the model generates new samples from the distribution p(x).
Applications include speech synthesis, i.e. producing new waveforms thatsound like natural human speech. This requires multiple output values and
a good model of the entire input. If the samples have even one element
drawn from the wrong distribution, then the sampling process is wrong.
For an example of the sampling tasks on small natural images, see Fig. 13.1.
Modeling a rich distribution over thousands or millions of random variables
is a challenging task, both computationally and statistically. Suppose we only
wanted to model binary variables. This is the simplest possible case, and yet
already it seems overwhelming. For a small, 32 × 32 pixel color (RGB) image,
there are 23072 possible binary images of this form. This number is over 10800
times larger than the estimated number of atoms in the universe.
In general, if we wish to model a distribution over a random vector x containing n discrete variables capable of taking on k values each, then the naive
approach of representing P (x) by storing a lookup table with one probability
value per possible outcome requires k n parameters!
This is not feasible for several reasons:
• Memory: the cost of storing the representation : For all but very
small values of n and k, representing the distribution as a table will require
too many values to store.
• Statistical eﬃciency: As the number of parameters in a model increases,
so does the amount of training examples needed to choose the values of those
parameters using a statistical estimator. Because the table-based model has
414

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Figure 13.1: Probabilistic modeling of natural images. Top: Example 32 × 32 pixel color
images from the CIFAR-10 dataset (Krizhevsky and Hinton, 2009). Bottom: Samples
drawn from a structured probabilistic model trained on this dataset. Each sample appears
at the same position in the grid as the training example that is closest to it in Euclidean
space. This comparison allows us to see that the model is truly synthesizing new images,
rather than memorizing the training data. Contrast of both sets of images has been
adjusted for display. Figure reproduced with permission from (Courville et al., 2011).

415

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

an astronomical number of parameters, it will require an astronomically
large training set to ﬁt accurately. Any such model will overﬁt the training
set very badly.
• Runtime: the cost of inference: Suppose we want to perform an inference task where we use our model of the joint distribution P (x) to compute
some other distribution, such as the marignal distribution P (x1) or the conditional distribution P (x2 | x1). Computing these distributions will require
summing across the entire table, so the runtime of these operations is as
high as the intractable memory cost of storing the model.
• Runtime: the cost of sampling: Likewise, suppose we want to draw a
sample from the model. The naive way to do this is to sample some value
u ∼ U (0, 1), then iterate through the table adding up the probability values
until they exceed u and return the outcome whose probability value was
added last. This requires reading through the whole table in the worst case,
so it has the same exponential cost as the other operations.
The problem with the table-based approach is that we are explicitly modeling
every possible kind of interaction between every possible subset of variables. The
probability distributions we encounter in real tasks are much simpler than this.
Usually, most variables inﬂuence each other only indirectly.
For example, consider modeling the ﬁnishing times of a team in a relay race.
Suppose the team consists of three runners, Alice, Bob, and Carol. At the start
of the race, Alice carries a baton and begins running around a track. After
completing her lap around the track, she hands the baton to Bob. Bob then runs
his own lap and hands the baton to Carol, who runs the ﬁnal lap. We can model
each of their ﬁnishing times as a continuous random variable. Alice’s ﬁnishing
time does not depend on anyone else’s, since she goes ﬁrst. Bob’s ﬁnishing time
depends on Alice’s, because Bob does not have the opportunity to start his lap
until Alice has completed hers. If Alice ﬁnishes faster, Bob will ﬁnish faster, all
else being equal. Finally, Carol’s ﬁnishing time depends on both her teammates.
If Alice is slow, Bob will probably ﬁnish late too, and Carol will have quite a late
starting time and thus is likely to have a late ﬁnishing time as well. However,
Carol’s ﬁnishing time depends only indirectly on Alice’s ﬁnishing time via Bob’s.
If we already know Bob’s ﬁnishing time, we won’t be able to estimate Carol’s
ﬁnishing time better by ﬁnding out what Alice’s ﬁnishing time was. This means
we can model the relay race using only two interactions: Alice’s eﬀect on Bob,
and Bob’s eﬀect on Carol. We can omit the third, indirect interaction between
Alice and Carol from our model.
Structured probabilistic models provide a formal framework for modeling only
direct interactions between random variables. This allows the models to have
416

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

signiﬁcantly fewer parameters which can in turn be estimated reliably from less
data. These smaller models also have dramatically reduced computation cost
in terms of storing the model, performing inference in the model, and drawing
samples from the model.

13.2

Using Graphs to Describe Model Structure

Structured probabilistc models use graphs (in the graph theory sense of “nodes”
or “vertices” connected by edges) to represent interactions between random variables. Each node represents a random variable. Each edge represents a direct
interaction. These direct interactions imply other, indirect interactions, but only
the direct interactions need to be explicitly modeled.
There is more than one way to describe the interactions in a probability distribution using a graph. In the following sections we describe some of the most
popular and useful approaches.

13.2.1

Directed Models

One kind of structured probabilistic model is the directed graphical model otherwise known as the belief network or Bayesian network 2 (Pearl, 1985).
Directed graphical models are called “directed” because their edges are directed, that is, they point from one vertex to another. This direction is represented in the drawing with an arrow. The direction of the arrow indicates which
variable’s probability distribution is deﬁned in terms of the other’s. Drawing an
arrow from a to b means that we deﬁne the probability distribution over b via
a conditional distribution, with a as one of the variables on the right side of the
conditioning bar. In other words, the distribution over b depends on the value of
a.
Let’s continue with the relay race example from Section 13.1. Suppose we
name Alice’s ﬁnishing time t0 , Bob’s ﬁnishing time t1 , and Carol’s ﬁnishing time
t2 . As we saw earlier, our estimate of t1 depends on t0. Our estimate of t 2
depends directly on t1 but only indirectly on t0 . We can draw this relationship
in a directed graphical model, illustrated in Fig. 13.2.
Formally, a directed graphical model deﬁned on variables x is deﬁned by a
directed acyclic graph G whose vertices are the random variables in the model, and
a set of local conditional probability distributions p(x i | P a G (xi )) where P aG (xi)
2

Judea Pearl suggested using the term Bayes Network when one wishes to “emphasize the
judgmental” nature of the values computed by the network, i.e. to highlight that they usually
represent degrees of belief rather than frequencies of events.

417

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Alice
t0

Bob
t1

Carol
t2

Figure 13.2: A directed graphical model depicting the relay race example. Alice’s ﬁnishing
time t 0 inﬂuences Bob’s ﬁnishing time t1 , because Bob does not get to start running until
Alice ﬁnishes. Likewise, Carol only gets to start running after Bob ﬁnishes, so Bob’s
ﬁnishing time t1 inﬂuences Carol’s ﬁnishing time t2 .

gives the parents of x i in G. The probability distribution over x is given by
p(x) = Πi p(xi | P aG (xi )).
In our relay race example, this means that, using the graph drawn in Fig. 13.2,
p(t0 , t1, t2 ) = p(t0 )p(t1 | t0 )p(t 2 | t 1).
This is our ﬁrst time seeing a structured probabilistic model in action. We
can examine the cost of using it, in order to observe how structured modeling has
many advantages relative to unstructured modeling.
Suppose we represented time by discretizing time ranging from minute 0 to
minute 10 into 6 second chunks. This would make t 0, t1 , and t 2 each be discrete
variables with 100 possible values. If we attempted to represent p(t0 , t1 , t2 ) with a
table, it would need to store 999,999 values (100 values of t0 × 100 values of t1 ×
100 values of t2 , minus 1, since the probability of one of the conﬁgurations is made
redundant by the constraint that the sum of the probabilities be 1). If instead,
we only make a table for each of the conditional probability distributions, then
the distribution over t0 requires 99 values, the table deﬁning t 1 given t0 requires
9900 values, and so does the table deﬁning t2 and t 1. This comes to a total of
19,899 values. This means that using the directed graphical model reduced our
number of parameters by a factor of more than 50!
In general, to model n discrete variables each having k values, the cost of the
single table approach scales like O(k n ), as we’ve observed before. Now suppose
we build a directed graphical model over these variables. If m is the maximum
number of variables appearing (on either side of the conditioning bar) in a single
conditional probability distribution, then the cost of the tables for the directed
model scales like O(k m ). As long as we can design a model such that m << n,
we get very dramatic savings.
In other words, so long as each variable has few parents in the graph, the
distribution can be represented with very few parameters. Some restrictions on
418

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

the graph structure (e.g. it is a tree) can also guarantee that operations like computing marginal or conditional distributions over subsets of variables are eﬃcient.
It’s important to realize what kinds of information can be encoded in the
graph, and what can’t be. The graph just encodes simplifying assumptions about
which variables are conditionally independent from each other. It’s also possible
to make other kinds of simplifying assumptions. For example, suppose we assume
Bob always runs the same regardless of how Alice performed. (In reality, Alice’s
performance probably inﬂuences Bob’s performance–depending on Bob’s personality, if Alice runs especially fast in a given race, this might encourage Bob to
push hard and match her exceptional performance, or it might make him overconﬁdent and lazy). Then the only eﬀect Alice has on Bob’s ﬁnishing time is
that we must add Alice’s ﬁnishing time to the total amount of time we think Bob
needs to run. This observation allows us to deﬁne a model with O(k) parameters
instead of O(k 2). However, note that t0 and t1 are still directly dependent with
this assumption, because t 1 represents the absolute time at which Bob ﬁnishes,
not the total time he himself spends running. This means our graph must still
contain an arrow from t0 to t1 . The assumption that Bob’s personal running time
is independent from all other factors cannot be encoded in a graph over t 0 , t1,
and t 2. Instead, we encode this information in the deﬁnition of the conditional
distribution itself. The conditional distribution is no longer a k × k − 1 element
table indexed by t0 and t 1 but is now a slightly more complicated formula using
only k − 1 parameters. The directed graphical model syntax does not place any
constraint on how we deﬁne our conditional distributions. It only deﬁnes which
variables they are allowed to take in as arguments.

13.2.2

Undirected Models

Directed graphical models give us one language for describing structured probabilistic models. Another popular language is that of undirected models, otherwise
known as Markov random ﬁelds (MRFs) or Markov networks (Kindermann,
1980). As their name implies, undirected models use graphs whose edges are
undirected.
Directed models are most naturally applicable to situations where there is
a clear reason to draw each arrow in one particular direction. Often these are
situations where we understand the causality, and the causality only ﬂows in one
direction. One such situation is the relay race example. Earlier runners aﬀects
the ﬁnishing times of later runners; later runners do not aﬀect the ﬁnishing times
of earlier runners.
Not all situations we might want to model have such a clear direction to
their interactions. When the interactions seem to have no intrinsic direction, or
to operate in both directions, it may be more appropriate to use an undirected
419

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

hr

hy

hc

Figure 13.3: An undirected graph representing how your roommate’s health hr , your
health hy , and your work colleague’s health h c aﬀect each other. You and your roommate
might infect each other with a cold, and you and your work colleague might do the same,
but assuming that your roommate and your colleague don’t know each other, they can
only infect each other indirectly via you.

model.
As an example of such a situation, suppose we want to model a distribution
over three binary variables: whether or not you are sick, whether or not your
coworker is sick, and whether or not your roommate is sick. As in the relay race
example, we can make simplifying assumptions about the kinds of interactions
that take place. Assuming that your coworker and your roommate do not know
each other, it is very unlikely that one of them will give the other a disease such
as a cold directly. This event can be seen as so rare that it is acceptable not
to model it. However, it is reasonably likely that either of them could give you
a cold, and that you could pass it on to the other. We can model the indirect
transmission of a cold from your coworker to your roommate by modeling the
transmission of the cold from your coworker to you and the transmission of the
cold from you to your roommate.
In this case, it’s just as easy for you to cause your roommate to get sick as
it is for your roommate to make you sick, so there is not a clean, uni-directional
narrative on which to base the model. This motivates using an undirected model.
As with directed models, if two nodes in an undirected model are connected by
an edge, then the random variables corresponding to those nodes interact with
each other directly. Unlike directed models, the edge in an undirected model has
no arrow, and is not associated with a conditional probability distribution.
Let’s call the random variable representing your health hy , the random variable
representing your roommate’s health hr , and the random variable representing
your colleague’s health h c . See Fig. 13.3 for a drawing of the graph representing
this scenario.
Formally, an undirected graphical model is a structured probabilistic model
deﬁned on an undirected graph G. For each clique C in the graph 3, a factor φ(C)
(also called a clique potential) measures the aﬃnity of the variables in that clique
for being in each of their possible joint states. The factors are constrained to be
3

A clique of the graph is a subset of nodes that are all connected to each other by an edge of
the graph.
420

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

non-negative. Together they deﬁne an unnormalized probability distribution
p̃(x) = ΠC∈G φ(C).
The unnormalized probability distribution is eﬃcient to work with so long as
all the cliques are small. It encodes the idea that states with higher aﬃnity are
more likely. However, unlike in a Bayesian network, there is little structure to the
deﬁnition of the cliques, so there is nothing to guarantee that multiplying them
together will yield a valid probability distribution. See Fig. 13.4 for an example
of reading factorization information from an undirected graph.
Our example of the cold spreading between you, your roommate, and your
colleague contains two cliques. One clique contains hy and hc . The factor for this
clique can be deﬁned by a table, and might have values resembling these:

hc = 0
hc = 1

hy = 0
2
1

hy = 1
1
10

A state of 1 indicates good health, while a state of 0 indicates poor health
(having been infected with a cold). Both of you are usually healthy, so the
corresponding state has the highest aﬃnity. The state where only one of you
is sick has the lowest aﬃnity, because this is a rare state. The state where both of
you are sick (because one of you has infected the other) is a higher aﬃnity state,
though still not as common as the state where both are healthy.
To complete the model, we would need to also deﬁne a similar factor for the
clique containing hy and hr.

13.2.3

The Partition Function

While the unnormalized probability distribution is guaranteed to be non-negative
everywhere, it is not guaranteed to sum or integrate to 1. To obtain a valid
probability distribution, we must use the corresponding normalized probability
distribution4 :
1
p(x) = p̃(x)
Z
where Z is the value that results in the probability distribution summing or
integrating to 1:
Z
Z=

p̃(x)dx.

4

A distribution deﬁned by normalizing a product of clique potentials is also called a Gibbs
distribution.
421

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

You can think of Z as a constant when the φ functions are held constant. Note
that if the φ functions have parameters, then Z is a function of those parameters.
It is common in the literature to write Z with its arguments omitted to save space.
Z is known as the partition function, a term borrowed from statistical physics.
Since Z is an integral or sum over all possible joint assignments of the state
x it is often intractable to compute. In order to be able to obtain the normalized
probability distribution of an undirected model, the model structure and the
deﬁnitions of the φ functions must be conducive to computing Z eﬃciently. In
the context of deep learning, Z is usually intractable, and we must resort to
approximations. Such approximate algorithms are the topic of Chapter 18.
One important consideration to keep in mind when designing undirected models is that it is possible for Z not to exist. This happens if some of the variables
in the model are continuous and the integral of p̃ over their domain diverges. For
example, suppose we want to model a single scalar variable x ∈ R with a single
clique potential φ(x) = x 2 . In this case,
Z
Z=
x2dx.
Since this integral diverges, there is no probability distribution corresponding
to this choice of φ(x). Sometimes the choice of some parameter of the φ functions determines whether the probability distribution is deﬁned. For example,


for φ(x; β) = exp −βx2 , the β parameter determines whether Z exists. Positive β results in a Gaussian distribution over x but all other values of β make φ
impossible to normalize.
One key diﬀerence between directed modeling and undirected modeling is that
directed models are deﬁned directly in terms of probability distributions from the
start, while undirected models are deﬁned more loosely by φ functions that are
then converted into probability distributions. This changes the intuitions one
must develop in order to work with these models. One key idea to keep in mind
while working with undirected models is that the domain of each of the variables
has dramatic eﬀect on the kind of probability distribution that a given set of φ
functions corresponds to. For example, consider an n-dimensional vector-valued
random variable x and an undirected model parameterized by a vector of biases
b. Suppose we have one clique for each element of x, φi(x i) = exp(bix i). What
kind of probability distribution does this result in? The answer is that we don’t
have enough information, because we have not yet speciﬁed the domain of x. If
x ∈ R n, then the integral deﬁning Z diverges and no probability distribution
exists. If x ∈ {0, 1} n , then p(x) factorizes into n independent distributions,
with p(xi = 1) = sigmoid (bi). If the domain of x is the set of elementary basis
vectors ({[1, 0, . . . , 0], [0, 1, . . . , 0], . . . , [0, 0, . . . , 1]} ) then p(x) = softmax(b), so a
large value of bi actually reduces p(xj = 1) for j 6
= i. Often, it is possible to
422

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A

B

C

D

E

F

Figure 13.4:
This graph implies that p(A, B, C, D, E, F ) can be written as
1
Z φA,B (A, B)φ B,C(B, C)φA,D (A, D)φB,E (B, E)φE,F (E, F ) for an appropriate choice of
the φ functions.

leverage the eﬀect of a carefully chosen domain of a variable in order to obtain
complicated behavior from a relatively simple set of φ functions. We’ll explore a
practical application of this idea later, in Chapter 20.7.

13.2.4

Energy-Based Models

Many interesting theoretical results about undirected models depend on the assumption that ∀x, p̃(x) > 0. A convenient way to enforce this to use an energybased model (EBM) where
p̃(x) = exp(−E(x))

(13.1)

and E(x) is known as the energy function. Because exp(z) is positive for all
z, this guarantees that no energy function will result in a probability of zero for
any state x. Being completely free to choose the energy function makes learning simpler. If we learned the clique potentials directly, we would need to use
constrained optimization, and we would need to arbitrarily impose some speciﬁc
minimal probability value. By learning the energy function, we can use unconstrained optimization 5, and the probabilities in the model can approach arbitrarily
close to zero but never reach it.
Any distribution of the form given by equation 13.1 is an example of a Boltzmann distribution. For this reason, many energy-based models are called Boltzmann machines. There is no accepted guideline for when to call a model an
energy-based model and when to call it a Boltzmann machines. The term Boltzmann machine was ﬁrst introduced to describe a model with exclusively binary
variables, but today many models such as the mean-covariance restricted Boltzmann machine incorporate real-valued variables as well.
5

For some models, we may still need to use constrained optimization to make sure Z exists.

423

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A

B

C

D

E

F

Figure 13.5: This graph implies that E(a, b, c, d, e, f) can be written as Ea,b (a, b) +
E b,c(b, c) + E a,d(a, d) + E b,e(b, e) + E e,f(e, f) for an appropriate choice of the per-clique
energy functions. Note that we can obtain the φ functions in Fig. 13.4 by setting each φ
to the exp of the corresponding negative energy, e.g., φa,b (a, b) = exp (−E(a, b)).

Cliques in an undirected graph correspond to factors of the unnormalized
probability function. Because exp(a) exp(b) = exp(a+b), this means that diﬀerent
cliques in the undirected graph correspond to the diﬀerent terms of the energy
function. In other words, an energy-based model is just a special kind of Markov
network: the exponentiation makes each term in the energy function correspond
to a factor for a diﬀerent clique. See Fig. 13.5 for an example of how to read the
form of the energy function from an undirected graph structure.
One part of the deﬁnition of an energy-based model serves no functional purpose from a machine learning point of view: the − sign in Eq. 13.1. This −
sign could be incorporated into the deﬁnition of E, or for many functions E the
learning algorithm could simply learn parameters with opposite sign. The − sign
is present primarily to preserve compatibility between the machine learning literature and the physics literature. Many advances in probabilistic modeling were
originally developed by statistical physicists, for whom E refers to actual, physical energy and does not have arbitrary sign. Terminology such as “energy” and
“partition function” remains associated with these techniques, even though their
mathematical applicability is broader than the physics context in which they were
developed. Some machine learning researchers (e.g., Smolensky (1986), who referred to negative energy as harmony) have chosen to emit the negation, but this
is not the standard convention.

13.2.5

Separation and D-Separation

The edges in a graphical model tell us which variables directly interact. We often
need to know which variables indirectly interact. Some of these indirect interactions can be enabled or disabled by observing other variables. More formally, we
would like to know which subsets of variables are conditionally independent from
each other, given the values of other subsets of variables.
424

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A

S

B

A

S

(a)

B

(b)

Figure 13.6: a) The path between random variable a and random variable b through s is
active, because s is not observed. This means that a and b are not separated. b) Here s
is shaded in, to indicate that it is observed. Because the only path between ra and b is
through s, and that path is inactive, we can conclude that a and b are separated given s.

A
B

C

D
Figure 13.7: An example of reading separation properties from an undirected graph. Here
b is shaded to indicate that it is observed. Because observing b blocks the only path from
a to c, we say that a and c are separated from each other given b. The observation of b
also blocks one path between a and d, but there is a second, active path between them.
Therefore, a and d are not separated given b.

Identifying the conditional independences in a graph is very simple in the case
of undirected models. In this case, conditional independence implied by the graph
is called separation. We say that a set of variables A is separated from another set
of variables B given a third set of variables S if the graph structure implies that A
is independent from B given S. If two variables a and b are connected by a path
involving only unobserved variables, then those variables are not separated. If no
path exists between them, or all paths contain an observed variable, then they
are separated. We refer to paths involving only unobserved variables as “active”
and paths including an observed variable as “inactive.”
When we draw a graph, we can indicate observed variables by shading them
in. See Fig. 13.6 for a depiction of how active and inactive paths in an undirected
look when drawn in this way. See Fig. 13.7 for an example of reading separation
from an undirected graph.
Similar concepts apply to directed models, except that in the context of directed models, these concepts are referred to as d-separation. The “d” stands for
“dependence.” D-separation for directed graphs is deﬁned the same as separation for undirected graphs: We say that a set of variables A is d-separated from
another set of variables B given a third set of variables S if the graph structure
425

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

implies that A is independent from B given S.
As with undirected models, we can examine the independences implied by
the graph by looking at what active paths exist in the graph. As before, two
variables are dependent if there is an active path between them, and d-separated
if no such path exists. In directed nets, determining whether a path is active is
somewhat more complicated. See Fig. 13.8 for a guide to identifying active paths
in a directed model. See Fig. 13.9 for an example of reading some properties from
a graph.
It is important to remember that separation and d-separation tell us only
about those conditional independences that are implied by the graph. There is
no requirement that the graph imply all independences that are present. In
particular, it is always legitimate to use the complete graph (the graph with all
possible edges) to represent any distribution. In fact, some distributions contain
independences that are not possible to represent with existing graphical notation.
Context-speciﬁc independences are independences that are present dependent on
the value of some variables in the network. For example, consider a model of three
binary variables, a, b, and c. Suppose that when a is 0, b and c are independent,
but when a is 1, b is deterministically equal to c. Encoding the behavior when
a = 1 requires an edge connecting b and c. The graph then fails to indicate that
b and c are independent when a = 0.
In general, a graph will never imply that an independence exists when it does
not. However, a graph may fail to encode an independence.

13.2.6

Converting Between Undirected and Directed Graphs

In common parlance, we often refer to certain model classes as being undirected
or directed. For example, we typically refer to RBMs as undirected and sparse
coding as directed. This way of speaking can be somewhat leading, because no
probabilistic model is inherently directed or undirected. Instead, some models
are most easily described using a directed graph, or most easily described using
an undirected graph.
Ever probability distribution can be represented by either a directed model
or by an undirected model. In the worst case, one can always represent any
distribution by using a “complete graph.” In the case of a directed model, the
complete graph is any directed acyclic graph where we impose some ordering
on the random variables, and each variable has all other variables that precede
it in the ordering as its ancestors in the graph. For an undirected model, the
complete graph is simply a graph containing a single clique encompassing all of
the variables.
Of course, the utility of a graphical model is that the graph implies that some
variables do not interact directly. The complete graph is not very useful because
426

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A

S

B
A

A

S

B

B

(a)

A

S

(b)

A

B

S

S

C

(c)

(d)

B

Figure 13.8: All of the kinds of active paths of length two that can exist between random
variables a and rb. a) Any path with arrows proceeding directly from a to b or vice versa.
This kind of path becomes blocked if s is observed. We have already seen this kind of
path in the relay race example. b) a and b are connected by a common cause s. For
example, suppose s is a variable indicating whether or not there is a hurricane and a and
b measure the wind speed at two diﬀerent nearby weather monitoring outposts. If we
observe very high winds at station a, we might expect to also see high winds at b. This
kind of path can be blocked by observing s. If we already know there is a hurricane, we
expect to see high winds at b, regardless of what is observed at a. A lower than expected
wind at a (for a hurricane) would not change our expectation of winds at b (knowing
there is a hurricane). However, if s is not observed, then a and b are dependent, i.e., the
path is inactive. c) a and b are both parents of s. This is called a V-structure or the
collider case, and it causes a and a to be related by the explaining away eﬀect. In this
case, the path is actually active when s is observed. For example, suppose s is a variable
indicating that your colleague is not at work. The variable a represents her being sick,
while b represents her being on vacation. If you observe that she is not at work, you
can presume she is probably sick or on vacation, but it’s not especially likely that both
have happened at the same time. If you ﬁnd out that she is on vacation, this fact is
suﬃcient to explain her absence, and you can infer that she is probably not also sick. d)
The explaining away eﬀect happens even if any descendant of s is observed! For example,
suppose that c is a variable representing whether you have received a report from your
colleague. If you notice that you have not received the report, this increases your estimate
of the probability that she is not at work today, which in turn makes it more likely that
she is either sick or on vacation. The only way to block a path through a V-structure is
to observe none of the descendants of the shared child.
427

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A

B
C

D

E

Figure 13.9

From this graph, we can read out several d-separation properties. Examples
include:
• a and b are d-separated given the empty set.
• a and e are d-separated given c.
• d and e are d-separated given c.
We can also see that some variables are no longer d-separated when we observe
some variables:
• a and b are not d-separated given c.
• a and b are not d-separated given d.

428

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

it does not imply any independences. TODO ﬁgure complete graph
When we represent a probability distribution with a graph, we want to choose
a graph that implies as many independences as possible, without implying any
independences that do not actually exist.
From this point of view, some distributions can be represented more eﬃciently using directed models, while other distributions can be represented more
eﬃciently using undirected models. In other words, directed models can encode
some independences that undirected models cannot encode, and vice versa.
Directed models are able to use one speciﬁc kind of substructure that undirected models cannot represent perfectly. This substructure is called an immorality. The structure occurs when two random variables a and b are both parents
of a third random variable c, and there is no edge directly connecting a and b
in either direction. (The name “immorality” may seem strange; it was coined in
the graphical models literature as a joke about unmarried parents) To convert a
directed model with graph D into an undirected model, we need to create a new
graph U . For every pair of variables x and y, we add an undirected edge connecting x and y to U if there is a directed edge (in either direction) connecting x and
y in D or if x and y are both parents in D of a third variable z. The resulting U
is known as a moralized graph. See Fig. 13.10 for examples of converting directed
models to undirected models via moralization.
Likewise, undirected models can include substructures that no directed model
can represent perfectly. Speciﬁcally, a directed graph D cannot capture all of
the conditional independences implied by an undirected graph U if U contains a
loop of length greater than three, unless that loop also contains a chord. A loop
is a sequence of variables connected by undirected edges, with the last variable
in the sequence connected back to the ﬁrst variable in the sequence. A chord is
a connection between any two non-consecutive variables in this sequence. If U
has loops of length four or greater and does not have chords for these loops, we
must add the chords before we can convert it to a directed model. Adding these
chords discards some of the independence information that was encoded in U .
The graph formed by adding chords to U is known as a chordal or triangulated
graph, because all the loops can now be described in terms of smaller, triangular
loops. To build a directed graph D from the chordal graph, we need to also assign
directions to the edges. When doing so, we must not create a directed cycle in
D, or the result does not deﬁne a valid directed probabilistic model. One way
to assign directions to the edges in D is to impose an ordering on the random
variables, then point each edge from the node that comes earlier in the ordering
to the node that comes later in the ordering. TODO point to ﬁg
IG HERE
TODO: started this above, need to scrap some some BNs encode indepen429

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

a
a

b

b

a
b

h2

h3

v1

v2

v3

h1

h2

h3

v1

v2

v3

c

c

a

h1

b

c

c

Figure 13.10: Examples of converting directed models to undirected models by constructing moralized graphs. Left) This simple chain can be converted to a moralized graph
merely by replacing its directed edges with undirected edges. The resulting undirected
model implies exactly the same set of independences and conditional independences. Center) This graph is the simplest directed model that cannot be converted to an undirected
model without losing some independences. This graph consists entirely of a single immorality. Because a and b are parents of c, they are connected by an active path when
c is observed. To capture this dependence, the undirected model must include a clique
encompassing all three variables. This clique fails to encode the fact that a⊥b. Right)
In general, moralization may add many edges to the graph, thus losing many implied
independences. For example, this sparse coding graph requires adding moralizing edges
between every pair of latent variables, thus introducing a quadratic number of new direct
dependences.

430

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

dences that MNs can’t encode, and vice versa example of BN that an MN can’t
encode: A and B are parents of C A is d-separated from B given the empty set
The Markov net requires a clique over A, B, and C in order to capture the active
path from A to B when C is observed This clique means that the graph cannot
imply A is separated from B given the empty set example of a MN that a BN
can’t encode: A, B, C, D connected in a loop BN cannot have both A d-sep D
given B, C and B d-sep C given A, D
In many cases, we may want to convert an undirected model to a directed
model, or vice versa. To do so, we choose the graph in the new format that implies
as many independences as possible, while not implying any independences that
were not implied by the original graph.
To convert a directed model D to an undirected model U, we re
TODO: conversion between directed and undirected models

13.2.7

Marginalizing Variables out of a Graph

TODO: marginalizing variables out of a graph

13.2.8

Factor Graphs

Factor graphs are another way of drawing undirected models that resolve an ambiguity in the graphical representation of standard undirected model syntax. In
an undirected model, the scope of every φ function must be a subset of some
clique in the graph. However, it is not necessary that there exist any φ whose
scope contains the entirety of every clique. Factor graphs explicitly represent the
scope of each φ function. Speciﬁcally, a factor graph is a graphical representation
of an undirected model that consists of a bipartite undirected graph. Some of the
nodes are drawn as circles. These nodes correspond to random variables as in a
standard undirected model. The rest of the nodes are drawn as squares. These
nodes correspond to the factors φ of the unnormalized probability distribution.
Variables and factors may be connected with undirected edges. A variable and a
factor are connected in the graph if and only if the variable is one of the arguments to the factor in the unnormalized probability distribution. No factor may
be connected to another factor in the graph, nor can a variable be connected to a
variable. See Fig. 13.11 for an example of how factor graphs can resolve ambiguity
in the interpretation of undirected networks.

13.3

Advantages of Structured Modeling

TODO– note that we have already shown that some things are cheaper in the
sections where we introduce the modeling syntax
431

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

A
A

B

A

f2

f1

B

f1
C
(a)

f3

C

C

(b)

(c)

Figure 13.11: An example of how a factor graph can resolve ambiguity in the interpretation of undirected networks. a) An undirected network with a clique involving three
variables a, b, and c. b) A factor graph corresponding to the same undirected model.
This factor graph has one factor over all three variables. c) Another valid factor graph
for the same undirected model. This factor graph has three factors, each over only two
variables. Note that representation, inference, and learning are all asymptotically cheaper
in (c) compared to (b), even though both require the same undirected graph to represent.
TODO: make sure ﬁgure respects random variable notation

TODO: revisit each of the three challenges from sec:unstructured TODO:
hammer point that graphical models convey information by leaving edges out
TODO: need to show reduced cost of sampling, but ﬁrst reader needs to know
about ancestral and gibbs sampling.... TODO: beneﬁt of separating representation from learning and inference

13.4

Learning About Dependencies

We consider here two types of random variables: observed or “visible” variables
v and latent or “hidden” variables h. The observed variables v correspond to the
variables actually provided in the data set during training. h consists of variables
that are introduced to the model in order to help it explain the structure in v.
Generally the exact semantics of h depend on the model parameters and are
created by the learning algorithm. The motivation for this is twofold.

13.4.1

B

Latent Variables Versus Structure Learning

Often the diﬀerent elements of v are highly dependent on each other. A good
model of v which did not contain any latent variables would need to have very
large numbers of parents per node in a Bayesian network or very large cliques in a
Markov network. Just representing these higher order interactions is costly–both
in a computational sense, because the number of parameters that must be stored
432

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

in memory scales exponentially with the number of members in a clique, but also
in a statistical sense, because this exponential number of parameters requires a
wealth of data to estimate accurately.
There is also the problem of learning which variables need to be in such large
cliques. An entire ﬁeld of machine learning called structure learning is devoted
to this problem . For a good reference on structure learning, see (Koller and
Friedman, 2009). Most structure learning techniques are a form of greedy search.
A structure is proposed, a model with that structure is trained, then given a score.
The score rewards high training set accuracy and penalizes model complexity.
Candidate structures with a small number of edges added or removed are then
proposed as the next step of the search, and the search proceeds to a new structure
that is expected to increase the score.
Using latent variables instead of adaptive structure avoids the need to perform
discrete searches and multiple rounds of training. A ﬁxed structure over visible
and hidden variables can use direct interactions between visible and hidden units
to impose indirect interactions between visible units. Using simple parameter
learning techniques we can learn a model with a ﬁxed structure that imputes the
right structure on the marginal p(v).

13.4.2

Latent Variables for Feature Learning

Another advantage of using latent variables is that they often develop useful
semantics.
As discussed in section 3.10.6, the mixture of Gaussians model learns a latent
variable that corresponds to which category of examples the input was drawn
from. This means that the latent variable in a mixture of Gaussians model can
be used to do classiﬁcation.
In Chapter 15 we saw how simple probabilistic models like sparse coding learn
latent variables that can be used as input features for a classiﬁer, or as coordinates
along a manifold. Other models can be used in this same way, but deeper models
and models with diﬀerent kids of interactions can create even richer descriptions
of the input. Most of the approaches mentioned in sec. 13.4.2 accomplish feature
learning by learning latent variables. Often, given some model of v and h, it
turns out that E[h | v] TODO: uh-oh, is there a collision between set notation
and expectation notation? or argmax hp(h, v) is a good feature mapping for v.
TODO: appropriate links to Monte Carlo methods chapter spun oﬀ from here

433

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

13.5

Inference and Approximate Inference Over Latent Variables

As soon as we introduce latent variables in a graphical model, this raises the
question: how to choose values of the latent variables h given values of the visible
variables x? This is what we call inference, in particular inference over the latent
variables. The general question of inference is to guess some variables given others.
TODO: inference has deﬁnitely been introduced above... TODO: mention
loopy BP, show how it is very expensive for DBMs
TODO: brieﬂy explain what variational inference is and reference approximate
inference chapter

13.5.1

Reparametrization Trick

Sometimes, in order to estimate the stochastic gradient of an expected loss over
some random variable h, with respect to parameters that inﬂuence h, we would
like to compute gradients through h, i.e., on the parameters that inﬂuenced the
probability distribution from which h was sampled. If h is continuous-valued,
this is generally possible by using the reparametrization trick, i.e., rewriting
h ∼ p(h | θ)

(13.2)

h = f (θ, η)

(13.3)

as
where η is some independent noise source of the appropriate dimension with
density p(η), and f is a continuous (diﬀerentiable almost everywhere) function.
Basically, the reparametrization trick is the idea that if the random variable to
be integrated over is continuous, we can back-propagate through the process that
gave rise to it in order to ﬁgure how to change that process.
For example, let us suppose we want to estimate the expected gradient
Z
∂
L(h)p(h | θ)dh
(13.4)
∂θ
where the parameters θ inﬂuences the random variable h which in term inﬂuence
our loss L. A very eﬃcient (Kingma and Welling, 2014b; Rezende et al., 2014)
way to achieve6 this is to perform the reparametrization in Eq. 13.3 and the
corresponding change of variable in the integral of Eq. 13.4, integrating over η
rather than h:
Z
∂
L(f (θ, η))p(eta)dη.
(13.5)
∂θ
6

compared to approaches that do not back-propagate through the generation of h
434

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

We can now more easily enter the derivative in the integral, getting
Z
∂L(f(θ, η))
g=
p(eta)dη.
∂θ
Finally, we get a stochastic gradient estimator
ĝ =

∂L(f(θ, η))
∂θ

where we sampled η ∼ p(η) and E[ĝ] = g.
This trick was used by Bengio (2013b); Bengio et al. (2013a) to train a neural network with stochastic hidden units. It was described at the same time
by Kingma (2013), but see the further developments in Kingma and Welling
(2014b). It was used to train generative stochastic networks (GSNs) (Bengio
et al., 2014a,b), described in Section 20.11, which can be viewed as recurrent
networks with noise injected both in input and hidden units (with each time step
corresponding to one step of a generative Markov chain). The reparametrization trick was also used to estimate the parameter gradient in variational autoencoders (Kingma and Welling, 2014a; Rezende et al., 2014; Kingma et al., 2014),
which are described in Section 20.9.3.

13.6

The Deep Learning Approach to Structured Probabilistic Models

Deep learning practictioners generally use the same basic computational tools
as other machine learning practitioners who work with structured probabilistic
models. However, in the context of deep learning, we usually make diﬀerent design
decisions about how to combine these tools, resulting in overall algorithms and
models that have a very diﬀerent ﬂavor from more traditional graphical models.
The most striking diﬀerence between the deep learning style of graphical model
design and the traditional style of graphical model design is that the deep learning style heavily emphasizes the use of latent variables. Deep learning models
typically have more latent variables than observed variables. Moreover, the practitioner typically does not intend for the latent variables to take on any speciﬁc
semantics ahead of time— the training algorithm is free to invent the concepts it
needs to model a particular dataset. The latent variables are usually not very easy
for a human to interpret after the fact, though visualization techniques may allow
some rough characterization of what they represent. Complicated non-linear interactions between variables are accomplished via indirect connections that ﬂow
through multiple latent variables. By contrast, traditional graphical models usually contain variables that are at least occasionally observed, even if many of the
435

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

variables are missing at random from some training examples. Complicated nonlinear interactions between variables are modeled by using higher-order terms,
with structure learning algorithms used to prune connections and control model
capacity. When latent variables are used, they are often designed with some speciﬁc semantics in mind—the topic of a document, the intelligence of a student,
the disease causing a patient’s symptoms, etc. These models are often much more
interpretable by human practitioners and often have more theoretical guarantees,
yet are less able to scale to complex problems and are not reuseable in as many
diﬀerent contexts as deep models.
Another obvious diﬀerence is the kind of graph structure typically used in
the deep learning approach. This is tightly linked with the choice of inference
algorithm. Traditional approaches to graphical models typically aim to maintain
the tractability of exact inference. When this constraint is too limiting, a popular
exact inference algorithm is loopy belief propagation. Both of these approaches
often work well with very sparsely connected graphs. By comparison, very few
interesting deep models admit exact inference, and loopy belief propagation is almost never used for deep learning. Most deep models are designed to make Gibbs
sampling or variational inference algorithms, rather than loopy belief propagation,
eﬃcient. Another consideration is that deep learning models contain a very large
number of latent variables, making eﬃcient numerical code essential. As a result
of these design constraints, most deep learning models are organized into regular repeating patterns of units grouped into layers, but neighboring layers may
be fully connected to each other. When sparse connections are used, they usually follow a regular pattern, such as the block connections used in convolutional
models.
Finally, the deep learning approach to graphical modeling is characterized by
a marked tolerance of the unknown. Rather than simplifying the model until
all quantities we might want can be computed exactly, we increase the power of
the model until it is just barely possible to train or use. We often use models
whose marginal distributions cannot be computed, and are satisﬁed simply to
draw approximate samples from these models. We often train models with an
intractable objective function that we cannot even approximate in a reasonable
amount of time, but we are still able to approximately train the model if we can
eﬃciently obtain an estimate of the gradient of such a function. The deep learning
approach is often to ﬁgure out what the minimum amount of information we
absolutely need is, and then to ﬁgure out how to get a reasonable approximation
of that information as quickly as possible.

436

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

h1

h2

v1

h3

v2

h4

v3

Figure 13.12: An example RBM drawn as a Markov network

13.6.1

Example: The Restricted Boltzmann Machine

TODO: rework this section. Add pointer to Chapter 20.2. TODO what do we
want to exemplify here?
The restricted Boltzmann machine (RBM) (Smolensky, 1986) or harmonium
is an example of a model that TODO what do we want to exemplify here?
It is an energy-based model with binary visible and hidden units. Its energy
function is
E(v, h) = −b >v − c >h − v > W h
where b, c, and W are unconstrained, real-valued, learnable parameters. The
model is depicted graphically in Fig. 13.12. As this ﬁgure makes clear, an important aspect of this model is that there are no direct interactions between any two
visible units or between any two hidden units (hence the “restricted,” a general
Boltzmann machine may have arbitrary connections).
The restrictions on the RBM structure yield the nice properties
p(h | v) = Π ip(h i | v)
and
p(v | h) = Πi p(vi | h).
The individual conditionals are simple to compute as well, for example


p(hi = 1 | v) = σ v >W :,i + bi .

Together these properties allow for eﬃcient block Gibbs sampling, alternating
between sampling all of h simultaneously and sampling all of v simultaneously.

437

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

Since the energy function itself is just a linear function of the parameters, it
is easy to take the needed derivatives. For example,
∂
E E(v, h) = −v ihj .
∂W i,j v,h
These two properties–eﬃcient Gibbs sampling and eﬃcient derivatives– make
it possible to train the RBM with stochastic approximations to ∇ θ log Z.

13.6.2

The Computational Challenge with High-Dimensional Distributions

TODO: this whole section should probably just be cut, IG thinks YB has written
the same thing in 2-3 other places (ml.tex for sure, and maybe also manifolds.tex
and prob.tex, possibly others IG hasn’t read yet) YB doesn’t seem to have read
the intro part of this chapter which discusses these things in more detail, double
check to make sure there’s not anything left out above If this section is kept, it
needs cleanup, i.e. a instead A, etc. If this section is cut, need to search for refs
to it and move them to one of the other versions of it
High-dimensional random variables actually bring two challenges: a statistical
challenge and a computational challenge.
The statistical challenge was introduced in Section 5.12 and regards generalization: the number of conﬁgurations we may want to distinguish can grow
exponentially with the number of dimensions of interest, and this quickly becomes much larger than the number of examples one can possibly have (or use
with bounded computational resources).
The computational challenge associated with high-dimensional distributions
arises because many algorithms for learning or using a trained model (especially
those based on estimating an explicit probability function) involve intractable
computations that grow exponentially with the number of dimensions.
With probabilistic models, this computational challenge arises because of intractable sums (summing over an exponential number of conﬁgurations) or intractable maximizations (ﬁnding the best out of an intractable number of conﬁgurations), discussed mostly in the third part of this book.
• Intractable inference: inference is discussed mostly in Chapter 19. It
regards the question of guessing the probable values of some variables A,
given other variables B, with respect to a model that captures the joint
distribution between A, B and C. In order to even compute such conditional
probabilities one needs to sum over the values of the variables C, as well as
compute a normalization constant which sums over the values of A and C.
438

CHAPTER 13. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING

• Intractable normalization constants (the partition function): the
partition function is discussed mostly in Chapter 18. Normalizing constants
of probability functions come up in inference (above) as well as in learning.
Many probabilistic models involve such a constant. Unfortunately, the parameters (which we want to tune) inﬂuence that constant, and computing
the gradient of the partition function with respect to the parameters is
generally as intractable as computing the partition function itself. MonteCarlo Markov chain (MCMC) methods (Chapter 14) are often used to deal
with the partition function (computing it or its gradient) but they may also
suﬀer from the curse of dimensionality, when the number of modes of the
distribution of interest is very large, and these modes are well separated
(Section 14.2).
One way to confront these intractable computations is to approximate them,
and many approaches have been proposed, discussed in the chapters listed above.
Another interesting way would be to avoid these intractable computations altogether by design, and methods that do not require such computations are thus
very appealing. Several generative models based on auto-encoders have been
proposed in recent years, with that motivation, and are discussed at the end of
Chapter 20.

439

Chapter 14

Monte Carlo Methods
TODO plan organization of chapter (spun oﬀ from graphical models chapter)

14.1

Markov Chain Monte Carlo Methods

Drawing a sample x from the probability distribution p(x) deﬁned by a structured model is an important operation. The following techniques are described
in (Koller and Friedman, 2009).
Sampling from an energy-based model is not straightforward. Suppose we
have an EBM deﬁning a distribution p(a, b). In order to sample a, we must draw
it from p(a | b), and in order to sample b, we must draw it from p(b | a). It seems
to be an intractable chicken-and-egg problem. Directed models avoid this because
their G is directed and acyclical. In ancestral sampling one simply samples each of
the variables in topological order, conditioning on each variable’s parents, which
are guaranteed to have already been sampled. This deﬁnes an eﬃcient, single-pass
method of obtaining a sample.
In an EBM, it turns out that we can get around this chicken and egg problem
by sampling using a Markov chain. A Markov chain is deﬁned by a state x and
a transition distribution T (x0 | x). Running the Markov chain means repeatedly
updating the state x to a value x0 sampled from T (x0 | x).
Under certain distributions, a Markov chain is eventually guaranteed to draw
x from an equilibrium distribution π(x0 ), deﬁned by the condition
X
∀x0 , π(x0 ) =
T (rvx0 | x)π(x).
x

TODO– this vector / matrix view needs a whole lot more exposition only literally a vector / matrix when the state is discrete unpack into multiple sentences,
440

CHAPTER 14. MONTE CARLO METHODS

the parenthetical is hard to parse is the term “stochastic matrix” deﬁned anywhere? make sure it’s in the index at least whoever ﬁnishes writing this section
should also ﬁnish making the math notation consistent terms in this section need
to be in the index
We can think of π as a vector (with the probability for each possible value x
in the element indexed by x, π(x)) and T as a corresponding stochastic matrix
(with row index x0 and column index x), i.e., with non-negative entries that sum
to 1 over elements of a column. Then, the above equation becomes
Tπ = π
an eigenvector equation that says that π is the eigenvector of T with eigenvalue
1. It can be shown (Perron-Frobenius theorem) that this is the largest possible
eigenvalue, and the only one with value 1 under mild conditions (for example
T (x0 | x) > 0). We can also see this equation as a ﬁxed point equation for the
update of the distribution associated with each step of the Markov chain. If we
start a chain by picking x0 ∼ p 0, then we get a distribution p1 = T p0 after one
step, and pt = T pt−1 = T tp0 after t steps. If this recursion converges (the chain
has a so-called stationary distribution), then it converges to a ﬁxed point which
is precisely pt = π for t → ∞, and the dynamical systems view meets and agrees
with the eigenvector view.
This condition guarantees that repeated applications of the transition sampling procedure don’t change the distribution over the state of the Markov chain.
Running the Markov chain until it reaches its equilibrium distribution is called
“burning in” the Markov chain.
Unfortunately, there is no theory to predict how many steps the Markov chain
must run before reaching its equilibrium distribution1 , nor any way to tell for sure
that this event has happened. Also, even though successive samples come from the
same distribution, they are highly correlated with each other, so to obtain multiple
samples one should run the Markov chain for many steps between collecting each
sample. Markov chains tend to get stuck in a single mode of π(x) for several
steps. The speed with which a Markov chain moves from mode to mode is called
its mixing rate. Since burning in a Markov chain and getting it to mix well may
take several sampling steps, sampling correctly from an EBM is still a somewhat
costly procedure.
TODO: mention Metropolis-Hastings
Of course, all of this depends on ensuring π(x) = p(x) . Fortunately, this
is easy so long as p(x) is deﬁned by an EBM. The simplest method is to use
Gibbs sampling, in which sampling from T (x0 | x) is accomplished by selecting
1

although in principle the ratio of the two leading eigenvalues of the transition operator gives
us some clue, and the largest eigenvalue is 1.
441

CHAPTER 14. MONTE CARLO METHODS

Figure 14.1: Paths followed by Gibbs sampling for three distributions, with the Markov
chain initialized at the mode in both cases. Left) A multivariate normal distribution
with two independent variables. Gibbs sampling mixes well because the variables are
independent. Center) A multivariate normal distribution with highly correlated variables.
The correlation between variables makes it diﬃcult for the Markov chain to mix. Because
each variable must be updated conditioned on the other, the correlation reduces the rate
at which the Markov chain can move away from the starting point. Right) A mixture of
Gaussians with widely separated modes that are not axis-aligned. Gibbs sampling mixes
very slowly because it is diﬃcult to change modes while altering only one variable at a
time.

one variable xi and sampling it from p conditioned on its neighbors in G. It is
also possible to sample several variables at the same time so long as they are
conditionally independent given all of their neighbors.
TODO: discussion of mixing example with 2 binary variables that prefer to
both have the same state IG’s graphic from lecture on adversarial nets
TODO: refer to this ﬁgure in the text:
TODO: refer to this ﬁgure in the text

14.1.1

Markov Chain Theory

TODO
State Perron’s theorem
DEFINE detailed balance

14.1.2

Importance Sampling

TODO write this section

14.2

The Diﬃculty of Mixing Between Well-Separated
Modes

442

CHAPTER 14. MONTE CARLO METHODS

Figure 14.2: An illustration of the slow mixing problem in deep probabilistic models.
Each panel should be read left to right, top to bottom. Left) Consecutive samples from
Gibbs sampling applied to a deep Boltzmann machine trained on the MNIST dataset.
Consecutive samples are similar to each other. Because the Gibbs sampling is performed
in a deep graphical model, this similarity is based more on semantic rather than raw visual
features, but it is still diﬃcult for the Gibbs chain to transition from one mode of the
distribution to another, for example by changing the digit identity. Right) Consecutive
ancestral samples from a generative adversarial network. Because ancestral sampling
generates each sample independently from the others, there is no mixing problem.

443

Chapter 15

Linear Factor Models and
Auto-Encoders
Linear factor models are generative unsupervised learning models in which we
imagine that some unobserved factors h explain the observed variables x through
a linear transformation. Auto-encoders are unsupervised learning methods that
learn a representation of the data, typically obtained by a non-linear parametric transformation of the data, i.e., from x to h, typically a feedforward neural
network, but not necessarily. They also learn a transformation going backwards
from the representation to the data, from h to x, like the linear factor models.
Linear factor models therefore only specify a parametric decoder, whereas autoencoder also specify a parametric encoder. Some linear factor models, like PCA,
actually correspond to an auto-encoder (a linear one), but for others the encoder
is implicitly deﬁned via an inference mechanism that searches for an h that could
have generated the observed x.
The idea of auto-encoders has been part of the historical landscape of neural
networks for decades (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel,
1994) but has really picked up speed in recent years. They remained somewhat
marginal for many years, in part due to what was an incomplete understanding of
the mathematical interpretation and geometrical underpinnings of auto-encoders,
which are developed further in Chapters 17 and 20.11.
An auto-encoder is simply a neural network that tries to copy its input to its output. The architecture of an auto-encoder is typically decomposed KEY
into the following parts, illustrated in Figure 15.1:
IDEA
• an input, x
• an encoder function f
• a “code” or internal representation h = f (x)
444

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

reconstruc,on!r!

Decoder.g!

code!h!

Encoder.f!

input!x!
Figure 15.1: General schema of an auto-encoder, mapping an input x to an output (called
reconstruction) r through an internal representation or code h. The auto-encoder has
two components: the encoder f (mapping x to h) and the decoder g (mapping h to r).

• a decoder function g
• an output, also called “reconstruction” r = g(h) = g(f (x))
• a loss function L computing a scalar L(r, x) measuring how good of a reconstruction r is of the given input x. The objective is to minimize the
expected value of L over the training set of examples {x}.

15.1

Regularized Auto-Encoders

Predicting the input may sound useless: what could prevent the auto-encoder
from simply copying its input into its output? In the 20th century, this was
achieved by constraining the architecture of the auto-encoder to avoid this, by
forcing the dimension of the code h to be smaller than the dimension of the input
x.
Figure 15.2 illustrates the two typical cases of auto-encoders: undercomplete
vs overcomplete, i.e., with the dimension of the representation h respectively
smaller vs larger than the input x. Whereas early work with auto-encoders, just
like PCA, uses the undercompleteness – i.e. a bottleneck in the sequence of layers
– to avoid learning the identity function, more recent work allows overcomplete
445

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

reconstruc4on!r!

Decoder*

Decoder*

Code*bo,leneck!h:!
undercomplete*
representa4on*

Code!h:!
overcomplete*
representa4on*

Encoder*

Encoder*

input!x!
Figure 15.2: Left: undercomplete representation (dimension of code h is less than dimension of input x). Right: overcomplete representation. Overcomplete auto-encoders
require some other form of regularization (instead of the constraint on the dimension of
h) to avoid the trivial solution where r = x for all x.

representations. What we have learned in recent years is that it is possible to
make the auto-encoder meaningfully capture the structure of the input distribution even if the representation is overcomplete, with other forms of constraint
or regularization. In fact, once you realize that auto-encoders can capture the
input distribution (indirectly, not as a an explicit probability function), you also
realize that it should need more capacity as one increases the complexity of the
distribution to be captured (and the amount of data available): it should not be
limited by the input dimension. This is a problem in particular with the shallow auto-encoders, which have a single hidden layer (for the code). Indeed, that
hidden layer size controls both the dimensionality reduction constraint (the code
size at the bottleneck) and the capacity (which allows to learn a more complex
distribution).
Besides the bottleneck constraint, alternative constraints or regularization
methods have been explored and can guarantee that the auto-encoder does something useful and not just learn some trivial identity-like function:
• Sparsity of the representation or of its derivative: even if the intermediate representation has a very high dimensionality, the eﬀective local
dimensionality (number of degrees of freedom that capture a coordinate sys446

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

tem among the probable x’s) could be much smaller if most of the elements
i
of h are zero (or any other constant, such that || ∂h
∂x || is close to zero). When
i
|| ∂h
|| is close to zero, hi does not participate in encoding local changes in
∂x
x. There is a geometrical interpretation of this situation in terms of manifold learning that is discussed in more depth in Chapter 17. The discussion
in Chapter 16 also explains how an auto-encoder naturally tends towards
learning a coordinate system for the actual factors of variation in the data.
At least four types of “auto-encoders” clearly fall in this category of sparse
representation:
– Sparse coding (Olshausen and Field, 1996) has been heavily studied
as an unsupervised feature learning and feature inference mechanism.
It is a linear factor model rather than an auto-encoder, because it has
no explicit parametric encoder, and instead uses an iterative optimization procedure to compute the maximally likely code. Sparse coding
looks for representations that are both sparse and explain the input
through the decoder. Instead of the code being a parametric function
of the input, it is considered like free variable that is obtained through
an optimization, i.e., a particular form of inference:
h∗ = f (x) = arg min L(g(h), x)) + λΩ(h)

(15.1)

h

where L is the reconstruction loss, f the (non-parametric) encoder, g
the (parametric) decoder, Ω(h) is a sparsity regularizer, and in practice
the minimization can be approximate. Sparse coding has a manifold
or geometric interpretation that is discussed in Section 15.8. It also
has an interpretation as a directed graphical model, described in more
details in Section 19.3. To achieve sparsity, the objective function to
optimize includes a term that is minimized when the representation
P has
many zero or near-zero values, such as the L1 penalty |h| 1 = i |hi |.

– An interesting variation of sparse coding combines the freedom to
choose the representation through optimization and a parametric encoder. It is called predictive sparse decomposition (PSD) (Kavukcuoglu
et al., 2008a) and is brieﬂy described in Section 15.8.2.
– At the other end of the spectrum are simply sparse auto-encoders,
which combine with the standard auto-encoder schema a sparsity penalty
which encourages the output of the encoder to be sparse. These are
described in Section 15.8.1. Besides the L1 penalty, other sparsity
penalties that have been explored include the Student-t penalty (Olshausen and Field, 1996; Bergstra, 2011), TODO: should the t be in
447

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

math mode, perhaps?

X

log(1 + α2h2i )

i

(i.e. where αhi has a Student-t prior density) and the KL-divergence
penalty (Lee et al., 2008; Goodfellow et al., 2009; Larochelle and Bengio, 2008a)
X
(t log h i + (1 − t) log(1 − hi )),
−
i

with a target sparsity level t, for h i ∈ (0, 1), e.g. through a sigmoid
non-linearity.

– Contractive autoencoders (Rifai et al., 2011b), covered in Sec2
tion 15.10, explicitly penalize || ∂h
∂x ||F , i.e., the sum of the squared norm
∂h i(x)
of the vectors ∂x
(each indicating how much each hidden unit hi
responds to changes in x and what direction of change in x that unit is
most sensitive to, around a particular x). With such a regularization
penalty, the auto-encoder is called contractive 1 because the mapping
from input x to representation h is encouraged to be contractive, i.e.,
to have small derivatives in all directions. Note that a sparsity regularization indirectly leads to a contractive mapping as well, when the
non-linearity used happens to have a zero derivative at h i = 0 (which
is the case for the sigmoid non-linearity).
• Robustness to injected noise or missing information: if noise is
injected in inputs or hidden units, or if some inputs are missing, while the
neural network is asked to reconstruct the clean and complete input, then it
cannot simply learn the identity function. It has to capture the structure
of the data distribution in order to optimally perform this reconstruction.
Such auto-encoders are called denoising auto-encoders and are discussed in
more detail in Section 15.9.

15.2

Denoising Auto-encoders

There is a tight connection between the denoising auto-encoders and the
contractive auto-encoders: it can be shown (Alain and Bengio, 2013) that
in the limit of small Gaussian injected input noise, the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction
function that maps x to r = g(f (x)). In other words, since both x and
1

A function f(x) is contractive if ||f(x)−f(y)|| < ||x−y|| for nearby x and y, or equivalently
if its derivative ||f 0 (x)|| < 1.
448

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

x + (where  is some small noise vector) must yield the same target output
x, the reconstruction function is encouraged to be insensitive to changes in
all directions . The only thing that prevents reconstruction r from simply
being a constant (completely insensitive to the input x), is that one also
has to reconstruct correctly for diﬀerent training examples x. However, the
auto-encoder can learn to be approximately constant around training examples x while producing a diﬀerent answer for diﬀerent training examples.
As discussed in Section 17.4, if the examples are near a low-dimensional
manifold, this encourages the representation to vary only on the manifold
and be locally constant in directions orthogonal to the manifold, i.e., the
representation locally captures a (not necessarily Euclidean, not necessarily
orthogonal) coordinate system for the manifold. In addition to the denoising
auto-encoder, the variational auto-encoder (Section 20.9.3) and the generative stochastic networks (Section 20.11) also involve the injection of noise,
but typically in the representation-space itself, thus introducing the notion
of h as a latent variable.
• Pressure of a Prior on the Representation: an interesting way to
generalize the notion of regularization applied to the representation is to
introduce in the cost function for the auto-encoder a log-prior term
− log P (h)
which captures the assumption that we would like to ﬁnd a representation
that has a simple distribution (if P (h) has a simple form, such as a factorized distribution2), or at least one that is simpler than the original data
distribution. Among all the encoding functions f , we would like to pick one
that
1. can be inverted (easily), and this is achieved by minimizing some reconstruction loss, and
2. yields representations h whose distribution is “simpler”, i.e., can be
captured with less capacity than the original training distribution itself.
The sparse variants described above clearly fall in that framework. The variational auto-encoder (Section 20.9.3) provides a clean mathematical framework for justifying the above pressure of a top-level prior when the objective
is to model the data generating distribution.
From the point of view of regularization (Chapter 7), adding the − log P (h)
term to the objective function (e.g. for encouraging sparsity) or adding a contractive penalty do not ﬁt the traditional view of a prior on the parameters. Instead,
2

all the sparse priors we have described correspond to a factorized distribution
449

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

the prior on the latent variables acts like a data-dependent prior, in the sense that
it depends on the particular values h that are going to be sampled (usually from a
posterior or an encoder), based on the input example x. Of course, indirectly, this
is also a regularization on the parameters, but one that depends on the particular
data distribution.

15.3

Representational Power, Layer Size and Depth

Nothing in the above description of auto-encoders restricts the encoder or decoder
to be shallow, but in the literature on the subject, most trained auto-encoders
have had a single hidden layer which is also the representation layer or code3
For one, we know by the usual universal approximator abilities of single
hidden-layer neural networks that a suﬃciently large hidden layer can represent
any function with a given accuracy. This observation justiﬁes overcomplete autoencoders: in order to represent a rich enough distribution, one probably needs
many hidden units in the intermediate representation layer. We also know that
Principal Components Analysis (PCA) corresponds to an undercomplete autoencoder with no intermediate non-linearity, and that PCA can only capture a set
of directions of variation that are the same everywhere in space. This notion is
discussed in more details in Chapter 17 in the context of manifold learning.
For two, it has also been reported many times that training a deep neural
network, and in particular a deep auto-encoder (i.e. with a deep encoder and a
deep decoder) is more diﬃcult than training a shallow one. This was actually a
motivation for the initial work on the greedy layerwise unsupervised pre-training
procedure, described below in Section 16.1, by which we only need to train a series
of shallow auto-encoders in order to initialize a deep auto-encoder. It was shown
early on (Hinton and Salakhutdinov, 2006) that, if trained properly, such deep
auto-encoders could yield much better compression than corresponding shallow or
linear auto-encoders (which are basically doing the same as PCA, see Section 15.6
below). As discussed in Section 16.7, deeper architectures can be in some cases
exponentially more eﬃcient (both in terms of computation and statistically) than
shallow ones. However, because we can usefully pre-train a deep net by training
and stacking shallow ones, it makes it interesting to consider single-layer (or at
least shallow and easy to train) auto-encoders, as has been done in most of the
literature discussed in this chapter.
3

as argued in this book, this is probably not a good choice, and we would like to independently
control the constraints on the representation, e.g. dimension and sparsity of the code, and the
capacity of the encoder.

450

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

15.4

Reconstruction Distribution

The above “parts” (encoder function f , decoder function g, reconstruction loss
L) make sense when the loss L is simply the squared reconstruction error, but
there are many cases where this is not appropriate, e.g., when x is a vector of
discrete variables or when P (x | h) is not well approximated by a Gaussian distribution 4 . Just like in the case of other types of neural networks (starting with the
feedforward neural networks, Section 6.3.2), it is convenient to deﬁne the loss L
as a negative log-likelihood over some target random variables. This probabilistic
interpretation is particularly important for the discussion in Sections 20.9.3, 20.10
and 20.11 about generative extensions of auto-encoders and stochastic recurrent
networks, where the output of the auto-encoder is interpreted as a probability distribution P (x | h), for reconstructing x, given hidden units h. This distribution
captures not just the expected reconstruction but also the uncertainty about the
original x (which gave rise to h, either deterministically or stochastically, given
h). In the simplest and most ordinary cases, this distribution factorizes, i.e.,
Q
P (x | h) = i P (xi | h). This covers the usual cases of x i | h being Gaussian (for
unbounded real values) and xi |h having a Bernoulli distribution (for binary values
xi ), but one can readily generalize this to other distributions, such as mixtures
(see Sections 3.10.6 and 6.3.2).
Thus we can generalize the notion of decoding function g(h) to decoding distribution P (x | h). Similarly, we can generalize the notion of encoding function
f (x) to encoding distribution Q(h | x), as illustrated in Figure 15.3. We use
this to capture the fact that noise is injected at the level of the representation
h, now considered like a latent variable. This generalization is crucial in the
development of the variational auto-encoder (Section 20.9.3) and the generalized
stochastic networks (Section 20.11).
We also ﬁnd a stochastic encoder and a stochastic decoder in the RBM, described in Section 20.2. In that case, the encoding distribution Q(h | x) and
P (x | h) “match”, in the sense that Q(h | x) = P (h | x), i.e., there is a unique
joint distribution which has both Q(h | x) and P (x | h) as conditionals. This is
not true in general for two independently parametrized conditionals like Q(h | x)
and P (x | h), although the work on generative stochastic networks (Alain et al.,
2015) shows that learning will tend to make them compatible asymptotically (with
enough capacity and examples).
4

See the link between squared error and normal density in Sections 5.6 and 6.3.2

451

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

h
Q(h|x)

P (x|h)

x
Figure 15.3: Basic scheme of a stochastic auto-encoder, in which both the encoder and
the decoder are not simple functions but instead involve some noise injection, meaning
that their output can be seen as sampled from a distribution, Q(h | x) for the encoder
and P (x | h) for the decoder. RBMs are a special case where P = Q (in the sense of
a unique joint corresponding to both conditinals) but in general these two distributions
are not necessarily conditional distributions compatible with a unique joint distribution
P (x, h).

15.5

Linear Factor Models

Now that we have introduced the notion of a probabilistic decoder, let us focus
on a very special case where the latent variable h generates x via a linear transformation plus noise, i.e., classical linear factor models, which do not necessarily
have a corresponding parametric encoder.
The idea of discovering explanatory factors that have a simple joint distribution among themselves is old, e.g., see Factor Analysis (see below), and has been
explored ﬁrst in the context where the relationship between factors and data is
linear, i.e., we assume that the data was generated as follows. First, sample the
real-valued factors,
h ∼ P (h),
(15.2)
and then sample the real-valued observable variables given the factors:
x = W h + b + noise

(15.3)

where the noise is typically Gaussian and diagonal (independent across dimensions). This is illustrated in Figure 15.4.

452

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

h ∼ P (h)
P (x|h)

x = W h + b + noise
Figure 15.4: Basic scheme of a linear factors model, in which we assume that an observed
data vector x is obtained by a linear combination of latent factors h, plus some noise.
Diﬀerent models, such as probabilistic PCA, factor analysis or ICA, make diﬀerent choices
about the form of the noise and of the prior P (h).

15.6

Probabilistic PCA and Factor Analysis

Probabilistic PCA (Principal Components Analysis), factor analysis and other
linear factor models are special cases of the above equations (15.2 and 15.3) and
only diﬀer in the choices made for the prior (over latent, not parameters) and
noise distributions.
In factor analysis (Bartholomew, 1987; Basilevsky, 1994), the latent variable
prior is just the unit variance Gaussian
h ∼ N (0, I )
while the observed variables x i are assumed to be conditionally independent, given
h, i.e., the noise is assumed to be coming from a diagonal covariance Gaussian
distribution, with covariance matrix ψ = diag(σ 2 ), with σ2 = (σ 21, σ22 , . . .) a
vector of per-variable variances.
The role of the latent variables is thus to capture the dependencies between
the diﬀerent observed variables x i. Indeed, it can easily be shown that x is just
a Gaussian-distribution (multivariate normal) random variable, with
x ∼ N (b, W W > + ψ)
where we see that the weights W induce a dependency between two variables xi
and xj through a kind of auto-encoder path, whereby xi inﬂuences ĥk = W kx
via w ki (for every k) and ĥk inﬂuences x j via w kj .
In order to cast PCA in a probabilistic framework, we can make a slight
modiﬁcation to the factor analysis model, making the conditional variances σ i
453

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

equal to each other. In that case the covariance of x is just W W > + σ2 I, where
σ2 is now a scalar, i.e.,
x ∼ N (b, W W > + σ 2 I)
or equivalently
x = W h + b + σz
where z ∼ N (0, I) is white noise. Tipping and Bishop (1999) then show an
iterative EM algorithm for estimating the parameters W and σ 2.
What the probabilistic PCA model is basically saying is that the covariance
is mostly captured by the latent variables h, up to some small residual reconstruction error σ 2. As shown by Tipping and Bishop (1999), probabilistic PCA
becomes PCA as σ → 0. In that case, the conditional expected value of h given
x becomes an orthogonal projection onto the space spanned by the d columns of
W , like in PCA. See Section 17.1 for a discussion of the “inference” mechanism
associated with PCA (probabilistic or not), i.e., recovering the expected value of
the latent factors hi given the observed input x. That section also explains the
very insightful geometric and manifold interpretation of PCA.
However, as σ → 0, the density model becomes very sharp around these d
dimensions spanned the columns of W , as discussed in Section 17.1, which would
not make it a very faithful model of the data, in general (not just because the
data may live on a higher-dimensional manifold, but more importantly because
the real data manifold may not be a ﬂat hyperplane - see Chapter 17 for more).

15.6.1

ICA

TODO: do we really want to put every linear factor model in the auto-encoder
chapter? if latent variable models are auto-encoders, what deep probabilistic
model would not be an auto-encoder? Independent Component Analysis (ICA)
is among the oldest representation learning algorithms (Herault and Ans, 1984;
Jutten and Herault, 1991; Comon, 1994; Hyvärinen, 1999; Hyvärinen et al., 2001).
It is an approach to modeling linear factors that seeks non-Gaussian projections of
the data. Like probabilistic PCA and factor analysis, it also ﬁts the linear factor
model of Eqs. 15.2 and 15.3. What is particular about ICA is that unlike PCA
and factor analysis it does not assume that the latent variable prior is Gaussian.
It only assumes that it is factorized, i.e.,
Y
P (h) =
P (hi ).
(15.4)
i

Since there is no parametric assumption behind the prior, we are really in front
of a so-called semi-parametric model, with parts of the model being parametric
(P (x | h)) and parts being non-speciﬁed or non-parametric (P (h)). In fact, this
454

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

typically yields to non-Gaussian priors: if the priors were Gaussian, then one
could not distinguish between the factors h and a rotation of h. Indeed, note
that if
h = Uz
with U an orthonormal (rotation) square matrix, i.e.,
z = U > h,
then, although h might have a Normal(0, I) distribution, the z also have a unit
covariance, i.e., they are uncorrelated:
V ar[z] = E[zz>] = E[U >hh> U ] = U > V ar[h]U = U >U = I .
In other words, imposing independence among Gaussian factors does not allow one
to disentangle them, and we could as well recover any linear rotation of these factors. It means that, given the observed x, even though we might assume the right
generative model, PCA cannot recover the original generative factors. However, if
we assume that the latent variables are non-Gaussian, then we can recover them,
and this is what ICA is trying to achieve. In fact, under these generative model
assumptions, the true underlying factors can be recovered (Comon, 1994). In fact,
many ICA algorithms are looking for projections of the data s = V x such that
they are maximally non-Gaussian. An intuitive explanation for these approaches
is that although the true latent variables h may be non-Gaussian, almost any
linear combination of them will look more Gaussian, because of the central limit
theorem. Since linear combinations of the x i’s are also linear combinations of
the hj ’s, to recover the h j ’s we just need to ﬁnd the linear combinations that are
maximally non-Gaussian (while keeping these diﬀerent projections orthogonal to
each other).
There is an interesting connection between ICA and sparsity, since the dominant form of non-Gaussianity in real data is due to sparsity, i.e., concentration
of probability at or near 0. Non-Gaussian distributions typically have more mass
around zero, although you can also get non-Gaussianity by increasing skewness,
asymmetry, or kurtosis.
Like PCA can be generalized to non-linear auto-encoders described later in
this chapter, ICA can be generalized to a non-linear generative model, e.g., x =
f (h)+ noise. See Hyvärinen and Pajunen (1999) for the initial work on non-linear
ICA and its successful use with ensemble learning by Roberts and Everson (2001);
Lappalainen et al. (2000).

455

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

15.6.2

Sparse Coding as a Generative Model

One particularly interesting form of non-Gaussianity arises with distributions that
are sparse. These typically have not just a peak at 0 but also a fat tail5 . Like the
other linear factor models (Eq. 15.3), sparse coding corresponds to a linear factor
model, but one with a “sparse” latent variable h, i.e., P (h) puts high probability
at or around 0. Unlike with ICA (previous section), the latent variable prior is
parametric. For example the factorized Laplace density prior is
P (h) =

Y

P (hi ) =

i

Yλ
i

2

e−λ|h i|

(15.5)

and the factorized Student-t prior is
P (h) =

Y
i

P (hi ) ∝

Y
i

ν+1
2

1
1+

h 2i
ν

.

(15.6)

Both of these densities have a strong preference for near-zero values but, unlike
the Gaussian, accomodate large values. In the standard sparse coding models,
the reconstruction noise is assumed to be Gaussian, so that the corresponding
reconstruction error is the squared error.
Regarding sparsity, note that the actual value h i = 0 has zero measure under
both densities, meaning that the posterior distribution P (h | x) will not generate
values h = 0. However, sparse coding is normally considered under a maximum
a posteriori (MAP) inference framework, in which the inferred values of h are
those that maximize the posterior, and these tend to often be zero if the prior
is suﬃciently concentrated around 0. The inferred values are those deﬁned in
Eq. 15.1, reproduced here,
h = f (x) = arg min L(g(h), x)) + λΩ(h)
h

where L(g(h), x) is interpreted as − log P (x | g(h)) and Ω(h) as − log P (h). This
MAP inference view of sparse coding and an interesting probabilistic interpretation of sparse coding are further discussed in Section 19.3.
To relate the generative model of sparse coding to ICA, note how the prior
imposes not just sparsity but also independence of the latent variables hi under
P (h), which may help to separate diﬀerent explanatory factors, unlike PCA,
factor analysis or probabilistic PCA, because these rely on a Gaussian prior,
which yields a factorized prior under any rotation of the factors, multiplication
by an orthonormal matrix, as demonstrated in Section 15.6.1.
5

with probability going to 0 as the values increase in magnitude at a rate that is slower than
the Gaussian, i.e., less than quadratic in the log-domain.
456

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

See Section 17.2 about the manifold interpretation of sparse coding.
TODO: relate to and point to Spike-and-slab sparse coding (Goodfellow et al.,
2012) (section?)

15.7

Reconstruction Error as Log-Likelihood

Although traditional auto-encoders (like traditional neural networks) were introduced with an associated training loss, just like for neural networks, that training
loss can generally be given a probabilistic interpretation as a conditional loglikelihood of the original input x, given the reprensentation h.
We have already covered negative log-likelihood as a loss function in general
for feedforward neural networks in Section 6.3.2. Like prediction error for regular
feedforward neural networks, reconstruction error for auto-encoders does not have
to be squared error. When we view the loss as negative log-likelihood, we interpret
the reconstruction error as
L = − log P (x | h)
where h is the representation, which may generally be obtained through an encoder taking x as input.

h = f(x)

f

g

x
L = − log P (x|g(f (x)))
Figure 15.5: The computational graph of an auto-encoder, which is trained to maximize
the probability assigned by the decoder g to the data point x, given the output of the
encoder h = f (x). The training objective is thus L = − log P (x | g(f(x))), which ends
up being squared reconstruction error if we choose a Gaussian reconstruction distribution
with mean g(f (x)), and cross-entropy if we choose a factorized Bernoulli reconstruction
distribution with means g(f(x)).
457

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

An advantage of this view is that it immediately tells us what kind of loss
function one should use depending on the nature of the input. If the input is realvalued and unbounded, then squared error is a reasonable choice of reconstruction
error, and corresponds to P (x | h) being Normal. If the input is a vector of
bits, then cross-entropy is a more reasonable choice, and corresponds to P (x |
Q
h) = i P (x i | h) with xi | h being Bernoulli-distributed. We then view the
decoder g(h) as computing the parameters of the reconstruction distribution, i.e.,
P (x | h) = P (x | g(h)).
Another advantage of this view is that we can think about the training of
the decoder as estimating the conditional distribution P (x | h), which comes
handy in the probabilistic interpretation of denoising auto-encoders, allowing us
to talk about the distribution P (x) explicitly or implicitly represented by the
auto-encoder (see Sections 15.9, 20.9.3 and 20.10 for more details). In the same
spirit, we can rethink the notion of encoder from a simple function to a conditional
distribution Q(h | x), with a special case being when Q(h | x) is a Dirac at
some particular value. Equivalently, thinking about the encoder as a distribution
corresponds to injecting noise inside the auto-encoder. This view is developed
further in Sections 20.9.3 and 20.11.

15.8

Sparse Representations

Sparse auto-encoders are auto-encoders which learn a sparse representation, i.e.,
one whose elements are often either zero or close to zero. Sparse coding was introduced in Section 15.6.2 as a linear factor model in which the prior P (h) on
the representation h = f (x) encourages values at or near 0. In Section 15.8.1, we
see how ordinary auto-encoders can be prevented from learning a useless identity
transformation by using a sparsity penalty rather than a bottleneck. The main
diﬀerence between a sparse auto-encoder and sparse coding is that sparse coding has no explicit parametric encoder, whereas sparse auto-encoders have one.
The “encoder” of sparse coding is the algorithm that performs the approximate
inference, i.e., looks for
||x − (b + W h)|| 2
h (x) = arg max log P (h | x) = arg min
− log P (h)
σ2
h
h
∗

(15.7)

where σ2 is a reconstruction variance parameter (which should equal the average
squared reconstruction error 6), and P (h) is a “sparse” prior that puts more prob6

but can be lumped into the regularizer λ which controls the strength of the sparsity prior,
deﬁned in Eq. 15.8, for example.

458

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

ability mass around h = 0, such as the Laplacian prior, with factorized marginals
λ
P (hi ) = e λ|hi|
2

(15.8)

or the Student-t prior, with factorized marginals
P (hi) ∝

1
(1 +

h2i
ν

)

ν+1
2

.

(15.9)

The advantages of such a non-parametric encoder and the sparse coding approach
over sparse auto-encoders are that
1. it can in principle minimize the combination of reconstruction error and
log-prior better than any parametric encoder,
2. it performs what is called explaining away (see Figure 13.8), i.e., it allows
to “choose” some “explanations” (hidden factors) and inhibits the others.
The disadvantages are that
1. computing time for encoding the given input x, i.e., performing inference
(computing the representation h that goes with the given x) can be substantially larger than with a parametric encoder (because an optimization
must be performed for each example x), and
2. the resulting encoder function could be non-smooth and possibly too nonlinear (with two nearby x’s being associated with very diﬀerent h’s), potentially making it more diﬃcult for the downstream layers to properly
generalize.
In Section 15.8.2, we describe PSD (Predictive Sparse Decomposition), which
combines a non-parametric encoder (as in sparse coding, with the representation
obtained via an optimization) and a parametric encoder (like in the sparse autoencoder). Section 15.9 introduces the Denoising Auto-Encoder (DAE), which puts
pressure on the representation by requiring it to extract information about the
underlying distribution and where it concentrates, so as to be able to denoise a
corrupted input. Section 15.10 describes the Contractive Auto-Encoder (CAE),
which optimizes an explicit regularization penalty that aims at making the representation as insensitive as possible to the input, while keeping the information
suﬃcient to reconstruct the training examples.

459

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

15.8.1

Sparse Auto-Encoders

A sparse auto-encoder is simply an auto-encoder whose training criterion involves
a sparsity penalty Ω(h) in addition to the reconstruction error:
L = − log P (x | g(h)) + Ω(h)

(15.10)

where g(h) is the decoder output and typically we have h = f(x), the encoder
output.
We can think of that penalty Ω(h) simply as a regularizer or as a log-prior
on the representations h. For example, the sparsity penalty corresponding to the
Laplace prior ( λ2 e−λ|hi| ) is the absolute value sparsity penalty (see also Eq. 15.8
above):
X
Ω(h) = λ
|h i|
i

− log P (h) =

X

log

i

λ
+ λ|h i| = const + Ω(h)
2

(15.11)

where the constant term depends only of λ and not h (which we typically ignore
in the training criterion because we consider λ as a hyperparameter rather than
a parameter). Similarly (as per Eq. 15.9), the sparsity penalty corresponding to
the Student-t prior (Olshausen and Field, 1997) is
X ν+1
h 2i
Ω(h) =
log(1 + )
(15.12)
2
ν
i

where ν is considered to be a hyperparameter.
The early work on sparse auto-encoders (Ranzato et al., 2007a, 2008) considered various forms of sparsity and proposed a connection between sparsity
regularization and the partition function gradient in energy-based models (see
Section TODO). The idea is that a regularizer such as sparsity makes it diﬃcult
for an auto-encoder to achieve zero reconstruction error everywhere. If we consider reconstruction error as a proxy for energy (unnormalized log-probability of
the data), then minimizing the training set reconstruction error forces the energy
to be low on training examples, while the regularizer prevents it from being low
everywhere. The same role is played by the gradient of the partition function in
energy-based models such as the RBM (Section TODO).
However, the sparsity penalty of sparse auto-encoders does not need to have
a probabilistic interpretation. For example, Goodfellow et al. (2009) successfully
used the following sparsity penalty, which does not try to bring hi all the way
down to 0, but only towards some low target value such as ρ = 0.05.
Ω(h) =
i

X

ρ log hi + (1 − ρ) log(1 − h i)
460

(15.13)

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

where 0 < hi < 1, usually with hi = sigmoid(ai ). This is just the cross-entropy between the Bernoulli distributions with probability p = hi and the target Bernoulli
distribution with probability p = ρ.
One way to achieve actual zeros in h for sparse (and denoising) auto-encoders
was introduced in Glorot et al. (2011c). The idea is to use a half-rectiﬁer (a.k.a.
simply as “rectiﬁer”) or ReLU (Rectiﬁed Linear Unit, introduced in Glorot et al.
(2011b) for deep supervised networks and earlier in Nair and Hinton (2010a) in
the context of RBMs) as the output non-linearity of the encoder. With a prior
that actually pushes the representations to zero (like the absolute value penalty),
one can thus indirectly control the average number of zeros in the representation.
ReLUs were ﬁrst successfully used for deep feedforward networks in Glorot et al.
(2011a), achieving for the ﬁrst time the ability to train fairly deep supervised
networks without the need for unsupervised pre-training, and this turned out to
be an important component in the 2012 object recognition breakthrough with
deep convolutional networks (Krizhevsky et al., 2012b).
Interestingly, the “regularizer” used in sparse auto-encoders does not conform
to the classical interpretation of regularizers as priors on the parameters. That
classical interpretation of the regularizer comes from the MAP (Maximum A
Posteriori) point estimation (see Section 5.5.1) of parameters associated with
the Bayesian view of parameters as random variables and considering the joint
distribution of data x and parameters θ (see Section 5.7):
arg max P (θ | x) = arg max (log P (x | θ) + log P (θ))
θ

θ

where the ﬁrst term on the right is the usual data log-likelihood term and the
second term, the log-prior over parameters, incorporates the preference over particular values of θ.
With regularized auto-encoders such as sparse auto-encoders and contractive
auto-encoders, instead, the regularizer corresponds to a log-prior over the representation, or over latent variables. In the case of sparse auto-encoders, predictive
sparse decomposition and contractive auto-encoders, the regularizer speciﬁes a
preference over functions of the data, rather than over parameters. This makes
such a regularizer data-dependent, unlike the classical parameter log-prior. Specifically, in the case of the sparse auto-encoder, it says that we prefer an encoder
whose output produces values closer to 0. Indirectly (when we marginalize over
the training distribution), this is also indicating a preference over parameters, of
course.

15.8.2

Predictive Sparse Decomposition

TODO: we have too many forward refs to this section. There are 150 lines about
PSD in this section and at least 20 lines of forward references to this section
461

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

in this chapter, some of which are just 100 lines away. Predictive sparse decomposition (PSD) is a variant that combines sparse coding and a parametric
encoder (Kavukcuoglu et al., 2008b), i.e., it has both a parametric encoder and
iterative inference. It has been applied to unsupervised feature learning for object recognition in images and video (Kavukcuoglu et al., 2009, 2010b; Jarrett
et al., 2009a; Farabet et al., 2011), as well as for audio (Henaﬀ et al., 2011). The
representation is considered to be a free variable (possibly a latent variable if we
choose a probabilistic interpretation) and the training criterion combines a sparse
coding criterion with a term that encourages the optimized sparse representation
h (after inference) to be close to the output of the encoder f(x):


L = arg min ||x − g(h)||2 + λ|h|1 + γ||h − f(x)|| 2
(15.14)
h

where f is the encoder and g is the decoder. Like in sparse coding, for each
example x an iterative optimization is performed in order to obtain a representation h. However, because the iterations can be initialized from the output of the
encoder, i.e., with h = f (x), only a few steps (e.g. 10) are necessary to obtain
good results. Simple gradient descent on h has been used by the authors. After h
is settled, both g and f are updated towards minimizing the above criterion. The
ﬁrst two terms are the same as in L1 sparse coding while the third one encourages
f to predict the outcome of the sparse coding optimization, making it a better
choice for the initialization of the iterative optimization. Hence f can be used as
a parametric approximation to the non-parametric encoder implicitly deﬁned by
sparse coding. It is one of the ﬁrst instances of learned approximate inference (see
also Sec. 19.6). Note that this is diﬀerent from separately doing sparse coding
(i.e., training g) and then training an approximate inference mechanism f , since
both the encoder and decoder are trained together to be “compatible” with each
other. Hence the decoder will be learned in such a way that inference will tend
to ﬁnd solutions that can be well approximated by the approximate inference.
TODO: this is probably too much forward reference, when we bring these things
in we can remind people that they resemble PSD, but it doesn’t really help the
reader to say that the thing we are describing now is similar to things they haven’t
seen yet A similar example is the variational auto-encoder, in which the encoder
acts as approximate inference for the decoder, and both are trained jointly (Section 20.9.3). See also Section 20.9.4 for a probabilistic interpretation of PSD in
terms of a variational lower bound on the log-likelihood.
In practical applications of PSD, the iterative optimization is only used during
training, and f is used to compute the learned features. It makes computation
fast at recognition time and also makes it easy to use the trained features f as
initialization (unsupervised pre-training) for the lower layers of a deep net. Like
other unsupervised feature learning schemes, PSD can be stacked greedily, e.g.,
462

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

training a second PSD on top of the features extracted by the ﬁrst one, etc.

15.9

Denoising Auto-Encoders

The Denoising Auto-Encoder (DAE) was ﬁrst proposed (Vincent et al., 2008,
2010) as a means of forcing an auto-encoder to learn to capture the data distribution without an explicit constraint on either the dimension or the sparsity of the
learned representation. It was motivated by the idea that in order to fully capture
a complex distribution, an auto-encoder needs to have at least as many hidden
units as needed by the complexity of that distribution. Hence its dimensionality
should not be restricted to the input dimension.
The principle of the denoising auto-encoder is deceptively simple and illustrated in Figure 15.6: the encoder sees as input a corrupted version of the input,
but the decoder tries to reconstruct the clean uncorrupted input.

h = f(x̃)

g

f
x̃
C(x̃|x)

L = − log P (x|g(f(x̃)))

x

Figure 15.6: The computational graph of a denoising auto-encoder, which is trained to
reconstruct the clean data point x from its corrupted version x̃, i.e., to minimize the loss
L = − log P (x | g(f ( x̃))), where x̃ is a corrupted version of the data example x, obtained
through a given corruption process C(x̃ | x).

Mathematically, and following the notations used in this chapter, this can be
formalized as follows. We introduce a corruption process C(x̃ | x) which represents a conditional distribution over corrupted samples x̃, given a data sample
x. The auto-encoder then learns a reconstruction distribution P (x | x̃) estimated
from training pairs (x, x̃), as follows:
1. Sample a training example x = x from the data generating distribution (the
463

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

training set).
2. Sample a corrupted version x̃ = x̃ from the conditional distribution C(x̃ |
x = x).
3. Use (x, x̃) as a training example for estimating the auto-encoder reconstruction distribution P (x | x̃) = P (x | g(h)) with h the output of encoder f (x̃)
and g(h) the output of the decoder.
Typically we can simply perform gradient-based approximate minimization (such
as minibatch gradient descent) on the negative log-likelihood − log P (x | h), i.e.,
the denoising reconstruction error, using back-propagation to compute gradients,
just like for regular feedforward neural networks (the only diﬀerence being the
corruption of the input and the choice of target output).
We can view this training objective as performing stochastic gradient descent
on the denoising reconstruction error, but where the “noise” now has two sources:
1. the choice of training sample x from the data set, and
2. the random corruption applied to x to obtain x̃.
We can therefore consider that the DAE is performing stochastic gradient
descent on the following expectation:
−Ex∼Q(x) Ex̃∼C(x|x)
log P (x | g(f ( x̃)))
˜
where Q(x) is the training distribution.

464

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

x̃
g(f(x̃)) ≈ E[x|x̃]

x̃
C(x̃|x)
x

Figure 15.7: A denoising auto-encoder is trained to reconstruct the clean data point x
from it corrupted version x̃. In the ﬁgure, we illustrate the corruption process C(x˜ | x)
by a grey circle of equiprobable corruptions, and grey arrow for the corruption process)
acting on examples x (red crosses) lying near a low-dimensional manifold near which
probability concentrates. When the denoising auto-encoder is trained to minimize the
average of squared errors ||g(f (x̃)) − x||2 , the reconstruction g(f(x̃)) estimates E[x | x̃],
which approximately points orthogonally towards the manifold, since it estimates the
center of mass of the clean points x which could have given rise to x̃. The auto-encoder
thus learns a vector ﬁeld g(f(x)) − x (the green arrows) and it turns out that this
∂ log Q(x)
vector ﬁeld estimates the gradient ﬁeld
(up to a multiplicative factor that is the
∂x
average root mean square reconstruction error), where Q is the unknown data generating
distribution.

15.9.1

Learning a Vector Field that Estimates a Gradient Field

As illustrated in Figure 15.7, a very important property of DAEs is that their
training criterion makes the auto-encoder learn a vector ﬁeld (g(f (x)) − x) that
Q(x)
estimates the gradient ﬁeld (or score) ∂ log∂x
, as per Eq. 15.15. A ﬁrst result in
this direction was proven by Vincent (2011a), showing that minimizing squared
reconstruction error in a denoising auto-encoder with Gaussian noise was related
to score matching (Hyvärinen, 2005a), making the denoising criterion a regularized form of score matching called denoising score matching (Kingma and LeCun,
2010a). Score matching is an alternative to maximum likelihood and provides a
consistent estimator. It is discussed further in Section 18.4. The denoising version
465

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

is discussed in Section 18.5.
The connection between denoising auto-encoders and score matching was ﬁrst
made (Vincent, 2011a) in the case where the denoising auto-encoder has a particular parametrization (one hidden layer, sigmoid activation functions on hidden
units, linear reconstruction), in which case the denoising criterion actually corresponds to a regularized form of score matching on a Gaussian RBM (with binomial
hidden units and Gaussian visible units). The connection between ordinary autoencoders and Gaussian RBMs had previously been made by Bengio and Delalleau
(2009), which showed that contrastive divergence training of RBMs was related to
an associated auto-encoder gradient, and later by Swersky (2010), which showed
that non-denoising reconstruction error corresponded to score matching plus a
regularizer.
The fact that the denoising criterion yields an estimator of the score for general encoder/decoder parametrizations has been proven (Alain and Bengio, 2012,
2013) in the case where the corruption and the reconstruction distributions are
Gaussian (and of course x is continuous-valued), i.e., with the squared error denoising error
||g(f (x̃)) − x||2
and corruption
C(x̃ = x̃|x) = N ( x̃; µ = x, Σ = σ2 I)
with noise variance σ 2.

466

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

Figure 15.8: Vector ﬁeld learned by a denoising auto-encoder around a 1-D curved manifold near which the data (orange circles) concentrates in a 2-D space. Each arrow is
proportional to the reconstruction minus input vector of the auto-encoder and points
towards higher probability according to the implicitly estimated probability distribution.
Note that the vector ﬁeld has zeros at both peaks of the estimated density function (on
the data manifolds) and at troughs (local minima) of that density function, e.g., on the
curve that separates diﬀerent arms of the spiral or in the middle of it.

More precisely, the main theorem states that
of

∂ log Q(x)
,
∂x

g(f(x))−x
σ2

is a consistent estimator

where Q(x) is the data generating distribution,
∂ log Q(x)
g(f (x)) − x
→
,
σ2
∂x

(15.15)

so long as f and g have suﬃcient capacity to represent the true score (and assuming that the expected training criterion can be minimized, as usual when proving
consistency associated with a training objective).
Note that in general, there is no guarantee that the reconstruction g(f (x))
minus the input x corresponds to the gradient of something (the estimated score
should be the gradient of the estimated log-density with respect to the input
467

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

x). That is why the early results (Vincent, 2011a) are specialized to particular
parametrizations where g(f (x)) − x is the derivative of something. See a more
general treatment by Kamyshanska and Memisevic (2015).
Although it was intuitively appealing that in order to denoise correctly one
must capture the training distribution, the above consistency result makes it
mathematically very clear in what sense the DAE is capturing the input distribution: it is estimating the gradient of its energy function (i.e., of its log-density),
i.e., learning to point towards more probable (lower energy) conﬁgurations. Figure 15.8 (see details of experiment in Alain and Bengio (2013)) illustrates this.
Note how the norm of reconstruction error (i.e. the norm of the vectors shown in
the ﬁgure) is related to but diﬀerent from the energy (unnormalized log-density)
associated with the estimated model. The energy should be low only where the
probability is high. The reconstruction error (norm of the estimated score vector)
is low where probability is near a peak of probability (or a trough of energy), but
it can also be low at maxima of energy (minima of probability).
Section 20.10 continues the discussion of the relationship between denoising
auto-encoders and probabilistic modeling by showing how one can generate from
the distribution implicitly estimated by a denoising auto-encoder. Whereas (Alain
and Bengio, 2013) generalized the score estimation result of Vincent (2011a) to
arbitrary parametrizations, the result from Bengio et al. (2013b), discussed in
Section 20.10, provides a probabilistic – and in fact generative – interpretation to
every denoising auto-encoder.

15.10

Contractive Auto-Encoders

The Contractive Auto-Encoder or CAE (Rifai et al., 2011a,c) introduces an explicit regularizer on the code h = f (x), encouraging the derivatives of f to be as
small as possible:


 ∂f(x) 2

Ω(h) = 
(15.16)
 ∂x 
F
which is the squared Frobenius norm (sum of squared elements) of the Jacobian
matrix of partial derivatives associated with the encoder function. Whereas the
denoising auto-encoder learns to contract the reconstruction function (the composition of the encoder and decoder), the CAE learns to speciﬁcally contract the
encoder. See Figure 17.13 for a view of how contraction near the data points
makes the auto-encoder capture the manifold structure.
If it weren’t for the opposing force of reconstruction error, which attempts
to make the code h keep all the information necessary to reconstruct training
examples, the CAE penalty would yield a code h that is constant and does not
468

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

depend on the input x. The compromise between these two forces yields an auto∂f (x)
encoder whose derivatives ∂x are tiny in most directions, except those that are
needed to reconstruct training examples, i.e., the directions that are tangent to
the manifold near which data concentrate. Indeed, in order to distinguish (and
thus, reconstruct correctly) two nearby examples on the manifold, one must assign
them a diﬀerent code, i.e., f (x) must vary as x moves from one to the other, i.e.,
in the direction of a tangent to the manifold.

Figure 15.9: Average (over test examples) of the singular value spectrum of the Jacobian
(x)
matrix ∂f
for the encoder f learned by a regular auto-encoder (AE) versus a contractive
∂x
auto-encoder (CAE). This illustrates how the contractive regularizer yields a smaller set
of directions in input space (those corresponding to large singular value of the Jacobian)
which provoke a response in the representation h while the representation remains almost
insensitive for most directions of change in the input.

What is interesting is that this penalty forces more strongly the representation
to be invariant in directions orthogonal to the manifold. This can be seen clearly
by comparing the singular value spectrum of the Jacobian ∂f∂x(x) for diﬀerent autoencoders, as shown in Figure 15.9. We see that the CAE manages to concentrate
the sensitivity of the representation in fewer dimensions than a regular (or sparse)
auto-encoder. Figure 17.3 illustrates tangent vectors obtained by a CAE on the
MNIST digits dataset, showing that the leading tangent vectors correspond to
small deformations such as translation. More impressively, Figure 15.10 shows
tangent vectors learned on 32×32 color (RGB) CIFAR-10 images by a CAE,
469

CHAPTER 15. LINEAR FACTOR MODELS AND AUTO-ENCODERS

compared to the tangent vectors by a non-distributed representation learner (a
mixture of local PCAs).

Figure 15.10: Illustration of tangent vectors (bottom) of the manifold estimated by a
contractive auto-encoder (CAE), at some input point (left, CIFAR-10 image of a dog).
See also Fig. 17.3. Each image on the right corresponds to a tangent vector, either
estimated by a local PCA (equivalent to a Gaussian mixture), top, or by a CAE (bottom).
The tangent vectors are estimated by the leading singular vectors of the Jacobian matrix
∂h of the input-to-code mappiing. Although both local PCA and CAE can capture local
∂x
tangents that are diﬀerent in diﬀerent points, the local PCA does not have enough training
data to meaningful capture good tangent directions, whereas the CAE does (because it
exploits parameter sharing across diﬀerent locations that share a subset of active hidden
units). The CAE tangent directions typically correspond to moving or changing parts of
the object (such as the head or legs), which corresponds to plausible changes in the input
image.

One practical issue with the CAE regularization criterion is that although it
is cheap to compute in the case of a single hidden layer auto-encoder, it becomes
much more expensive in the case of deeper auto-encoders. The strategy followed
by Rifai et al. (2011a) is to separately pre-train each single-layer auto-encoder
stacked to form a deeper auto-encoder. However, a deeper encoder could be
advantageous in spite of the computational overhead, as argued by Schulz and
Behnke (2012).
Another practical issue is that the contraction penalty on the encoder f could
yield useless results if the decoder g would exactly compensate (e.g. by being
scaled up by exactly the same amount as f is scaled down). In Rifai et al.
(2011a), this is compensated by tying the weights of f and g, both being of the
form of an aﬃne transformation followed by a non-linearity (e.g. sigmoid), i.e.,
the weights of g and the transpose of the weights of f.
470

Chapter 16

Representation Learning
What is a good representation? Many answers are possible, and this remains a
question to be further explored in future research. What we propose as answer
in this book is that in general, a good representation is one that makes further
learning tasks easy. In an unsupervised learning setting, this could mean that the
joint distribution of the diﬀerent elements of the representation (e.g., elements of
the representation vector h) is one that is easy to model (e.g., in the extreme,
these elements are marginally independent of each other). But that would not
be enough: a representation that throws away all information (e.g., h = 0 for
all inputs x) is very easy to model but is also useless. Hence we want to learn a
representation that keeps the information (or at least all the relevant information,
in the supervised case) and makes it easy to learn functions of interest from this
representation.
In Chapter 1, we have introduced the notion of representation, the idea that
some representations were more helpful (e.g. to classify objects from images or
phonemes from speech) than others. As argued there, this suggests learning representations in order to “select” the best ones in a systematic way, i.e., by optimizing
a function that maps raw data to its representation, instead of - or in addition
to - handcrafting them. This motivation for learning input features is discussed
in Section 6.6, and is one of the major side-eﬀects of training a feedforward deep
network (treated in Chapter 6), typically via supervised learning, i.e., when one
has access to (input,target) pairs1 , available for some task of interest. In the case
of supervised learning of deep nets, we learn a representation with the objective
of selecting one that is best suited to the task of predicting targets given inputs.
Whereas supervised learning has been the workhorse of recent industrial successes of deep learning, the authors of this book believe that it is likely that a key
1

typically obtained by labeling inputs with some target answer that we wish the computer
would produce

471

CHAPTER 16. REPRESENTATION LEARNING

element of future advances will be unsupervised learning of representations.
So how can we exploit the information in data if we don’t have labeled examples? Or too few? Pure supervised learning with few labeled examples can easily
overﬁt. On the other hand, humans (and other animals) can sometimes learn a
task from just one or very few examples. How is that possible? Clearly they must
rely on previously acquired knowledge, either innate or (more likely the case for
humans) via previous learning experience. Can we discover good representations
purely out of unlabeled examples? (this is treated in the ﬁrst four sections of this
chapter). Can we combine unlabeled examples (which are often easy to obtain)
with labeled examples? (this is semi-supervised learning, Section 16.3). And what
if instead of one task we have many tasks that could share the same representation or parts of it? (this is multi-task learning, discussed in Section 7.12). What
if we have “training tasks” (on which enough labeled examples are available) as
well as “test tasks” (not known at the time of learning the representation, and for
which only very few labeled examples will be provided)? What if the test task is
similar but diﬀerent from the training task? (this is transfer learning and domain
adaptation, discussed in Section 16.2).

16.1

Greedy Layerwise Unsupervised Pre-Training

Unsupervised learning played a key historical role in the revival of deep neural
networks, allowing for the ﬁrst time to train a deep supervised network. We
call this procedure unsupervised pre-training, or more precisely, greedy layer-wise
unsupervised pre-training, and it is the topic of this section.
This recipe relies on a one-layer representation learning algorithm such as
those introduced in this part of the book, i.e., the auto-encoders (Chapter 15)
and the RBM (Section 20.2). Each layer is pre-trained by unsupervised learning,
taking the output of the previous layer and producing as output a new representation of the data, whose distribution (or its relation to other variables such as
categories to predict) is hopefully simpler.
Greedy layerwise unsupervised pre-training was introduced in Hinton et al.
(2006); Hinton and Salakhutdinov (2006); Bengio et al. (2007a); Ranzato et al.
(2007a). These papers are generally credited with founding the renewed interest in
learning deep models as it provided a means of initializing subsequent supervised
training and often led to notable performance gains when compared to models
trained without unsupervised pretraining, at least for the small kinds of datasets
(like the 60,000 examples of MNIST) used in these experiments.
It is called layerwise because it proceeds one layer at a time, training the
k-th layer while keeping the previous ones ﬁxed. It is called unsupervised because
each layer is trained with an unsupervised representation learning algorithm. It
472

CHAPTER 16. REPRESENTATION LEARNING

is called greedy because the diﬀerent layers are not jointly trained with respect
to a global training objective, which could make the procedure sub-optimal. In
particular, the lower layers (which are ﬁrst trained) are not adapted after the
upper layers are introduced. However it is also called pre-training, because it is
supposed to be only a ﬁrst step before a joint training algorithm is applied to ﬁnetune all the layers together with respect to a criterion of interest. In the context
of a supervised learning task, it can be viewed as a regularizer (see Chapter 7)
and a sophisticated form of parameter initialization.
When we refer to pre-training we will be referring to a speciﬁc protocol with
two main phases of training: the pretraining phase and the ﬁne-tuning phase.
No matter what kind of unsupervised learning algorithm or what model type you
employ, in the vast majority of cases, the overall training scheme is nearly the
same. While the choice of unsupervised learning algorithm will obviously impact
the details, in the abstract, most applications of unsupervised pre-training follows
this basic protocol.
As outlined in Algorithm 16.1, in the pretraining phase, the layers of the
model are trained, in order, in an unsupervised way on their input, beginning
with the bottom layer, i.e. the one in direct contact with the input data. Next,
the second lowest layer is trained taking the activations of the ﬁrst layer hidden
units as input for unsupervised training. Pretraining proceeds in this fashion,
from bottom to top, with each layer training on the “output” or activations of
the hidden units of the layer below. After the last layer is pretrained, a supervised
layer is put on top, and all the layers are jointly trained with respect to the overall
supervised training criterion. In other words, the pre-training was only used to
initialize a deep supervised neural network (which could be a convolutional neural
network (Ranzato et al., 2007a)). This is illustrated in Figure 16.1.
However, greedy layerwise unsupervised pre-training can also be used as initialization for other unsupervised learning algorithms, such as deep auto-encoders (Hinton and Salakhutdinov, 2006), deep belief networks (Hinton et al., 2006) (Section 20.4), or deep Boltzmann machines (Salakhutdinov and Hinton, 2009a) (Section 20.5).
As discussed in Section 8.7.4, it is also possible to have greedy layerwise supervised pre-training, to help optimize deep supervised networks. This builds on
the premise that training a shallow network is easier than training a deep one,
which seems to have been validated in several contexts (Erhan et al., 2010).

16.1.1

Why Does Unsupervised Pre-Training Work?

What has been observed on several datasets starting in 2006 (Hinton et al., 2006;
Bengio et al., 2007a; Ranzato et al., 2007a) is that greedy layer-wise unsupervised
pre-training can yield substantial improvements in test error for classiﬁcation
473

CHAPTER 16. REPRESENTATION LEARNING

Algorithm 16.1 Greedy layer-wise unsupervised pre-training protocol.
Given the following: Unsupervised feature learner L, which takes a training set
D of examples and returns an encoder or feature function f = L(D). The raw
input data is X, with one row per example and f (X) is the dataset used by the
second level unsupervised feature learner. In the case ﬁne-tuning is performed, we
use a learner T which takes an initial function f, input examples X (and in the
supervised ﬁne-tuning case, associated targets Y ), and returns a tuned function.
The number of stages is M .
D(0) = X
f ← Identity function
for k = 1 . . . , M do
f (k) = L(D)
f ← f (k) ◦ f
end for
if ﬁne-tuning then
f ← T (f, X, Y )
end if
Return f

Figure 16.1: Illustration of the greedy layer-wise unsupervised pre-training scheme, in
the case of a network with 3 hidden layers. The protocol proceeds in 4 phases (one per
hidden layer, plus the ﬁnal supervised ﬁne-tuning phase), from left to right. For the
unsupervised steps, each layer (darker grey) is trained to learn a better representation of
the output of the previously trained layer (initially, the raw input). These representations
learned by unsupervised learning form the initialization of a deep supervised net, which
is then trained (ﬁne-tuned) as usual (last phase, right), with all parameters being free to
change (darker grey).
474

CHAPTER 16. REPRESENTATION LEARNING

tasks. Later work suggested that the improvements were less marked (or not even
visible) when very large labeled datasets are available, although the boundary
between the two behaviors remains to be clariﬁed, i.e., it may not just be an issue
of number of labeled examples but also how this relates to the complexity of the
function to be learned.
A question that thus naturally arises is the following: why and when does
unsupervised pre-training work? Although previous studies have mostly focused
on the case when the ﬁnal task is supervised (with supervised ﬁne-tuning), it is
also interesting to keep in mind that one gets improvements in terms of both
training and test performance in the case of unsupervised ﬁne-tuning, e.g., when
training deep auto-encoders (Hinton and Salakhutdinov, 2006).
This “why does it work” question is at the center of the paper by Erhan et al.
(2010), and their experiments focused on the supervised ﬁne-tuning case. They
consider diﬀerent machine learning hypotheses to explain the results observed,
and attempted to conﬁrm those via experiments. We summarize some of this
investigation here.
First of all, they studied the trajectories of neural networks during supervised
ﬁne-tuning, and evaluated how diﬀerent they were depending on initial conditions,
i.e., due to random initialization or due to performing unsupervised pre-training
or not. The main result is illustrated and discussed in Figures 16.2 and 16.2.
Note that it would not make sense to plot the evolution of parameters of these
networks directly, because the same input-to-output function can be represented
by diﬀerent parameter values (e.g., by relabeling the hidden units). Instead,
this work plots the trajectories in function space, by considering the output of a
network (the class probability predictions) for a given set of test examples as a
proxy for the function computed. By concatenating all these outputs (over say
1000 examples) and doing dimensionality reduction on these vectors, we obtain
the kinds of plots illustrated in the ﬁgure.
The main conclusions of these kinds of plots are the following:
1. Each training trajectory goes to a diﬀerent place, i.e., diﬀerent trajectories
do not converge to the same place. These “places” might be in the vicinity of
a local minimum or as we understand it better now (Dauphin et al., 2014)
these are more likely to be an “apparent local minimum” in the region
of ﬂat derivatives near a saddle point. This suggests that the number of
these apparent local minima is huge, and this also is in agreement with
theory (Dauphin et al., 2014; Choromanska et al., 2014).
2. Depending on whether we initialize with unsupervised pre-training or not,
very diﬀerent functions (in function space) are obtained, covering regions
that do not overlap. Hence there is a qualitative eﬀect due to unsupervised
475

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.2: Illustrations of the trajectories of diﬀerent neural networks in function space
(not parameter space, to avoid the issue of many-to-one mapping from parameter vector
to function), with diﬀerent random initializations and with or without unsupervised pretraining. Each plus or diamond point corresponds to a diﬀerent neural network, at a
particular time during its training trajectory, with the function it computes projected to
2-D by t-SNE (van der Maaten and Hinton, 2008a) (this ﬁgure) or by Isomap (Tenenbaum
et al., 2000) (Figure 16.3). TODO: should the t be in math mode? Color indicates the
number of training epochs. What we see is that no two networks converge to the same
function (so a large number of apparent local minima seems to exist), and that networks
initialized with pre-training learn very diﬀerent functions, in a region of function space
that does not overlap at all with those learned by networks without pre-training. Such
curves were introduced by Erhan et al. (2010) and are reproduced here with permission.

476

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.3: See Figure 16.2’s caption.
This ﬁgure only diﬀers in the use of
Isomap (Tenenbaum et al., 2000) rather than t-SNE (van der Maaten and Hinton, 2008b)
for dimensionality reduction. Note that Isomap tries to preserve global relative distances
(and hence volumes), whereas t-SNE only cares about preserving local geometry and
neighborhood relationships. We see with the Isomap dimensionality reduction that the
volume in function space occupied by the networks with pre-training is much smaller (in
fact that volume gets reduced rather than increased, during training), suggesting that
the set of solutions enjoy smaller variance, which would be consistent with the observed
improvements in generalization error. Such curves were introduced by Erhan et al. (2010)
and are reproduced here with permission.

477

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.4: Histograms presenting the test errors obtained on MNIST using denoising
auto-encoder models trained with or without pre-training (400 diﬀerent initializations
each). Left: 1 hidden layer. Right: 4 hidden layers. We see that the advantage brought
by pre-training increases with depth, both in terms of mean error and in terms of the
variance of the error (w.r.t. random initialization).

TODO: ﬁgure credit saying these came from Erhan 2010....
pre-training.
3. With unsupervised pre-training, the region of space covered by the solutions
associated with diﬀerent initializations shrinks as we consider more training
iterations, whereas it grows without unsupervised pre-training. This is only
apparent in the visualization of Figure 16.3, which attempts to preserve
volume. A larger region is bad for generalization (because not all these
functions can be the right one together), yielding higher variance. This is
consistent with the better generalization observed with unsupervised pretraining.
Another interesting eﬀect is that the advantage of pre-training seems to increase with depth, as illustrated in Figure 16.4, with both the mean and the
variance of the error decreasing more for deeper networks.
An important question is whether the advantage brought by pre-training can
be seen as a form of regularizer (which could help test error but hurt training error)
or simply a way to ﬁnd a better minimizer of training error (e.g., by initializing
near a better minimum of training error). The experiments suggest pre-training
actually acts as a regularizer, i.e., hurting training error at least in some cases
(with deeper networks). So if it also helps optimization, it is only because it
initializes closer to a good solution from the point of view of generalization, not
necessarily from the point of view of the training set.
478

CHAPTER 16. REPRESENTATION LEARNING

How could unsupervised pre-training act as regularizer? Simply by imposing
an extra constraint: the learned representations should not only be consistent
with better predicting outputs y but they should also be consistent with better
capturing the variations in the input x, i.e., modeling P (x). This is associated
implicitly with a prior, i.e., that P (y|x) and P (x) share structure, i.e., that
learning about P (x) can help to generalize better on P (y | x). Obviously this
needs not be the case in general, e.g., if y is an eﬀect of x. However, if y is a cause
of x, then we would expect this a priori assumption to be correct, as discussed at
greater length in Section 16.4 in the context of semi-supervised learning.
A disadvantage of unsupervised pre-training is that it is diﬃcult to choose
the capacity hyperparameters (such as when to stop training) for the pre-training
phases. An expensive option is to try many diﬀerent values of these hyperparameters and choose the one which gives the best supervised learning error after
ﬁne-tuning. Another potential disadvantage is that unsupervised pre-training
may require larger representations than what would be necessarily strictly for the
task at hand, since presumably, y is only one of the factors that explain x.
Today, as many deep learning researchers and practitioners have moved to
working with very large labeled datasets, unsupervised pre-training has become
less popular in favor of other forms of regularization such as dropout – to be
discussed in section 7.11. Nevertheless, unsupervised pre-training remains an
important tool in the deep learning toolbox and should particularly be considered
when the number of labeled examples is low, such as in the semi-supervised,
domain adaptation and transfer learning settings, discussed next.

16.2

Transfer Learning and Domain Adaptation

Transfer learning and domain adaptation refer to the situation where what has
been learned in one setting (i.e., distribution P 1 ) is exploited to improve generalization in another setting (say distribution P 2 ).
In the case of transfer learning, we consider that the task is diﬀerent but many
of the factors that explain the variations in P1 are relevant to the variations that
need to be captured for learning P2 . This is typically understood in a supervised
learning context, where the input is the same but the target may be of a diﬀerent
nature, e.g., learn about visual categories that are diﬀerent in the ﬁrst and the
second setting. If there is a lot more data in the ﬁrst setting (sampled from P1 ),
then that may help to learn representations that are useful to quickly generalize
when examples of P2 are drawn. For example, many visual categories share lowlevel notions of edges and visual shapes, the eﬀects of geometric changes, changes
in lighting, etc. In general, transfer learning, multi-task learning (Section 7.12),
and domain adaptation can be achieved via representation learning when there
479

CHAPTER 16. REPRESENTATION LEARNING

exist features that would be useful for the diﬀerent settings or tasks, i.e., there
are shared underlying factors. This is illustrated in Figure 7.6, with shared lower
layers and task-dependent upper layers.
However, sometimes, what is shared among the diﬀerent tasks is not the semantics of the input but the semantics of the output, or maybe the input needs
to be treated diﬀerently (e.g., consider user adaptation or speaker adaptation).
In that case, it makes more sense to share the upper layers (near the output)
of the neural network, and have a task-speciﬁc pre-processing, as illustrated in
Figure 16.5.

Y

selection switch

h2

h1

X1

X2

h3

X3

Figure 16.5: Example of architecture for multi-task or transfer learning when the output
variable Y has the same semantics for all tasks while the input variable X has a diﬀerent
meaning (and possibly even a diﬀerent dimension) for each task (or, for example, each
user), called X 1, X 2 and X3 for three tasks in the ﬁgure. The lower levels (up to the
selection switch) are task-speciﬁc, while the upper levels are shared. The lower levels
learn to translate their task-speciﬁc input into a generic set of features.

In the related case of domain adaptation, we consider that the task (and the
optimal input-to-output mapping) is the same but the input distribution is slightly
diﬀerent. For example, if we predict sentiment (positive or negative judgement)
associated with textual comments posted on the web, the ﬁrst setting may refer to
480

CHAPTER 16. REPRESENTATION LEARNING

consumer comments about books, videos and music, while the second setting may
refer to televisions or other products. One can imagine that there is an underlying
function that tells whether any statement is positive, neutral or negative, but
of course the vocabulary, style, accent, may vary from one domain to another,
making it more diﬃcult to generalize across domains. Simple unsupervised pretraining (with denoising auto-encoders) has been found to be very successful for
sentiment analysis with domain adaptation (Glorot et al., 2011c).
A related problem is that of concept drift, which we can view as a form of
transfer learning due to gradual changes in the data distribution over time. Both
concept drift and transfer learning can be viewed as particular forms of multi-task
learning (Section 7.12). Whereas multi-task learning is typically considered in
the context of supervised learning, the more general notion of transfer learning is
applicable for unsupervised learning and reinforcement learning as well. Figure 7.6
illustrates an architecture in which diﬀerent tasks share underlying features or
factors, taken from a larger pool that explain the variations in the input.
In all of these cases, the objective is to take advantage of data from a ﬁrst
setting to extract information that may be useful when learning or even when
directly making predictions in the second setting. One of the potential advantages
of representation learning for such generalization challenges, and especially of
deep representation learning, is that it may considerably help to generalize by
extracting and disentangling a set of explanatory factors from data of the ﬁrst
setting, some of which may be relevant to the second setting. In the case of
object recognition from an image, many of the factors of variation that explain
visual categories in natural images remain the same when we move from one set
of categories to another.
This discussion raises a very interesting and important question which is one
of the core questions of this book: what is a good representation? Is it possible to
learn representations that disentangle the underlying factors of variation? This
theme is further explored at the end of this chapter (Section 16.4 and beyond). We
claim that learning the most abstract features helps to maximize our chances of
success in transfer learning, domain adaptation, or concept drift. More abstract
features are more general and more likely to be close to the underlying causal
factor, i.e., be relevant over many domains, many categories, and many time
periods.
A good example of the success of unsupervised deep learning for transfer
learning is its success in two competitions that took place in 2011, with results
presented at ICML 2011 (and IJCNN 2011) in one case (Mesnil et al., 2011) (the
Transfer Learning Challenge, http://www.causality.inf.ethz.ch/unsupervised-learning.php)
and at NIPS 2011 (Goodfellow et al., 2011) in the other case (the Transfer Learning Challenge that was held as part of the NIPS’2011 workshop on Challenges in
481

CHAPTER 16. REPRESENTATION LEARNING

learning hierarchical models, https://sites.google.com/site/nips2011workshop/transfer-lea
In the ﬁrst of these competitions, the experimental setup is the following.
Each participant is ﬁrst given a dataset from the ﬁrst setting (from distribution
P1 ), basically illustrating examples of some set of categories. The participants
must use this to learn a good feature space (mapping the raw input to some
representation), such that when we apply this learned transformation to inputs
from the transfer setting (distribution P 2 ), a linear classiﬁer can be trained and
generalize well from very few labeled examples. Figure 16.6 illustrates one of the
most striking results: as we consider deeper and deeper representations (learned
in a purely unsupervised way from data of the ﬁrst setting P 1 ), the learning curve
on the new categories of the second (transfer) setting P2 becomes much better, i.e.,
less labeled examples of the transfer tasks are necessary to achieve the apparently
asymptotic generalization performance.
An extreme form of transfer learning is one-shot learning or even zero-shot
learning or zero-data learning, where one or even zero example of the new task
are given.
One-shot learning (Fei-Fei et al., 2006) is possible because, in the learned
representation, the new task corresponds to a very simple region, such as a balllike region or the region around a corner of the space (in a high dimensional
space, there are exponentially many corners). This works to the extent that the
factors of variation corresponding to these invariances have been cleanly separated
from other factors, in the learned representation space, and we have somehow
learned which factors do and do not matter when discriminating objects of certain
categories.
Zero-data learning (Larochelle et al., 2008) and zero-shot learning (Richard Socher
and Ng, 2013) are only possible because additional information has been exploited
during training that provides representations of the “task” or “context”, helping
the learner ﬁgure out what is expected, even though no example of the new task
has ever been seen. For example, in a multi-task learning setting, if each task
is associated with a set of features, i.e., a distributed representation (that is always provided as an extra input, in addition to the ordinary input associated
with the task), then one can generalize to new tasks based on the similarity between the new task and the old tasks, as illustrated in Figure 16.7. One learns
a parametrized function from inputs to outputs, parametrized by the task representation. In the case of zero-shot learning (Richard Socher and Ng, 2013), the
“task” is a representation of a semantic object (such as a word), and its representation has already been learned from data relating diﬀerent semantic objects
together (such as natural language data, relating words together). On the other
hand, for some of the tasks (e.g., words) one has data associating the variables
of interest (e.g., words, pixels in images). Thus one can generalize and associate
482

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.6: Results obtained on the Sylvester validation set (Transfer Learning Challenge). From left to right and top to bottom, respectively 0, 1, 2, 3, and 4 pre-trained
layers. Horizontal axis is logarithm of number of labeled training examples on transfer
setting (test task). Vertical axis is Area Under the Curve, which reﬂects classiﬁcation
accuracy. With deeper representations (learned unsupervised), the learning curves considerably improve, requiring fewer labeled examples to achieve the best generalization.
483

CHAPTER 16. REPRESENTATION LEARNING

output f t (x)

task speciﬁcation t
input x
Figure 16.7: Figure illustrating how zero-data or zero-shot learning is possible. The trick
is that the new context or task on which no example is given but on which we want a
prediction is represented (with an input t), e.g., with a set of features, i.e., a distributed
representation, and that representation is used by the predictor f t (x). If t was a one-hot
vector for each task, then it would not be possible to generalize to a new task, but with
a distributed representation the learner can beneﬁt from the meaning of the individual
task features (as they inﬂuence the relationship between inputs x and targets y, say),
learned on other tasks for which examples are available.

images to words for which no labeled images were previously shown to the learner.
A similar phenomenon happens in machine translation (Klementiev et al., 2012;
Mikolov et al., 2013; Gouws et al., 2014): we have words in one language, and
the relationships between words can be learned from unilingual corpora; on the
other hand, we have translated sentences which relate words in one language with
words in the other. Even though we may not have labeled examples translating
word A in language X to word B in language Y, we can generalize and guess a
translation for word A because we have learned a distributed representation for
words in language X, a distributed representation for words in language Y, and
created a link (possibly two-way) relating the two spaces, via translation examples. Note that this transfer will be most successful if all three ingredients (the
two representations and the relations between them) are learned jointly.

484

CHAPTER 16. REPRESENTATION LEARNING

X

Y
Figure 16.8: Transfer learning between two domains corresponds to zero-shot learning.
A ﬁrst set of data (dashed arrows) can be used to relate examples in one domain (top
left, X) and ﬁx a relationship between their representations, a second set of data (dotted
arrows) can be used to similarly relate examples and their representation in the other
domain (bottom right, Y ), while a third dataset (full large arrows) anchors the two
representations together, with examples consisting of pairs (x, y) taken from the two
domains. In this way, one can for example associate an image to a word, even if no
images of that word were ever presented, simply because word-representations (top) and
image-representations (bottom) have been learned jointly with a two-way relationship
between them.

This is illustrated in Figure 16.8, where we see that zero-shot learning is a
particular form of transfer learning. The same principle explains how one can
perform multi-modal learning, capturing a representation in one modality, a representation in the other, and the relationship (in general a joint distribution)
between pairs (x, y) consisting of one observation x in one modality and another
observation y in the other modality (Srivastava and Salakhutdinov, 2012). By
learning all three sets of parameters (from x to its representation, from y to its
485

CHAPTER 16. REPRESENTATION LEARNING

representation, and the relationship between the two representation), concepts in
one map are anchored in the other, and vice-versa, allowing one to meaningfully
generalize to new pairs.

16.3

Semi-Supervised Learning

As discussed in Section 16.1.1 on the advantages of unsupervised pre-training, unsupervised learning can have a regularization eﬀect in the context of supervised
learning. This ﬁts in the more general category of combining unlabeled examples
with unknown distribution P (x) with labeled examples (x, y), with the objective
of estimating P (y | x). Exploiting unlabeled examples to improve performance
on a labeled set is the driving idea behind semi-supervised learning (Chapelle
et al., 2006). For example, one can use unsupervised learning to map X into a
representation (also called embedding) such that two examples x 1 and x2 that
belong to the same cluster (or are reachable through a short path going through
neighboring examples in the training set) end up having nearby embeddings. One
can then use supervised learning (e.g., a linear classiﬁer) in that new space and
achieve better generalization in many cases (Belkin and Niyogi, 2002; Chapelle
et al., 2003). A long-standing variant of this approach is the application of Principal Components Analysis as a pre-processing step before applying a classiﬁer
(on the projected data). In these models, the data is ﬁrst transformed in a new
representation using unsupervised learning, and a supervised classiﬁer is stacked
on top, learning to map the data in this new representation into class predictions.
Instead of having separate unsupervised and supervised components in the
model, one can consider models in which P (x) (or P (x, y)) and P (y | x) share
parameters (or whose parameters are connected in some way), and one can tradeoﬀ the supervised criterion − log P (y | x) with the unsupervised or generative one
(− log P (x) or − log P (x, y)). It can then be seen that the generative criterion
corresponds to a particular form of prior (Lasserre et al., 2006), namely that the
structure of P (x) is connected to the structure of P (y | x) in a way that is captured by the shared parametrization. By controlling how much of the generative
criterion is included in the total criterion, one can ﬁnd a better trade-oﬀ than
with a purely generative or a purely discriminative training criterion (Lasserre
et al., 2006; Larochelle and Bengio, 2008b).
In the context of deep architectures, a very interesting application of these
ideas involves adding an unsupervised embedding criterion at each layer (or only
one intermediate layer) to a traditional supervised criterion (Weston et al., 2008).
This has been shown to be a powerful semi-supervised learning strategy, and is
an alternative to the unsupervised pre-training approach described earlier in this
chapter, which also combine unsupervised learning with supervised learning.
486

CHAPTER 16. REPRESENTATION LEARNING

In the context of scarcity of labeled data (and abundance of unlabeled data),
deep architectures have shown promise as well. Salakhutdinov and Hinton (2008)
describe a method for learning the covariance matrix of a Gaussian Process, in
which the usage of unlabeled examples for modeling P (x) improves P (y | x)
quite signiﬁcantly. Note that such a result is to be expected: with few labeled
samples, modeling P (x) usually helps, as argued below (Section 16.4). These
results show that even in the context of abundant labeled data, unsupervised pretraining can have a pronounced positive eﬀect on generalization: a somewhat
surprising conclusion.

16.4

Semi-Supervised Learning and Disentangling Underlying Causal Factors

What we put forward as a hypothesis, going a bit further, is that an ideal representation is one that disentangles the underlying causal factors of variation that
generated the observed data. Note that this may be diﬀerent from “easy to model”,
but we further assume that for most problems of interest, these two properties
coincide: once we “understand” the underlying explanations for what we observe,
it generally becomes easy to predict one thing from others.
A very basic question is whether unsupervised learning on input variables x
can yield representations that are useful when later trying to learn to predict some
target variable y, given x. More generally, when does semi-supervised learning
work? See also Section 16.3 for an earlier discussion.
It turns out that the answer to this question is very diﬀerent dependent on the
underlying relationship between x and y. Put diﬀerently, the question is whether
P (y | x), seen as a function of x has anything to do with P (x). If not, then
unsupervised learning of P (x) can be of no help to learn P (y | x). Consider
for example the case where P (x) is uniformly distributed and E[y | x] is some
function of interest. Clearly, observing x alone gives us no information about
P (y | x). As a better case, consider the situation where x arises from a mixture,
with one mixture component per value of y, as illustrated in Figure 16.9. If the
mixture components are well separated, then modeling P (x) tells us precisely
where each component is, and a single labeled example of each example will then
be enough to perfectly learn P (y | x). But more generally, what could make
P (y | x) and P (x) tied together?
If y is closely associated with one of the causal factors of x, then, as ﬁrst argued
by Janzing et al. (2012), P (x) and P (y | x) will be strongly tied, and unsupervised
representation learning that tries to disentangle the underlying factors of variation
is likely to be useful as a semi-supervised learning strategy.
Consider the assumption that y is one of the causal factors of x, and let h
487

CHAPTER 16. REPRESENTATION LEARNING

P (x)

y=1

y=2

y=2

x
Figure 16.9: Example of a density over x that is a mixture over three components. The
component identity is an underlying explanatory factor, y. Because the mixture components (e.g., natural object classes in image data) are statistically salient, just modeling
P (x) in an unsupervised way with no labeled example already reveals the factor y.

represent all those factors. Then the true generative process can be conceived as
structured according to this directed graphical model, with h as the parent of x:
P (h, x) = P (x | h)P (h).
As a consequence, the data has marginal probability
Z
P (x) = P (x | h)p(h)dh
or, in the discrete case (like in the mixture example above):
X
P (x) =
P (x | h)P (h).
h

From this straightforward observation, we conclude that the best possible model
of x (from a generalization point of view) is the one that uncovers the above
“true” structure, with h as a latent variable that explains the observed variations
in x. The “ideal” representation learning discussed above should thus recover
these latent factors. If y is one of them (or closely related to one of them), then
it will be very easy to learn to predict y from such a representation. We also
see that the conditional distribution of y given x is tied by Bayes rule to the
components in the above equation:
P (y | x) =

P (x | y)P (y)
.
P (x)
488

CHAPTER 16. REPRESENTATION LEARNING

Thus the marginal P (x) is intimately tied to the conditional P (y | x) and knowledge of the structure of the former should be helpful to learn the latter, i.e.,
semi-supervised learning works. Furthermore, not knowing which of the factors
in h will be the one of the interest, say y = hi , an unsupervised learner should
learn a representation that disentangles all the generative factors h j from each
other, then making it easy to predict y from h.
In addition, as pointed out by Janzing et al. (2012), if the true generative
process has x as an eﬀect and y as a cause, then modeling P (x | y) is robust to
changes in P (y). If the cause-eﬀect relationship was reversed, it would not be true,
since by Bayes rule, P (x | y) would be sensitive to changes in P (y). Very often,
when we consider changes in distribution due to diﬀerent domains, temporal nonstationarity, or changes in the nature of the task, the causal mechanisms remain
invariant (“the laws of the universe are constant”) whereas what changes are the
marginal distribution over the underlying causes (or what factors are linked to
our particular task). Hence, better generalization and robustness to all kinds of
changes can be expected via learning a generative model that attempts to recover
the causal factors h and P (x | h).

16.5

Assumption of Underlying Factors and Distributed
Representation

A very basic notion that comes out of the above discussion and of the notion
of “disentangled factors” is the very idea that there are underlying factors that
generate the observed data. It is a core assumption behind most neural network
and deep learning research, more precisely relying on the notion of distributed
representation.
What we call a distributed representation is one which can express an exponentially large number of concepts by allowing to compose the activation of
many features. An example of distributed representation is a vector of n binary
features, which can take 2 n conﬁgurations, each potentially corresponding to a
diﬀerent region in input space. This can be compared with a symbolic representation, where the input is associated with a single symbol or category. If there
are n symbols in the dictionary, one can imagine n feature detectors, each corresponding to the detection of the presence of the associated category. In that case
only n diﬀerent conﬁgurations of the representation-space are possible, carving n
diﬀerent regions in input space. Such a symbolic representation is also called a
one-hot representation, since it can be captured by a binary vector with n bits
that are mutually exclusive (only one of them can be active). These ideas are
developed further in the next section.
Examples of learning algorithms based on non-distributed representations in489

CHAPTER 16. REPRESENTATION LEARNING

clude:
• Clustering methods, including the k-means algorithm: only one cluster
“wins” the competition.
• k-nearest neighbors algorithms: only one template or prototype example is
associated with a given input.
• Decision trees: only one leaf (and the nodes on the path from root to leaf)
is activated when an input is given.
• Gaussian mixtures and mixtures of experts: the templates (cluster centers)
or experts are now associated with a degree of activation, which makes the
posterior probability of components (or experts) given input look more like
a distributed representation. However, as discussed in the next section,
these models still suﬀer from a poor statistical scaling behavior compared
to those based on distributed representations (such as products of experts
and RBMs).
• Kernel machines with a Gaussian kernel (or other similarly local kernel): although the degree of activtion of each “support vector” or template example
is now continuous-valued, the same issue arises as with Gaussian mixtures.
• Language or translation models based on n-grams: the set of contexts (sequences of symbols) is partitioned according to a tree structure of suﬃxes
(e.g. a leaf may correspond to the last two words being w1 and w2 ), and
separate parameters are estimated for each leaf of the tree (with some sharing being possible of parameters associated with internal nodes, between
the leaves of the sub-tree rooted at the same internal node).

490

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.10: Illustration of how a learning algorithm based on a non-distributed representation breaks up the input space into regions, with a separate set of parameters for
each region. For example, a clustering algorithm or a 1-nearest-neighbor algorithm associates one template (colored X) to each region. This is also true of decision trees, mixture
models, and kernel machines with a local (e.g., Gaussian) kernel. In the latter algorithms,
the output is not constant by parts but instead interpolates between neighboring regions,
but the relationship between the number of parameters (or examples) and the number
of regions they can deﬁne remains linear. The advantage is that a diﬀerent answer (e.g.,
density function, predicted output, etc.) can be independently chosen for each region.
The disadvantage is that there is no generalization to new regions, except by extending
the answer for which there is data, exploiting solely a smoothness prior. It makes it
diﬃcult to learn a complicated function, with more ups and downs than the available
number of examples. Contrast this with a distributed representation, Figure 16.11.

An important related concept that distinguishes a distributed representation
from a symbolic one is that generalization arises due to shared attributes between
diﬀerent concepts. As pure symbols, “ tt cat” and “dog” are as far from each
other as any other two symbols. However, if one associates them with a meaningful
distributed representation, then many of the things that can be said about cats
can generalize to dogs and vice-versa. This is what allows neural language models
to generalize so well (Section 12.4). Distributed representations induce a rich
similarity space, in which semantically close concepts (or inputs) are close in
distance, a property that is absent from purely symbolic representations. Of
491

CHAPTER 16. REPRESENTATION LEARNING

course, one would get a distributed representation if one would associated multiple
symbolic attributes to each symbol.

Figure 16.11: Illustration of how a learning algorithm based on a distributed representation breaks up the input space into regions, with exponentially more regions than
parameters. Instead of a single partition (as in the non-distributed case, Figure 16.10),
we have many partitions, one per “feature”, and all their possible intersections. In the
example of the ﬁgure, there are 3 binary features C1, C2, and C3, each corresponding to
partitioning the input space in two regions according to a hyperplane, i.e., each is a linear
classiﬁer. Each possible intersection of these half-planes forms a region, i.e., each region
corresponds to a conﬁguration of the bits specifying whether each feature is 0 or 1, on
which side of their hyperplane is the input falling. If the input space is large enough, the
number of regions grows exponentially with the number of features, i.e., of parameters.
However, the way these regions carve the input space still depends on few parameters:
this huge number of regions are not placed independently of each other. We can thus
represent a function that looks complicated but actually has structure. Basically, the assumption is that one can learn about each feature without having to see the examples for
all the conﬁgurations of all the other features, i.e., these features corespond to underlying
factors explaining the data.

Note that a sparse representation is a distributed representation where the
number of attributes that are active together is small compared to the total number of attributes. For example, in the case of binary representations, one might
492

CHAPTER 16. REPRESENTATION LEARNING

have only k  n of the n bits that are non-zero. The power of representation
grows exponentially with the number of active attributes, e.g., O(nk ) in the above
example of binary vectors. At the extreme, a symbolic representation is a very
sparse representation where only one attribute at a time can be active.

16.6

Exponential Gain in Representational Eﬃciency
from Distributed Representations

When and why can there be a statistical advantage from using a distributed
representation as part of a learning algorithm?
Figures 16.10 and 16.11 explain that advantage in intuitive terms. The argument is that a function that “looks complicated” can be compactly represented
using a small number of parameters, if some “structure” is uncovered by the
learner. Traditional “non-distributed” learning algorithms generalize only due to
the smoothness assumption, which states that if u ≈ v, then the target function f
to be learned has the property that f (u) ≈ f(v), in general. There are many ways
of formalizing such an assumption, but the end result is that if we have an example (x, y) for which we know that f(x) ≈ y, then we choose an estimator f̂ that
approximately satisﬁes these constraints while changing as little as possible. This
assumption is clearly very useful, but it suﬀers from the curse of dimensionality:
in order to learn a target function that takes many diﬀerent values (e.g. many
ups and downs) in a large number of regions2 , we may need a number of examples
that is at least as large as the number of distinguishible regions. One can think
of each of these regions as a category or symbol: by having a separate degree
of freedom for each symbol (or region), we can learn an arbitrary mapping from
symbol to value. However, this does not allow us to generalize to new symbols,
new regions.
If we are lucky, there may be some regularity in the target function, besides
being smooth. For example, the same pattern of variation may repeat itself many
times (e.g., as in a periodic function or a checkerboard). If we only use the
smoothness prior, we will need additional examples for each repetition of that
pattern. However, as discussed by Montufar et al. (2014), a deep architecture
could represent and discover such a repetition pattern and generalize to new instances of it. Thus a small number of parameters (and therefore, a small number
of examples) could suﬃce to represent a function that looks complicated (in the
sense that it would be expensive to represent with a non-distributed architecture). Figure 16.11 shows a simple example, where we have n binary features
2

e.g., exponentially many regions: in a d-dimensional space with at least 2 diﬀerent values
to distinguish per dimension, we might want f to diﬀer in 2d diﬀerent regions, requiring O(2 d)
training examples.
493

CHAPTER 16. REPRESENTATION LEARNING

in a d-dimensional space, and where each binary feature corresponds to a linear
classiﬁer that splits the input space in two parts. The exponentially large number
of intersections of n of the corresponding half-spaces corresponds to as many distinguishable regions that a distributed representation learner could capture. How
many regions are generated by an arrangement of n hyperplanes in Rd ? This
corresponds to the number of regions that a shallow neural network (one hidden
layer) can distinguish (Pascanu et al., 2014b), which is
d  
X
n
= O(nd),
j
j=0

following a more general result from Zaslavsky (1975), known as Zaslavsky’s theorem, one of the central results from the theory of hyperplane arrangements.
Therefore, we see a growth that is exponential in the input size and polynomial
in the number of hidden units.
Although a distributed representation (e.g. a shallow neural net) can represent
a richer function with a smaller number of parameters, there is no free lunch: to
construct an arbitrary partition (say with 2 d diﬀerent regions) one will need a
correspondingly large number of hidden units, i.e., of parameters and of examples.
The use of a distributed representation therefore also corresponds to a prior, which
comes on top of the smoothness prior. To return to the hyperplanes examples of
Figure 16.11, we see that we are able to get this generalization because we can
learn about the location of each hyperplane with only O(d) examples: we do not
need to see examples corresponding to all O(n d) regions.
Let us consider a concrete example. Imagine that the input is the image of a
person, and that we have a classiﬁer that detects whether the person is a child or
not, another that detects if that person is a male or a female, another that detects
whether that person wears glasses or not, etc. Keep in mind that these features are
discovered automatically, not ﬁxed a priori. We can learn about the male vs female
distinction, or about the glasses vs no-classes case, without having to consider all
of the conﬁgurations of the n features. This form of statistical separability is
what allows one to generalize to new conﬁgurations of a person’s features that
have never been seen during training. It corresponds to the prior discussed above
regarding the existence of multiple underlying explanatory factors. This prior
is very plausible for most of the data distributions on which human intelligence
would be useful, but it may not apply to every possible distribution. However, this
apparently innocuous assumption buys us a lot, statistically speaking, because
it allows the learner to discover structure with a reasonably small number of
examples that would otherwise require exponentially more training data.
Another interesting result illustrating the statistical eﬀect of a distributed representations versus a non-distributed one is the mathematical analysis (Montufar
494

CHAPTER 16. REPRESENTATION LEARNING

and Morton, 2014) of products of mixtures (which include the RBM as a special
case) versus mixture of products (such as the mixture of Gaussians). The analysis shows that a mixture of products can require an exponentially larger number
of parameters in order to represent the probability distributions arising out of a
product of mixtures.

16.7

Exponential Gain in Representational Eﬃciency
from Depth

In the above example with the input being an image of a person, it would not
be reasonable to expect factors such as gender, age, and the presence of glasses
to be detected simply from a linear classiﬁer, i.e., a shallow neural network. The
kinds of factors that can be chosen almost independently in order to generate
data are more likely to be very high-level and related in highly non-linear ways to
the input. This demands deep distributed representations, where the higher level
features (seen as functions of the input) or factors (seen as generative causes) are
obtained through the composition of many non-linearities.
It turns out that organizing computation through the composition of many
non-linearities and a hierarchy of reused features can give another exponential
boost to statistical eﬃciency. Although 2-layer networks (e.g., with saturating
non-linearities, boolean gates, sum/products, or RBF units) can generally be
shown to be universal approximators3 , the required number of hidden units may
be very large. The main results on the expressive power of deep architectures
state that there are families of functions that can be represented eﬃciently with
a deep architecture (say depth k) but would require an exponential number of
components (with respect to the input size) with insuﬃcient depth (depth 2 or
depth k − 1).
More precisely, a feedforward neural network with a single hidden layer is
a universal approximator (of Borel measurable functions) (Hornik et al., 1989;
Cybenko, 1989). Other works have investigated universal approximation of probability distributions by deep belief networks (Le Roux and Bengio, 2010; Montúfar
and Ay, 2011), as well as their approximation properties (Montúfar, 2014; Krause
et al., 2013).
Regarding the advantage of depth, early theoretical results have focused on
circuit operations (neural net unit computations) that are substantially diﬀerent from those being used in real state-of-the-art deep learning applications,
such as logic gates (Håstad, 1986) and linear threshold units with non-negative
weights (Håstad and Goldmann, 1991). More recently, Delalleau and Bengio
3

with enough hidden units they can approximate a large class of functions (e.g. continuous
functions) up to some given tolerance level
495

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.12: A sum-product network (Poon and Domingos, 2011) composes summing
units and product units, so that each node computes a polynomial. Consider the product
node computing x2 x3 : its value is reused in its two immediate children, and indirectly
incorporated in its grand-children. In particular, in the top node shown the product
x 2x 3 would arise 4 times if that node’s polynomial was expanded as a sum of products.
That number could double for each additional layer. In general a deep sum of product
can represent polynomials with a number of min-terms that is exponential in depth,
and some families of polynomials are represented eﬃciently with a deep sum-product
network but not eﬃciently representable with a simple sum of products, i.e., a 2-layer
network (Delalleau and Bengio, 2011).

(2011) showed that a shallow network requires exponentially many more sumproduct hidden units 4 than a deep sum-product network (Poon and Domingos,
2011) in order to compute certain families of polynomials. Figure 16.12 illustrates a sum-product network for representing polynomials, and how a deeper
network can be exponentially more eﬃcient because the same computation can
be reused exponentially (in depth) many times. Note however that Martens and
Medabalimi (2014) showed that sum-product networks may be have limitations
in their expressive power, in the sense that there are distributions that can easily
be represented by other generative models but that cannot be eﬃciently represented under the decomposability and completeness conditions associated with
the probabilistic interpretation of sum-product networks (Poon and Domingos,
2011).
Closer to the kinds of deep networks actually used in practice (Pascanu et al.,
2014a; Montufar et al., 2014) showed that piecewise linear networks (e.g. obtained from rectiﬁer non-linearities or maxout units) could represent functions
4

Here, a single sum-product hidden layer summarizes a layer of product units followed by a
layer of sum units.
496

CHAPTER 16. REPRESENTATION LEARNING

Figure 16.13: An absolute value rectiﬁcation unit has the same output for every pair
of mirror points in its input. The mirror axis of symmetry is given by the hyperplane
deﬁned by the weights and bias of the unit. If one considers a function computed on
top of that unit (the green decision surface), it will be formed of a mirror image of a
simpler pattern, across that axis of symmetry. The middle image shows how it can be
obtained by folding the space around that axis of symmetry, and the right image shows
how another repeating pattern can be folded on top of it (by another downstream unit)
to obtain another symmetry (which is now repeated four times, with two hidden layers).
This is an intuitive explanation of the exponential advantage of deeper rectiﬁer networks
formally shown in Pascanu et al. (2014a); Montufar et al. (2014).

with exponentially more piecewise-linear regions, as a function of depth, compared to shallow neural networks. Figure 16.13 illustrates how a network with
absolute value rectiﬁcation creates mirror images of the function computed on top
of some hidden unit, with respect to the input of that hidden unit. Each hidden
unit speciﬁes where to fold the input space in order to create mirror responses
(on both sides of the absolute value non-linearity). By composing these folding
operations, we obtain an exponentially large number of piecewise linear regions
which can capture all kinds of regular (e.g. repeating) patterns.
More precisely, the main theorem in Montufar et al. (2014) states that the
number of linear regions carved out by a deep rectiﬁer network with d inputs,
depth L, and n units per hidden layer, is
 d(L−1) !
n
O
nd ,
d
i.e., exponential in the depth L. In the case of maxout networks with k ﬁlters per
unit, the number of linear regions is


(L−1)+d
O k
.

16.8

Priors Regarding The Underlying Factors

To close this chapter, we come back to the original question: what is a good
representation? We proposed that an ideal representation is one that disentangles
497

CHAPTER 16. REPRESENTATION LEARNING

the underlying causal factors of variation that generated the data, especially those
factors that we care about in our applications. It seems clear that if we have direct
clues about these factors (like if a factor y = hi , a label, is observed at the same
time as an input x), then this can help the learner separate these observed factors
from the others. This is already what supervised learning does. But in general,
we may have a lot more unlabeled data than labeled data: can we use other
clues, other hints about the underlying factors, in order to disentangle them more
easily?
What we propose here is that indeed we can provide all kinds of broad priors
which are as many hints that can help the learner discover, identify and disentangle these factors. The list of such priors is clearly not exhaustive, but it is a
starting point, and yet most learning algorithms in the machine learning literature only exploit a small subset of these priors. With absolutely no priors, we
know that it is not possible to generalize: this is the essence of the no-free-lunch
theorem for machine learning. In the space of all functions, which is huge, with
any ﬁnite training set, there is no general-purpose learning recipe that would
dominate all other learning algorithms. Whereas some assumptions are required,
when our goal is to build AI or understand human intelligence, it is tempting to
focus our attention on the most general and broad priors, that are relevant for
most of the tasks that humans are able to successfully learn.
This list was introduced in section 3.1 of Bengio et al. (2013c).
• Smoothness: we want to learn functions f s.t. x ≈ y generally implies
f (x) ≈ f(y). This is the most basic prior and is present in most machine
learning, but is insuﬃcient to get around the curse of dimensionality, as
discussed abov and in Bengio et al. (2013c). below.
• Multiple explanatory factors: the data generating distribution is generated by diﬀerent underlying factors, and for the most part what one learns
about one factor generalizes in many conﬁgurations of the other factors.
This assumption is behind the idea of distributed representations, discussed in Section 16.5 above.
• Depth, or a hierarchical organization of explanatory factors: the
concepts that are useful at describing the world around us can be deﬁned in
terms of other concepts, in a hierarchy, with more abstract concepts higher
in the hierarchy, being deﬁned in terms of less abstract ones. This is the
assumption exploited by having deep representations.
• Causal factors: the input variables x are consequences, eﬀects, while the
explanatory factors are causes, and not vice-versa. As discussed above, this
enables the semi-supervised learning assumption, i.e., that P (x) is tied
498

CHAPTER 16. REPRESENTATION LEARNING

to P (y | x), making it possible to improve the learning of P (y | x) via
the learning of P (x). More precisely, this entails that representations that
are useful for P (x) are useful when learning P (y | x), allowing sharing of
statistical strength between the unsupervised and supervised learning tasks.
• Shared factors across tasks: in the context where we have many tasks,
corresponding to diﬀerent yi’s sharing the same input x or where each task
is associated with a subset or a function fi (x) of a global input x, the assumption is that each yi is associated with a diﬀerent subset from a common
pool of relevant factors h. Because these subsets overlap, learning all the
P (yi | x) via a shared intermediate representation P (h | x) allows sharing
of statistical strength between the tasks.
• Manifolds: probability mass concentrates, and the regions in which it concentrates are locally connected and occupy a tiny volume. In the continuous
case, these regions can be approximated by low-dimensional manifolds that
a much smaller dimensionality than the original space where the data lives.
This is the manifold hypothesis and is covered in Chapter 17, especially
with algorithms related to auto-encoders.
• Natural clustering: diﬀerent values of categorical variables such as object
classes5 are associated with separate manifolds. More precisely, the local
variations on the manifold tend to preserve the value of a category, and a
linear interpolation between examples of diﬀerent classes in general involves
going through a low density region, i.e., P (x | y = i) for diﬀerent i tend
to be well separated and not overlap much. For example, this is exploited
explicitly in the Manifold Tangent Classiﬁer discussed in Section 17.5. This
hypothesis is consistent with the idea that humans have named categories
and classes because of such statistical structure (discovered by their brain
and propagated by their culture), and machine learning tasks often involves
predicting such categorical variables.
• Temporal and spatial coherence: this is similar to the cluster assumption but concerns sequences or tuples of observations; consecutive or spatially nearby observations tend to be associated with the same value of
relevant categorical concepts, or result in a small move on the surface of
the high-density manifold. More generally, diﬀerent factors change at different temporal and spatial scales, and many categorical concepts of interest change slowly. When attempting to capture such categorical variables,
this prior can be enforced by making the associated representations slowly
5

it is often the case that the y of interest is a category
499

CHAPTER 16. REPRESENTATION LEARNING

changing, i.e., penalizing changes in values over time or space. This prior
was introduced in Becker and Hinton (1992).
• Sparsity: for any given observation x, only a small fraction of the possible
factors are relevant. In terms of representation, this could be represented
by features that are often zero (as initially proposed by Olshausen and Field
(1996)), or by the fact that most of the extracted features are insensitive
to small variations of x. This can be achieved with certain forms of priors
on latent variables (peaked at 0), or by using a non-linearity whose value is
often ﬂat at 0 (i.e., 0 and with a 0 derivative), or simply by penalizing the
magnitude of the Jacobian matrix (of derivatives) of the function mapping
input to representation. This is discussed in Section 15.8.
• Simplicity of Factor Dependencies: in good high-level representations,
the factors are related to each other through simpleQdependencies. The
simplest possible is marginal independence, P (h) = i P (hi ), but linear
dependencies or those captured by a shallow auto-encoder are also reasonable assumptions. This can be seen in many laws of physics, and is assumed
when plugging a linear predictor or a factorized prior on top of a learned
representation.

500

Chapter 17

The Manifold Perspective on
Representation Learning
Manifold learning is an approach to machine learning that is capitalizing on the
manifold hypothesis (Cayton, 2005; Narayanan and Mitter, 2010): the data generating distribution is assumed to concentrate near regions of low dimensionality.
The notion of manifold in mathematics refers to continuous spaces that locally
resemble Euclidean space, and the term we should be using is really submanifold,
which corresponds to a subset which has a manifold structure. The use of the
term manifold in machine learning is much looser than its use in mathematics,
though:
• the data may not be strictly on the manifold, but only near it,
• the dimensionality may not be the same everywhere,
• the notion actually referred to in machine learning naturally extends to
discrete spaces.
Indeed, although the very notions of a manifold or submanifold are deﬁned
for continuous spaces, the more general notion of probability concentration applies equally well to discrete data. It is a kind of informal prior assumption about
the data generating distribution that seems particularly well-suited for AI tasks
such as those involving images, video, speech, music, text, etc. In all of these
cases the natural data has the property that randomly choosing conﬁgurations of
the observed variables according to a factored distribution (e.g. uniformly) are
very unlikely to generate the kind of observations we want to model. What is the
probability of generating a natural looking image by choosing pixel intensities
independently of each other? What is the probability of generating a meaningful natural language paragraph by independently choosing each character in a
501

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

⇓

Figure 17.1: Top: data sampled from a distribution in a high-dimensional space (one 2
dimensions shown for illustration) that is actually concentrated near a one-dimensional
manifold, which here is like a twisted string. Bottom: the underlying manifold that the
learner should infer.

string? Doing a thought experiment should give a clear answer: an exponentially
tiny probability. This is because the probability distribution of interest concentrates in a tiny volume of the total space of conﬁgurations. That means that to
the ﬁrst degree, the problem of characterizing the data generating distribution
can be reduced to a binary classiﬁcation problem: is this conﬁguration probable
or not?. Is this a grammatically and semantically plausible sentence in English?
Is this a natural-looking image? Answering these questions tells us much more
about the nature of natural language or text than the additional information one
would have by being able to assign a precise probability to each possible sequence
of characters or set of pixels. Hence, simply characterizing where probability
concentrates is a fundamental importance, and this is what manifold learning
algorithms attempt to do. Because it is a where question, it is more about geometry than about probability distributions, although we ﬁnd both views useful
502

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

when designing learning algorithms for AI tasks.
tangent directions
tangent plane

Data on a curved manifold

Figure 17.2: A two-dimensional manifold near which training examples are concentrated,
along with a tangent plane and its associated tangent directions, forming a basis that
specify the directions of small moves one can make to stay on the manifold.

Figure 17.3: Illustration of tangent vectors of the manifold estimated by a contractive
auto-encoder (CAE), at some input point (top left, image of a zero). Each image on the
top right corresponds to a tangent vector. They are obtained by picking the dominant
∂f (x)
singular vectors (with largest singular value) of the Jacobian ∂x (see Section 15.10).
Taking the original image plus a small quantity of any of these tangent vectors yields
another plausible image, as illustrated in the bottom. The leading tangent vectors seem
to correspond to small deformations, such as translation, or shifting ink around locally in
the original image. Reproduced with permission from the authors of Rifai et al. (2011a).

In addition to the property of probability concentration, there is another one
503

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

that characterizes the manifold hypothesis: when a conﬁguration is probable it
is generally surrounded (at least in some directions) by other probable conﬁgurations. If a conﬁguration of pixels looks like a natural image, then there are tiny
changes one can make to the image (like translating everything by 0.1 pixel to the
left) which yield another natural-looking image. The number of independent ways
(each characterized by a number indicating how much or whether we do it) by
which a probable conﬁguration can be locally transformed into another probable
conﬁguration indicates the local dimension of the manifold. Whereas maximum
likelihood procedures tend to concentrate probability mass on the training examples (which can each become a local maximum of probability when the model
overﬁts), the manifold hypothesis suggests that good solutions instead concentrate probability along ridges of high probability (or their high-dimensional generalization) that connect nearby examples to each other. This is illustrated in
Figure 17.1.
What is most commonly learned to characterize a manifold is a representation
of the data points on (or near, i.e. projected on) the manifold. Such a representation for a particular example is also called its embedding. It is typically given by a
low-dimensional vector, with less dimensions than the “ambient” space of which
the manifold is a low-dimensional subset. Some algorithms (non-parametric manifold learning algorithms, discussed below) directly learn an embedding for each
training example, while others learn a more general mapping, sometimes called
an encoder, or representation function, that maps any point in the ambient space
(the input space) to its embedding.
Another important characterization of a manifold is the set of its tangent
planes. At a point x on a d-dimensional manifold, the tangent plane is given by d
basis vectors that span the local directions of variation allowed on the manifold.
As illustrated in Figure 17.2, these local directions specify how one can change x
inﬁnitesimally while staying on the manifold.
Manifold learning has mostly focused on unsupervised learning procedures
that attempt to capture these manifolds. Most of the initial machine learning research on learning non-linear manifolds has focused on non-parametric methods
based on the nearest-neighbor graph. This graph has one node per training example and edges connecting near neighbors. Basically, these methods (Schölkopf
et al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000; Brand, 2003; Belkin
and Niyogi, 2003; Donoho and Grimes, 2003; Weinberger and Saul, 2004; Hinton
and Roweis, 2003; van der Maaten and Hinton, 2008a) associate each of these
nodes with a tangent plane that spans the directions of variations associated with
the diﬀerence vectors between the example and its neighbors, as illustrated in
Figure 17.4.
A global coordinate system can then be obtained through an optimization or
504

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

Figure 17.4: Non-parametric manifold learning procedures build a nearest neighbor graph
whose nodes are training examples and arcs connect nearest neighbors. Various procedures can thus obtain the tangent plane associated with a neighborhood of the graph,
and a coordinate system that associates each training example with a real-valued vector
position, or embedding. It is possible to generalize such a representation to new examples
by a form of interpolation. So long as the number of examples is large enough to cover
the curvature and twists of the manifold, these approaches work well. Images from the
QMUL Multiview Face Dataset (Gong et al., 2000).

solving a linear system. Figure 17.5 illustrates how a manifold can be tiled by a
large number of locally linear Gaussian-like patches (or “pancakes”, because the
Gaussians are ﬂat in the tangent directions).

505

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

Figure 17.5: If the tangent plane at each location is known, then they can be tiled to
form a global coordinate system or a density function. In the ﬁgure, each local patch
can be thought of as a local Euclidean coordinate system or as a locally ﬂat Gaussian,
or “pancake”, with a very small variance in the directions orthogonal to the pancake and
a very large variance in the directions deﬁning the coordinate system on the pancake.
The average of all these Gaussians would provide an estimated density function, as in the
Manifold Parzen algorithm (Vincent and Bengio, 2003) or its non-local neural-net based
variant (Bengio et al., 2006c).

tangent image
tangent directions

high−contrast image

shifted
image
tangent image
tangent directions

Figure 17.6: When the data are images, the tangent vectors can also be visualized like
506
images. Here we show the tangent vector associated
with translation: it corresponds to
the diﬀerence between an image and a slightly translated version. This basically extracts

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

However, there is a fundamental diﬃculty with such non-parametric neighborhoodbased approaches to manifold learning, raised in Bengio and Monperrus (2005):
if the manifolds are not very smooth (they have many ups and downs and twists),
one may need a very large number of training examples to cover each one of
these variations, with no chance to generalize to unseen variations. Indeed, these
methods can only generalize the shape of the manifold by interpolating between
neighboring examples. Unfortunately, the manifolds of interest in AI have many
ups and downs and twists and strong curvature, as illustrated in Figure 17.6. This
motivates the use of distributed representations and deep learning for capturing
manifold structure, which is the subject of this chapter.

Figure 17.7: Training examples of a face dataset – the QMUL Multiview Face
Dataset (Gong et al., 2000) – for which the subjects were asked to move in such a way as
to cover the two-dimensional manifold corresponding to two angles of rotation. We would
like learning algorithms to be able to discover and disentangle such factors. Figure 17.8
illustrates such a feat.

The hope of many manifold learning algorithms, including those based on deep
learning and auto-encoders, is that one learns an explicit or implicit coordinate
system for the leading factors of variation that explain most of the structure in
the unknown data generating distribution. An example of explicit coordinate
system is one where the dimensions of the representation (e.g., the outputs of the
encoder, i.e., of the hidden units that compute the “code” associated with the
input) are directly the coordinates that map the unknown manifold. Training
examples of a face dataset in which the images have been arranged visually on a
2-D manifold are shown in Figure 17.7, with the images laid down so that each
of the two axes corresponds to one of the two angles of rotation of the face.
However, the objective is to discover such manifolds, and Figure 17.8 illustrates the images generated by a variational auto-encoder (Kingma and Welling,
2014a) when the two-dimensional auto-encoder code (representation) is varied
507

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

on the 2-D plane. Note how the algorithm actually discovered two independent
factors of variation: angle of rotation and emotional expression.
Another kind of interesting illustration of manifold learning involves the discovery of distributed representations for words. Neural language models were
initiated with the work of Bengio et al. (2001c, 2003b), in which a neural network
is trained to predict the next word in a sequence of natural language text, given
the previous words, and where each word is represented by a real-valued vector,
called embedding or neural word embedding.

Figure 17.8: Two-dimensional representation space (for easier visualization), i.e., a Euclidean coordinate system for Frey faces (left) and MNIST digits (right), learned by a
variational auto-encoder (Kingma and Welling, 2014a). Figures reproduced with permission from the authors. The images shown are not examples from the training set but
images x actually generated by the model P (x | h), simply by changing the 2-D “code”
h (each image corresponds to a diﬀerent choice of “code” h on a 2-D uniform grid). On
the left, one dimension that has been discovered (horizontal) mostly corresponds to a
rotation of the face, while the other (vertical) corresponds to the emotional expression.
The decoder deterministically maps codes (here two numbers) to images. The encoder
maps images to codes (and adds noise, during training).

Figure 17.9 shows such neural word embeddings reduced to two dimensions
(originally 50 or 100) using the t-SNE non-linear dimensionality reduction algorithm (van der Maaten and Hinton, 2008a). The ﬁgures zooms into diﬀerent areas
of the word-space and illustrates that words that are semantically and syntacti508

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

cally close end up having nearby embeddings.

Figure 17.9: Two-dimensional representation space (for easier visualization), of English
words, learned by a neural language model as in Bengio et al. (2001c, 2003b), with t-SNE
for the non-linear dimensionality reduction from 100 to 2. Diﬀerent regions are zoomed
to better see the details. At the global level one can identify big clusters corresponding to part-of-speech, while locally one sees mostly semantic similarity explaining the
neighborhood structure.

17.1

Manifold Interpretation of PCA and Linear AutoEncoders

The above view of probabilistic PCA as a thin “pancake” of high probability
is related to the manifold interpretation of PCA and linear auto-encoders, in
which we are looking for projections of x into a subspace that preserves as much
information as possible about x. This is illustrated in Figure 17.10. Let the
encoder be
h = f (x) = W > (x − µ)
509

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

computing such a projection, a low-dimensional representation of h. With the
auto-encoder view, we have a decoder computing the reconstruction
x̂ = g(h) = b + V h.

Figure 17.10: Flat Gaussian capturing probability concentration near a low-dimensional
manifold. The ﬁgure shows the upper half of the “pancake” above the “manifold plane”
which goes through its middle. The variance in the direction orthogonal to the manifold
is very small (upward red arrow) and can be considered like “noise”, where the other
variances are large (larger red arrows) and correspond to “signal”, and a coordinate
system for the reduced-dimension data.

It turns out that the choices of linear encoder and decoder that minimize
reconstruction error
E[||x − x̂||2]
correspond to V = W , µ = b = E[x] and the rows of W form an orthonormal
basis which spans the same subspace as the principal eigenvectors of the covariance
matrix
C = E[(x − µ)(x − µ)> ].
In the case of PCA, the rows of W are these eigenvectors, ordered by the magnitude of the corresponding eigenvalues (which are all real and non-negative). This
is illustrated in Figure 17.11.

510

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

+/*
+.*

!"#$%&'!(#)$%,x-*
!"#$%&'!(#)$%*"!!$!*+"#'$!*
x"

Figure 17.11: Manifold view of PCA and linear auto-encoders. The data distribution is
concentrated near a manifold aligned with the leading eigenvectors (here, this is just v1 )
of the data covariance matrix. The other eigenvectors (here, just v2 ) are orthogonal to the
manifold. A data point (in red, x) is encoded into a lower-dimensional representation or
code h (here the scalar which indicates the position on the manifold, starting from h = 0).
The decoder (transpose of the encoder) maps h to the data space, and corresponds to a
point lying exactly on the manifold (green cross), the orthogonal projection of x on the
manifold. The optimal encoder and decoder minimize the sum of reconstruction errors
(diﬀerence vector between x and its reconstruction).

One can also show that eigenvalue λ i of C corresponds to the variance of x
in the direction of eigenvector v i . If x ∈ RD and h ∈ R d with d < D, then the
optimal reconstruction error (choosing µ, b, V and W as above) is
2

min E[||x − x̂|| ] =

D
X

λi .

i=d+1

Hence, if the covariance has rank d, the eigenvalues λ d+1 to λD are 0 and reconstruction error is 0.
Furthermore, one can also show that the above solution can be obtained by
maximizing the variances of the elements of h, under orthonormal W , instead of
minimizing reconstruction error.

511

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

17.2

Manifold Interpretation of Sparse Coding

Sparse coding was introduced in Section 15.6.2 a linear factors generative model.
It also has an interesting manifold learning interpretation. The codes h inferred
with the above equation do not ﬁll the space in which h lives. Instead, probability
mass is concentrated on axis-aligned subspaces: sets of values of h for which most
of the axes are set at 0. We can thus decompose h into two pieces of information:
P
• A binary pattern β which speciﬁes which hi are non-zero, with N a = i βi
the number of “active” (non-zero) dimensions.
• A variable-length real-valued vector α ∈ R Na which speciﬁes the coordinates
for each of the active dimensions.
The pattern β can be viewed as specifying an Na -dimensional region in input
space (the set of x = W h + b where hi = 0 if bi = 0). That region is actually a
linear manifold, an Na -dimensional hyperplane. All those hyperplanes go through
a “center” x = b. The vector α then speciﬁes a Euclidean coordinate on that
hyperplane.
Because the prior P (h) is concentrated around 0, the probability mass of P (x)
is concentrated on the regions of these hyperplanes near x = b. Depending on
the amount of reconstruction error (output variance for P (x | g(h))), there is also
probability mass bleeding around these hyperplanes and making them look more
like pancakes. Each of these hyperplane-aligned manifolds and the associated
distribution is just like the ones we associate to probabilistic PCA and factor
analysis. The crucial diﬀerence is that instead of one hyperplane, we have 2 d
hyperplanes if h ∈ R d. Due to the sparsity prior, however, most of these ﬂat
Gaussians are unlikely: only the ones corresponding to a small Na (with only
a few of the axes being active) are likely. For example, if we were to restrict
 
ourselves to only those values of b for which Na = k, then one would have dk
Gaussians. With this exponentially large number of Gaussians, the interesting
thing to observe is that the sparse coding model only has a number of parameters
linear in the number of dimensions of h. This property is shared with other
distributed representation learning algorithms described in this chapter, such as
the regularized auto-encoders.

17.3

The Entropy Bias from Maximum Likelihood

TODO: how the log-likelihood criterion forces a learner that is not able to generalize perfectly to yield an estimator that is much smoother than the target
distribution. Phrase it in terms of entropy, not smoothness.
512

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

17.4

Manifold Learning via Regularized Auto-Encoders

Auto-encoders have been described in Section 15. What is their connection to
manifold learning? This is what we discuss here.
We denote f the encoder function, with h = f (x) the representation of x,
and g the decoding function, with x̂ = g(h) the reconstruction of x, although in
some cases the encoder is a conditional distribution q(h | x) and the decoder is a
conditional distribution P (x | h).
What all auto-encoders have in common, when they are prevented from simply learning the identity function for all possible input x, is that training them
involves a compromise between two “forces”:
1. Learning a representation h of training examples x such that x can be
approximately recovered from h through a decoder. Note that this needs
not be true for any x, only for those that are probable under the data
generating distribution.
2. Some constraint or regularization is imposed, either on the code h or on the
composition of the encoder/decoder, so as to make the transformed data
somehow simpler or to prevent the auto-encoder from achieving perfect
reconstruction everywhere. We can think of these constraints or regularization as a preference for solutions in which the representation is as simple
as possible, e.g., factorized or as constant as possible, in as many directions
as possible. In the case of the bottleneck auto-encoder a ﬁxed number of
representation dimensions is allowed, that is smaller than the dimension of
x. In the case of sparse auto-encoders (Section 15.8) the representation
elements h i are pushed towards 0. In the case of denoising auto-encoders
(Section 15.9), the encoder/decoder function is encouraged to be contractive (have small derivatives). In the case of the contractive auto-encoder
(Section 15.10), the encoder function alone is encouraged to be contractive,
while the decoder function is tied (by symmetric weights) to the encoder
function. In the case of the variational auto-encoder (Section 20.9.3), a
prior log P (h) is imposed on h to make its distribution factorize and concentrate as much as possible. Note how in the limit, for all of these cases,
the regularization prefers representations that are insensitive to the input.
Clearly, the second type of force alone would not make any sense (as would
any regularizer, in general). How can these two forces (reconstruction error on
one hand, and “simplicity” of the representation on the other hand) be reconciled? The solution of the optimization problem is that only the variations that
are needed to distinguish training examples need to be represented. If the data
generating distribution concentrates near a low-dimensional manifold, this yields
513

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

Figure 17.12: A regularized auto-encoder or a bottleneck auto-encoder has to reconcile
two forces: reconstruction error (which forces it to keep enough information to distinguish
training examples from each other), and a regularizer or constraint that aims at reducing
its representational ability, to make it as insensitive as possible to the input in as many
directions as possible. The solution is for the learned representation to be sensitive to
changes along the manifold (green arrow going to the right, tangent to the manifold) but
invariant to changes orthogonal to the manifold (blue arrow going down). This yields to
contraction of the representation in the directions orthogonal to the manifold.

representations that implicitly capture a local coordinate for this manifold: only
the variations tangent to the manifold around x need to correspond to changes
in h = f (x). Hence the encoder learns a mapping from the embedding space
x to a representation space, a mapping that is only sensitive to changes along
the manifold directions, but that is insensitive to changes orthogonal to the manifold. This idea is illustrated in Figure 17.12. A one-dimensional example is
illustrated in Figure 17.13, showing that by making the auto-encoder contractive
around the data points (and the reconstruction point towards the nearest data
point), we recover the manifold structure (of a set of 0-dimensional manifolds in
a 1-dimensional embedding space, in the ﬁgure).

17.5

Tangent Distance, Tangent-Prop, and Manifold
Tangent Classiﬁer

One of the early attempts to take advantage of the manifold hypothesis is the
Tangent Distance algorithm (Simard et al., 1993, 1998). It is a non-parametric
nearest-neighbor algorithm in which the metric used is not the generic Euclidean
distance but one that is derived from knowledge of the manifolds near which
probability concentrates. It is assumed that we are trying to classify examples
514

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

r(x)"

x 1"

x2"

x3"

x"

Figure 17.13: If the auto-encoder learns to be contractive around the data points, with
the reconstruction pointing towards the nearest data points, it captures the manifold
structure of the data. This is a 1-dimensional version of Figure 17.12. The denoising
auto-encoder explicitly tries to make the derivative of the reconstruction function r(x)
small around the data points. The contractive auto-encoder does the same thing for the
encoder. Although the derivative of r(x) is asked to be small around the data points, it
can be large between the data points (e.g. in the regions between manifolds), and it has
to be large there so as to reconcile reconstruction error (r(x) ≈ x for data points x) and
contraction (small derivatives of r(x) near data points).

and that examples on the same manifold share the same category. Since the
classiﬁer should be invariant to the local factors of variation that correspond
to movement on the manifold, it would make sense to use as nearest-neighbor
distance between points x1 and x 2 the distance between the manifolds M 1 and
M2 to which they respectively belong. Although that may be computationally
diﬃcult (it would require an optimization, to ﬁnd the nearest pair of points on
M1 and M2 ), a cheap alternative that makes sense locally is to approximate Mi
by its tangent plane at xi and measure the distance between the two tangents,
or between a tangent plane and a point. That can be achieved by solving a lowdimensional linear system (in the dimension of the manifolds). Of course, this
algorithm requires one to specify the tangent vectors at any point
In a related spirit, the Tangent-Prop algorithm (Simard et al., 1992) proposes
to train a neural net classiﬁer with an extra penalty to make the output f(x) of
the neural net locally invariant to known factors of variation. These factors of
variation correspond to movement of the manifold near which examples of the
∂f (x)
same class concentrate. Local invariance is achieved by requiring ∂x to be
orthogonal to the known manifold tangent vectors v i at x, or equivalently that

515

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

the directional derivative of f at x in the directions vi be small:
2
X  ∂f(x)
·vi .
regularizer = λ
∂x

(17.1)

i

Like for tangent distance, the tangent vectors are derived a priori, e.g., from the
formal knowledge of the eﬀect of transformations such as translation, rotation,
and scaling in images. Tangent-Prop has been used not just for supervised learning (Simard et al., 1992) but also in the context of reinforcement learning (Thrun,
1995).

∂f
∂x

∂h
∂x

Figure 17.14: Illustration of the main idea of the tangent-prop algorithm (Simard et al.,
1992) and manifold tangent classiﬁer (Rifai et al., 2011d), which both regularize the
classiﬁer output function f (x) (e.g. estimating conditional class probabilities given the
input) so as to make it invariant to the local directions of variations ∂h
(manifold tangent
∂x
directions). This can be achieved by penalizing the magnitude of the dot product of
∂f
all the rows of ∂h
∂x (the tangent directions) with all the rows of ∂x (the directions of
sensitivity of each output to the input). In the case of the tangent-prop algorithm, the
tangent directions are given a priori, whereas in the case of the manifold tangent classiﬁer,
they are learned, with h(x) being the learned representation of the input x. The ﬁgure
illustrates two manifolds, one per class, and we see that the classiﬁer output increases
the most as we move from one manifold to the other, in input space.

A more recent paper introduces the Manifold Tangent Classiﬁer (Rifai et al.,
2011d), which eliminates the need to know the tangent vectors a priori, and
516

CHAPTER 17. THE MANIFOLD PERSPECTIVE ON REPRESENTATION LEARNING

instead uses a contractive auto-encoder to estimate them at any point. As we
have seen in the previous section and Figure 15.9, auto-encoders in general, and
contractive auto-encoders especially well, learn a representation h that is most
sensitive to the factors of variation present in the data x, so that the leading
singular vectors of ∂h
∂x correspond to the estimated tangent vectors. As illustrated
in Figure 15.10, these estimated tangent vectors go beyond the classical invariants
that arise out of the geometry of images (such as translation, rotation and scaling)
and include factors that must be learned because they are object-speciﬁc (such
as adding or moving body parts). The algorithm proposed with the manifold
tangent classiﬁer is therefore simple: (1) use a regularized auto-encoder such
as the contractive auto-encoder to learn the manifold structure by unsupervised
learning (2) use these tangents to regularize a neural net classiﬁer as in TangentProp (Eq. 17.1). TODO Tangent Prop or Tangent-Prop?

517

Chapter 18

Confronting the Partition
Function
TODO– make sure the book explains asymptotic consistency somewhere, add
links to it here
In Section 13.2.2 we saw that many probabilistic models (commonly known
as undirected graphical models) are deﬁned by an unnormalized probability distribution p̃(x; θ) or energy function (Section 13.2.4)
E(x) = − log p̃(x).

(18.1)

Because the analytic formulation of the model is via this energy function or unnormalized probability, the complete formulation of the probability function or
probability density requires a normalization constant called partition function
Z(θ) such that
1
p(x; θ) =
p̃(x; θ)
Z(θ)
is a valid, normalized probability distribution. The partition function is an integral or sum over the unnormalized probability of all states. This operation is
intractable for many interesting models.
As we will see in chapter 20, many deep learning models are designed to have
a tractable normalizing constant, or are designed to be used in ways that do not
involve computing p(x) at all. However, other models directly confront the challenge of intractable partition functions. In this chapter, we describe techniques
used for training and evaluating models that have intractable partition functions.

518

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.1

The Log-Likelihood Gradient of Energy-Based
Models

What makes learning by maximum likelihood particularly diﬃcult is that the
partition function depends on the parameters, so that the log-likelihood gradient
has a term corresponding to the gradient of the partition function:
∂ log p(x; θ)
∂E(x)
log Z(θ)
=−
−
.
∂θ
∂θ
∂θ

(18.2)

In the case where the energy function is analytically tractable (e.g., RBMs), the
diﬃcult part is estimating the the gradient of the partition function. Unsurprisingly, since computing Z itself is intractable, we ﬁnd that computing its gradient
is also intractable, but the good news is that it corresponds to an expectation
over the model distribution, which can be estimated by Monte-Carlo methods.
Though the gradient of the log partition function is intractable to evaluate
accurately, it is straightforward to analyze algebraically. The derivatives we need
for learning are of the form
∂
log p(x)
∂θ
where θ is one of the parameters of p(x). These derivatives are given simply by
∂
∂
log p(x) =
(log p̃(x) − log Z) .
∂θ
∂θ
In this chapter, we are primarily concerned with the estimation of the term
on the right:
∂
log Z
∂θ
=
=
=

∂
∂θ

∂
Z
∂θ

Z
P

x p̃(x)

P

Z

∂
x ∂θ

p̃(x)

.
Z
For models that guarantee p(x) > 0 for all x, we can substitute exp (log p̃(x))
for p̃(x):
P ∂
exp (log p̃(x))
= x ∂θ
Z
=

x

P

∂ log p̃(x)
exp (log p̃(x)) ∂θ
Z

519

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

=
=

P

∂
x p̃(x) ∂θ

X
x

log p̃(x)

Z
∂
p(x) log p̃(x)
∂θ

∂
log p̃(x).
∂θ
This derivation made use of summation over discrete x, but a similar result
applies using integration over continuous x. In the continuous version of the
derivation, we use Leibniz’s rule for diﬀerentiation under the integral sign to
obtain the identity
Z
Z
∂
∂
p̃(x)dx =
p̃(x)dx.
∂θ
∂θ
This identity is only applicable under certain regularity conditions on p̃ and
∂
1
∂θ p̃(x) . Fortunately, most machine learning models of interest have these properties.
This identity
∂
∂
log Z = Ex∼p(x)
log p̃(x)
(18.3)
∂θ
∂θ
is the basis for a variety of Monte Carlo methods for approximately maximizing
the likelihood of models with intractable partition functions.
Putting this result together with Eq. 18.2, we obtain the following well-known
decomposition of the gradient in terms of the gradient of the energy function on
the observed x and in average over the model distribution:
= Ex∼p(x)

∂E(x)
∂
∂ − log p(x; θ)
=
− Ex∼p(x) E(x).
∂θ
∂θ
∂θ

(18.4)

The ﬁrst term is called the positive phase contribution to the gradient and it corresponds to pushing the energy down on the “positive” examples and reinforcing
the interactions that are observed between random variables when x is observed,
while the second term is called the negative phase contribution to the gradient and
it corresponds to pushing the energy up everywhere else, with proportionally more
push where the model currently puts more probability mass. When a minimum
of the negative log-likelihood is found, the two terms must of course cancel each
other, and the only thing that prevents the model from putting probability mass
in exactly the same way as the training distribution is that it may be regularized
or have some constraints, e.g. be parametric.
1

In measure theoretic terms, the conditions are: (i) p̃ must be a Lebesgue-integrable function
∂
of x for every value of θ; (ii) ∂θ
p̃(x) must exist for all θ and almost all x; (iii) There exists
∂
an integrable function R(x) that bounds ∂θ
p̃(x) (i.e. such that | ∂∂θ p̃(x)| ≤ R(x) for all θ and
almost all x).
520

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.2

Stochastic Maximum Likelihood and Contrastive
Divergence

The naive way of implementing equation 18.3 is to compute it by burning in a set
of Markov chains from a random initialization every time the gradient is needed.
When learning is performed using stochastic gradient descent, this means the
chains must be burned in once per gradient step. This approach leads to the
training procedure presented in Algorithm 18.1. The high cost of burning in the
Markov chains in the inner loop makes this procedure computationally infeasible,
but this procedure is the starting point that other more practical algorithms aim
to approximate.
Algorithm 18.1 A naive MCMC algorithm for maximizing the log-likelihood
with an intractable partition function using gradient ascent.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow burn in. Perhaps 100
to train an RBM on a small image patch.
while Not converged do
Sample a minibatch of m examples {x(1) , . . . , x (m)} from the training set.
P
(i)
g ← 1m m
i=1 ∇ θ log p̃(x ; θ)
Initialize a set of m samples { x̃(1), . . . , x̃ (m) } to random values (e.g., from
a uniform or normal distribution, or possibly a distribution with marginals
matched to the model’s marginals)
for i = 1 to k do
for j = 1 to m do
x̃(j) ← gibbs update(x̃(j) )
end for
end for
P
(i)
g ← g − m1 m
i=1 ∇θ log p̃( x̃ ; θ)
θ ← θ + g
end while
We can view the MCMC approach to maximum likelihood as trying to achieve
balance between two forces, one pushing up on the model distribution where the
data occurs, and another pushing down on the model distribution where the model
samples occur. Fig. 18.1 illustrates this process. The two forces correspond to
maximizing log p̃ and minimizing log Z. In this chapter, we assume the positive
phase is tractable and may be performed exactly, but other chapters, especially
chapter 19 deal with intractable positive phases. In this chapter, we present
several approximations to the negative phase. Each of these approximations can
521

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Figure 18.1: The view of Algorithm 18.1 as having a “positive phase” and “negative
phase”. Left) In the positive phase, we sample points from the data distribution, and
push up on their unnormalized probability. This means points that are likely in the data
get pushed up on more. Right) In the negative phase, we sample points from the model
distribution, and push down on their unnormalized probability. This counteracts the
positive phase’s tendency to just add a large constant to the unnormalized probability
everywhere. When the data distribution and the model distribution are equal, the positive
phase has the same chance to push up at a point as the negative phase has to push down.
At this point, there is no longer any gradient (in expectation) and training must terminate.

be understood as making the negative phase computationally cheaper but also
making it push down in the wrong locations.
Because the negative phase involves drawing samples from the model’s distribution, we can think of it as ﬁnding points that the model believes in strongly.
Because the negative phase acts to reduce the probability of those points, they are
generally considered to represent the model’s incorrect beliefs about the world.
They are frequently referred to in the literature as “hallucinations” or “fantasy
particles.” In fact, the negative phase has been proposed as a possible explanation for dreaming in humans and other animals (Crick and Mitchison, 1983), the
idea being that the brain maintains a probabilistic model of the world and follows the gradient of log p̃ while experiencing real events while awake and follows
the negative gradient of log p̃ to minimize log Z while sleeping and experiencing
events sampled from the current model. This view explains much of the language
used to describe algorithms with a positive and negative phase, but it has not
been proven to be correct with neuroscientiﬁc experiments. In machine learning
models, it is usually necessary to use the positive and negative phase simultaneously, rather than in separate time periods of wakefulness and REM sleep. As
we will see in chapter 19.6, other machine learning algorithms draw samples from
the model distribution for other purposes and such algorithms could also provide
an account for the function of dream sleep.
522

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Given this understanding of the role of the positive and negative phase of
learning, we can attempt to design a less expensive alternative to Algorithm 18.1.
The main cost of the naive MCMC algorithm is the cost of burning in the Markov
chains from a random initialization at each step. A natural solution is to initialize
the Markov chains from a distribution that is very close to the model distribution,
so that the burn in operation does not take as many steps.
The contrastive divergence (CD, or CD-k to indicate CD with k Gibbs steps)
algorithm initializes the Markov chain at each step with samples from the data
distribution (Hinton, 2000). This approach is presented as Algorithm 18.2. Obtaining samples from the data distribution is free, because they are already available in the data set. Initially, the data distribution is not close to the model
distribution, so the negative phase is not very accurate. Fortunately, the positive
phase can still accurately increase the model’s probability of the data. After the
positive phase has had some time to act, the model distribution is closer to the
data distribution, and the negative phase starts to become accurate.
Algorithm 18.2 The contrastive divergence algorithm, using gradient ascent as
the optimization procedure.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow a Markov chain of
p(x; θ) to mix when initializedfrom pdata. Perhaps 1-20 to train an RBM on a
small image patch.
while Not converged do
Sample P
a minibatch of m examples {x(1) , . . . , x (m)} from the training set.
m
g ← 1m i=1 ∇ θ log p̃(x(i) ; θ)
for i = 1 to m do
x̃ (i) ← x(i)
end for
for i = 1 to k do
for j = 1 to m do
x̃(j) ← gibbs update(x̃(j) )
end for
end for
Pm
g ← g − m1 i=1 ∇θ log p̃( x̃(i) ; θ)
θ ← θ + g
end while
Of course, CD is still an approximation to the correct negative phase. The
main way that CD qualitatively fails to implement the correct negative phase
is that it fails to suppress “spurious modes” — regions of high probability that
are far from actual training examples. Fig. 18.2 illustrates why this happens.
523

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Essentially, it is because modes in the model distribution that are far from the data
distribution will not be visited by Markov chains initialized at training points,
unless k is very large.
Carreira-Perpiñan and Hinton (2005) showed experimentally that the CD estimator is biased for RBMs and fully visible Boltzmann machines, in that it
converges to diﬀerent points than the maximum likelihood estimator. They argue
that because the bias is small, CD could be used as an inexpensive way to initialize a model that could later be ﬁne-tuned via more expensive MCMC methods.
Bengio and Delalleau (2009) showed that CD can be interpreted as discarding the
smallest terms of the correct MCMC update gradient, which explains the bias.
CD is useful for training shallow models like RBMs. These can in turn be
stacked to initialize deeper models like DBNs or DBMs. However, CD does not
provide much help for training deeper models directly. This is because it is diﬃcult
to obtain samples of the hidden units given samples of the visible units. Since
the hidden units are not included in the data, initializing from training points
cannot solve the problem. Even if we initialize the visible units from the data,
we will still need to burn in a Markov chain sampling from the distribution over
the hidden units conditioned on those visible samples. Most of the approximate
inference techniques described in chapter 19 for approximately marginalizing out
the hidden units cannot be used to solve this problem. This is because all of the
approximate marginalization methods based on giving a lower bound on p̃ would
give a lower bound on log Z. We need to minimize log Z, and minimizing a lower
bound is not a useful operation.
The CD algorithm can be thought of as penalizing the model for having a
Markov chain that changes the input rapidly when the input comes from the
data. This means training with CD somewhat resembles autoencoder training.
Even though CD is more biased than some of the other training methods, it
can be useful for pre-training shallow models that will later be stacked. This is
because the earliest models in the stack are encouraged to copy more information
up to their latent variables, thereby making it available to the later models. This
should be thought of more of as an often-exploitable side eﬀect of CD training
rather than a principled design advantage.
Sutskever and Tieleman (2010) showed that the CD update direction is not
the gradient of any function. This allows for situations where CD could cycle
forever, but in practice this is not a serious problem.
A diﬀerent strategy that resolves many of the problems with CD is to initialize
the Markov chains at each gradient step with their states from the previous gradient step. This approach was ﬁrst discovered under the name stochastic maximum
likelihood (SML) in the applied mathematics and statistics community (Younes,
1998) and later independently rediscovered under the name persistent contrastive
524

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Figure 18.2: An illustration of how the negative phase of contrastive divergence (Algorithm 18.2) can fail to suppress spurious modes. A spurious mode is a mode that is
present in the model distribution but absent in the data distribution. Because contrastive
divergence initializes its Markov chains from data points and runs the Markov chain for
only a few steps, it is unlikely to visit modes in the model that are far from the data
points. This means that when sampling from the model, we will sometimes get samples
that do not resemble the data. It also means that due to wasting some of its probability
mass on these modes, the model will struggle to place high probability mass on the correct
modes. Note that this ﬁgure uses a somewhat simpliﬁed concept of distance–the spurious
mode is far from the correct mode along the number line in R. This corresponds to a
Markov chain based on making local moves with a single x variable in R. For most deep
probabilistic models, the Markov chains are based on Gibbs sampling and can make nonlocal moves of individual variables but cannot move all of the variables simultaneously.
For these problems, it is usually better to consider the edit distance between modes,
rather than the Euclidean distance. However, edit distance in a high dimensional space
is diﬃcult to depict in a 2-D plot.

525

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

divergence (PCD, or PCD-k to indicate the use of k Gibbs steps per update) in
the deep learning community (Tieleman, 2008). See Algorithm 18.3. The basic
idea of this approach is that, so long as the steps taken by the stochastic gradient algorithm are small, then the model from the previous step will be similar
to the model from the current step. It follows that the samples from the previous model’s distribution will be very close to being fair samples from the current
model’s distribution, so a Markov chain initialized with these samples will not
require much time to mix.
Because each Markov chain is continually updated throughout the learning
process, rather than restarted at each gradient step, the chains are free to wander
far enough to ﬁnd all of the model’s modes. SML is thus considerably more
resistant to forming models with spurious modes than CD is. Moreover, because
it is possible to store the state of all of the sampled variables, whether visible or
latent, SML provides an initialization point for both the hidden and visible units.
CD is only able to provide an initialization for the visible units, and therefore
requires burn-in for deep models. SML is able to train deep models eﬃciently.
Marlin et al. (2010) compared SML to many of the other criteria presented in this
chapter. They found that SML results in the best test set log-likelihood for an
RBM, and if the RBM’s hidden units are used as features for an SVM classiﬁer,
SML results in the best classiﬁcation accuracy.
SML is vulnerable to becoming inaccurate if k is too small or  is too large
— in other words, if the stochastic gradient algorithm can move the model faster
than the Markov chain can mix between steps. There is no known way to test
formally whether the chain is successfully mixing between steps. Subjectively, if
the learning rate is too high for the number of Gibbs steps, the human operator
will be able to observe that there is much more variance in the negative phase
samples across gradient steps rather than across diﬀerent Markov chains. For
example, a model trained on MNIST might sample exclusively 7s on one step.
The learning process will then push down strongly on the mode corresponding to
7s, and the model might sample exclusively 9s on the next step.
Care must be taken when evaluating the samples from a model trained with
SML. It is necessary to draw the samples starting from a fresh Markov chain
initialized from a random starting point after the model is done training. The
samples present in the persistent negative chains used for training have been
inﬂuenced by several recent versions of the model, and thus can make the model
appear to have greater capacity than it actually does.
Berglund and Raiko (2013) performed experiments to examine the bias and
variance in the estimate of the gradient provided by CD and SML. CD proves to
have low variance than the estimator based on exact sampling. SML has higher
variance. The cause of CD’s low variance is its use of the same training points
526

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Algorithm 18.3 The stochastic maximum likelihood / persistent contrastive
divergence algorithm using gradient ascent as the optimization procedure.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow a Markov chain of
p(x; θ + g) toburn in, starting from samples from p(x; θ). Perhaps 1 for RBM
on a small image patch, or 5-50 for a morecomplicated model like a DBM.
Initialize a set of m samples {x̃(1) , . . . , x̃(m) } to random values (e.g., from a uniform or normal distribution, or possibly a distribution with marginals matched
to the model’s marginals)
while Not converged do
Sample a minibatch of m examples {x(1) , . . . , x (m)} from the training set.
P
(i)
g ← 1m m
i=1 ∇ θ log p̃(x ; θ)
for i = 1 to k do
for j = 1 to m do
x̃(j) ← gibbs update(x̃(j) )
end for
end for
P
(i)
g ← g − m1 m
i=1 ∇θ log p̃( x̃ ; θ)
θ ← θ + g
end while
in both the positive and negative phase. If the negative phase is initialized from
diﬀerent training points, the variance rises above that of the estimator based on
exact sampling.
TODO– FPCD? TODO– Rates-FPCD?
TODO– mention that all these things can be coupled with enhanced samplers,
which I believe are mentioned in the intro to graphical models chapter
One key beneﬁt to the MCMC-based methods described in this section is that
they provide an estimate of the gradient of log Z, and thus we can essentially
decompose the problem into the log p̃ contribution and the log Z contribution.
We can then use any other method to tackle log p̃(x), and just add our negative
phase gradient onto the other method’s gradient. In particular, this means that
our positive phase can make use of methods that provide only a lower bound on
p̃. Most of the other methods of dealing with log Z presented in this chapter are
incompatible with bound-based positive phase methods.

527

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.3

Pseudolikelihood

Monte Carlo approximations to the partition function and its gradient directly
confront the partition function. Other approaches sidestep the issue, by training
the model without computing the partition function. Most of these approaches
are based on the observation that it is easy to compute ratios of probabilities
in an unnormalized probabilistic model. This is because the partition function
appears in both the numerator and the denominator of the ratio and cancels out:
p(x)
=
p(y)

1
Z p̃(x)
1 p̃(y)
Z

=

p̃(x)
.
p̃(y)

The pseudolikelihood is based on the observation that conditional probabilities
take this ratio-based form, and thus can be computed without knowledge of the
partition function. Suppose that we partition x into a, b, and c, where a contains
the variables we want to ﬁnd the conditional distribution over, b contains the
variables we want to condition on, and c contains the variables that are not part
of our query.
p(a | b) =

p(a, p(b)
p(a, b)
p̃(a, b)
= P
= P
.
p(b)
p(a, b, c)
p̃(a, b, c)
a,c
a,c

This quantity requires marginalizing out a, which can be a very eﬃcient operation
provided that a and c do not contain very many variables. In the extreme case, a
can be a single variable and c can be empty, making this operation require only
as many evaluations of p̃ as there are values of a single random variable.
Unfortunately, in order to compute the log-likelihood, we need to marginalize
out large sets of variables. If there are n variables total, we must marginalize a
set of size n − 1. By the chain rule of probability,
log p(x) = log p(x1) + log p(x 2 | x 1 ) + · · · + p(xn | x1:n−1 ).
In this case, we have made a maximally small, but c can be as large as x2:n.
What if we simply move c into b to reduce the computational cost? This yields
the pseudolikelihood (Besag, 1975) objective function:
n
X
i=1

log p(xi | x −i ).

If each random variable has k diﬀerent values, this requires only k × n evaluations of p̃ to compute, as opposed to the k n evaluations needed to compute the
partition function.
528

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

This may look like an unprincipled hack, but it can be proven that estimation
by maximizing the log pseudolikelihood is asymptotically consistent (Mase, 1995).
Of course, in the case of datasets that do not approach the large sample limit,
pseudolikelihood may display diﬀerent behavior from the maximum likelihood
estimator.
It is possible to trade computational complexity for deviation from maximum
likelihood behavior by using the generalized pseudolikelihood estimator (Huang
and Ogata, 2002). The generalized pseudolikelihood estimator uses m diﬀerent
sets S (i) , i = 1, . . . , m of indices variables that appear together on the left side of
the conditioning bar. In the extreme case of m = 1 and S (1) = 1, . . . , n the generalized pseudolikelihood recovers the log-likelihood. In the extreme case of m = n
and S(i) = {i}, the generalized pseudolikelihood recovers the pseudolikelihood.
The generalized pseudolikelihood objective function is given by
m
X
i=1

log p(xS (i) | x −S(i) ).

The performance of pseudolikelihood-based approaches depends largely on
how the model will be used. Pseudolikelihood tends to perform poorly on tasks
that require a good model of the full joint p(x), such as density estimation and
sampling. However, it can perform better than maximum likelihood for tasks
that require only the conditional distributions used during training, such as ﬁlling in small amounts of missing values. Generalized pseudolikelihood techniques
are especially powerful if the data has regular structure that allows the S index
sets to be designed to capture the most important correlations while leaving out
groups of variables that only have negligible correlation. For example, in natural
images, pixels that are widely separated in space also have weak correlation, so
the generalized pseudolikelihood can be applied with each S set being a small,
spatially localized window.
One weakness of the pseudolikelihood estimator is that it cannot be used with
other approximations that provide only a lower bound on p̃(x), such as variational
inference, which will be covered in chapter 19.4. This is because p̃ appears in the
denominator. A lower bound on the denominator provides only an upper bound
on the expression as a whole, and there is no beneﬁt to maximizing an upper
bound. This makes it diﬃcult to apply pseudolikelihood approaches to deep
models such as deep Boltzmann machines, since variational methods are one of
the dominant approaches to approximately marginalizing out the many layers of
hidden variables that interact with each other. However, pseudolikelihood is still
useful for deep learning, because it can be used to train single layer models, or
deep models using approximate inference methods that are not based on lower
bounds.
529

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

Pseudolikelihood has a much greater cost per gradient step than SML, due
its explicit computation of all of the conditionals. However, generalized pseudolikelihood and similar criteria can still perform well if only one randomly selected
conditional is computed per example (Goodfellow et al., 2013b), thereby bringing
the computational cost down to match that of SML.
Though the pseudolikelihood estimator does not explicitly minimize log Z, it
can still be thought of as having something resembling a negative phase. The
denominators of each conditional distribution result in the learning algorithm
suppressing the probability of all states that have only one variable diﬀering from
a training example.

18.4

Score Matching and Ratio Matching

Score matching (Hyvärinen, 2005b) provides another consistent means of training
a model without estimating Z or its derivatives. The strategy used by score
matching is to minimize the expected squared diﬀerence between the derivatives
of the model’s log pdf with respect to the input and the derivatives of the data’s
log pdf with respect to the input:
θ ∗ = min J(θ) =
θ

1
E ||∇ log pmodel (x; θ) − ∇x log pdata (x)||22 .
2 x x

Because the ∇x Z = 0, this objective function avoids the diﬃculties associated
with diﬀerentiating the partition function. However, it appears to have another
diﬃcult: it requires knowledge of the true distribution generating the training
data, pdata . Fortunately, minimizing J (θ) turns out to be equivalent to minimizing

2 !
m n
1 XX ∂2
1
∂
J̃(θ) =
log pmodel (x; θ) +
log pmodel (x; θ)
m
∂x2j
2 ∂xi
i=1 j=1

where {x (1) , . . . , x(m) } is the training set and n is the dimensionality of x.
Because score matching requires taking derivatives with respect to x, it is not
applicable to models of discrete data. However, the latent variables in the model
may be discrete.
Like the pseudolikelihood, score matching only works when we are able to evaluate log p̃(x) and its derivatives directly. It is not compatible with methods that
only provide a lower bound on log p̃(x), because we are not able to conclude anything about the relationship between the derivatives and second derivatives of the
lower bound, and the relationship of the true derivatives and second derivatives
needed for score matching. This means that score matching cannot be applied to
estimating models with complicated interactions between the hidden units, such
530

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

as sparse coding models or deep Boltzmann machines. Score matching can be
used to pretrain the ﬁrst hidden layer of a larger model. Score matching has not
been applied as a pretraining strategy for the deeper layers of a larger model,
because the hidden layers of such models usually contain some discrete variables.
While score matching does not explicitly have a negative phase, it can be
viewed as a version of contrastive divergence using a speciﬁc kind of Markov
chain (Hyvärinen, 2007a). The Markov chain in this case is not Gibbs sampling,
but rather a diﬀerent approach that makes local moves guided by the gradient.
Score matching is equivalent to CD with this type of Markov chain when the size
of the local moves approaches zero.
Lyu (2009) generalized score matching to the discrete case (but made an
error in their derivation that was corrected by Marlin et al. (2010)). Marlin
et al. (2010) found that generalized score matching (GSM) does not work in high
dimensional discrete spaces where the observed probability of many events is 0.
A more successful approach to extending the basic ideas of score matching
to discrete data is ratio matching (Hyvärinen, 2007b). Ratio matching applies
speciﬁcally to binary data. Ratio matching consists of minimizing the following
objective function:

2
m
n
1 XX
1

J(RM)(θ) =


(i) ;θ)
(
p
x
m i=1 j=1 1 + model
pmodel (f (x(i) ,j);θ)
where f (x, j) return x with the bit at position j ﬂipped. Ratio matching avoids
the partition function using the same trick as the pseudolikelihood estimator: in
a ratio of two probabilities, the partition function cancels out. Marlin et al.
(2010) found that ratio matching outperforms SML, pseudolikelihood, and GSM
in terms of the ability of models trained with ratio matching to denoise test set
images.
Like the pseudolikelihood estimator, ratio matching requires n evaluations of p̃
per data point, making its computational cost per update roughly n times higher
than that of SML.
Like the pseudolikelihood estimator, ratio matching can be thought of as pushing down on all fantasy states that have only one variable diﬀerent from a training
example. Since ratio matching applies speciﬁcally to binary data, this means that
it acts on all fantasy states within Hamming distance 1 of the data.
Ratio matching can also be useful as the basis for dealing with high-dimensional
sparse data, such as word count vectors. This kind of data poses a challenge for
MCMC-based methods because the data is extremely expensive to represent in
dense format, yet the MCMC sampler does not yield sparse values until the model
531

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

has learned to represent the sparsity in the data distribution. Dauphin and Bengio (2013) overcame this issue by designing an unbiased stochastic approximation
to ratio matching. The approximation evaluates only a randomly selected subset
of the terms of the objective, and does not require the model to generate complete
fantasy samples.

18.5

Denoising Score Matching

In some cases we may wish to regularize score matching, by ﬁtting a distribution
Z
p smoothed(x) = p data(x + y)q(y | x)dy
rather than the true p data. This is especially useful because in practice we usually
do not have access to the true pdata but rather only an empirical distribution
deﬁned by samples from it. Any consistent estimator will, given enough capacity, make pmodel into a set of Dirac distributions centered on the training points.
Smoothing by q helps to reduce this problem, at the loss of the asymptotic consistency property. Kingma and LeCun (2010b) introduced a procedure for performing regularized score matching with the smoothing distribution q being normally
distributed noise.
Surprisingly, some denoising autoencoder training algorithms correspond to
training energy-based models with denoising score matching (Vincent, 2011b).
The denoising autoencoder variant of the algorithm is signiﬁcantly less computationally expensive than score matching. Swersky et al. (2011) showed how to
derive the denoising autoencoder for any energy-based model of real data. This
approach is known as denoising score matching (SMD).

18.6

Noise-Contrastive Estimation

Most techniques for estimating models with intractable partition functions do not
provide an estimate of the partition function. SML and CD estimate only the
gradient of the log partition function, rather than the partition function itself.
Score matching and pseudolikelihood avoid computing quantities related to the
partition function altogether.
Noise-contrastive estimation (NCE) (Gutmann and Hyvarinen, 2010) takes a
diﬀerent strategy. In this approach, the probability distribution by the model is
represented explicitly as
log p model(x) = log p̃ model (x; θ) + c,
532

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

where c is explicitly introduced as an approximation of − log Z(θ). Rather than
estimating only θ, the noise contrastive estimation procedure treats c as just
another parameter and estimates θ and c simultaneously, using the same algorithm
for both. The resulting thus may not correspond exactly to a valid probability
distribution, but will become closer and closer to being valid as the estimate of c
improves.2
Such an approach would not be possible using maximum likelihood as the
criterion for the estimator. The maximum likelihood criterion would choose to set
c arbitrarily high, rather than setting c to create a valid probability distribution.
NCE works by reducing the unsupervised learning problem of estimating p(x)
to a supervised learning problem. This supervised learning problem is constructed
in such a way that maximum likelihood estimation in this supervised learning
problem deﬁnes an asymptotically consistent estimator of the original problem.
Speciﬁcally, we introduce a second distribution, the noise distribution p noise(x).
The noise distribution should be tractable to evaluate and to sample from. We
can now construct a model over both x and a new, binary class variable y. In the
new joint model, we specify that
1
p joint model (y = 1) = ,
2
pjoint model (x | y = 1) = pmodel (x),
and

p joint model (x | y = 0) = p noise(x).

In other words, y is a switch variable that determines whether we will generate x
from the model or from the noise distribution.
We can construct a similar joint model of training data. In this case, the
switch variable determines whether we draw x from the data or from the noise
distribution. Formally, ptrain (y = 1) = 12 , p train (x | y = 1) = pdata (x), and
ptrain (x | y = 0) = pnoise (x).
We can now just use standard maximum likelihood learning on the supervised
learning problem of ﬁtting p joint model to ptrain :
θ, c = arg max E x,y∼ptrain log pjoint model (y | x).
θ,c

It turns out that p joint model is essentially a logistic regression model applied
to the diﬀerence in log probabilities of the model and the noise distribution:
p joint model(y = 1 | x) =
2

pmodel (x)
p model (x) + pnoise (x)

NCE is also applicable to problems with a tractable partition function, where there is no
need to introduce the extra parameter c. However, it has generated the most interest as a means
of estimating models with diﬃcult partition functions.
533

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

=

1
1+

p noise(x)
pmodel(x)

1


(x)
1 + exp log pp noise
model(x)


pnoise(x)
= σ − log
pmodel (x)

=

= σ (log pmodel (x) − log p noise(x)) .

NCE is thus simple to apply so long as log p̃model is easy to backpropagate
through, and, as speciﬁed above, noise is easy to evaluate (in order to evaluate
pjoint model ) and sample from (in order to generate the training data).
NCE is most successful when applied to problems with few random variables,
but can work well even if those random variables can take on a high number of
values. For example, it has been successfully applied to modeling the conditional
distribution over a word given the context of the word (Mnih and Kavukcuoglu,
2013). Though the word may be drawn from a large vocabulary, there is only one
word.
When NCE is applied to problems with many random variables, it becomes
less eﬃcient. The logistic regression classiﬁer can reject a noise sample by identifying any one variable whose value is unlikely. This means that learning slows down
greatly after pmodel has learned the basic marginal statistics. Imagine learning a
model of images of faces, using unstructured Gaussian noise as pnoise . If pmodel
learns about eyes, it can reject almost all unstructured noise samples without
having learned anything other facial features, such as mouths.
The constraint that pnoise must be easy to evaluate and easy to sample from
can be overly restrictive. When p noise is simple, most samples are likely to be too
obviously distinct from the data to force pmodel to improve noticeably.
Like score matching and pseudolikelihood, NCE does not work if only a lower
bound on p̃ is available. Such a lower bound could be used to construct a lower
bound on p joint model(y = 1 | x), but it can only be used to construct an upper
bound on p joint model (y = 0 | x), which appears in half the terms of the NCE
objective. Likewise, a lower bound on p noise is not useful, because it provides only
an upper bound on p joint model(y = 1 | x).
TODO– put herding in this chapter? and if not, where to put it?
TODO– cite the Bregman divergence paper?

18.7

Estimating the Partition Function

While much of this chapter is dedicated to describing methods for working around
the unknown and intractable partition function Z(θ) associated with an undi534

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

rected graphical model; in this section we will discuss several methods for directly
estimating the partition function.
Estimating the partition function can be important because we require it if
we wish to compute the normalized likelihood of data. This is often important in
evaluating the model, monitoring training performance, and comparing models
to each other.
For example, imagine we have two models: MA : p A(x; θ A) = Z1A p̃A (x; θA )
and M B : pB (x; θB) = Z1B p̃B (x; θ B). A common way to compare the models is to evaluate the likelihood of an i.i.d. test dataset of size Ntest : Dtest =
Q
Q
Ntest under both models. If
(t) θ ) >
(t)
{x(t)
}
p
(x
A
A
i
t
t p B(x θ B) or equivalently
P
P
if t ln pA (x(t) θ A)− t ln p B (x(t) θ B ) > 0, then we say that MA is a better model
than M B (or, at least, it is a better model of the test set). More speciﬁcally, to
say that M A is better than MB, we need that:
X
X
(t)
ln pA(x θ A ) −
ln pB (x(t)θ B ) > 0
X
t

X
t

t



t


X
ln p̃A(x (t)θA ) − ln Z(θA ) −
ln p̃ B(x (t) θ B) − ln Z(θB ) > 0

 t
ln p̃A (x (t)θA ) − ln p̃ B(x (t); θ B ) − Ntest ln Z(θ A ) + Ntest ln Z(θB ) > 0
X
t

!
p̃A (x(t); θA )
Z(θA)
ln
−
N
ln
> 0.
test
Z(θB )
p̃B (x(t); θB )

TODO: too much repetition of ”to know” TODO: be more speciﬁc about what
it means to ”compare”, does this mean to take the ratio of two likelihoods? In
order to compare two models we need to compare not only their unnormalized
probabilities, but also their partition functions. It is interesting to note that, in
order to compare these models, we do not actually need to know the value of their
partition function. We need only know their ratio. That is, we need to know their
relative value, up to some shared constant. If, however, we wanted to know the
actual probability of the test data under either MA or MB , we would need to
know the actual value of the partition functions. That said, if we knew the ratio
B)
of two partition functions, R = Z(θ
, and we knew the actual value of just one
Z(θA )
of the two, say Z(θA ), we can compute the value of the other:
Z(θB ) = R × Z(θA ) =

Z(θB )
Z(θA)
Z(θA )

We can make use of this observation to estimate the partition functions of
undirected graphical models.
535

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

For a given probability distribution, say p 1(x), the partition function is deﬁned
as
Z
Z1 = p̃ 1 (x) dx
(18.5)
where the integral is over the domain of x. Of course, in the case of discrete
x, we replace the integral with a sum. For convenience, we have suppressed the
dependency of both the partition functions and the unnormalized distributions
on the model parameters.
A simple way to estimate the partition function is to use a Monte Carlo method
such as simple importance sampling. Here we consider a proposal distribution,
say p 0 (x), from which we can sample and evaluate both its partition function Z0 ,
and its unnormalized distribution p̃ 0 (x).
Z
Z1 = p̃1 (x) dx
Z
p0 (x)
=
p̃ 1 (x) dx
p0 (x)
Z
p̃1(x)
= Z 0 p0(x)
dx
p̃0(x)
K

X p̃(x (k))
Z1
≈
Z0
p̃ 0(x(k) )
k=1

s.t. : x (k) ∼ p0

(18.6)

In the last line, we make a Monte Carlo approximation of the integral using
samples drawn from p 0(x) and then weigh each sample with the ratio of the
unnormalized p̃1 and the proposal p 0 each evaluated at that sample.
If the distribution p0 is close to p1, this can be an eﬀective way of estimating
the partition function (Minka, 2005). Unfortunately, most of the time p 1 is both
complicated, i.e. multimodal, and deﬁned over a high dimensional space. It is
diﬃcult to ﬁnd a tractable p 0 that is simple enough to evaluate while still being
close enough to p1 to result in a high quality approximation. If p0 and p 1 are not
close, most samples from p 0 will have low probability under p1 and therefore make
(relatively) negligible contribution to the sum in Eq. 18.6. Having few samples
with signiﬁcant weights in this sum will result in an estimator with high variance,
i.e. a poor quality estimator.
TODO: quantify this
We now turn to two related strategies developed to cope with the challenging task of estimating partition functions for complex distributions over highdimensional spaces: annealed importance sampling and Bennett’s ratio acceptance method. Both start with the simple importance sampling strategy introduced above and both attempt to overcome the problem of the proposal p 0 being
536

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

too far from p 1 by introducing intermediate distributions that attempt to bridge
the gap between p0 and p1 .

18.7.1

Annealed Importance Sampling

TODO– describe how this is the main way of evaluating p(x) when you want
to get test set likelihoods but can’t be used for training TODO– also mention
Guillaume’s ”tracking the partition function” paper?
In situations where D KL (p0 |p1 ) is large (i.e., where there is little overlap between p0 and p1), AIS attempts to bridge the gap by introducing intermediate
distributions. Consider a sequence of distributions pη0 , . . . , pηn , with 0 = η0 <
η1 < · · · < η n−1 < ηn = 1 so that the ﬁrst and last distributions in the sequence
1
are p 0 and p1 respectively. We can now write the ratio Z
Z 0 as
Z1
Z1 Z η1
Zηn−1
=
···
Z0
Z0 Z η1
Zηn−1
Zη 1 Z η2
Zη n−1 Z 1
···
=
Z 0 Z η1
Zη n−2 Zηn−1
=

n−1
Y

j=0

Z ηj+1
Zηj

(18.7)

Provided the distributions pηj and p ηj +1 , for all 0 ≤ j ≤ n − 1, are suﬃciently
Zη j+1
Z η j using
1
estimate of Z
Z 0.

close, we can reliably estimate each of the factors

simple importance

sampling and then use these to obtain an
Where do these intermediate distributions come from? Just as the original
proposal distribution p0 is a design choice, so is the sequence of distributions
pη1 . . . pηn−1 . That is, it can be speciﬁcally constructed to suit the problem domain. One general-purpose and popular choice for the intermediate distributions
is to use the weighted geometric average of the target distribution p 1 and the
starting proposal distribution (for which the partition function is known) p0:
η

1−η j

pηj ∝ p 1j p0

(18.8)

In order to sample from these intermediate distributions, we deﬁne a series of
Markov chain transition functions T ηj (x 0, x) that deﬁne the probability distribution of transitioning from x 0 to x. T η j(x 0, x) is deﬁned to leave p ηj (x) invariant:
Z
pηj (x) = pηj (x0 )Tηj (x0 , x) dx0
(18.9)
537

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

These transitions may be constructed as any Markov chain Monte Carlo method
(e.g.. Metropolis-Hastings, Gibbs), including methods involving multiple scans or
other iterations.
The AIS sampling strategy is then to generate samples from p 0 and then use
the transition operators to sequentially generate samples from the intermediate
distributions until we arrive at samples from the target distribution p 1 :
• for k = 1 . . . K

(k)

– Sample x η1 ∼ p0 (x)
– Sample
(k)
(k)
vx η2 ∼ Tη 1(x η1 , x)
– ...

(k)

(k)

– Sample x ηn−1 ∼ T ηn−2 (xη n−2 , x)
(k)

(k)

– Sample x ηn ∼ T ηn−1 (x ηn−1 , x)
• end
For sample k, we can derive the importance weight by chaining together the
importance weights for the jumps between the intermediate distributions given in
Eq. 18.7.
(k)
(k)
p̃η1 (x (k)
p̃1 (x 1 )
η 1 ) p̃ η2(x η 2 )
(k)
w =
...
(18.10)
(k)
(k)
(k)
p̃0(x 0 ) p̃1(xη1 )
p̃ηn−1 (xηn−1)
To avoid computational issues such as overﬂow, it is probably best to do the
computation in log space, i.e. ln w (k) = ln p̃η 1(x) − ln p̃ 0(x) + . . . .
With the sampling procedure thus deﬁne and the importance weights given
in Eq. 18.10, the estimate of the ratio of partition functions is given by:
K
Z1
1 X (k)
≈
w
Z0 K

(18.11)

k=1

In order to verify that this procedure deﬁnes a valid importance sampling
scheme, we can show that the AIS procedure corresponds to simple importance
sampling on an extended state space with points sampled over the product space:
[xη1 , . . . , x ηn−1 , x 1] Neal (2001).
We deﬁne the distribution over the extended space as:
p̃(xη 1 , . . . , xηn−1 , x1 ) = p̃1(x 1)T̃ηn−1 (x1, x ηn−1 )T̃η n−2(x ηn−1 , xηn−2 ) . . . T̃η 1(x η2, x η 1 )
(18.12)
538

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

where T̃ a is the reverse of the transition operator deﬁned by Ta (via an application
of Bayes’ rule):
T̃a (x, x0 ) =

p a(x0 )
p̃a(x 0 )
T a(x 0 , x) =
Ta (x 0, x).
p a (x)
p̃ a(x)

(18.13)

Plugging the above into the expression for the joint distribution on the extended
state space given in Eq. 18.12, we get:
p̃(x η1 , . . . , xη n−1 , x 1)
p̃ ηn−1 (x ηn−1 )
p̃η (x η )
p̃ (x )
T ηn−1 (xηn−1, x 1 ) n−2 n−2 T ηn−2 (x ηn−2 , xη n−1) . . . η 1 η 1 Tη 1 (xη1 , xη2 )
p̃η n−1 (x1 )
p̃η n−2 (x ηn−1 )
p̃η 1 (xη 2 )
p̃ η (xη n−1 )
p̃ η (xη n−2)
p̃1(x1 )
=
T ηn−1 (xη n−1, x 1 ) n−1
T η n−2(xη n−2, xη n−1 ) . . . n−2
T η 1 (x η1, xη2 )p̃η 1 (xη1 )
p̃η n−1(x 1)
p̃ ηn−2 (xη n−1 )
p̃η 1 (xη 2)
(18.14)
= p̃1(x1 )

If we now consider the sampling scheme given above as a means of generating
samples from a proposal distribution q over the extended state, with its distribution given by:
q(xη 1 , . . . , x ηn−1 , x1 ) = p 0 (xη1 )Tη1 (xη1, x η2 ) . . . Tηn−1 (xηn−1 , x1)

(18.15)

We have a joint distribution on the extended space given by Eq. 18.14. Taking
q(x η 1 , . . . , xηn−1 , x1) as the proposal distribution on the extended state space from
which we will draw samples, it remains to determine the importance weights:
(k)

w

(k)

(k)

(k)

p̃(xη 1 , . . . , xηn−1 , x1 )
p̃1(x 1 )
p̃η (x η ) p̃η1 (x η1 )
=
=
. . . 2 (k)2
(k)
(k)
q(xη 1 , . . . , xηn−1 , x1 ) p̃η
p̃ 1(xη 1 ) p̃ 0(x0 )
n−1 (xηn−1 )

(18.16)

These weights are the same as proposed for AIS. Thus we can interpret AIS as
simple importance sampling applied to an extended state and its validity follows
immediately from the validity of importance sampling.
Annealed importance sampling (AIS) was ﬁrst discovered by Jarzynski (1997)
and then again, independently, by Neal (2001). It is currently the most common
way of estimating the partition function for undirected probabilistic models. The
reasons for this may have more to do with the publication of an inﬂuential paper
Salakhutdinov and Murray (2008) describing its application to estimating the
partition function of restricted Boltzmann machines and deep belief networks
than with any inherent advantage the method has over the other method described
below.
A discussion of the properties of the AIS estimator (e.g.. its variance and
eﬃciency) can be found in Neal (2001).
539

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

18.7.2

Bridge Sampling

Bridge sampling Bennett (1976) is another method that, like AIS, addresses the
shortcomings of importance sampling; however it does so in a diﬀerent but related manner. Rather than chaining together a series of intermediate distributions, bridge sampling relies on a single distribution p∗, known as the bridge,
to interpolate between a distribution with known partition function, p0 , and a
distribution p1 for which we are trying to estimate the partition function Z1.
Bridge sampling estimates the ratio Z 1/Z 0 as the ratio of the expected importance weights between p̃0 and p̃∗ and between p̃ 1 and p̃∗ :
,
K
(k)
K
(k)
X
X
Z1
p̃ ∗ (x0 )
p̃ ∗(x1 )
≈
(18.17)
(k)
(k)
Z0
p̃ 0 (x )
p̃ 1(x )
k=1

0

k=1

1

If the bridge distribution p ∗ is chosen carefully to have a large overlap of support
with both p0 and p1, then bridge sampling can allow the distance between two
distributions (or more formally, DKL (p0|p1)) to be much larger than with standard
importance sampling.
(opt)
It can be shown than the optimal bridging distribution is given by p∗ (x) ∝
p̃ 0(x)p̃1 (x)
rp̃0 (x)+p̃ 1(x) where r = Z1 /Z 0.
This appears to be an unworkable solution as it would seem to require the
very quantity we are trying to estimate, i.e. Z1 /Z0 . However, it is possible to
start with a coarse estimate of r and use the resulting bridge distribution to reﬁne
our estimate recursively Neal (2005).
TODO: illustration of the bridge distribution

18.7.3

Extensions

Linked importance sampling Both AIS and bridge sampling have their advantages. If DKL (p0|p1 ) is not too large (i.e. if p0 and p1 are suﬃciently close)
bridge sampling can be a more eﬀective means of estimating the ratio of partition
functions than AIS. If, however, the two distributions are too far apart for a single distribution p ∗ to bridge the gap then one can at least use AIS with potential
many intermediate distributions to span the distance between p 0 and p 1. Neal
(2005) showed how his linked importance sampling method leveraged the power
of the bridge sampling strategy to bridge the intermediate distributions used in
AIS to signiﬁcantly improve the overall partition function estimates.
Tracking the partition function while training Using a combination of
bridge sampling, AIS and parallel tempering, Desjardins et al. (2011) devised
a scheme to track the partition function of an RBM throughout the training
540

CHAPTER 18. CONFRONTING THE PARTITION FUNCTION

process. The strategy is based on the maintenance of independent estimates of
the partition functions of the RBM at every temperature operating in the parallel
tempering scheme. The authors combined bridge sampling estimates of the ratios
of partition functions of neighboring chains (i.e. from parallel tempering) with
AIS estimates across time to come up with a low variance estimate of the partition
functions at every iteration of learning.

541

Chapter 19

Approximate inference
TODO: somewhere in this chapter, point out that variational inference implicitly deﬁnes a recurrent net, stochastic approximate inference implicitly deﬁnes a
stochastic recurrent net
Misplaced TODO: discussion of the diﬀerent directions of the KL divergence,
and the eﬀects on ignoring / preserving modes
Many probabilistic models are diﬃcult to train because it is diﬃcult to perform
inference in them. In the context of deep learning, we usually have a set of visible
variables v and a set of latent variables h. The challenge of inference usually refers
to the diﬃcult problem of computing p(h | v) or taking expectations with respect
to it. Such operations are often necessary for tasks like maximum likelihood
learning.
Many simple graphical models with only one hidden layer, such as restricted
Boltzmann machines and probabilistic PCA are deﬁned in a way that makes inference operations like computing p(h | v) or taking expectations with respect to
it simple. Unfortunately, most graphical models with multiple layers of hidden
variables, such as deep belief networks and deep Boltzmann machines have intractable posterior distributions. Exact inference requires an exponential amount
of time in these models. Even some models with only a single layer, such as sparse
coding, have this problem.
Intractable inference problems usually arise from interactions between latent
variables in a structured graphical model. See Fig. 19.1 for some examples. These
interactions may be due to direct interactions in undirected models or “explaining
away” interactions between mutual ancestors of the same visible unit in directed
models.

542

CHAPTER 19. APPROXIMATE INFERENCE

Figure 19.1: Intractable inference problems are usually the result of interactions between
latent variables in a structured graphical model. These can be due to direct edges,
or due to paths that are activated when the child of a V-structure is observed. Left)
A semi-restricted Boltzmann machine with connections between hidden units. These
direct connections between latent variables make the posterior distribution complicated.
Center) A deep Boltzmann machine, organized into layers of variables without intralayer connections, still has an intractable posterior distribution due to the connections
between layers. Right) This directed model has interactions between latent variables
when the visible variables are observed, because ever two latent variables are co-parents.
Note that it is still possible to have these graph structures yet have tractable inference.
For example, probabilistic PCA has the graph structure shown in the right, yet simple
inference due to special properties of the speciﬁc conditional distributions it uses (linearGaussian conditionals with mutually orthogonal basis vectors). MISPLACED TODO–
make sure probabilistic PCA is at least deﬁned somewhere in the book

543

CHAPTER 19. APPROXIMATE INFERENCE

19.1

Inference as Optimization

Many approaches to confronting the problem of diﬃcult inference make use of the
observation that exact inference can be described as an optimization problem.
Speciﬁcally, assume we have a probabilistic model consisting of observed variables v and latent variables h. We would like to compute the log probability of
the observed data, log p(v; θ). Sometimes it is too diﬃcult to compute log p(v; θ)
if it is costly to marginalize out h. Instead, we can compute a lower bound on it.
This bound is called the evidence lower bound (ELBO). Other names for this lower
bound include the negative variational free energy and the negative Helmholtz free
energy.
Speciﬁcally, this lower bound is deﬁned as TODO– ﬁgure out why I
was making q be bm in some but not all places
L(v, θ, q) = log p(v; θ) − DKL (q(h)kp(h | v; θ))
where q is an arbitrary probability distribution over h.
TODO: the below equations framebust It is straightforward to see that this
is a lower bound on log p(v):

 X


X
p(v, h)
p(v, h)
ln p(v) = ln p(v) +
q(h | v) ln
−
q(h | v) ln
q(h | v)
q(h | v)
h
h

 X

 X
X
p(v, h)
p(v, h)
=
q(h | v) ln
−
q(h | v) ln
+
q(h | v) log p(v)
q(h | v)
q(h | v)
h
h


 
 h

X
X
p(v, h)
p(v, h)
=
q(h | v) ln
−
q(h | v) ln
− ln p(v)
q(h
|
v)
q(h
|
v)
h
h

 X


X
p(v, h)
p(v, h)
=
q(h | v) ln
−
q(h | v) ln
q(h | v)
p(v)q(h | v)
h
h




X
X
p(h | v)
p(v, h)
=
q(h | v) ln
−
q(h | v) ln
q(h | v)
q(h | v)
h

h

= L(q) + KL(qkp)

Because the diﬀerence log p(v) and L(v, θ, q) is given by the KL-divergence and
because the KL-divergence is always non-negative, we can see that L always has
at most the same value as the desired log probability, and is equal to it if and
only if q is the same distribution as p(h | v).
Surprisingly, L can be considerably easier to compute for some distributions
q. Simple algebra shows that we can rearrange L into a much more convenient
form:
L(v, θ, q) = log p(v; θ) − DKL (q(h)kp(h | v; θ))
544

CHAPTER 19. APPROXIMATE INFERENCE

= log p(v; θ) − Eh∼q

log q(h)
log p(h | v)

= log p(v; θ) − Eh∼q

log q(h)

log

p(h,v)
p(v)

= log p(v; θ) − Eh∼q [log q(h) − log p(h, v) + log p(v)]
= −Eh∼q [log q(h) − log p(h, v)] .
This yields the more canonical deﬁnition of the evidence lower bound,
L(v, θ, q) = Eh∼q [log p(h, v)] + H(q).

(19.1)

The ﬁrst term of L is known as the energy term. The second term is known
as the entropy term. For an appropriate choice of q, both terms can be easy to
compute. The only question is how close to p(h | v) the distribution q will be.
This determines how good of an approximation L will be for log p(v).
We can thus think of inference as the procedure for ﬁnding the q that maximizes L. Exact inference maximizes L perfectly. Throughout this chapter, we
will show how many forms of approximate inference are possible. No matter what
choice of q we use, L will give us a lower bound on the likelihood. We can get
tighter or looser bounds that are cheaper or more expensive to compute depending
on how we choose to approach this optimization problem. We can obtain a poorly
matched q but reduce the computational cost by using an imperfect optimization
procedure, or by using a perfect optimization procedure over a restricted family
of q distributions.

19.2

Expectation Maximization

Expectation maximization (EM) is a popular training algorithm for models with
latent variables. It consists of alternating between two steps until convergence:
• The E-step (Expectation step): Set q(h(i) ) = p(h (i) | v ( i); θ) for all indices i
of the training examples v (i) we want to train on (both batch and minibatch
variants are valid). By this we mean q is deﬁned in terms of the current
value of θ; if we vary θ then p(h | v; θ) will change but q(h) will not.
P
• The M-step (Maximization step): Completely or partially maximize i L(v( i), θ, q)
with respect to θ using your optimization algorithm of choice.
This can be viewed as a coordinate ascent algorithm to maximize L. On one
step, we maximize L with respect to q, and on the other, we maximize L with
respect to θ.
545

CHAPTER 19. APPROXIMATE INFERENCE

Stochastic gradient ascent on latent variable models can be seen as a special
case of the EM algorithm where the M step consists of taking a single gradient
step. Other variants of the EM algorithm can make much larger steps. For some
model families, the M step can even be performed analytically, jumping all the
way to the optimal solution given the current q.
Even though the E-step involves exact inference, we can think of the EM
algorithm as using approximate inference in some sense. Speciﬁcally, the M-step
assumes that the same value of q can be used for all values of θ. This will introduce
a gap between L and the true log p(v) as the M-step moves further and further.
Fortunately, the E-step reduces the gap to zero again as we enter the loop for the
next time.
The EM algorithm is a workhorse of classical machine learning, and it can be
considered to be used in deep learning in the sense that stochastic gradient ascent
can be seen as EM with a very simple and small M step. However, because L
can not be analytically maximized for many interesting deep models, the more
general EM framework as a whole is typically not explored in the deep learning
research community.
TODO–cite the emview paper

19.3

MAP Inference: Sparse Coding as a Probabilistic Model

TODO synch up with other sections on sparse coding
Many versions of sparse coding can be cast as probabilistic models. For example, suppose we encode visible data v ∈ R n with latent variables h ∈ R m . We
can use a prior to encourage our latent code variables to be sparse:
p(h) = T ODO.
We can deﬁne the visible units to be Gaussian with an aﬃne transformation from
the code to the mean of the Gaussian:
v ∼ N (v | µ + W h, β −1)
where β is a diagonal precision matrix to maintain tractability.
Computing p(h | v) is diﬃcult. TODO explain why
One operation that we can do is perform maximum a posteriori (MAP) inference, which means solving the following optimization problem:
h∗ = arg max p(h | v).
This yields the familiar optimization problem
546

CHAPTER 19. APPROXIMATE INFERENCE

TODO synch with other sparse coding sections, make sure the other sections
talk about using gradient descent, feature sign, ISTA, etc.
This shows that the popular feature extraction strategy for sparse coding can
be justiﬁed as having a probabilistic interpretation–it may be MAP inference in
this probabilistic model ( there are other probabilistic models that yield the same
optimization problem, so we cannot positively identify this speciﬁc model from
the feature extraction process ).
Excitingly, MAP inference of h given v also has an interpretation in terms
of maximizing the evidence lower bound. Speciﬁcally, MAP inference maximizes
L with respect to q under the constraint that q take the form form of a Dirac
distribution. During learning of sparse coding, we alternate between using convex
optimization to extract the codes, and using convex optimization to update W
to achieve the optimal reconstruction given the codes. This turns out to be
equivalent to maximizing L with respect to θ for the q that was obtained from
MAP inference. The learning algorithm can be thought of as EM restricted to
using a Dirac posterior. In other words, rather than performing learning exactly
using standard inference, we learn to maximize a bound on the true likelihood,
using exact MAP inference.

19.4

Variational Inference and Learning

One common diﬃculty in probabilistic modeling is that the posterior distribution
p(h | v) is infeasible to compute for many models with hidden variables h and
visible variables v. Expectations with respect to this distribution may also be
intractable.
Consider as an example the binary sparse coding model. In this model, the
input v ∈ Rn is formed by adding Gaussian noise to the sum of m diﬀerent
components which can each be present or absent. Each component is switched
on or oﬀ by the corresponding hidden unit in h ∈ {0, 1}m :
p(hi = 1) = σ(b i)
p(v | h) = N (v | W h, β −1 )
where b is a learn-able set of biases, W is a learn-able weight matrix, and β
is a learn-able, diagonal precision matrix.
Training this model with maximum likelihood requires taking the derivative
with respect to the parameters. Consider the derivative with respect to one of
the biases:
∂
log p(v)
∂bi
547

CHAPTER 19. APPROXIMATE INFERENCE

=
=
=

∂
∂bi

∂
∂b i

P

=

P

=

X

= Eh

h p(h)p(v

p(v)

| h)

| h) ∂b∂ p(h)
i

p(v)

X
h

p(v)
p(v)
P
h p(h, v)
p(v)

h p(v

h

=

∂
∂b i

p(h | v)
p(h | v)

p(h|v)

∂
∂bi p(h)

p(h)
∂
∂bi p(h)

p(h)

∂
log p(h).
∂bi

This requires computing expectations with respect to p(h | v). Unfortunately,
p(h | v) is a complicated distribution. See Fig. 19.2 for the graph structure of
p(h, v) and p(h | v). The posterior distribution corresponds to the complete
graph over the hidden units, so variable elimination algorithms do not help us to
compute the required expectations any faster than brute force.
One solution to this problem is to use variational methods. Variational methods involve using a simple distribution q(h) to approximate the true, complicated
posterior p(h | v). The name “variational” derives from their frequent use of a
branch of mathematics called calculus of variations. However, not all variational
methods use calculus of variations.
TODO variational inference involves maximization of a BOUND TODO variational inference also usually involves a restriction on the function family
TODO

19.4.1

Discrete Latent Variables

TODO– BSC example

548

CHAPTER 19. APPROXIMATE INFERENCE

h1

h2

v1

h3

v2

h4

v3

h1

h3

h2

h4

Figure 19.2: The graph structure of a binary sparse coding model with four hidden units.
Left) The graph structure of p(h, v). Note that the edges are directed, and that every
two hidden units co-parents of every visible unit. Right) The graph structure of p(h | v).
In order to account for the active paths between co-parents, the posterior distribution
needs an edge between all of the hidden units.

19.4.2

Calculus of Variations

Many machine learning techniques are based on minimizing a function J(θ) by
ﬁnding the input vector θ ∈ Rn for which it takes on its minimal value. This can
be accomplished with multivariate calculus and linear algebra, by solving for the
critical points where ∇ θ J(θ) = 0. In some cases, we actually want to solve for a
function f (x), such as when we want to ﬁnd the probability density function over
some random variable. This is what calculus of variations enables us to do.
A function of a function f is known as a functional J [f]. Much as we can
take partial derivatives of a function with respect to elements of its vector-valued
argument, we can take functional derivatives, also known as variational derivatives
of a functional J[f ] with respect to respect to individual values of the function
f (x). The functional derivative of the functional J with respect to the value of
δ
the function f at point x is denoted δf(x)
J.
A complete formal development of functional derivatives is beyond the scope
of this book. For our purposes, it is suﬃcient to state that for diﬀerentiable
functions f (x) and diﬀerentiable functions g(y, x) with continuous derivatives,
that
Z
δ
∂
g (f (x), x) dx =
g(f(x), x).
(19.2)
δf(x)
∂y
To gain some intuition for this identity, one can think of f(x) as being a vector
with uncountably many elements, indexed by a real vector x. In this (somewhat
incomplete view), the identity providing the functional derivatives is the same as

549

CHAPTER 19. APPROXIMATE INFERENCE

we would obtain for a vector θ ∈ R n indexed by positive integers:
∂ X
∂
g(θ j , j) =
g(θi , i).
∂θi
∂θ i
j

Many results in other machine learning publications are presented using the more
general Euler-Lagrange equation which allows g to depend on the derivatives of f
as well as the value of f , but we do not need this fully general form for the results
presented in this book.
To optimize a function with respect to a vector, we take the gradient of the
function with respect to the vector and solve for the point where every element
of the gradient is equal to zero. Likewise, we can optimize a functional by solving
for the function where the functional derivative at every point is equal to zero.
As an example of how this process works, consider the problem of ﬁnding the
probability distribution function over x ∈ R that has maximal Shannon entropy.
Recall that the entropy of a probability distribution p(x) is deﬁned as
H[p] = −E x log p(x).
For continuous values, the expectation is an integral:
Z
H[p] = − p(x) log p(x)dx.
We cannot simply maximize H(x) with respect to the function p(x), because
the result might not be a probability distribution. Instead, we need to use Lagrange multipliers, to add a constraint that p(x) integrate to 1. Also, the entropy
increases without bound as the variance increases, so we can only search for the
distribution with maximal entropy for ﬁxed variance σ 2 . Finally, the problem
is underdetermined because the distribution can be shifted arbitrarily without
changing the entropy. To impose a unique solution, we add a constraint that the
mean of the distribution be µ. The Lagrangian functional for this optimization
problem is
Z



L[p] = λ 1
p(x)dx − 1 + λ 2 (E[x] − µ) + λ3 E[(x − µ) 2] − σ2 + H[p]
=

Z



λ1p(x) + λ2 p(x)x + λ3 p(x)(x − µ)2 − p(x) log p(x) dx − λ1 − µλ 2 − σ2 λ3.

To minimize the Lagrangian with respect to p, we set the functional derivatives
equal to 0:
∀x,

δ
L = λ1 + λ2x + λ 3(x − µ) 2 − 1 − log p(x) = 0.
δp(x)
550

CHAPTER 19. APPROXIMATE INFERENCE

This condition now tells us the functional form of p(x). By algebraically rearranging the equation, we obtain


p(x) = exp −λ1 − λ2x + λ3 (x − µ)2 + 1 .

We never assumed directly that p(x) would take this functional form; we
obtained the expression itself by analytically minimizing a functional. To ﬁnish
the minimization problem, we must choose the λ values to ensure that all of our
constraints are satisﬁed. We are free to choose any λ values, because the gradient
of the Lagrangian with respect to the λ variables is zero so long as the
√ constraints
are satisﬁed. To satisfy all of the constraints, we may set λ1 = log σ 2π, λ 2 = 0,
and λ3 = − 2σ12 to obtain
p(x) = N (x | µ, σ 2).
This is one reason for using the normal distribution when we do not know the
true distribution. Because the normal distribution has the maximum entropy, we
impose the least possible amount of structure by making this assumption.
What about the probability distribution function that minimizes the entropy?
It turns out that there is no speciﬁc function that achieves minimal entropy. As
functions place more mass on x = µ ± σ and less on all other values of x, they lose
entropy. However, any function placing exactly zero mass on all but two points
does not integrate to one, and is not a valid probability distribution. There thus
is no single minimal entropy probability distribution function, much as there is
no single minimal positive real number.

19.4.3

Continuous Latent Variables

TODO: Gaussian example from IG’s thesis? TODO: S3C example

19.5

Stochastic Inference

TODO: Charlie Tang’s SFNNs? Is there anything else where sampling-based
inference actually gets used?

19.6

Learned Approximate Inference

TODO: wake-sleep algorithm
In chapter 18.2 we saw that one possible explanation for the role of dream
sleep in human beings and animals is that dreams could provide the negative
phase samples that Monte Carlo training algorithms use to approximate the negative gradient of the log partition function of undirected models. Another possible
551

CHAPTER 19. APPROXIMATE INFERENCE

explanation for biological dreaming is that it is providing samples from p(h, v)
which can be used to train an inference network to predict h given v. In some
senses, this explanation is more satisfying than the partition function explanation.
Monte Carlo algorithms generally do not perform well if they are run using only
the positive phase of the gradient for several steps then with only the negative
phase of the gradient for several steps. Human beings and animals are usually
awake for several consecutive hours then asleep for several consecutive hours, and
it is not readily apparent how this schedule could support Monte Carlo training
of an undirected model. Learning algorithms based on maximizing L can be run
with prolonged periods of improving q and prolonged periods of improving θ,
however. If the role of biological dreaming is to train networks for predicting q,
then this explains how animals are able to remain awake for several hours (the
longer they are awake, the greater the gap between L and log p(v), but L will remain a lower bound) and to remain asleep for several hours (the generative model
itself is not modiﬁed during sleep) without damaging their internal models. Of
course, these ideas are purely speculative, and there is no hard evidence to suggest that dreaming accomplishes either of these goals. Dreaming may also serve
reinforcement learning rather than probabilistic modeling, by sampling synthetic
experiences from the animal’s transition model, on which to train the animal’s
policy. Or sleep may serve some other purpose not yet anticipated by the machine
learning community.
TODO: DARN and NVIL? TODO: fast DBM inference

552

Chapter 20

Deep Generative Models
In this chapter, we present several of the speciﬁc kinds of generative models that
can be built and trained using the techniques presented in chapters 13, 18 and
19. All of these models represent probability distributions over multiple variables
in some way. Some allow the probability distribution function to be evaluated
explicitly. Others do not allow the evaluation of the probability distribution
function, but support operations that implicitly require knowledge of it, such as
sampling. Some of these models are structured probabilistic models described in
terms of graphs and factors, as described in chapter 13. Others can not easily be
described in terms of factors, but represent probability distributions nonetheless.

20.1

Boltzmann Machines

Boltzmann machines were originally introduced in Ackley et al. (1985) as a general “connectionist” approach to learning arbitrary probability distributions over
binary vectors. Boltzmann Machines form the basis of a large number of popular
variants. Indeed, these variants have long ago surpassed the popularity of the
original and most general incarnation of the Boltzmann machine. In this section
we brieﬂy introduce the general Boltzmann machine and discuss the issues that
come up when trying to train and perform inference in the model.
We deﬁne our Boltzmann machine over a d-dimensional binary random vector
x ∈ {0, 1}d . The Boltzmann machine is an energy-based model1 , meaning we
deﬁne the joint probability distribution over the model variable using an energy
function.
exp (−E(x))
P (x) =
.
(20.1)
Z
Where E(x) is the energy function and Z is the partition function, that ensures
1

For a general discussion of energy based models see Sec. 13.2.4

553

CHAPTER 20. DEEP GENERATIVE MODELS

h1 (2)

h1

h2

v1

h3

v2

h4

v3

h1 (1)

v1

h2 (2)

h2 (1)

h3 (2)

h3 (1)

v2

h4 (1)

v3

h1 (2)

h 1(1)

h2

h2 (1)

v1

(2)

h3 (2)

h3 (1)

v2

h4 (1)

v3

Figure 20.1: Examples of models that may be built with restricted Boltzmann machines.
a) The restricted Boltzmann machine itself is an undirected graphical model based on a
bipartite graph. There are no connections among the visible units, nor any connections
among the hidden units. Typically every visible unit is connected to every hidden unit
but it is possible to construct sparsely connected RBMs such as convolutional RBMs. b)
A deep belief network is a hybrid graphical model involving both directed and undirected
connections. Like an RBM, it has no intra-layer connections. However, a DBN has
multiple hidden layers, and thus there are connections between hidden units that are in
separate layers. All of the local conditional probability distributions needed by the deep
belief network are copied directly from the local conditional probability distributions of
its constituent RBMs. Note that we could also represent the deep belief network with
a completely undirected graph, but it would need intra-layer connections to capture the
dependencies between parents. c) A deep Boltzmann machine is an undirected graphical
model with several layers of latent variables. Like RBMs and DBNs, DBMs lack intralayer connections. DBMs are less closely tied to RBMs than DBNs are. When initializing
a DBM from a stack of RBMs, it is necessary to modify the RBM parameters slightly.
Some kinds of DBMs may be trained without ﬁrst training a set of RBMs.

554

CHAPTER 20. DEEP GENERATIVE MODELS

that the
by:

P

x P (x)

= 1. The energy function of the Boltzmann machine is given
E(x) = −x>U x − b> x,

(20.2)

where U is the “weight” matrix of model parameters and b are the oﬀsets for
each x.
In the general setting of the Boltzmann machine, we could consider that the
goal is that we are given a set of observations, each of which are d-dimensional and
that we are to use the joint probability distribution given in Eq. 20.1 describes
the joint probability distribution over the observed variables (also called visible
units). While this scenario is certainly viable, it does limit the kinds of interactions between the observed variables to those described by the weight matrix.
Speciﬁcally it limits the model to 2nd-order interactions.
In the spirit of the “connectionist” approach to density modeling that originally inspired the Boltzmann machine, it is interesting to consider the case where
not all the variables are observed. In this case, the non-observed variables, or
latent variables can act similarly to hidden units in a multi-layer perceptron and
model higher-order interactions among the visible units.
Formally, we decompose the units into two subsets: the visible units x v and
the latent (or hidden) units xh . Without loss of generality, we can re-express the
energy function decomposing x into subsets x v and xh:
>
>
>
>
E(x v, xh ) = −x>
v Rxv − xv W x h − xh Sxh − b xv − c x h,

(20.3)

Boltzmann Machine Learning As a probabilistic model, it is natural to consider maximum likelihood as the learning paradigm for Boltzmann machines. According to the ML paradigm, we are interested in choosing the parameters that
(locally) maximize the probability of the visible units over a dataset.
(1)
(t)
Consider a dataset of n examples X v = [x v , . . . , x v , . . . , xnv ]. Our goal is to
maximize the likelihood of this dataset under the Boltzmann machine. Assuming
the data is i.i.d, this amounts to maximizing the following:
`(θ) = log P (Xv ) =

n
X

log P (x(t)
v ).

(20.4)

t=1

Of course, our Boltzmann machine does not explicitly parametrize a distribution
(t)
over the visible units as P (xv ), instead, as given in Eq. 20.3, it is parametrized
via an energy function to joint probability distribution over xv and xh , the hidden
(t)
units. In order to recover P (x v ), we need to marginalize out the inﬂuence of x h.
(t)

P (x(t)
v )=

P (x(t)
v ,xh ) =
xh

X

xh

555X

1
(t)
exp −E(x(t)
v ,xh ) .
Z
n

o

(20.5)

CHAPTER 20. DEEP GENERATIVE MODELS

Combining Eqs. 20.4 and 20.5 gives us our objective function we wish to maximize. Unfortunately, because Z is a function of the model parameters, maximizing
likelihood function is not amenable to analytically solution. Instead we will do as
we do for the vast majority of deep learning models, we will follow the gradient of
our objective function. The Boltzmann machine likelihood gradient is given by:
"
#!
n
n
o
X
∂
∂ X 1
(t)
`(θ) =
log
exp −E(x (t)
(20.6)
v , xh )
∂θ
∂θ
Z
xh
t=1
"
#
n
n
o
X ∂
X
∂Z
(t)
−
=
log
exp −E(x(t)
(20.7)
v , xh )
∂θ
∂θ
xh
t=1
n
o


(t) (t)
n
X X exp −E(x v , x h )
∂
∂Z
(t)

n
o
=
E(x (t)
, xh ) −
(20.8)
v
P
(t)
(t)
∂θ
∂θ
exp
−E(x
,
x
)
x
v
t=1

20.2

h

h

xh

Restricted Boltzmann Machines

Restricted Boltzmann machines are some of the most common building blocks
of deep probabilistic models. They are undirected probabilistic graphical models
containing a layer of observable variables and a single layer of latent variables.
RBMs may be stacked (one on top of the other) to form deeper models. See
Fig. 20.1 for some examples. In particular, Fig. 20.1a shows the graph structure
of an RBM itself. It is a bipartite graph: with no connections permitted between
any variables in the observed layer or between any units in the latent layer.
TODO– review and pointers to other sections of the book This should be the
main place where they are described in detail, earlier they are just an example of
undirected models or an example of a feature learning algorithm.
TODO: please use lower-case letter names for scalars, none of this D and N
stuﬀ. do we even use these variable names anywhere, or do we just deﬁne them
and never refer back to them? if they are never used, delete them, don’t make
the reader hold variables in their head for no payoﬀ
We begin with the binary version of the restricted Boltzmann machine, but
as we see later there are extensions to other types of visible and hidden units.
More formally, we will consider the observed layer to consist of a set of D
binary random variables which we refer to collectively with the vector v, where
the ith element, i.e. vi is a binary random variable. We will refer to the latent or
hidden layer of N binary random variables collectively as h, with the jth random
elements as h j.
Like the general Boltzmann machine, the restricted Boltzmann machine is an
energy-based model with the joint probability distribution speciﬁed by its energy
556

CHAPTER 20. DEEP GENERATIVE MODELS

function:

1
exp {−E(v, h)} .
Z
Where E(v, h) is the energy function that parametrizes the relationship between
the visible and hidden variables:
P (v = v, h = h) =

E(v, h) = −b> v − c>h − v > W h,

(20.9)

and the Z is the normalizing constant known as the partition function:
XX
exp {−E(v, h)} .
Z=
v

h

For many undirected models, it is apparent from the deﬁnition of the partition
function Z that the naive method of computing Z (exhaustively summing over
all states) would be computationally intractable. However, it is still possible that
a more cleverly designed algorithm could exploit regularities in the probability
distribution to compute Z faster than the naive algorithm, so the exponential cost
of the naive algorithm is not a guarantee of the partiion function’s intractability.
In the case of restricted Boltzmann machines, there is actually a hardness result,
proven by Long and Servedio (2010).

20.2.1

Conditional Distributions

The intractable partition function Z, implies that the joint probability distribution is also intractable (in the sense that the normalized probability of a given
joint conﬁguration of [v, h] is generally not available). However, due the bipartite
graph structure, the restricted Boltzmann machine has the very special property
that its conditional distributions P (h | v) and P (v | h) are factorial and relatively
simple to compute and sample from. Indeed, it is this property that has made
the RBM a relatively popular model for a wide range of applications including
image modeling (TODO CITE), speech processing (TODO CITE) and natural
language processing (TODO CITE).
Deriving the conditional distributions from the joint distribution is straight-

557

CHAPTER 20. DEEP GENERATIVE MODELS

forward.
p(h, v)
p(v)
p(h, v)
=
p(v)
n
o
1 1
>
>
>
=
exp b v + c h + v W h
p(v) Z
n
o
1
= 0 exp c>h + v >W h
Z


n
n


X
X
1
>
= 0 exp
cjh j +
v W :,jh j


Z

p(h | v) =

j=1

j=1

n
n
o
1 Y
>
= 0
exp c j hj + v W :,jhj
Z
j=1

Since we are conditioning on the visible units v, we can treat these as constants
w.r.t. the distribution p(h | v). The factorial nature of the conditional p(h | v)
follows immediately from our ability to wright the joint probability over the vector
h as the product of (unnormalized) distributions over the individual elements,
h j . It is now a simple matter of normalizing the distributions over the individual
binary hj .
P̃ (h j = 1 | v)
P˜(h j = 0 | v) +P˜(h j = 1 | v)


exp cj + v >W :,j
=
exp {0} + exp {c j + v> W:,j }


>
= sigmoid cj + v W:,j .

P (h j = 1 | v) =

(20.10)

We can now express the full conditional over the hidden layer as the factorial
distribution:
n


Y
>
P (h | v) =
sigmoid cj + v W:,j .
(20.11)
j=1

A similar derivation will show that the other condition of interest to us, P (v |
h), is also a factorial distribution:
d

P (v | h) =

sigmoid (b i + Wi,: h) .
i=1

Y

558

(20.12)

CHAPTER 20. DEEP GENERATIVE MODELS

20.2.2

RBM Gibbs Sampling

The factorial nature of these conditions is a very useful property of the RBM,
and allows us to eﬃciently draw samples from the joint distribution via a block
Gibbs sampling strategy (see section 14.1 for a more complete discussion of Gibbs
sampling methods).
Block Gibbs sampling simply refers to the situation where in each step of Gibbs
sampling, multiple variables (or a “block” of variables) are sampled jointly. In
the case of the RBM, each iteration of block Gibbs sampling consists of two steps.
Step 1: Sample h (l) ∼ P (h | v(l)). Due to the factorial nature of the conditionals,
we can simultaneously and independently sample from all the elements of h (l)
given v (l). Step 2: Sample v (l+1) ∼ P (v | h(l) ). Again, the factorial nature
of the conditional P (v | h (l)) allows us can simultaneously and independently
sample from all the elements of v (l+1) given h (l).

20.3

Training Restricted Boltzmann Machines

Despite the simplicity of the RBM conditionals, training these models is not without its complications. As a probabilistic model, a sensible inductive principle for
estimating the model parameters is maximum likelihood – though other possibilities are certainly possible Marlin et al. (2010) and will be discussed later in
Sec. 20.3.3. In the following we derive the maximum likelihood gradient with
respect to the model parameters.
Let us consider that we have a batch (or minibatch) of n examples taken from
an i.i.d dataset (independently and identically distributed examples) {v (1) , . . . , v (t), . . . , v (n) }.
The log-likelihood under the RBM with parameters b (visible unit biases), c (hidden unit biases) and W (interaction weights) is given by:
`(W , b, c) =
=

n
X
t=1
n
X

log P (v(t))
log

t=1

=
=

n
X
t=1
n
X
t=1

X

(t), h)
P (vn,:

h

log

X
h

log

X
h

n
o
exp −E(v(t) , h)
n
o
exp −E(v(t) , h)

559

!
!

− n log Z
− n log

X
v,h

exp {−E(v, h)}
(20.13)

CHAPTER 20. DEEP GENERATIVE MODELS

In the last line of the equation above, we have used the deﬁnition of the partition
function.
To maximize the likelihood of the data under the restricted Boltzmann machine, we consider the gradient of the likelihood with respect to the model parameters, which we will refer to collectively as θ = {b, c, W }:
!
n
n
o
X
X
X
∂
∇θ `(θ) = ∇θ
−n
exp {−E(v, h)}
log
exp −E(v (t), h)
log
∂θ
h
t=1
v,h
P


P
n
(t)
(t)
X
, h)
v,h exp {−E(v, h)} ∇θ − E(v, h)
h exp −E(v  , h) ∇θ − E(v

P
=
P
−
n
(t) , h)
exp
−E(v
v,h exp {−E(v, h)}
h
=

t=1
n
X
t=1

h
E P (h|v(t) ) ∇ θ − E(v

(t)

i
, h) − nEP (v,h) [∇ θ − E(v, h)]

(20.14)

As we can see from Eq. 20.14, the gradient of the log likelihood is speciﬁed as the
diﬀerence between two expectations of the gradient of the energy function, The
ﬁrst expectation (the data term) is with respect to the product of the empirical
P
distribution over the data, P (v) = 1/n nt=1 δ(x − v (t)) 2 and the conditional
distribution P (h | v (t) ). The second expectation (the model term) is with respect
to the joint model distribution P (v, h).
This diﬀerence between a data-driven term and a model-driven term is not
unique to RBMs, as discussed in some detail in Sec. 18.2, this is a general feature
of the maximum likelihood gradient for all undirected models.
We can complete the derivation of log-likelihood gradient by expanding the
term: ∇θ − E(v, h). We will consider ﬁrst the gradient of the negative energy
function of W .


∂  >
b v + c> h + v >W h
∂W
>
= hv

(20.15)

nablaW − E(v, h) =

(20.16)

The gradients with respect to b and c are similarly derived:
∇b − E(v, h) = v, ∇c − E(v, h) = h

2 As

(20.17)

discussed in Sec. 3.10.5, we use the term empirical distribution to refer to a mixture over
delta functions placed on training examples
560

CHAPTER 20. DEEP GENERATIVE MODELS

Putting it all together we can the following equations for the gradients with
respect to the RBM parameters and given n training examples:
∇W `(W , b, c) =
∇b `(W , b, c) =
∇c `(W , b, c) =

n
X
t=1
n
X
t=1
n
X
t=1

where we have deﬁned ĥ(t) as
ĥ

(t)

= EP (h|v(t))

ĥ(t) v(t)

>

h
i
− NE P (v,h) hv >

v(t) − nE P (v,h) [v]
(t)

ĥ

− nEP (v,h) [h]



(t)
[h] = sigmoid c + v W .

(20.18)

While we are able to write down these expressions for the log-likelihood gradient, unfortunately, in most situations of interest, we are not able to use them
directly to calculate gradients. The problem is the expectations over the joint
model distribution P(v, h). While we have conditional distributions P (v | h)
and P (h | v) that are easy to work with, the RBM joint distribution is not
amenable to analytic evaluation of the expectation EP (v,h) [f(v, h)].
This is bad news—it implies that in most cases it is impractical to compute
the exact log-likelihood gradient. Fortunately, as discussed in Sec. 18.2, there
are two widely used approximation strategies that have been applied to the training of RBM with some degree of success: contrastive divergence and stochastic
maximum likelihood.
In the following sections we discuss two diﬀerent strategies to approximate this
gradient that have been applied to training the RBM. However, before getting into
the actual training algorithms, it is worth considering what general approaches
are available to us in approximating the log-likelihood gradient. As we mentioned,
our problem stems from the expectation over the joint distribution P (v, h), but we
know that we have access to factorial conditionals and that we can use these as the
basis of a Gibbs sampling procedure to recover samples form the joint distribution
(as discussed in Sec. 20.2.2). Thus, we can imagine using, for example, T MCMC
samples from P (v, h) to form a Monte Carlo estimate of the expectations over
the joint distribution:
T

1X
f(v (t), h (t)).
E P (v,h) [f(v, h)] ≈
T

(20.19)

t=1

There is a problem with this strategy that has to do with the initialization of the
MCMC chain. MCMC chains typically require a burn-in period, where the chain
561

CHAPTER 20. DEEP GENERATIVE MODELS

20.3.1

Contrastive Divergence Training of the RBM

As discussed in a more general context in Sec. 18.2, Contrastive divergence (CD)
seeks to approximate the expectation over the joint distribution with samples
drawn from short Gibbs sampling chains. CD deals with the typical requirement
for an extended burn-in sample sequence by initializing these chains at the data
points used in the data-dependent, conditional term. The result is a biased approximation of the log-likelihood gradient (Carreira-Perpiñan and Hinton, 2005;
Bengio and Delalleau, 2009; Fischer and Igel, 2011), that never-the-less has been
empirically shown to be eﬀective. The constrastive divergence algorithm, as applied to RBMs, is given in Algorithm 20.1.
Algorithm 20.1 The contrastive divergence algorithm, using gradient ascent as
the optimization procedure.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow a Markov chain of
p(v; θ) to mix when initializedfrom p data. Perhaps 1-20 to train an RBM on a
small image patch.
while Not converged do
Sample a minibatch
of m examples from the training set {v (1) , . . . , v (m)}.
P
(t) (t) >
∆W ← m1 m
t=1 v ĥ
P
m
(t)
∆b ← m1
t=1 v
P
m
(t)
∆c ← m1
t=1 ĥ
for t = 1 to m do
v˜(t) ← v (t)
end for
for l = 1 to k do
for t = 1 to m do Q


h̃(t) sampled from nj=1 sigmoid cj + ṽ (t) >W:,j .


Qd
(t)
(t)
ṽ sampled from i=1 sigmoid bi + W i,: h̃ .
end for
end for


h̄(t) ← sigmoid c + ṽ (t) > W
P
(t) (t) >
∆W ← ∆ W − 1m m
t=1 ṽ h̄
P
m
1
(t)
∆b ← ∆ b − m
t=1 ṽ
P
1
m
(t)
∆c ← ∆ b − m
t=1 h̄
W ← W + ∆W
b ← b + ∆b
c ← c + ∆c
end while
562

CHAPTER 20. DEEP GENERATIVE MODELS

20.3.2

Stochastic Maximum Likelihood (Persistent Contrastive
Divergence) for the RBM

While contrastive divergence has been the most popular method of training
RBMs, the stochastic maximum likelihood (SML) algorithm (Younes, 1998; Tieleman, 2008) is known to be a competitive alternative – especially if we are interested in recovering the best possible generative model (i.e. achieving the highest
possible test set likelihood). As with CD, the general SML algorithm is described
in Sec. 18.2. Here we are concerned with how to apply the algorithm to training
an RBM.
In comparison to the CD algorithm, SML uses an alternative solution to the
problem of how to approximate the partition function’s contribution to the loglikelihood gradient. Instead of initializing the k-step MCMC chain with the current example from the training set, in SML we initialize the MCMC chain for
training iteration s with the last state of the MCMC chain from the last training
iteration (s−1). Assuming that the gradient updates to the model parameters do
not signiﬁcantly change the model, the MCMC state of the last iteration should
be close to the equilibrium distribution at iteration s – minimizing the number of
“burn-in” MCMC steps needed to reach equilibrium at the current iteration. As
with CD, in practice we often use just one Gibbs step between learning iterations.
Algorithm 20.2 describes the SML algorithm as applied to RBMs.
TODO: include experimental examples, i.e. an RBM trained with CD on
MNIST

20.3.3

Other Inductive Principles

TODO:Other inductive principles have been used to train RBMs. In this section
we brieﬂy discuss these.

20.4

Deep Belief Networks

Deep belief networks (DBNs) were one of the ﬁrst successful non-convolutional
architectures. The introduction of deep belief networks in 2006 began the current deep learning renaissance. Prior to the introduction of deep belief networks,
deep models were considered too diﬃcult to optimize, due to the vanishing and
exploding gradient problems and the existence of plateaus, negative curvature,
and suboptimal local minima that can arise in neural network objective functions. Kernel machines with convex objective functions dominated the research
landscape. Deep belief networks demonstrated that deep architectures can be
successful, by outperforming kernelized support vector machines on the MNIST
dataset (Hinton et al., 2006). Today, deep belief networks have mostly fallen out
563

CHAPTER 20. DEEP GENERATIVE MODELS

Algorithm 20.2 The stochastic maximum likelihood / persistent contrastive
divergence algorithm for training an RBM.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow a Markov chain of
p(v, h; θ +∆θ ) toburn in, starting from samples from p(v, h; θ). Perhaps 1 for
RBM on a small image patch.
Initialize a set of m samples {ṽ (1), . . . , ṽ(m)} to random values (e.g., from a uniform or normal distribution, or possibly a distribution with marginals matched
to the model’s marginals)
while Not converged do
Sample a minibatch of m examples {v (1), . . . , v (m) } from the training set.
P
(t) (t) >
∆W ← m1 m
t=1 ĥ v
P
m
∆b ← m1
v (t)
t=1
Pm (t)
∆c ← m1
t=1 ĥ
for l = 1 to k do
for t = 1 to m do


Q
h̃(t) sampled from nj=1 sigmoid cj + ṽ (t) >W:,j .


Q
ṽ (t)sampled from di=1 sigmoid bi + W i,: h̃(t) .
end for
end for
1 Pm
(t) (t) >
∆W ← ∆ W − P
m
t=1 ṽ h̃
m
1
(t)
∆b ← ∆ b − m
t=1 ṽ
P
1
m
(t)
∆c ← ∆ b − m
t=1 h̃
W ← W + ∆W
b ← b + ∆b
c ← c + ∆c
end while
of favor and are rarely used, even compared to other unsupervised or generative
learning algorithms, but they are still deservedly recognized for their important
role in deep learning history.
Deep belief networks are generative models with several layers of latent variables. The latent variables are typically binary, and the visible units may be
binary or real. There are no intra-layer connections. Usually, every unit in each
layer is connected to every unit in each neighboring layer, though it is possible to
construct more sparsely connected DBNs. The connections between the top two
layers are undirected. The connections between all other layers are directed, with
the arrows pointed toward the layer that is closest to the data. See Fig. 20.1b for
an example.
564

CHAPTER 20. DEEP GENERATIVE MODELS

A DBN with L hidden layers contains L weight matrices: W (1) , . . . , W (L) . It
also contains L + 1 bias vectors: b (0) , . . . , b(L) with b (0) providing the biases for
the visible layer. The probability distribution represented by the DBN is given
by


(L) (L−1)
(L)> (L)
(L−1)> (L−1)
(L−1)>
(L) (L)
) ∝ exp b
p(h , h
h +b
h
+h
W h
,
(l)
p(h i

=1|h

(l+1)

)=σ



(l)
bi

+

(l+1)>
W:,i


(0)
p(v i = 1 | h ) = σ b i
(1)

(l+1)



∀i, ∀l ∈ 1, . . . , L − 2,

(1)> (1)
+ W:,i h
∀i.
h

In the cause of real-valued visible units, substitute


(0)
(1)> (1)
−1
v∼N v|b +W
h ,β

with β diagonal for tractability. Generalizations to other exponential family visible units are straightforward, at least in theory. Note that a DBN with only one
hidden layer is just an RBM.
To generate a sample from a DBN, we ﬁrst run several steps of Gibbs sampling
on the top two hidden layers. This stage is essentially drawing a sample from the
RBM deﬁned by the top two hidden layers. We can then use a single pass of
ancestral sampling through the rest of the model to draw a sample from the
visible units.
Inference in a deep belief network is intractable due to the explaining away
eﬀect within each directed layer, and due to the interaction between the two ﬁnal
hidden layers. Evaluating or maximizing the standard evidence lower bound on
the log-likelihood is also intractable, because the evidence lower bound takes the
expectation of cliques whose size is equal to the network width.
Evaluating or maximizing the log-likelihood requires not just confronting the
problem of intractable inference to marginalize out the latent variables, but also
the problem of an intractable partition function within the undirected model of
the last two layers.
As a hybrid of directed and undirected models, deep belief networks encounter
many of the diﬃculties associated with both families of models. Because deep belief networks are partially undirected, they require Markov chains for sampling
and have an intractable partition function. Because they are directed and generally consist of binary random variables, their evidence lower bound is intractable.
TODO–training procedure TODO–discriminative ﬁne-tuning TODO–view of
MLP as variational inference with very loose bound comment on how this does
not capture intra-layer explaining away interactions comment on how this does
565

CHAPTER 20. DEEP GENERATIVE MODELS

not capture inter-layer feedback interactions TODO–quantitative analysis with
AIS TODO–wake sleep?
The term “deep belief network” is commonly used incorrectly to refer to any
kind of deep neural network, even networks without latent variable semantics. The
term “deep belief network” should refer speciﬁcally to models with undirected
connections in the deepest layer and directed connections pointing downward
between all other pairs of sequential layers.
The term “deep belief network” may also cause some confusion because the
term “belief network” is sometimes used to refer to purely directed models, while
deep belief networks contain an undirected layer. Deep belief networks also share
the acronym DBN with dynamic Bayesian networks, which are Bayesian networks
for representing Markov chains.

20.5

Deep Boltzmann Machines

A deep Boltzmann machine (DBM) is another kind of deep, generative model
(Salakhutdinov and Hinton, 2009a). Unlike the deep belief network (DBN), it
is an entirely undirected model. Unlike the RBM, the DBM has several layers
of latent variables (RBMs have just one). But like the RBM, within each layer,
each of the variables are mutually independent, conditioned on the variables in
the neighboring layers. See Fig. 20.2 for the graph structure.
Like RBMs and DBNs, DBMs typically contain only binary units – as we
assume in our development of the model – but it may sometimes contain realvalued visible units.
A DBM is an energy-based model, meaning that the the joint probability
distribution over the model variables is parametrized by an energy function E. In
the case of a deep Boltzmann machine with one visible layer, v, and three hidden
layers, h(1) , h (2)andh (3), the joint probability is given by:
P



v, h(1) , h (2), h(3)





1
(1)
(2)
(3)
=
exp −E(v, h , h , h ; θ) .
Z(θ)

(20.20)

The DBM energy function is:
E(v, h(1) , h(2), h (3); θ) = −v> W (1) h(1) − h(1)> W(2) h(2) − h(2)> W (3) h (3) .
(20.21)
In comparison to the RBM energy function (Eq. 20.9), the DBM energy
function includes connections between the hidden units (latent variables) in the
form of the weight matrices (W (2) and W (3)). As we will see, these connections
have signiﬁcant consequences for both the model behavior as well as how we go
about performing inference in the model.
566

CHAPTER 20. DEEP GENERATIVE MODELS

h(3)
hidden layers
(binary units)

W (3)
h(2)

W (2)

h(1)
visible layer
(binary units)

connections
(weights)

W (1)

v

Figure 20.2: The deep Boltzmann machine (oﬀsets on all units are present but suppressed
to simplify notation).

567

CHAPTER 20. DEEP GENERATIVE MODELS

In comparison to fully connected Boltzmann machines (with every unit connected to every other unit), the DBM oﬀers some similar advantages as oﬀered by
the RBM. Speciﬁcally, as illustrated in Fig. (TODO: include ﬁgure), the DBM
layers can be organized into a bipartiite graph, with odd layers on one side and
even layers on the other. This immediately implies that when we condition on the
variables in the even layer, the variables in the odd layers become conditionally
independent. Of course, when we condition on the variables in the odd layers,
the variables in the even layers also become conditionally independent.
We show this explicitly for the conditional distribution P (h (1) = 1 | v, h (2)),
in the case of a DBM with two hidden layers (of course, this result generalizes to
a DBM with any number of layers).
P (h(1) , v, h (2))
P (v, h(2) )


exp v> W(1) h(1) + h (1)>W (2) h(2)
 > (1) (1)

= P1
P
(1)>
(2) (2)
(1)
· · · 1h (1)
exp
v
W
h
+
h
W
h
h1 =0
=0
n


exp v> W(1) h(1) + h (1)>W (2) h(2)
P1


= P1
(1)
· · · h (1)=0 exp v> W (1) h(1) + h (1)>W (2) h(2)
h1 =0
nP

n
(1) (1)
(1)>
(2) (2)
>
exp
+ hj Wj,: h
j=1 v W:,j hj


= P1
P1
Pn
> W (1)h (1) + h(1)>W (2) h(2)
(1)
(1)
·
·
·
exp
v
0
0
0
0
0
j =1
:,j
j
j
j ,:
h1 =0
h n =0


Q
(1)>
(1) (1)
(2)
>
Wj,: h(2)
j exp v W:,j hj + h j


=P
P
Q
(1)>
(1) (1)
(2) (2)
1
>
· · · 1 (1)
exp
v
W
h
+
h
W
h
(1)
j0
:,j 0 j 0
j0
j0 ,:
h1 =0
h n =0


(1) (1)
(1)>
(2)
exp v >W:,j h j + hj
Wj,: h (2)
Y


=
P1
> W (1) h(1) + h(1)> W (2)h (2)
exp
v
(1)
j
:,j j
j
j,:
hj =0


(1) (1)
(1)>
(2)
Y exp v >W:,j h j + hj Wj,: h (2)


=
(1)
(2)
1 + exp v> W:,j + W j,: h (2)
j
Y
(1)
=
P (h j | v, h (2)).
(20.22)

P (h(1) | v, h (2)) =

j

From the above we can conclude that the conditional distribution for any
layer of the DBM conditioned on the neighboring layers, is fractorial (i.e. all
variables in the layer are conditionally independent). Further, we’ve shown that

568

CHAPTER 20. DEEP GENERATIVE MODELS

this conditional distribution is given by a logistic sigmoid function:


(1)
(2) (2)
>
exp v W:,j + Wj,: h
(1)
(2)


P (h j = 1 | v, h ) =
(1)
(2) (2)
>
1 + exp v W:,j + Wj,: h
1


(1)
(2) (2)
>
1 + exp −v W :,j − Wj,: h


(1)
(2) (2)
>
= sigmoid v W:,j + Wj,: h
.
=

(20.23)

For the two layer DBM, the conditional distributions of the remaining two
Qd
layers (v, h(2) ) also factorize. That is P (v | h(1) ) = i=1 P (vi | h (1) ), where


(1) (1)
(1)
P (v i = 1 | h ) = sigmoid Wi,: h
.
(20.24)
Also, P (h (2) | h(1) ) =

Qm

(2)

P (hk

20.5.1

(2)

| h(1) ), where


(2)
(1)
(1)>
= 1 | h ) = sigmoid h
W:,k .

k=1

P (hk

(20.25)

Interesting Properties

TODO: comparison to DBNs TODO: comparison to neuroscience (local learning) “most biologically plausible” TODO: description of easy mean ﬁeld TODO:
description of sampling, comparison to general Boltzmann machines,DBNs

20.5.2

DBM Mean Field Inference

For the two hidden layer DBM, the conditional distributions, P (v | h(1) ), P (h (1) |
v, h (2)), and P (h (2) | h(1)) are factorial, however the posterior distribution over all
the hidden units given the visible unit, i.e. P(h (1) , h(2) | v), can be complicated.
This is, of course, due to the interaction weights W (2) between h (1) and h(2)
which render these variables mutually dependent, given an observed v.
So, like the DBN we are left to seek out methods to approximate the DBM
posterior distribution. However, unlike the DBN, the DBM posterior distribution
over their hidden units – while complicated – is easy to approximate with a variational approximation (as discussed in Sec. 19.1), speciﬁcally a mean ﬁeld approximation. The mean ﬁeld approximation is a simple form of variational inference,
where we restrict the approximating distribution to fully factorial distributions.
In the context of DBMs, the mean ﬁeld equations capture the bidirectional interactions between layers. In this section we derive the iterative approximate
inference procedure originally introduced in Salakhutdinov and Hinton (2009a)
569

CHAPTER 20. DEEP GENERATIVE MODELS

In variational approximations to inference, we approach the task of approximating a particular target distribution – in our case, the posterior distribution
over the hidden units given the visible units – by some reasonably simple family
of distributions. In the case of the mean ﬁeld approximation, the approximating family is the set of distributions where the hidden units are conditionally
independent.
Let Q(h(1) , h(2) | v) be the approximation of P (h(1), h (2) | v). The mean ﬁeld
assumption implies that
Q(h(1) , h (2)

| v) =

n
Y

(1)
Q(hj

j=1

| v)

m
Y

k=1

(2)

Q(h k | v).

(20.26)

The mean ﬁeld approximation attempts to ﬁnd for every observation a member of this family of distributions that “best ﬁts” the true posterior P (h (1), h(2) |
v). By best ﬁt, we speciﬁcally mean that we wish to ﬁnd the approximation Q
that minimizes the KL-divergence with P , i.e. KL(QkP ) where:
!
(1), h (2) | v)
X
Q(h
Q(h (1), h (2) | v) log
(20.27)
KL(QkP ) =
(1), h (2) | v)
P
(h
h
In general, we do not have to provide a parametric form of the approximating
distribution beyond enforcing the independence assumptions. The variational
approximation procedure is generally able to recover a functional form of the
approximate distribution. However, in the case of a mean ﬁeld assumption on
binary hidden units (the case we are considering here) there is no loss of generality
by ﬁxing a parametrization of the model in advance.
We parametrize Q as a product of Bernoulli distributions, that is we consider
the probability of each element of h(1) to be associated with a parameter. Specif(1)
(1)
(1)
ically, for each j ∈ {1, . . . , n}, ĥ j = P (hj = 1), where ĥ j ∈ [0, 1] and for
(2)
(2)
(2)
each k ∈ {1, . . . , m}, hˆ = P (h = 1), where ĥ ∈ [0, 1]. Thus we have the
k

k

k

following approximation to the posterior:
Q(h (1), h(2)

| v) =
=

n
Y

j=1
n
Y
j=1

Q(h(1)
j

| v)

m
Y

k=1

(1) h (1)
(ĥj ) j (1 −

Q(h (2)
| v)
k

(1) (1−h (1)
j )
ĥj )

×

m
Y

k=1

(2)

(2)

(2)

(2)

( ĥk )hk (1 − ĥk ) (1−h k

)

(20.28)

Of course, for DBMs with more layers the approximate posterior parametrization
can be extended in the obvious way.
570

CHAPTER 20. DEEP GENERATIVE MODELS

Now that we have speciﬁed our family of approximating distributions Q. It
remains to specify a procedure for choosing the member of this family that best
ﬁts P . One way to do this is to explicitly minimize KL(QkP ) with respect to the
variational parameters of Q. We will approach the selection of Q from a slightly
diﬀerent, but entirely equivalent, path. Rather than minimize KL(QkP ), we will
maximize the variational lower bound (or evidence lower bound: see Sec. 19.1),
which in the context of the 2-hidden-layer deep Boltzmann machine is given by:
!
(1) (2)
X
P (v, h , h ; θ)
L(Q) =
Q(h(1) , h (2) | v) log
q(h(1), h (2) | v)
h(1),h(2)
X
=−
Q(h(1) , h(2) | v)E(v, h(1) , h(2) ; θ) − log Z(θ) + H(Q), (20.29)
h(1),h (2)

where Z(θ) is the DBM partition function and H(Q) is the entropy of the mean
ﬁeld distribution.
We wish to maximize the variational lower bound in Eq. 20.29 with respect
to the mean ﬁeld parameters of Q(h(1) , h (2) | v). Substituting Eq. 20.28 for
Q(h (1), h (2) | v) in the variational lower bound, we get:
XX
X X (1) (2) (2)
(1) (1)
L(q) =
(20.30)
viW ij0 ĥj 0 +
ĥ j 0 W j0 k0 ĥk 0 − ln Z(θ) + H(q).
i

j0

j0

k0

We maximize the above expression (Eq. 20.30) by taking deriatives with
respect to the variational parameters and solving for the system of ﬁxed point
equations:
∂
(1)
∂ ĥ j

L(q) = 0

∂

∀j ∈ {1, . . . , n},

(2)

∂ ĥk

L(q) = 0
(1)

∀k ∈ {1, . . . , m}

The gradient with respect to, for example, ĥ j is reasonable straightforward

571

CHAPTER 20. DEEP GENERATIVE MODELS

to evaluate:



X X (1) (2) (2)
∂
∂ X X
(1) (1)
L(q)
=
v
W
ĥ
+
ĥj 0 W j0 k0 ĥk 0 − ln Z(θ) + H(q) 
0
0
i
ij j
(1)
(1)
∂ ĥ j
∂ĥ j
i
j0
j0
k0

X X (1) (2) (2)
∂ X X
(1) (1)
=
v
W
ĥj 0 W j0 k0 ĥk 0 − ln Z(θ)
0 ĥj 0 +
i
ij
∂ĥ (1)
0
0
0
i
j
j
k
j

X (1)
(1)
(1)
(1)
−
ĥj0 ln ĥ j0 + (1 − ĥ j 0 ) ln(1 − ĥ j0 )
j0

−
=

X
i

(1)

X
k0

vi W ij +

ĥ(2)
ln ĥ (2)
+ (1 −
k0
k0

ĥ (2)
) ln(1
k0







− ĥ (2)
)
k0

(1)
X (2) (2)
ĥj 0
,
W jk0 ĥ k0 − ln 
(1)
1 − ĥj 0
k0

#

where in the second line, we have just expanded the terms involved in the entropy
(1)
H(q). Setting this derivative to zero and solving for ĥ j , we have
!
(1)
X
X
ĥ
∂
(2) (2)
j
vi Wij(1) +
Wjk
L(q) = 0 =
0 ĥk0 − ln
(1)
1 − ĥ (1)
∂ ĥj
i
k0
j
!
X
X (2) (2)
(1)
(1)
ĥj = sigmoid
v iWij +
W jk0 ĥk 0
i

k0

A similar derivation leads to the other set of equations for the second hidden
layer variational parameters. Putting these together, we have the following system
of equations:
!
X
X
(1)
(1)
(2) (2)
, ∀j
(20.31)
ĥ j = sigmoid
v i W ij +
W jk0 ĥ k0
i

k0



X (2) (1)
(2)
ĥ k = sigmoid 
W j0 k ĥ j0 , ∀k

(20.32)

j0

At a ﬁxed point of this system of equations, we have a local maximum of our
variational lower bound L(q). Thus they deﬁne a iterative algorithm where we
(1)
(2)
intersperse updates of ĥj (using Eq. 20.31) and updates of ĥk (using Eq.
20.32). So variational inference in the two hidden layer deep Boltzmann machine
(1)
(2)
amounts to iterating these update equations for ĥ j and ĥk until convergence. In
practice, ≈ 10 iterations is usually suﬃcient. Extending approximate variational
inference to deeper DBMs is straightforward.
572

CHAPTER 20. DEEP GENERATIVE MODELS

20.5.3

DBM Parameter Learning

Because a deep Boltzmann machine contains restricted Boltzmann machines as
components, the hardness results for computing the partition function and sampling that apply to restricted Boltzmann machines also apply to deep Boltzmann
machines. This means that evaluating the probability mass function of a Boltzmann machine requires approximate methods such as annealed importance sampling. Likewise, training the model requires approximations to the gradient of the
log partition function. See chapter 18 for a general description of these methods.
The posterior distribution over the hidden units in a deep Boltzmann machine is intractable, due to the interactions between diﬀerent hidden layers. This
means that we must use approximate inference during learning. The standard
approach is to use stochastic gradient ascent on the mean ﬁeld lower bound, as
described in chapter 19. Mean ﬁeld is incompatible with most of the methods for
approximating the gradients of the log partition function described in chapter 18.
Moreover, it has been observed that for contrastive divergence to work well, it is
important that the samples from the posterior (e.g., for the 2 hidden layer DBM:
P (h (1), h (2) | v)) be exact (Salakhutdinov and Hinton, 2009b). In the case of
the DBM, the intractability of the posterior means that we would have to run
a Gibbs sampler until the samples converged to samples from the true posterior
(i.e. until they “burned in”). Thus for the DBM, CD oﬀers no speedup relative
to naive MCMC methods. Instead, DBMs are usually trained using a variant of
stochastic maximum likelihood. The negative phase samples can be generated
simply by running a Gibbs sampling chain that alternates between sampling the
odd-numbered layers and sampling the even-numbered layers.
Learning in the DBM can equivalently be considered as performing a variational form of the Expectation Maximization (EM) algorithm. Speciﬁcally, consider the variational lower bound for the two-layer DBM (making the dependency
on the model parameters explicit):
XX
XX (1) (2) (2)
(1) (1)
v iWij 0 ĥj 0 +
ĥj 0 Wj 0 k0 ĥk 0 − ln Z(θ) + H(Q).
L(Q, θ) =
i

j0

j0

k0

This expression lower bounds the likelihood P(v | θ). So by maximizing this
bound we hope to improve the likelihood. Thus we can think of L(Q, θ) as a
surrogate objective function for the DBM. From this perspective it is natural
to consider a 2-step optimization procedure. In the ﬁrst step (the E-step or
expectation step), we optimize L(Q, θ) with respect to the variational parameters.
In the case of the two-layer DBM this amounts to solving for ĥ (1) and ĥ (2) via
the iterative scheme introduced above. Then in the second step (the M-step or
maximization step), we optimize L(Q, θ) with respect to the model parameters
θ.
573

CHAPTER 20. DEEP GENERATIVE MODELS

Note that maximizing the variational lower bound with respect to the parameters does not guarantee that we improve the true likelihood P (v | θ) on every
step. 3 That said, in practice we often ﬁnd that we are able to make progress in
training DBMs by maximizing the lower bound L(Q, θ).
Unlike the standard M-step we typically have as part of the EM algorithm, our
M-step will not actually maximize L(Q, θ) with respect to θ (holding Q ﬁxed).
The presence of the partition function makes it impractical to solve the system of
equations ∇ θL(Q, θ) = 0 for θ. Instead we will be content to make incremental
progress toward this maximum by taking a small step in the direction of the
gradient ∇ θL(Q, θ). In the case of the 2-hidden layer DBM, this is given by:


X X (1) (2) (2)
∂ X X
(1) (1)
∇ θL(Q, θ) =
viWij 0 ĥj 0 +
ĥ j 0 Wj0k 0ĥ k0 − ln Z(θ) + H(Q) 
∂θ
i
j0
j 0 k0


X X (1) (2) (2)
∂ X X
∂
(1) (1)
=
viWij 0 ĥj 0 +
ĥ j 0 Wj0k 0ĥ k0  −
ln Z(θ)
∂θ
∂θ
0
0
0
i

j

j

k

(20.33)

The ﬁrst term in Eq. 20.33 is straightforward, once the values of ĥ(1) and ĥ(2)
have been computed in the E-step. Our use of variation approximate inference
has rendered learning in the DBM as analogous to training in the RBM where
the likelihood gradient (Eq. 20.14) is also composed of a analytically tractable
term and a term involving the gradient of the partition function:
h
i
− nEP (v,h(1) ,h(2) ) ∇ θ − E(v, h(1), h (2))
(20.34)

Similar to the RBM case, the partition function’s contribution to the gradient
of the variational lower bound is intractable. We approximate it using a variational version of stochastic maximum likelihood 4 (VSML) algorithm. The nonvariational version of stochastic maximum likelihood algorithm is discussed in
Sec. 18.2 and is applied to RBMs in Sec. 20.3.2.
Unlike in the RBM, the interaction between the hidden units of the DBM
precludes a direct application of the contrastive divergence training algorithm.
Speciﬁcally the issue is that, in the positive phase, in order to get samples from
3

In standard EM we do have just a guarantee. The diﬀerence is that in the case of standard
(t)
EM we assume the true posterior is tractable and therefore we can set Q(h | v, θ ) = P (h |
(t)
v, θ ). Under these conditions the lower bound is tight, i.e. L(Q, θ) = P (v | θ)
4
Salakhutdinov and Hinton (2009a) refer to this algorithm as persistent contrastive divergence. We prefer to distinguish the variational version of the algorithm as applied to DBMs
from the original stochastic maximum likelihood algorithm that directly (though stochastically)
maximizes the likelihood rather than a lower bound on the likelihood as we are doing here.
574

CHAPTER 20. DEEP GENERATIVE MODELS

the posterior P (h | v), one may have to wait a signiﬁcant amount of time for the
samples to “burn-in”. The necessity for this burn-in renders CD an impractical
algorithm for training CD. As far as we known, variants of CD that make use of
the variational approximation for the positive phase gradient approximation have
not been unexplored.
Variational stochastic maximum likelihood as applied to the DBM is given in
Algorithm 20.3. Recall that we have included the oﬀset parameters in the weight
matrices W (1) and W (2) . Note that the Gibbs sampling in the negative phase of
the stochastic maximum likelihood algorithm can be divided into two blocks of
updates, one including all odd layers (including the visible layer) and the other
including all even layers. Due to the DBM connection pattern, given the even
layers, the distribution over the odd layers is factorial and thus can be sampled
simultaneously and independently as a block. Likewise given the odd layers, the
even layers can be sampled simultaneously and independently as a block.

20.5.4

Practical Training Strategies

Unfortunately, training a DBM using stochastic maximum likelihood (as described
above) from a random initialization usually results in failure. In some cases, the
model fails to learn to represent the distribution adequately. In other cases, the
DBM may represent the distribution well, but with no higher likelihood than
could be obtained with just an RBM. Note that a DBM with very small weights
in all but the ﬁrst layer represents approximately the same distribution as an
RBM.
It is not clear exactly why this happens. When DBMs are initialized from a
pretrained conﬁguration, training usually succeeds. See section 20.5.4 for details.
One possibility is that it is diﬃcult to coordinate the learning rate of the stochastic
gradient algorithm with the number of Gibbs steps used in the negative phase
of stochastic maximum likelihood. SML relies on the learning rate being small
enough relative to the number of Gibbs steps that the Gibbs chain can mix again
after each update to the model parameters. The distribution represented by
the model can change very rapidly during the earlier parts of training, and this
may make it diﬃcult for the negative chains employed by SML to fully mix.
As described in section 20.5.5, multi-prediction deep Boltzmann machines avoid
the potential inaccuracy of SML by training with a diﬀerent objective function
that is less principled but easier to compute. Another possible explanation for
the failure of joint training with mean ﬁeld and SML is that the Hessian matrix
could be poorly conditioned. This perspective motivates centered deep Boltzmann
machines, presented in section 20.5.6, which modify the model family in order to
obtain a better conditioned Hessian matrix.
575

CHAPTER 20. DEEP GENERATIVE MODELS

a)

b)

c)

d)

Figure 20.3: The deep Boltzmann machine training procedure used to obtain the state of
the art classiﬁcation accuracy on the MNIST dataset (Srivastava et al., 2014; Salakhutdinov and Hinton, 2009a). TODO: this is not state of the art anymore, just best DBM result
a) Train an RBM by using CD to approximately maximize logP (v). b) Train a second
RBM that models h(1) and y by using CD-k to approximately maximize log P (h (1), y)
where h(1) is drawn from the ﬁrst RBM’s posterior conditioned on the data. Increase
k from 1 to 20 during learning. c) Combine the two RBMs into a DBM. Train it to
approximately maximize logP (v, y) using stochastic maximum likelihood with k = 5. d)
Delete y from the model. Deﬁne a new set of features h(1) and h ( 2) that are obtained
by running mean ﬁeld inference in the model lacking y. Use these features as input to an
MLP whose structure is the same as an additional pass of mean ﬁeld, with an additional
output layer for the estimate of y. Initialize the MLP’s weights to be the same as the
DBM’s weights. Train the MLP to approximately maximize log P (y | v) using stochastic
gradient descent and dropout. Figure reprinted from (Goodfellow et al., 2013b).

Layerwise Pretraining
The original and most popular method for overcoming the joint training problem
of DBMs is greedy layerwise pretraining. In this method, each layer of the DBM is
trained in isolation as an RBM. The ﬁrst layer is trained to model the input data.
Each subsequent RBM is trained to model samples from the previous RBM’s
posterior distribution. After all of the RBMs have been trained in this way, they
can be combined to form a DBM. The DBM may then be trained with PCD.
Typically PCD training will only make a small change in the model’s parameters
and its performance as measured by the log likelihood it assigns to the data, or
its ability to classify inputs.
Note that this greedy layerwise training procedure is not just coordinate ascent. It bears some passing resemblance to coordinate ascent because we optimize
one subset of the parameters at each step. However, in the case of the greedy
layerwise training procedure, we actually use a diﬀerent objective function at each
step.
TODO: details of combining stacked RBMs into a DBM TODO: partial mean
ﬁeld negative phase
576

CHAPTER 20. DEEP GENERATIVE MODELS

20.5.5

Multi-Prediction Deep Boltzmann Machines

TODO– cite stoyanov TODO

20.5.6

Centered Deep Boltzmann Machines

TODO
This chapter has described the tools needed to ﬁt a very broad class of probabilistic models. Which tool to use depends on which aspects of the log-likelihood
are problematic.
For the simplest distributions p, the log-likelihood is tractable, and the model
can be ﬁt with a straightforward application of maximum likelihood estimation
and gradient ascent as described in chapter
In this chapter, I’ve shown what to do in two diﬀerent diﬃcult cases. If Z is intractable, then one may still use maximum likelihood estimation via the sampling
approximation techniques described in section 18.2. If p(h | v) is intractable, one
may still train the model using the negative variational free energy rather than
the likelihood, as described in 19.4.
It is also possible that both of these diﬃculties will arise. An example of this
occurs with the deep Boltzmann machine (Salakhutdinov and Hinton, 2009b),
which is essential a sequence of RBMs composed together. The model is depicted
graphically in Fig. 20.1c.
This model still has the same problem with computing the partition function
as the simpler RBM does. It has also discarded the restricted structure that
made P (h | v) easy to represent in the RBM. The typical way to train the DBM
is to minimize the variational free energy rather than maximize the likelihood.
Of course, the variational free energy still depends on the partition function, so
it is necessary to use sampling techniques to approximate its gradient.
TODO: k-NADE

20.6

Boltzmann Machines for Real-Valued Data

While Boltzmann machines were originally developed for use with binary data,
many applications such as image and audio modeling seem to require the ability
to represent probability distributions over real values. In some cases, it is possible
to treat real-valued data in the interval [0, 1] as representing the expectation of a
binary variable (TODO cite some examples). However, this is not a particularly
theoretically satisfying approach.

577

CHAPTER 20. DEEP GENERATIVE MODELS

Figure 20.4: TODO caption and label, reference from text Figure reprinted from (Goodfellow et al., 2013b).
578

CHAPTER 20. DEEP GENERATIVE MODELS

20.6.1

Gaussian-Bernoulli RBMs

TODO– cite exponential family harmoniums? TODO– multiple ways of parametrizing them (citations?)

20.6.2

mcRBMs

TODO–mcRBMs

20.6.3

5

TODO–HMC

mPoT Model

TODO–mPoT

20.6.4

Spike and Slab Restricted Boltzmann Machines

Spike and slab restricted Boltzmann machines (Courville et al., 2011) or ssRBMs
provide another means of modeling the covariance structure of real-valued data.
Compared to mcRBMs, ssRBMs have the advantage of requiring neither matrix
inversion nor Hamiltonian Monte Carlo methods.
The spike and slab RBM has two sets of hidden units: the spike units h which
are binary, and the slab units s which are real-valued. The mean of the visible
units conditioned on the hidden units is given by (h  s)W > . In other words,
each column W:,i deﬁnes a component that can be appear in the input. The corresponding spike variable h i determines whether that component is present at all.
The corresponding slab variable s i determines the brightness of that component,
if it is present. When a spike variable is active, the corresponding slab variable
adds variance to the input along the axis deﬁned by W:,i . This allows us to model
the covariance of the inputs. Fortunately, contrastive divergence and persistent
contrastive divergence with Gibbs sampling are still applicable. There is no need
to invert any matrix.
Gating by the spike variables means that the true marginal distribution over
h  s is sparse. This is diﬀerent from sparse coding, where samples from the
model “almost never” (in the measure theoretic sense) contain zeros in the code,
and MAP inference is required to impose sparsity.
The primary disadvantage of the spike and slab restricted Boltzmann machine
is that some settings of the parameters can correspond to a covariance matrix
that is not positive deﬁnite. Such a covariance matrix places more unnormalized
probability on values that are farther from the mean, causing the integral over
all possible outcomes to diverge. Generally this issue can be avoided with simple
heuristic tricks. There is not yet any theoretically satisfying solution. Using
5

The term “mcRBM” is pronounced by saying the name of the letters M-C-R-B-M; the ”mc”
is not pronounced like the “Mc” in “McDonald’s.”
579

CHAPTER 20. DEEP GENERATIVE MODELS

constrained optimization to explicitly avoid the regions where the probability is
undeﬁned is diﬃcult to do without being overly conservative and also preventing
the model from accessing high-performing regions of parameter space.
Qualitatively, convolutional variants of the ssRBM produce excellent samples
of natural images. Some examples are shown in Fig. 13.1.
The ssRBM allows for several extensions. Including higher-order interactions
and average-pooling of the slab variables (Courville et al., 2014) enables the model
to learn excellent features for a classiﬁer when labeled data is scarce. Adding a
term to the energy function that prevents the partition function from becoming
undeﬁned results in a sparse coding model, spike and slab sparse coding (Goodfellow et al., 2013c), also known as S3C.

20.7

Convolutional Boltzmann Machines

As seen in chapter 9, extremely high dimensional inputs such as images place
great strain on the computation, memory, and statistical requirements of machine
learning models. Replacing matrix multiplication by discrete convolution with a
small kernel is the standard way of solving these problems for inputs that have
translation invariant spatial or temporal structure. Desjardins and Bengio (2008)
showed that this approach works well when applied to RBMs.
Deep convolutional networks usually require a pooling operation so that the
spatial size of each successive layer decreases. Feedforward convolutional networks
often use a pooling function such as the maximum of the elements to be pooled. It
is unclear how to generalize this to the setting of energy-based models. We could
introduce a binary pooling unit p over n binary detector units d and enforce
p = maxi di by setting the energy function to be ∞ whenever that constraint is
violated. This does not scale well though, as it requires evaluating 2 n diﬀerent
energy conﬁgurations to compute the normalization constant. For a small 3 × 3
pooling region this requires 2 9 = 512 energy function evaluations per pooling unit!
Lee et al. (2009) developed a solution to this problem called probabilistic max
pooling (not to be confused with “stochastic pooling,” which is a technique for
implicitly constructing ensembles of convolutional feedforward networks). The
strategy behind probabilistic max pooling is to constrain the detector units so
at most one may be active at a time. This means there are only n + 1 total
states (one state for each of the n detector units being on, and an additional state
corresponding to all of the detector units being oﬀ). The pooling unit is on if
and only if one of the detector units is on. The state with all units oﬀ is assigned
energy zero. We can think of this as describing a model with a single variable that
has n + 1 states, or equivalently as model that has n + 1 variables that assigns
energy ∞ to all but n + 1 joint assignments of variables.
580

CHAPTER 20. DEEP GENERATIVE MODELS

While eﬃcient, probabilistic max pooling does force the detector units to be
mutually exclusive, which may be a useful regularizing constraint in some contexts
or a harmful limit on model capacity in other contexts. It also does not support
overlapping pooling regions. Overlapping pool regions are usually required to
obtain the best performance from feedforward convolutional networks, so this
constraint probably greatly reduces the performance of convolutional Boltzmann
machines.
Lee et al. (2009) demonstrated that probabilistic max pooling could be used
to build convolutional deep Boltzmann machines 6. This model is able to perform
operations such as ﬁlling in missing portions of its input. However, it has not
proven especially useful as a pretraining strategy for supervised learning, performing similarly to shallow baseline models introduced by Pinto et al. (2008).
TODO: comment on partition function changing when you change the image
size, boundary issues

20.8

Other Boltzmann Machines

TODO–Conditional Boltzmann machine TODO-RNN-RBM TODO–discriminative
Boltzmann machine TODO–Heng’s class relevant and irrelevant Boltzmann machines TODO– Honglak’s recent work

20.9

Directed Generative Nets

So far in this chapter we have focused on undirected generative models, in the
deep learning context these are almost always parametrized via an energy function
E and possess an intractable partition function Z. The exception being the deep
belief net which can be characterized as a hybrid directed / undirected model.
As discussed in Chapter 13, directed graphical models make up a second
prominent class of graphical models. While directed graphical models have been
the very popular within the greater Machine Learning community, within the
smaller Deep Learning community they have until recently been overshadowed
by undirected models such as the RBM.
In this section we will consider some of the standard directed graphical models
that have traditionally associated with the deep learning community 7 .
6

The publication describes the model as a ”deep belief network” but because it can be described as a purely undirected model with tractable layer-wise mean ﬁeld ﬁxed point updates, it
best ﬁts the deﬁnition of a deep Boltzmann machine.
7
The list of directed graphical models that we cover here is inevitably going to be incomplete.
The choice of models we include has more to do their prominence within the Deep Learning
581

CHAPTER 20. DEEP GENERATIVE MODELS

TODO: sigmoid belief nets TODO: refer back to DBN TODO: sparse coding
(maybe drop this from the list, covered elsewhere) TODO: deconvolutional nets?
(AC votes for dropping this) TODO: refer back to S3C and BSC (binary sparse
coding) TODO: NADE will be in RNN chapter, refer back to it here make sure
k-NADE and multi-NADE are mentioned somewhere
TODO: refer to DARN and NVIL?
TODO: Stochastic Feedforward nets

20.9.1

Sigmoid Belief Nets

Sigmoid Belief Nets were originally conceived in response to the Neal (1992)
is one of the ﬁrst

20.9.2

Diﬀerentiable Generator Nets

TODO describe how VAEs and GANs both use the same kind of generator net
cite the generating chairs paper to show how this generator net can be trained
with a procedure that isn’t explicitly unsupervised cite both Kevin and Zoubin’s
version of training a generator net with MMD

20.9.3

Variational Autoencoders

The variational autoencoder is model
L

(20.35)

TODO

20.9.4

Variational Interpretation of PSD

TODO, develop the explanation of Sec. 9.1 of Bengio et al. (2013c).

20.9.5

Generative Adversarial Networks

TODO: do we want to still use the capital value function here? Should we say
it’s a functional? Note that the G is OK because it’s a distribution
Generative adversarial networks (TODO cite) are another kind of generative
model based on diﬀerentiable mappings from input noise to samples that resemble
the data. In this sense, they closely resemble variational autoencoders. However,
the training procedure is diﬀerent, and generative model is not necessarily coupled
with an inference network. It is theoretically possible to train an inference network
community and the perceived impact that they have had on the community.
582

CHAPTER 20. DEEP GENERATIVE MODELS

using a strategy similar to the wake-sleep algorithm, but there is no need to infer
posterior variables during training.
Generative adversarial networks are based on game theory. A generator network is trained to map input noise z to samples x. This function g(z) deﬁnes
the generative model. The distribution p(z) is not learned; it is simply ﬁxed to
some distribution at the start of training (usually a very unstructured distribution such as a normal or uniform distribution). We can think of g(z) as deﬁning
a conditional distribution


1
p(x | z) = N (x | g z), I ,
β
but in all learning rules we take limit as β → ∞ so we can treat g(z) itself as a
sample and ignore the parametrization of the output distribution.
The generator g is pitted against an adversary: a discriminator network d.
The discriminator network receives data or samples x as input and outputs its
estimate of the probability that x was sampled from the data rather than the
model. During training, d tries to maximize and g tries to minimize a value
function measuring the log probability of d being correct:
g ∗ = arg min maxV (g, d)
d

g

where

v(g, d) = Ex∼p data log d(x) + E x P pmodel log (1 − d(x)) .

The optimization of g can be done simply by backpropagating through d then
g, so the learning process requires neither approximate inference nor approximation of a partition function gradient. In the case where max d v(g, d) is convex
(such as the case where optimization is performed directly in the space of probability density functions) then the procedure is guaranteed to converge and is
asymptotically consistent. In practice, the procedure can be diﬃcult to make
work, because it can be diﬃcult to keep d optimized well enough to provide a
good estimate of how to update g at all times.

20.9.6

Convolutional Generative Networks

TODO– discuss convolutional generator nets (was GANs paper the ﬁrst?) and
be sure to cover “unpooling” include the unpooling technique from generatoring
chairs paper, and include others if there are relevant others

20.10

A Generative View of Autoencoders

Many kinds of autoencoders can be viewed as probabilistic models. Diﬀerent
autoencoders can be interpreted as probabilistic models in diﬀerent ways.
583

CHAPTER 20. DEEP GENERATIVE MODELS

One of the ﬁrst probabilistic interpretations of autoencoders was the view
denoising autoencoders as energy-based models trained using regularized score
matching. See Sections 15.9.1 and 18.5 for details. Since the early work (Vincent, 2011a) made the connection with Gaussian RBMs, this gave denoising autoencoders with a particular parametrization a generative interpretation (they could
be sampled from using the MCMC sampling techniques for Gaussian RBMs) .
The next milestone in connecting auto-encoders with a generative interpretation came with the work of Rifai et al. (2012). It relied on the view of contractive
auto-encoders as estimators of the tangent of the manifold near which probability
concentrates, discussed in Section 15.10 (see also Figures 15.9, 17.3). In this context, Rifai et al. (2012) demonstrated experimentally that good samples could
be obtained from a trained contractive auto-encoder by alternating encoding,
decoding, and adding noise in a particular way.
As discussed in Section 15.9.1, the application of the encoder/decoder pair
moves the input conﬁguration towards a more probable one. This can be exploited
to actually sample from the estimated distribution. If you consider most MonteCarlo Markov Chain (MCMC) algorithms, they have two elements:
1. move from lower probability conﬁgurations towards higher probability conﬁgurations, and
2. inject randomness so that the chain moves around (and does not stay stuck
at some peak of probability, or mode) and has a chance to visit every conﬁguration in the whole space, with a relative frequency equal to its probability
under the underlying model.
So conceptually all one needs to do is to perform encode-decode operations (go
towards more probable conﬁgurations) as well as inject noise (to move around the
probable conﬁgurations), as hinted at in (Mesnil et al., 2012; Rifai et al., 2012).

20.10.1

Markov Chain Associated with any Denoising Auto-Encoder

The above discussion left open the question of what noise to inject and where, in
order to obtain a Markov chain that would generate from the distribution estimated by the auto-encoder. Bengio et al. (2013b) showed how to construct such
a Markov chain for generalized denoising autoencoders. Generalized denoising autoencoders are speciﬁed by a denoising distribution for sampling an estimate of
the clean input given the corrupted input.

584

CHAPTER 20. DEEP GENERATIVE MODELS

ht = f(x̃ t )
g

f

ωt = g(h t)

x̃t
C(X̃|X)

P (X|ω)
xt+1

xt

Figure 20.5: Each step of the Markov chain associated with a trained denoising autoencoder, that generates the samples from the probabilistic model implicitly trained by
the denoising reconstruction criterion. Each step consists in (a) injecting corruption
C in state x, yielding x̃, (b) encoding it with f, yielding h = f( x̃), (c) decoding the
result with g, yielding parameters ω for the reconstruction distribution, and (d) given ω,
sampling a new state from the reconstruction distribution P (x | ω = g(f (x̃))). In the
typical squared reconstruction error case, g(h) = x̂, which estimates E[x | x̃], corruption
consists in adding Gaussian noise and sampling from P (x | ω) consists in adding another
Gaussian noise to the reconstruction x̂. The latter noise level should correspond to the
mean squared error of reconstructions, whereas the injected noise is a hyperparameter
that controls the mixing speed as well as the extent to which the estimator smoothes the
empirical distribution (Vincent, 2011b). In the ﬁgure, only the C and P conditionals
are stochastic steps (f and g are deterministic computations), although noise can also
be injected inside the auto-encoder, as in generative stochastic networks (Bengio et al.,
2014b)

Each step of the Markov chain that generates from the estimated distribution
consists of the following sub-steps, illustrated in Figure 20.5:
1. starting from the previous state x, inject corruption noise, sampling x̃ from
C(x̃ | x).
2. Encode x˜ into h = f ( x̃).
3. Decode h to obtain the parameters ω = g(h) of P (x | ω = g(h)) = P(x | x̃).
4. Sample the next state x from P (x | ω = g(h)) = P (x | x̃).
The theorem states that if the auto-encoder P (x | x̃) forms a consistent estimator
of corresponding true conditional distribution, then the stationary distribution of
585

CHAPTER 20. DEEP GENERATIVE MODELS

the above Markov chain forms a consistent estimator (albeit an implicit one) of
the data generating distribution of x.

P (x|x̃)
PC(x̃|x) P (x̃|x)

Figure 20.6: Illustration of one step of the sampling Markov chain associated with a
denoising auto-encoder (see also Figure 20.5). In the ﬁgure, the data (black circles) are
sitting near a low-dimensional manifold (a spiral, here), and the two stochastic steps of
the Markov chain are ﬁrst to corrupt x (clean image of dog, green circle) into x̃ (noisy
image of dog, blue circle) via C(x̃ | x) (here an isotropic Gaussian noise in green), and
then to sample a new x via the estimated denoising P (x | x̃). Note how there are many
possible x which could have given rise to x̃, and these all lie on the manifold in the
neighborhood of x̃, hence the ﬂattened shape of P (x | x̃) (in blue). Modiﬁed from a
ﬁgure ﬁrst created and graciously authorized by Jason Yosinski.

Figure 20.6 illustrates the sampling process of the DAE in a way that complements Figure 20.5, with a speciﬁc imagined example. For a more elaborate
discussion of the probabilistic nature of denoising auto-encoders, and their generalization (Bengio et al., 2014b), Generative Stochastic Networks (GSNs), see
Section 20.11 below. In particular, the noise does not have to be injected only in
the input, and it could be injected anywhere along the chain. GSNs also generalize DAEs by allowing the state of the Markov chain to be extended beyond the
visible variable x, to include also some latent variable h. Finally, Section 20.11
discusses training strategies for DAEs that are aimed at making it a better gen586

CHAPTER 20. DEEP GENERATIVE MODELS

erative model and not just a good feature learner.

Figure 20.7: Illustration of the eﬀect of the walk-back training procedure, used for denoising auto-encoders or GSNs in general. The objective is to remove spurious modes faster
by letting the Markov chain go towards them (along the red path, starting on the purple
data manifold and following the arrows plus noise), and then punishing the Markov chain
for this behavior (i.e., walking back to the right place) by telling the chain to return
towards the data manifold (reconstruct the original data).

20.10.2

Clamping and Conditional Sampling

Similarly to Boltzmann machines, denoising auto-encoders and GSNs can be used
to sample from a conditional distribution P (x f | xo ), simply by clamping the
observed units x f and only resampling the free units xo given xf and the sampled
latent variables (if any). This has been introduced by Bengio et al. (2014b).
However, note that Proposition 1 of that paper is missing a condition: the
transition operator (deﬁned by the stochastic mapping going from one state of
the chain to the next) should satisfy detailed balance, described in Section 14.1.1.
An experiment in clamping half of the pixels (the right part of the image) and
running the Markov chain on the other half is shown in Figure 20.8.

587

CHAPTER 20. DEEP GENERATIVE MODELS

Figure 20.8: Illustration of clamping the right half of the image and running the Markov
by resampling only the left half at each step. These samples come from a GSN trained
to reconstruct MNIST digits at each time step, i.e., using the walkback procedure.

20.10.3

Walk-Back Training Procedure

The walk-back training procedure was proposed by Bengio et al. (2013b) as a way
to speed-up the convergence of generative training of denoising auto-encoders.
Instead of performing a one-step encode-decode reconstruction, this procedure
consists in alternative multiple stochastic encode-decode steps (as in the generative Markov chain) initialized at a training example (just like with the contrastive
divergence algorithm, described in Sections 18.2) and 20.3.1) and penalizing the
last probabilistic reconstructions (or all of the reconstructions along the way).
It was shown in that paper that training with k steps is equivalent (in the
sense of achieving the same stationary distribution) as training with one step,
but practically has the advantage that spurious modes farther from the data can
be removed more eﬃciently, as illustrated in Figure 20.7.
Figure 20.9 illustrates the application of the walk-back procedure in a generative stochastic network, which is described in the next section.

588

CHAPTER 20. DEEP GENERATIVE MODELS

H0

H1

H2

W3

H3
H 1W

X0

X1

X2

W1

dataX 0

W3

H2W

2

W1

target

X1

W3

H3

2

W2

W1

target

X2

target

X3

Figure 20.9: Left: graphical model of the generative Markov chain associated with a generative stochastic network (GSN). Right speciﬁc case where the latent variable is formed
of several layers, each connected to the one above and the one below, making the generative process very similar to Gibbs sampling in a deep Boltzmann machine (Salakhutdinov
and Hinton, 2009b). The walk-back training procedure is used, i.e., at every step the reconstruction probability distribution is pushed towards generating the training example
(which also initializes the chain).

20.11

Generative Stochastic Networks

Generative stochastic networks (Bengio et al., 2014b) or GSNs are generalizations
of denoising auto-encoders that include latent variables in the generative Markov
chain, in addition to the visible variables (usually denoted x). The generative
Markov chain looks like the one in Figure 20.10. An example of a GSN structured
like a deep Boltzmann machine and trained by the walk-back procedure is shown
in Figure 20.9.

H0

H1
X0

H2
X1

X2

Figure 20.10: Markov chain of a GSN (Generative Stochastic Network) with latent variables with H and visible variable X, i.e., an unfolding of the generative process with X k
and H k at step k of the chain. TODO: please use h and x etc. throughout the GSN
section

A GSN is parametrized by two conditional probability distributions which
specify one step of the Markov chain:
1. P (X k | Hk ) tells how to generate the next visible variable given the current
latent state. Such a “reconstruction distribution” is also found in denoising
auto-encoders, RBMs, DBNs and DBMs.
2. P (H k | Hk−1 , Xk−1) tells how to update the latent state variable, given the
previous latent state and visible variable.
589

CHAPTER 20. DEEP GENERATIVE MODELS

Denoising auto-encoders and GSNs diﬀer from classical probabilistic models
(directed or undirected) in that it parametrizes the generative process itself rather
than the mathematical speciﬁcation of the joint distribution of visible and latent
variables. Instead, the latter is deﬁned implicitly, if it exists, as the stationary
distribution of the generative Markov chain. The conditions for existence of the
stationary distribution are mild (basically, the chain mixes) but can be violated
by some choices of the transition distributions (for example, if they were deterministic).
One could imagine diﬀerent training criteria for GSNs. The one proposed and
evaluated by Bengio et al. (2014b) is simply reconstruction log-probability on the
visible units, just like for denoising auto-encoders. This is achieved by clamping
X0 = x to the observed example and maximizing the probability of generating x
at some subsequent time steps, i.e., maximizing log P (Xk = x | H k), where H k
is sampled from the chain, given X 0 = x. In order to estimate the gradient of
log P (X k = x | Hk ) with respect to the other pieces of the model, Bengio et al.
(2014b) use the reparametrization trick, introduced in Section 13.5.1.
The walk-back training protocol (described in Section 20.10.3 was used (Bengio
et al., 2014b) to improve training convergence of GSNS.

20.11.1

Discriminant GSNs

Whereas the original formulation of GSNs (Bengio et al., 2014b) was meant for
unsupervised learning and implicitly modeling P (x) for observed data x, it is
possible to modify the framework to optimize P (y | x).
For example, Zhou and Troyanskaya (2014) generalize GSNs in this way, by
only back-propagating the reconstruction log-probability over the output variables, keeping the input variables ﬁxed. They applied this successfully to model
sequences (protein secondary structure) and introduced a (one-dimensional) convolutional structure in the transition operator of the Markov chain. Keep in mind
that, for each step of the Markov chain, one generates a new sequence for each
layer, and that sequence is the input for computing other layer values (say the
one below and the one above) at the next time step, as illustrated in Figure 20.11.
Hence the Markov chain is really over the output variable (and associated
higher-level hidden layers), and the input sequence only serves to condition that
chain, with back-propagation allowing to learn how the input sequence can condition the output distribution implicitly represented by the Markov chain. It is
therefore a case of using the GSN in the context of structured outputs, where
P (y | x) does not have a simple parametric form but instead the components of
y are statistically dependent of each other, given x, in complicated ways.
Zöhrer and Pernkopf (2014) considered a hybrid model that combines a supervised objective (as in the above work) and an unsupervised objective (as in
590

CHAPTER 20. DEEP GENERATIVE MODELS

Figure 20.11: Markov chain arising out of a discriminant GSN, i.e., where a GSN is used
as a structured output model over a variable y, conditioned on an input X. Reproduced
with permission from Zhou and Troyanskaya (2014). The structure is as in a GSN (over
the output) but with computations being conditioned on the input X at each step.

the original GSN work), by simply adding (with a diﬀerent weight) the supervised and unsupervised costs i.e., the reconstruction log-probabilities of y and x
respectively. Such a hybrid criterion had previously been introduced for RBMs
by Larochelle and Bengio (2008a). They show improved classiﬁcation performance
using this scheme.

20.12

Methodological Notes

Researchers studying generative models often need to compare one generative
model to another, usually in order to demonstrate that a newly invented generative model is better at capturing some distribution than the pre-existing models.
This can be a diﬃcult and subtle task. In many cases, we can not actually
evaluate the log probability of the data under the model, but only an approximation. In these cases, it’s important to think and communicate clearly about
exactly what is being measured. For example, suppose we can evaluate a stochastic estimate of the log-likelihood for model A, and a deterministic lower bound
on the log-likelihood for model B. If model A gets a higher score than model B,
which is better? If we care about determining which model has a better internal
representation of the distribution, we actually cannot tell, unless we have some
way of determining how loose the bound for model B is. However, if we care about
how well we can use the model in practice, for example to perform anomaly detection, then it is fair to say that model A is better based on a criterion speciﬁc
to the practical task of interest, e.g., based on ranking test examples and ranking
criterian such as precision and recall.
Another subtletly of evaluating generative models is that the evaluation metrics are often hard research problems in and of themselves. It can be very diﬃcult
to establish that models are being compared fairly. For example, suppose we use
591

CHAPTER 20. DEEP GENERATIVE MODELS

AIS to estimate log Z in order to compute log p̃(x) − log Z for a new model we
have just invented. A computationally economical implementation of AIS may
fail to ﬁnd several modes of the model distribution and underestimate Z, which
will result in us overestimating log p(x). It can thus be diﬃcult to tell whether a
good likelihood estimate is due to a good model or a bad AIS implementation.
Other ﬁelds of machine learning usually allow for some variation in the preprocessing of the data. For example, when comparing the accuracy of object
recognition algorithms, it is usually acceptable to preprocess the input images
slightly diﬀerently for each algorithm based on what kind of input requirements
it has. Generative modeling is diﬀerent because changes in preprocessing, even
very small and subtle ones, are completely unacceptable. Any change to the input
data changes the distribution to be captured and fundamentally alters the task.
For example, multiplying the input by 0.1 will artiﬁcially increase likelihood by
10.
Issues with preprocessing commonly arise when benchmarking generative models on the MNIST dataset, one of the more popular generative modeling benchmarks. MNIST consists of grayscale images. Some models treat MNIST images
as points in a real vector space, while others treat them as binary. Yet others
treat the grayscale values as probabilities for a binary samples. It is essential to
compare real-valued models only to other real-valued models and binary-valued
models only to other binary-valued models. Otherwise the likelihoods measured
are not on the same space. (For the binary-valued models, the log-likelihood can
be at most 0., while for real-valued models it can be arbitrarily high, since it is
the measurement of a density) Among binary models, it is important to compare models using exactly the same kind of binarization. For example, we might
binarize a gray pixel to 0 or 1 by thresholding at 0.5, or by drawing a random
sample whose probability of being 1 is given by the gray pixel intensity. If we use
the random binarization, we might binarize the whole dataset once, or we might
draw a diﬀerent random example for each step of training and then draw multiple
samples for evaluation. Each of these three schemes yields wildly diﬀerent likelihood numbers, and when comparing diﬀerent models it is important that both
models use the same binarization scheme for training and for evaluation. In fact,
researchers who apply a single random binarization step share a ﬁle containing
the results of the random binarization, so that there is no diﬀerence in results
based on diﬀerent outcomes of the binarization step.
Finally, in some cases the likelihood seems not to measure any attribute of
the model that we really care about. For example, real-valued models of MNIST
can obtain arbitrarily high likelihood by assigning arbitrarily low variance to
background pixels that never change. Models and algorithms that detect these
constant features can reap unlimited rewards, even though this is not a very useful
592

CHAPTER 20. DEEP GENERATIVE MODELS

thing to do. This strongly suggests a need for developing other ways of evaluating
generative models.
Although this is still an open question, this might be achieved by converting
the problem into a classiﬁcation task. For example, we have seen that the NCE
method (Noise Contrastive Estimation, Section 18.6) compares the density of the
training data according to a learned unnormalized model with its density under
a background model. However, generative models do not always provide us with
an energy function (equivalently, an unnormalized density), e.g., deep Boltzmann
machines, generative stochastic networks, most denoising auto-encoders (that are
not guaranteed to correspond to an energy function), deep Belief networks, etc.
Therefore, it would be interesting to consider a classiﬁcation task in which one
tries to distinguish the training examples from the generated examples. This is
precisely what is achieved by the discriminator network of generative adversarial
networks (Section 20.9.5). However, it would require an expensive operation
(training a discriminator) each time one would have to evaluate performance

593

CHAPTER 20. DEEP GENERATIVE MODELS

Algorithm 20.3 The variational stochastic maximum likelihood algorithm for
training a 2 hidden-layer DBM.
Set , the step size, to a small positive number
Set k, the number of Gibbs steps, high enough to allow a Markov chain of
p(v, h(1) , h(2) ; θ +∆ θ) toburn in, starting from samples from p(v, h (1) , h (2) ; θ).
Initialize three random matrices, V˜, H̃(1) and H̃ (2) each with m columns set
to random values (e.g., from bernoulli distributions, possibly with marginals
matched to the model’s marginals).
while Not converged (learning loop) do
Sample a minibatch of m examples
from the training
data and arrange them
 (1)

(m)
as the columns of a matrix V = v , . . . , v
from the training set.
while Not converged
 (Mean-ﬁeld inference loop)
 do
H̃ (1) ← sigmoid V > W (1) + H̃ (2) > W (2) > .


(2)
(1)
(2)
H̃ ← sigmoid H̃ W
.
end while
∆W (1) ← 1mV Ĥ(1) >
∆W (2) ← 1mĤ (1)Ĥ (2) >
for l = 1 to k (Gibbs sampling) do
Gibbs block 1:


Q m Qd
(1)
(2)
Ṽ sampled from i=1 a=1 sigmoid W a,: H̃ :,i .


Qm Q m
(1) >
(2)
(2)
H̃ sampled from i=1 b=1 sigmoid H̃ :,i W :,b .
Gibbs block 2:


Q
Qn
(1)
(2)
(2)
>
H̃ (1)sampled from m
sigmoid
Ṽ
W
+
W
H̃
.
:,i
i=1
j=1
:,j
j,:
:,i
end for
1 V Ĥ (1) >
∆W (1) ← ∆ W (1) − m
1 Ĥ (1) Ĥ(2) >
∆W (2) ← ∆ W (2) − m
W (1) ← W (1) + ∆ W (1)
W (2) ← W (2) + ∆ W (2)
end while

594

Bibliography
Ackley, D. H., Hinton, G. E., and Sejnowski, T. J. (1985). A learning algorithm for
Boltzmann machines. Cognitive Science, 9, 147–169. 553
Alain, G. and Bengio, Y. (2012). What regularized auto-encoders learn from the data generating distribution. Technical Report Arxiv report 1211.4246, Université de Montréal.
466
Alain, G. and Bengio, Y. (2013). What regularized auto-encoders learn from the data
generating distribution. In ICLR’2013 . also arXiv report 1211.4246. 448, 466, 468
Alain, G., Bengio, Y., Yao, L., Éric Thibodeau-Laufer, Yosinski, J., and Vincent, P.
(2015). GSNs: Generative stochastic networks. arXiv:1503.05571. 451
Anderson, E. (1935). The Irises of the Gaspe Peninsula. Bulletin of the American Iris
Society, 59, 2–5. 19
Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly
learning to align and translate. Technical report, arXiv:1409.0473. 23, 95, 325, 398,
407, 408
Bahl, L. R., Brown, P., de Souza, P. V., and Mercer, R. L. (1987). Speech recognition
with continuous-parameter hidden Markov models. Computer, Speech and Language,
2, 219–234. 64, 356
Baldi, P. and Brunak, S. (1998). Bioinformatics, the Machine Learning Approach. MIT
Press. 358
Baldi, P. and Sadowski, P. J. (2013). Understanding dropout. In Advances in Neural
Information Processing Systems 26 , pages 2814–2822. 232
Baldi, P., Brunak, S., Frasconi, P., Soda, G., and Pollastri, G. (1999). Exploiting the
past and the future in protein secondary structure prediction. Bioinformatics, 15(11),
937–946. 323
Baldi, P., Sadowski, P., and Whiteson, D. (2014). Searching for exotic particles in highenergy physics with deep learning. Nature communications , 5. 23

595

BIBLIOGRAPHY

Barron, A. E. (1993). Universal approximation bounds for superpositions of a sigmoidal
function. IEEE Trans. on Information Theory, 39, 930–945. 189
Bartholomew, D. J. (1987). Latent variable models and factor analysis. Oxford University
Press. 453
Basilevsky, A. (1994). Statistical Factor Analysis and Related Methods: Theory and
Applications. Wiley. 453
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A.,
Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. 78, 186, 379
Basu, S. and Christensen, J. (2013). Teaching classiﬁcation boundaries to humans. In
AAAI’2013 . 273
Baum, L. E. and Petrie, T. (1966). Statistical inference for probabilistic functions of
ﬁnite state Markov chains. Ann. Math. Stat., 37, 1559–1563. 354
Baxter, J. (1995). Learning internal representations. In Proceedings of the 8th International Conference on Computational Learning Theory (COLT’95), pages 311–320,
Santa Cruz, California. ACM Press. 234
Baydin, A. G., Pearlmutter, B. A., Radul, A. A., and Siskind, J. M. (2015). Automatic
diﬀerentiation in machine learning: a survey. arXiv preprint arXiv:1502.05767 . 184
Bayer, J. and Osendorfer, C. (2014). Learning stochastic recurrent networks. arXiv
preprint arXiv:1411.7610 . 231
Becker, S. and Hinton, G. (1992). A self-organizing neural network that discovers surfaces
in random-dot stereograms. Nature, 355, 161–163. 500
Beiu, V., Quintana, J. M., and Avedillo, M. J. (2003). Vlsi implementations of threshold
logic-a comprehensive survey. Neural Networks, IEEE Transactions on, 14(5), 1217–
1243. 383
Belkin, M. and Niyogi, P. (2002). Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS’01 , Cambridge, MA. MIT Press. 486
Belkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimensionality reduction and
data representation. Neural Computation, 15(6), 1373–1396. 152, 504
Bengio, S. and Bengio, Y. (2000a). Taking on the curse of dimensionality in joint distributions using neural networks. IEEE Transactions on Neural Networks, special issue
on Data Mining and Knowledge Discovery, 11(3), 550–557. 330
Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. (2015). Scheduled sampling for sequence prediction with recurrent neural networks. Technical report, arXiv:1506.03099.
314
596

BIBLIOGRAPHY

Bengio, Y. (1991). Artiﬁcial Neural Networks and their Application to Sequence Recognition. Ph.D. thesis, McGill University, (Computer Science), Montreal, Canada. 336,
358
Bengio, Y. (1993). A connectionist approach to speech recognition. International Journal
on Pattern Recognition and Artiﬁcial Intelligence, 7(4), 647–668. 356
Bengio, Y. (1999a). Markovian models for sequential data. Neural Computing Surveys,
2, 129–162. 356
Bengio, Y. (1999b). Markovian models for sequential data. Neural Computing Surveys,
2, 129–162. 359
Bengio, Y. (2000). Gradient-based optimization of hyperparameters. Neural Computation, 12(8), 1889–1900. 372
Bengio, Y. (2002). New distributed probabilistic language models. Technical Report
1215, Dept. IRO, Université de Montréal. 400
Bengio, Y. (2009). Learning deep architectures for AI . Now Publishers. 147, 190
Bengio, Y. (2013a). Deep learning of representations: looking forward. In Statistical
Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science,
pages 1–37. Springer, also in arXiv at http://arxiv.org/abs/1305.0445. 381
Bengio, Y. (2013b). Estimating or propagating gradients through stochastic neurons.
Technical Report arXiv:1305.2982, Universite de Montreal. 435
Bengio, Y. and Bengio, S. (2000b). Modeling high-dimensional discrete data with multilayer neural networks. In NIPS’99 , pages 400–406. MIT Press. 329, 330, 332, 333
Bengio, Y. and Delalleau, O. (2009). Justifying and generalizing contrastive divergence.
Neural Computation, 21(6), 1601–1621. 466, 524, 562
Bengio, Y. and Frasconi, P. (1996). Input/Output HMMs for sequence processing. IEEE
Transactions on Neural Networks, 7(5), 1231–1249. 358
Bengio, Y. and Grandvalet, Y. (2004). No unbiased estimator of the variance of k-fold
cross-validation. In NIPS’03 , Cambridge, MA. MIT Press, Cambridge. 114
Bengio, Y. and LeCun, Y. (2007a). Scaling learning algorithms towards AI. In L. Bottou,
O. Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines . MIT
Press. 17, 192
Bengio, Y. and LeCun, Y. (2007b). Scaling learning algorithms towards AI. In Large
Scale Kernel Machines. 147
Bengio, Y. and Monperrus, M. (2005). Non-local manifold tangent learning. In NIPS’04 ,
pages 129–136. MIT Press. 150, 506, 507
597

BIBLIOGRAPHY

Bengio, Y. and Sénécal, J.-S. (2003). Quick training of probabilistic neural nets by
importance sampling. In Proceedings of AISTATS 2003 . 403
Bengio, Y. and Sénécal, J.-S. (2008). Adaptive importance sampling to accelerate training
of a neural probabilistic language model. IEEE Trans. Neural Networks, 19(4), 713–
722. 403
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1991). Phonetically motivated
acoustic parameters for continuous speech recognition using artiﬁcial neural networks.
In Proceedings of EuroSpeech’91 . 24, 391
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1992a). Global optimization of a
neural network-hidden Markov model hybrid. IEEE Transactions on Neural Networks ,
3(2), 252–259. 356, 358
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1992b). Neural network - gaussian
mixture hybrid for speech recognition or density estimation. In NIPS 4 , pages 175–182.
Morgan Kaufmann. 391
Bengio, Y., Frasconi, P., and Simard, P. (1993). The problem of learning long-term dependencies in recurrent networks. In IEEE International Conference on Neural Networks,
pages 1183–1195, San Francisco. IEEE Press. (invited paper). 247, 343
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with
gradient descent is diﬃcult. IEEE Tr. Neural Nets. 247, 248, 249, 334, 341, 343, 344
Bengio, Y., LeCun, Y., Nohl, C., and Burges, C. (1995). Lerec: A NN/HMM hybrid for
on-line handwriting recognition. Neural Computation, 7(6), 1289–1303. 358
Bengio, Y., Latendresse, S., and Dugas, C. (1999). Gradient-based learning of hyperparameters. Learning Conference, Snowbird. 372
Bengio, Y., Ducharme, R., and Vincent, P. (2001a). A neural probabilistic language
model. In NIPS’00 , pages 932–938. MIT Press. 16, 380
Bengio, Y., Ducharme, R., and Vincent, P. (2001b). A neural probabilistic language
model. In NIPS’2000 , pages 932–938. 394, 395, 396, 405
Bengio, Y., Ducharme, R., and Vincent, P. (2001c). A neural probabilistic language
model. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, NIPS’2000 , pages
932–938. MIT Press. 508, 509
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003a). A neural probabilistic
language model. JMLR, 3, 1137–1155. 395, 399, 405
Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003b). A neural probabilistic
language model. Journal of Machine Learning Research, 3, 1137–1155. 508, 509
Bengio, Y., Le Roux, N., Vincent, P., Delalleau, O., and Marcotte, P. (2006a). Convex
neural networks. In NIPS’2005 , pages 123–130. 227
598

BIBLIOGRAPHY

Bengio, Y., Delalleau, O., and Le Roux, N. (2006b). The curse of highly variable functions
for local kernel machines. In NIPS’2005 . 147
Bengio, Y., Larochelle, H., and Vincent, P. (2006c). Non-local manifold Parzen windows.
In NIPS’2005 . MIT Press. 150, 506
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007a). Greedy layer-wise
training of deep networks. In NIPS’2006 . 12, 16, 472, 473
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007b). Greedy layer-wise
training of deep networks. In NIPS 19 , pages 153–160. MIT Press. 190
Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In
ICML’09 . 272, 273
Bengio, Y., Léonard, N., and Courville, A. (2013a). Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv:1308.3432. 188, 381,
435
Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013b). Generalized denoising autoencoders as generative models. In NIPS’2013 . 468, 584, 588
Bengio, Y., Courville, A., and Vincent, P. (2013c). Representation learning: A review and
new perspectives. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI),
35(8), 1798–1828. 498, 582
Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014a). Deep generative
stochastic networks trainable by backprop. Technical Report arXiv:1306.1091. 435
Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. (2014b). Deep generative
stochastic networks trainable by backprop. In ICML’2014 . 435, 585, 586, 587, 589,
590
Bennett, C. (1976). Eﬃcient estimation of free energy diﬀerences from Monte Carlo data.
Journal of Computational Physics, 22(2), 245–268. 540
Berger, A. L., Della Pietra, V. J., and Della Pietra, S. A. (1996). A maximum entropy
approach to natural language processing. Computational Linguistics, 22, 39–71. 406
Berglund, M. and Raiko, T. (2013). Stochastic gradient estimate variance in contrastive
divergence and persistent contrastive divergence. CoRR, abs/1312.6002. 526
Bergstra, J. (2011). Incorporating Complex Cells into Neural Networks for Pattern Classiﬁcation. Ph.D. thesis, Université de Montréal. 226, 447
Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. J.
Machine Learning Res., 13, 281–305. 369, 370, 371

599

BIBLIOGRAPHY

Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,
J., Warde-Farley, D., and Bengio, Y. (2010a). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference
(SciPy). Oral Presentation. 78, 379
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,
J., Warde-Farley, D., and Bengio, Y. (2010b). Theano: a CPU and GPU math expression compiler. In Proc. SciPy. 186
Bergstra, J., Bardenet, R., Bengio, Y., and Kégl, B. (2011). Algorithms for hyperparameter optimization. In NIPS’2011 . 372
Bertsekas, D. P. (2004). Nonlinear programming . Athena Scientiﬁc, 2 edition. 250
Besag, J. (1975). Statistical analysis of non-lattice data. The Statistician, 24(3), 179–195.
528
Bishop, C. M. (1994). Mixture density networks. 171
Bishop, C. M. (1995a). Regularization and complexity control in feed-forward networks.
In Proceedings International Conference on Artiﬁcial Neural Networks ICANN’95 , volume 1, page 141–148. 212, 221
Bishop, C. M. (1995b). Training with noise is equivalent to Tikhonov regularization.
Neural Computation, 7(1), 108–116. 212
Bishop, C. M. (2006). Pattern Recognition and Machine Learning . Springer. 92, 138
Blum, A. L. and Rivest, R. L. (1992). Training a 3-node neural network is np-complete.
249
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M. K. (1989). Learnability
and the vapnik–chervonenkis dimension. Journal of the ACM , 36(4), 929––865. 106
Bonnet, G. (1964). Transformations des signaux aléatoires à travers les systèmes non
linéaires sans mémoire. Annales des Télécommunications, 19(9–10), 203–220. 187
Bordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012). Joint learning of words and
meaning representations for open-text semantic parsing. AISTATS’2012 . 328
Boser, B. E., Guyon, I. M., and Vapnik, V. N. (1992). A training algorithm for optimal
margin classiﬁers. In COLT ’92: Proceedings of the ﬁfth annual workshop on Computational learning theory , pages 144–152, New York, NY, USA. ACM. 16, 134, 148,
164
Bottou, L. (1991). Une approche théorique de l’apprentissage connexioniste; applications
à la reconnaissance de la parole. Ph.D. thesis, Université de Paris XI. 358
Bottou, L. (1998). Online algorithms and stochastic approximations. In D. Saad, editor,
Online Learning in Neural Networks . Cambridge University Press, Cambridge, UK.
252
600

BIBLIOGRAPHY

Bottou, L. (2011). From machine learning to machine reasoning. Technical report,
arXiv.1102.1808. 326, 328
Bottou, L. and Bousquet, O. (2008). The tradeoﬀs of large scale learning. In NIPS’2008 .
251, 253
Bottou, L., Fogelman-Soulié, F., Blanchet, P., and Lienard, J. S. (1990). Speaker independent isolated digit recognition: multilayer perceptrons vs dynamic time warping.
Neural Networks, 3, 453–465. 358
Bottou, L., Bengio, Y., and LeCun, Y. (1997). Global training of document processing
systems using graph transformer networks. In Proceedings of the Computer Vision and
Pattern Recognition Conference (CVPR’97), pages 490–494, Puerto Rico. IEEE. 349,
357, 358, 359, 360, 361
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation
and transcription. In ICML’12 . 408
Boureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis of feature pooling in
vision algorithms. In Proc. International Conference on Machine learning (ICML’10).
287
Boureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011). Ask the locals:
multi-way local pooling for image recognition. In Proc. International Conference on
Computer Vision (ICCV’11). IEEE. 287
Bourlard, H. and Kamp, Y. (1988). Auto-association by multilayer perceptrons and
singular value decomposition. Biological Cybernetics, 59, 291–294. 444
Bourlard, H. and Morgan, N. (1993). Connectionist Speech Recognition. A Hybrid Approach, volume 247 of The Kluwer international series in engineering and computer
science. Kluwer Academic Publishers, Boston. 358
Bourlard, H. and Wellekens, C. (1989). Speech pattern discrimination and multi-layered
perceptrons. Computer Speech and Language, 3, 1–19. 391
Bourlard, H. and Wellekens, C. (1990). Links between hidden Markov models and multilayer perceptrons. IEEE Transactions on Pattern Analysis and Machine Intelligence,
12, 1167–1178. 358
Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University
Press, New York, NY, USA. 88
Brady, M. L., Raghavan, R., and Slawny, J. (1989). Back-propagation fails to separate
where perceptrons succeed. IEEE Transactions on Circuits and Systems, 36, 665–674.
242
Brand, M. (2003). Charting a manifold. In NIPS’2002 , pages 961–968. MIT Press. 152,
504
601

BIBLIOGRAPHY

Breiman, L. (1994). Bagging predictors. Machine Learning , 24(2), 123–140. 226
Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. (1984). Classiﬁcation and
Regression Trees. Wadsworth International Group, Belmont, CA. 137
Bridle, J. S. (1990). Alphanets: a recurrent ‘neural’ network architecture with a hidden
Markov model interpretation. Speech Communication, 9(1), 83–92. 167
Brown, P. (1987). The Acoustic-Modeling problem in Automatic Speech Recognition.
Ph.D. thesis, Dept. of Computer Science, Carnegie-Mellon University. 356
Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Laﬀerty, J. D.,
Mercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation.
Computational linguistics , 16(2), 79–85. 19
Brown, P. F., Pietra, V. J. D., DeSouza, P. V., Lai, J. C., and Mercer, R. L. (1992).
Class-based n-gram models of natural language. Computational Linguistics, 18, 467–
479. 395
Bryson, A. and Ho, Y. (1969). Applied optimal control: optimization, estimation, and
control . Blaisdell Pub. Co. 195
Bryson, Jr., A. E. and Denham, W. F. (1961). A steepest-ascent method for solving
optimum programming problems. Technical Report BR-1303, Raytheon Company,
Missle and Space Division. 195
Buchberger, B., Collins, G. E., Loos, R., and Albrecht, R. (1983). Computer Algebra .
Springer-Verlag. 186
Buciluǎ, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery
and data mining , pages 535–541. ACM. 380
Cai, M., Shi, Y., and Liu, J. (2013). Deep maxout neural networks for speech recognition.
In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop
on, pages 291–296. IEEE. 194
Carreira-Perpiñan, M. A. and Hinton, G. E. (2005). On contrastive divergence learning.
In R. G. Cowell and Z. Ghahramani, editors, AISTATS’2005 , pages 33–40. Society for
Artiﬁcial Intelligence and Statistics. 524, 562
Caruana, R. (1993). Multitask connectionist learning. In Proc. 1993 Connectionist Models
Summer School , pages 372–379. 232
Cauchy, A. (1847a). Méthode générale pour la résolution de systèmes d’équations simultanées. In Compte rendu des séances de l’académie des sciences, pages 536–538.
80
Cauchy, L. A. (1847b). Méthode générale pour la résolution des systèmes d’équations
simultanées. Compte Rendu à l’Académie des Sciences . 194
602

BIBLIOGRAPHY

Cayton, L. (2005). Algorithms for manifold learning. Technical Report CS2008-0923,
UCSD. 152, 501
Chapelle, O., Weston, J., and Schölkopf, B. (2003). Cluster kernels for semi-supervised
learning. In NIPS’02 , pages 585–592, Cambridge, MA. MIT Press. 486
Chapelle, O., Schölkopf, B., and Zien, A., editors (2006). Semi-Supervised Learning. MIT
Press, Cambridge, MA. 486
Chellapilla, K., Puri, S., and Simard, P. (2006). High Performance Convolutional Neural Networks for Document Processing. In Guy Lorette, editor, Tenth International
Workshop on Frontiers in Handwriting Recognition, La Baule (France). Université de
Rennes 1, Suvisoft. http://www.suvisoft.com. 21, 24, 378
Chen, S. F. and Goodman, J. T. (1999). An empirical study of smoothing techniques for
language modeling. Computer, Speech and Language, 13(4), 359–393. 348, 406
Chen, T., Du, Z., Sun, N., Wang, J., Wu, C., Chen, Y., and Temam, O. (2014a). Diannao:
A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Proceedings of the 19th international conference on Architectural support for programming
languages and operating systems, pages 269–284. ACM. 383
Chen, Y., Luo, T., Liu, S., Zhang, S., He, L., Wang, J., Li, L., Chen, T., Xu, Z., Sun, N.,
et al. (2014b). Dadiannao: A machine-learning supercomputer. In Microarchitecture
(MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pages 609–622.
IEEE. 383
Chilimbi, T., Suzue, Y., Apacible, J., and Kalyanaraman, K. (2014). Project adam:
Building an eﬃcient and scalable deep learning training system. In 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI’14). 380
Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F., Schwenk, H., and Bengio,
Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical
machine translation. In Proceedings of the Empiricial Methods in Natural Language
Processing (EMNLP 2014). 323, 341, 407
Choromanska, A., Henaﬀ, M., Mathieu, M., Arous, G. B., and LeCun, Y. (2014). The
loss surface of multilayer networks. 242, 475
Chorowski, J., Bahdanau, D., Cho, K., and Bengio, Y. (2014). End-to-end continuous
speech recognition using attention-based recurrent nn: First results. arXiv:1412.1602.
393
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated
recurrent neural networks on sequence modeling. NIPS’2014 Deep Learning workshop,
arXiv 1412.3555. 341, 393
Ciresan, D., Meier, U., Masci, J., and Schmidhuber, J. (2012). Multi-column deep neural
network for traﬃc sign classiﬁcation. Neural Networks , 32, 333–338. 22, 190
603

BIBLIOGRAPHY

Ciresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber, J. (2010). Deep big
simple neural nets for handwritten digit recognition. Neural Computation, 22, 1–14.
21, 24, 378
Coates, A. and Ng, A. Y. (2011). The importance of encoding versus training with sparse
coding and vector quantization. In ICML’2011 . 24
Coates, A., Lee, H., and Ng, A. Y. (2011). An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Thirteenth International Conference
on Artiﬁcial Intelligence and Statistics (AISTATS 2011). 386
Coates, A., Huval, B., Wang, T., Wu, D., Catanzaro, B., and Andrew, N. (2013). Deep
learning with cots hpc systems. In S. Dasgupta and D. McAllester, editors, Proceedings
of the 30th International Conference on Machine Learning (ICML-13), volume 28 (3),
pages 1337–1345. JMLR Workshop and Conference Proceedings. 21, 24, 299, 380
Collobert, R. (2004). Large Scale Machine Learning. Ph.D. thesis, Université de Paris
VI, LIP6. 164
Collobert, R. (2011). Deep learning for eﬃcient discriminative parsing. In AISTATS’2011 .
95
Collobert, R. and Weston, J. (2008). A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In ICML’2008 . 404
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P.
(2011a). Natural language processing (almost) from scratch. Journal of Machine
Learning Research, 12, 2493–2537. 273
Collobert, R., Kavukcuoglu, K., and Farabet, C. (2011b). Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop. 379
Comon, P. (1994). Independent component analysis - a new concept? Signal Processing,
36, 287–314. 454, 455
Cortes, C. and Vapnik, V. (1995). Support vector networks. Machine Learning, 20,
273–297. 16, 134, 148
Couprie, C., Farabet, C., Najman, L., and LeCun, Y. (2013). Indoor semantic segmentation using depth information. In International Conference on Learning Representations
(ICLR2013). 22, 190
Courbariaux, M., Bengio, Y., and David, J.-P. (2015). Low precision arithmetic for deep
learning. In Arxiv:1412.7024, ICLR’2015 Workshop. 384
Courville, A., Bergstra, J., and Bengio, Y. (2011). Unsupervised models of images by
spike-and-slab RBMs. In ICML’11 . 415, 579
Courville, A., Desjardins, G., Bergstra, J., and Bengio, Y. (2014). The spike-and-slab
RBM and extensions to discrete and sparse data distributions. Pattern Analysis and
Machine Intelligence, IEEE Transactions on, 36(9), 1874–1887. 580
604

BIBLIOGRAPHY

Cover, T. M. and Thomas, J. A. (2006). Elements of Information Theory, 2nd Edition.
Wiley-Interscience. 56
Cox, D. and Pinto, N. (2011). Beyond simple features: A large-scale feature search
approach to unconstrained face recognition. In Automatic Face & Gesture Recognition
and Workshops (FG 2011), 2011 IEEE International Conference on, pages 8–15. IEEE.
298
Cramér, H. (1946). Mathematical methods of statistics. Princeton University Press. 126
Crick, F. H. C. and Mitchison, G. (1983). The function of dream sleep. Nature, 304,
111–114. 522
Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems , 2, 303–314. 188, 495
Dahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010). Phone recognition
with the mean-covariance restricted Boltzmann machine. In NIPS’2010 . 22
Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2012). Context-dependent pre-trained
deep neural networks for large vocabulary speech recognition. IEEE Transactions on
Audio, Speech, and Language Processing, 20(1), 33–42. 392
Dahl, G. E., Jaitly, N., and Salakhutdinov, R. (2014). Multi-task neural networks for
QSAR predictions. arXiv:1406.1231. 23
Dauphin, Y. and Bengio, Y. (2013). Stochastic ratio matching of RBMs for sparse highdimensional inputs. In NIPS26 . NIPS Foundation. 532
Dauphin, Y., Glorot, X., and Bengio, Y. (2011). Large-scale learning of embeddings with
reconstruction sampling. In ICML’2011 . 403
Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. (2014).
Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In NIPS’2014 . 242, 475
Davis, A., Rubinstein, M., Wadhwa, N., Mysore, G., Durand, F., and Freeman, W. T.
(2014). The visual microphone: Passive recovery of sound from video. ACM Transactions on Graphics (Proc. SIGGRAPH), 33(4), 79:1–79:10. 384
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Le, Q., Mao, M., Ranzato, M.,
Senior, A., Tucker, P., Yang, K., and Ng, A. Y. (2012). Large scale distributed deep
networks. In NIPS’2012 . 380
Dean, T. and Kanazawa, K. (1989). A model for reasoning about persistence and causation. Computational Intelligence, 5(3), 142–150. 347
Delalleau, O. and Bengio, Y. (2011). Shallow vs. deep sum-product networks. In NIPS .
17, 189, 495, 496
605

BIBLIOGRAPHY

Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). ImageNet: A
Large-Scale Hierarchical Image Database. In CVPR09 . 19, 142
Deng, J., Berg, A. C., Li, K., and Fei-Fei, L. (2010a). What does classifying more than
10,000 image categories tell us? In Proceedings of the 11th European Conference on
Computer Vision: Part V , ECCV’10, pages 71–84, Berlin, Heidelberg. Springer-Verlag.
19
Deng, J., Ding, N., Jia, Y., Frome, A., Murphy, K., Bengio, S., Li, Y., Neven, H., and
Adam, H. (2014). Large-scale object classiﬁcation using label relation graphs. In
ECCV’2014 , pages 48–64. 349
Deng, L. and Yu, D. (2014). Deep learning – methods and applications. Foundations and
Trends in Signal Processing. 392
Deng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton, G. (2010b). Binary coding of speech spectrograms using a deep auto-encoder. In Interspeech 2010 ,
Makuhari, Chiba, Japan. 22
Desjardins, G. and Bengio, Y. (2008). Empirical evaluation of convolutional RBMs
for vision. Technical Report 1327, Département d’Informatique et de Recherche
Opérationnelle, Université de Montréal. 580
Desjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the partition function.
In NIPS’2011 . 540
Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R., and Makhoul, J. (2014). Fast
and robust neural network joint models for statistical machine translation. In Proc.
ACL’2014 . 406
DiCarlo, J. J. (2013). Mechanisms underlying visual object recognition: Humans vs.
neurons vs. machines. NIPS Tutorial. 23, 301
Do, T.-M.-T. and Artières, T. (2010). Neural conditional random ﬁelds. In International
Conference on Artiﬁcial Intelligence and Statistics, pages 177–184. 349
Donahue, J., Hendricks, L. A., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko,
K., and Darrell, T. (2014). Long-term recurrent convolutional networks for visual
recognition and description. arXiv:1411.4389. 95
Donoho, D. L. and Grimes, C. (2003). Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data. Technical Report 2003-08, Dept. Statistics,
Stanford University. 152, 504
Doya, K. (1993). Bifurcations of recurrent neural networks in gradient descent learning.
IEEE Transactions on Neural Networks, 1, 75–80. 249, 334
Dreyfus, S. E. (1962). The numerical solution of variational problems. Journal of Mathematical Analysis and Applications, 5(1), 30–45. 195
606

BIBLIOGRAPHY

Dreyfus, S. E. (1973). The computational solution of optimal control problems with time
lag. IEEE Transactions on Automatic Control , 18(4), 383–385. 195
Dugas, C., Bengio, Y., Bélisle, F., and Nadeau, C. (2001). Incorporating second-order
functional knowledge for better option pricing. In NIPS’00 , pages 472–478. MIT Press.
65, 164
Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., and LeytonBrown, K. (2013). Towards an empirical foundation for assessing bayesian optimization
of hyperparameters. NIPS workshop on Bayesian Optimization in Theory and Practice.
372
El Hihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term
dependencies. In NIPS 8 . MIT Press. 326, 347
ElHihi, S. and Bengio, Y. (1996). Hierarchical recurrent neural networks for long-term
dependencies. In NIPS’1995 . 337
Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., and Vincent, P. (2009). The diﬃculty
of training deep architectures and the eﬀect of unsupervised pre-training. In Proceedings
of AISTATS’2009 . 190
Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. (2010).
Why does unsupervised pre-training help deep learning? J. Machine Learning Res.
473, 475, 476, 477
Fang, H., Gupta, S., Iandola, F., Srivastava, R., Deng, L., Dollár, P., Gao, J., He, X.,
Mitchell, M., Platt, J. C., Zitnick, C. L., and Zweig, G. (2015). From captions to visual
concepts and back. arXiv:1411.4952. 95
Farabet, C., LeCun, Y., Kavukcuoglu, K., Culurciello, E., Martini, B., Akselrod, P.,
and Talay, S. (2011). Large-scale FPGA-based convolutional networks. In R. Bekkerman, M. Bilenko, and J. Langford, editors, Scaling up Machine Learning: Parallel and
Distributed Approaches. Cambridge University Press. 462
Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013a). Learning hierarchical
features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence. 22, 190
Farabet, C., Couprie, C., Najman, L., and LeCun, Y. (2013b). Learning hierarchical
features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8), 1915–1929. 349
Fei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4), 594–611.
482
Fischer, A. and Igel, C. (2011). Bounding the bias of contrastive divergence learning.
Neural Computation, 23(3), 664–73. 562
607

BIBLIOGRAPHY

Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. Annals
of Eugenics , 7, 179–188. 19, 98
Frasconi, P., Gori, M., and Sperduti, A. (1997). On the eﬃcient classiﬁcation of data
structures by neural networks. In Proc. Int. Joint Conf. on Artiﬁcial Intelligence. 326,
328
Frasconi, P., Gori, M., and Sperduti, A. (1998). A general framework for adaptive processing of data structures. IEEE Transactions on Neural Networks, 9(5), 768–786.
326, 328
Freund, Y. and Schapire, R. E. (1996a). Experiments with a new boosting algorithm. In
Machine Learning: Proceedings of Thirteenth International Conference, pages 148–156,
USA. ACM. 227
Freund, Y. and Schapire, R. E. (1996b). Game theory, on-line prediction and boosting.
In Proceedings of the Ninth Annual Conference on Computational Learning Theory ,
pages 325–332. 227
Frey, B. J. (1998). Graphical models for machine learning and digital communication.
MIT Press. 329
Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn
good density estimators? In NIPS’95 , pages 661–670. MIT Press, Cambridge, MA.
329
Fukushima, K. (1980). Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaﬀected by shift in position. Biological Cybernetics, 36,
193–202. 14, 21, 24, 302
Garson, J. (1900). The metric system of identiﬁcation of criminals, as used in in great
britain and ireland. The Journal of the Anthropological Institute of Great Britain and
Ireland, (2), 177–227. 19
Gers, F. A., Schmidhuber, J., and Cummins, F. (2000). Learning to forget: Continual
prediction with LSTM. Neural computation, 12(10), 2451–2471. 342
Glorot, X. and Bengio, Y. (2010a). Understanding the diﬃculty of training deep feedforward neural networks. In AISTATS’2010 . 163
Glorot, X. and Bengio, Y. (2010b). Understanding the diﬃculty of training deep feedforward neural networks. In JMLR W&CP: Proceedings of the Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010), volume 9, pages
249–256. 266
Glorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer neural networks. In
AISTATS’2011 . 14, 164, 461

608

BIBLIOGRAPHY

Glorot, X., Bordes, A., and Bengio, Y. (2011b). Deep sparse rectiﬁer neural networks.
In JMLR W&CP: Proceedings of the Fourteenth International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS 2011). 193, 461
Glorot, X., Bordes, A., and Bengio, Y. (2011c). Domain adaptation for large-scale sentiment classiﬁcation: A deep learning approach. In ICML’2011 . 461, 481
Gong, S., McKenna, S., and Psarrou, A. (2000). Dynamic Vision: From Images to Face
Recognition. Imperial College Press. 505, 507
Goodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009). Measuring invariances in deep
networks. In NIPS’2009 , pages 646–654. 226, 448, 460
Goodfellow, I., Koenig, N., Muja, M., Pantofaru, C., Sorokin, A., and Takayama, L.
(2010). Help me help you: Interfaces for personal robots. In Proc. of Human Robot
Interaction (HRI), Osaka, Japan. ACM Press, ACM Press. 93
Goodfellow, I., Courville, A., and Bengio, Y. (2012). Large-scale feature learning with
spike-and-slab sparse coding. In ICML’2012 . 457
Goodfellow, I. J. (2010). Technical report: Multidimensional, downsampled convolution
for autoencoders. Technical report, Université de Montréal. 293
Goodfellow, I. J., Courville, A., and Bengio, Y. (2011). Spike-and-slab sparse coding
for unsupervised feature discovery. In NIPS Workshop on Challenges in Learning
Hierarchical Models. 190, 481
Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013a).
Maxout networks. In S. Dasgupta and D. McAllester, editors, ICML’13 , pages 1319–
1327. 193, 230, 300, 386
Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. (2013b). Multi-prediction
deep Boltzmann machines. In NIPS26 . NIPS Foundation. 94, 530, 576, 578
Goodfellow, I. J., Courville, A., and Bengio, Y. (2013c). Scaling up spike-and-slab models
for unsupervised feature learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35(8), 1902–1914. 580
Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y. (2014a). An empirical investigation of catastrophic forgeting in gradient-based neural networks. In
ICLR’2014 . 194
Goodfellow, I. J., Shlens, J., and Szegedy, C. (2014b). Explaining and harnessing adversarial examples. CoRR, abs/1412.6572. 235
Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., and Bengio, Y. (2014c). Generative adversarial networks. In NIPS’2014 .
188

609

BIBLIOGRAPHY

Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. (2014d). Multidigit number recognition from Street View imagery using deep convolutional neural
networks. In International Conference on Learning Representations . 22, 94, 190, 365,
382
Goodman, J. (2001). Classes for fast maximum entropy training. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), Utah. 400
Gori, M. and Tesi, A. (1992). On the problem of local minima in backpropagation. IEEE
Transactions on Pattern Analysis and Machine Intelligence, PAMI-14(1), 76–86. 242
Gosset, W. S. (1908). The probable error of a mean. Biometrika , 6(1), 1–25. Originally
published under the pseudonym “Student”. 19
Gouws, S., Bengio, Y., and Corrado, G. (2014). Bilbowa: Fast bilingual distributed
representations without word alignments. Technical report, arXiv:1410.2455. 408, 484
Graf, H. P. and Jackel, L. D. (1989). Analog electronic neural network circuits. Circuits
and Devices Magazine, IEEE , 5(4), 44–49. 383
Graves, A. (2011a). Practical variational inference for neural networks. In J. ShaweTaylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, editors, Advances in
Neural Information Processing Systems 24 , pages 2348–2356. Curran Associates, Inc.
211, 212
Graves, A. (2011b). Practical variational inference for neural networks. In NIPS’2011 .
214
Graves, A. (2012). Supervised Sequence Labelling with Recurrent Neural Networks. Studies
in Computational Intelligence. Springer. 309, 323, 340, 341, 349, 393
Graves, A. (2013). Generating sequences with recurrent neural networks. Technical
report, arXiv:1308.0850. 172, 340
Graves, A. and Jaitly, N. (2014). Towards end-to-end speech recognition with recurrent
neural networks. In ICML’2014 . 340
Graves, A. and Schmidhuber, J. (2005). Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5), 602–610.
323
Graves, A. and Schmidhuber, J. (2009). Oﬄine handwriting recognition with multidimensional recurrent neural networks. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, NIPS’2008 , pages 545–552. 323
Graves, A., Fernández, S., Gomez, F., and Schmidhuber, J. (2006). Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks.
In ICML’2006 , pages 369–376, Pittsburgh, USA. 349, 393

610

BIBLIOGRAPHY

Graves, A., Liwicki, M., Bunke, H., Schmidhuber, J., and Fernández, S. (2008). Unconstrained on-line handwriting recognition with recurrent neural networks. In J. Platt,
D. Koller, Y. Singer, and S. Roweis, editors, NIPS’2007 , pages 577–584. 323
Graves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., and Schmidhuber,
J. (2009). A novel connectionist system for unconstrained handwriting recognition.
Pattern Analysis and Machine Intelligence, IEEE Transactions on , 31(5), 855–868.
340
Graves, A., Mohamed, A.-r., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. In ICASSP’2013 , pages 6645–6649. 323, 325, 326, 340, 341,
392, 393
Graves, A., Wayne, G., and Danihelka, I. (2014a).
arXiv:1410.5401. 23

Neural Turing machines.

Graves, A., Wayne, G., and Danihelka, I. (2014b). Neural turing machines. arXiv preprint
arXiv:1410.5401 . 343
Greﬀ, K., Srivastava, R. K., Koutnı́k, J., Steunebrink, B. R., and Schmidhuber, J. (2015).
LSTM: a search space odyssey. arXiv preprint arXiv:1503.04069 . 342
Gregor, K. and LeCun, Y. (2010). Emergence of complex-like cells in a temporal product
network with local receptive ﬁelds. Technical report, arXiv:1006.0448. 292
Gülçehre, Ç. and Bengio, Y. (2013). Knowledge matters: Importance of prior information for optimization. In International Conference on Learning Representations
(ICLR’2013). 22, 270
Guo, H. and Gelfand, S. B. (1992). Classiﬁcation trees with neural network feature
extraction. Neural Networks, IEEE Transactions on, 3(6), 923–933. 382
Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. (2015). Deep learning
with limited numerical precision. CoRR, abs/1502.02551. 384
Gutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’10). 532
Hadsell, R., Sermanet, P., Ben, J., Erkan, A., Han, J., Muller, U., and LeCun, Y.
(2007). Online learning for oﬀroad robots: Spatial label propagation to learn longrange traversability. In Proceedings of Robotics: Science and Systems, Atlanta, GA,
USA. 385
Haﬀner, P., Franzini, M., and Waibel, A. (1991). Integrating time alignment and neural
networks for high performance continuous speech recognition. In International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 105–108, Toronto.
358
611

BIBLIOGRAPHY

Håstad, J. (1986). Almost optimal lower bounds for small depth circuits. In Proceedings
of the 18th annual ACM Symposium on Theory of Computing, pages 6–20, Berkeley,
California. ACM Press. 189, 495
Håstad, J. and Goldmann, M. (1991). On the power of small-depth threshold circuits.
Computational Complexity , 1, 113–129. 189, 495
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The elements of statistical learning:
data mining, inference and prediction. Springer Series in Statistics. Springer Verlag.
138
Hebb, D. O. (1949). The Organization of Behavior . Wiley, New York. 15
Henaﬀ, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011). Unsupervised learning
of sparse features for scalable audio classiﬁcation. In ISMIR’11 . 462
Herault, J. and Ans, B. (1984). Circuits neuronaux à synapses modiﬁables: Décodage de
messages composites par apprentissage non supervisé. Comptes Rendus de l’Académie
des Sciences, 299(III-13), 525––528. 454
Hinton, G. (2012). Neural networks for machine learning. Coursera, video lectures. 257
Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V.,
Nguyen, P., Sainath, T., and Kingsbury, B. (2012a). Deep neural networks for acoustic
modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82–97. 22,
392
Hinton, G. E. (2000). Training products of experts by minimizing contrastive divergence.
Technical Report GCNU TR 2000-004, Gatsby Unit, University College London. 523
Hinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding. In NIPS’2002 . 504
Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimensionality of data with
neural networks. Science, 313(5786), 504–507. 450, 472, 473
Hinton, G. E. and Salakhutdinov, R. (2006). Reducing the Dimensionality of Data with
Neural Networks. Science, 313, 504–507. 475
Hinton, G. E. and Zemel, R. S. (1994). Autoencoders, minimum description length, and
Helmholtz free energy. In NIPS’1993 . 444
Hinton, G. E., Osindero, S., and Teh, Y. (2006). A fast learning algorithm for deep belief
nets. Neural Computation, 18, 1527–1554. 12, 16, 24, 135, 472, 473, 563
Hinton, G. E., Deng, L., Yu, D., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,
Vanhoucke, V., Nguyen, P., Sainath, T. N., and Kingsbury, B. (2012b). Deep neural
networks for acoustic modeling in speech recognition: The shared views of four research
groups. IEEE Signal Process. Mag., 29(6), 82–97. 94

612

BIBLIOGRAPHY

Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.
(2012c). Improving neural networks by preventing co-adaptation of feature detectors.
Technical report, arXiv:1207.0580. 208
Hinton, G. E., Vinyals, O., and Dean, J. (2014). Dark knowledge. Invited talk at the
BayLearn Bay Area Machine Learning Symposium. 381
Hochreiter, S. (1991). Untersuchungen zu dynamischen neuronalen Netzen. Diploma
thesis, T.U. Münich. 247, 334, 343
Hochreiter, S. and Schmidhuber, J. (1995). Simplifying neural nets by discovering ﬂat
minima. In Advances in Neural Information Processing Systems 7 , pages 529–536.
MIT Press. 215
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780. 23, 340, 341
Hochreiter, S., Informatik, F. F., Bengio, Y., Frasconi, P., and Schmidhuber, J. (2000).
Gradient ﬂow in recurrent nets: the diﬃculty of learning long-term dependencies. In
J. Kolen and S. Kremer, editors, Field Guide to Dynamical Recurrent Networks. IEEE
Press. 341
Holi, J. L. and Hwang, J.-N. (1993). Finite precision error analysis of neural network
hardware implementations. Computers, IEEE Transactions on, 42(3), 281–290. 383
Holt, J. L. and Baker, T. E. (1991). Back propagation simulations using limited precision calculations. In Neural Networks, 1991., IJCNN-91-Seattle International Joint
Conference on, volume 2, pages 121–126. IEEE. 383
Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are
universal approximators. Neural Networks , 2, 359–366. 188, 495
Hornik, K., Stinchcombe, M., and White, H. (1990). Universal approximation of an
unknown mapping and its derivatives using multilayer feedforward networks. Neural
networks, 3(5), 551–560. 188
Horst, R., Pardalos, P., and Thoai, N. (2000). Introduction to Global Optimization.
Kluwer Academic Publishers. Second Edition. 271
Hsu, F.-H. (2002). Behind Deep Blue: Building the Computer That Defeated the World
Chess Champion . Princeton University Press, Princeton, NJ, USA. 2
Huang, F. and Ogata, Y. (2002). Generalized pseudo-likelihood estimates for markov
random ﬁelds on lattice. Annals of the Institute of Statistical Mathematics, 54(1),
1–18. 529
Hubel, D. and Wiesel, T. (1968). Receptive ﬁelds and functional architecture of monkey
striate cortex. Journal of Physiology (London), 195, 215–243. 299

613

BIBLIOGRAPHY

Hubel, D. H. and Wiesel, T. N. (1959). Receptive ﬁelds of single neurons in the cat’s
striate cortex. Journal of Physiology , 148, 574–591. 299
Hubel, D. H. and Wiesel, T. N. (1962). Receptive ﬁelds, binocular interaction, and
functional architecture in the cat’s visual cortex. Journal of Physiology (London),
160, 106–154. 299
Hutter, F., Hoos, H., and Leyton-Brown, K. (2011). Sequential model-based optimization
for general algorithm conﬁguration. In LION-5 . Extended version as UBC Tech report
TR-2010-10. 372
Hyotyniemi, H. (1996). Turing machines are recurrent neural networks. In STeP’96 ,
pages 13–24. 311
Hyvärinen, A. (1999). Survey on independent component analysis. Neural Computing
Surveys, 2, 94–128. 454
Hyvärinen, A. (2005a). Estimation of non-normalized statistical models using score
matching. J. Machine Learning Res., 6. 465
Hyvärinen, A. (2005b). Estimation of non-normalized statistical models using score
matching. Journal of Machine Learning Research, 6, 695–709. 530
Hyvärinen, A. (2007a). Connections between score matching, contrastive divergence,
and pseudolikelihood for continuous-valued variables. IEEE Transactions on Neural
Networks, 18, 1529–1531. 531
Hyvärinen, A. (2007b). Some extensions of score matching. Computational Statistics and
Data Analysis, 51, 2499–2512. 531
Hyvärinen, A. and Pajunen, P. (1999). Nonlinear independent component analysis: Existence and uniqueness results. Neural Networks , 12(3), 429–439. 455
Hyvärinen, A., Karhunen, J., and Oja, E. (2001). Independent Component Analysis.
Wiley-Interscience. 454
Hyvärinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural Image Statistics: A probabilistic approach to early computational vision. Springer-Verlag. 305
Ioﬀe, S. and Szegedy, C. (2015). Batch normalization: Accelerating deep network training
by reducing internal covariate shift. 22, 93, 263
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation.
Neural networks, 1(4), 295–307. 256
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixture
of local experts. Neural Computation, 3, 79–87. 171
Jaeger, H. (2003). Adaptive nonlinear system identiﬁcation with echo state networks. In
Advances in Neural Information Processing Systems 15 . 335
614

BIBLIOGRAPHY

Jaeger, H. (2007a). Discovering multiscale dynamical features with hierarchical echo state
networks. Technical report, Jacobs University. 326
Jaeger, H. (2007b). Echo state network. Scholarpedia, 2(9), 2330. 334
Jaeger, H. and Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and
saving energy in wireless communication. Science, 304(5667), 78–80. 24, 334
Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., Mooij, J. M., and Schölkopf, B. (2012).
On causal and anticausal learning. In ICML’2012 , pages 1255–1262. 487, 489
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009a). What is the best
multi-stage architecture for object recognition? In ICCV’09 . 14, 164, 462
Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009b). What is the best
multi-stage architecture for object recognition? In Proc. International Conference on
Computer Vision (ICCV’09), pages 2146–2153. IEEE. 21, 24, 192, 193, 298, 299
Jarzynski, C. (1997). Nonequilibrium equality for free energy diﬀerences. Phys. Rev.
Lett., 78, 2690–2693. 539
Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge University
Press. 48
Jean, S., Cho, K., Memisevic, R., and Bengio, Y. (2014). On using very large target
vocabulary for neural machine translation. arXiv:1412.2007. 407
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of markov source parameters
from sparse data. In E. S. Gelsema and L. N. Kanal, editors, Pattern Recognition in
Practice. North-Holland, Amsterdam. 348, 406
Jia, Y., Huang, C., and Darrell, T. (2012). Beyond spatial pyramids: Receptive ﬁeld learning for pooled image features. In Computer Vision and Pattern Recognition (CVPR),
2012 IEEE Conference on, pages 3370–3377. IEEE. 287
Jim, K.-C., Giles, C. L., and Horne, B. G. (1996). An analysis of noise in recurrent neural
networks: convergence and generalization. IEEE Transactions on Neural Networks,
7(6), 1424–1438. 211, 214
Jordan, M. I. (1998). Learning in Graphical Models . Kluwer, Dordrecht, Netherlands. 16
Jozefowicz, R., Zaremba, W., and Sutskever, I. (2015a). An empirical evaluation of
recurrent network architectures. In ICML’2015 . 342
Jozefowicz, R., Zaremba, W., and Sutskever, I. (2015b). An empirical exploration of
recurrent network architectures. In Proceedings of The 32nd International Conference
on Machine Learning , pages 2342–2350. 268
Juang, B. H. and Katagiri, S. (1992). Discriminative learning for minimum error classiﬁcation. IEEE Transactions on Signal Processing, 40(12), 3043–3054. 356
615

BIBLIOGRAPHY

Judd, J. S. (1989). Neural Network Design and the Complexity of Learning. MIT press.
249
Jutten, C. and Herault, J. (1991). Blind separation of sources, part I: an adaptive algorithm based on neuromimetic architecture. Signal Processing, 24, 1–10. 454
Kahou, S. E., Pal, C., Bouthillier, X., Froumenty, P., Gülçehre, c., Memisevic, R., Vincent, P., Courville, A., Bengio, Y., Ferrari, R. C., Mirza, M., Jean, S., Carrier, P.-L.,
Dauphin, Y., Boulanger-Lewandowski, N., Aggarwal, A., Zumer, J., Lamblin, P., Raymond, J.-P., Desjardins, G., Pascanu, R., Warde-Farley, D., Torabi, A., Sharma, A.,
Bengio, E., Côté, M., Konda, K. R., and Wu, Z. (2013). Combining modality speciﬁc
deep neural networks for emotion recognition in video. In Proceedings of the 15th ACM
on International Conference on Multimodal Interaction. 190
Kalchbrenner, N. and Blunsom, P. (2013). Recurrent continuous translation models. In
EMNLP’2013 . 407
Kamyshanska, H. and Memisevic, R. (2015). The potential energy of an autoencoder.
IEEE Transactions on Pattern Analysis and Machine Intelligence. 468
Kanazawa, K., Koller, D., and Russell, S. (1995). Stochastic simulation algorithms for
dynamic probabilistic networks. In Proc. UAI’1995 , pages 346–351. 347
Karpathy, A. and Li, F.-F. (2015). Deep visual-semantic alignments for generating image
descriptions. In CVPR’2015 . arXiv:1412.2306. 95
Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., and Fei-Fei, L. (2014).
Large-scale video classiﬁcation with convolutional neural networks. In CVPR. 19
Karush, W. (1939). Minima of Functions of Several Variables with Inequalities as Side
Constraints. Master’s thesis, Dept.˜of Mathematics, Univ.˜of Chicago. 90
Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model
component of a speech recognizer. IEEE Transactions on Acoustics, Speech, and Signal
Processing, ASSP-35(3), 400–401. 348, 406
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008a). Fast inference in sparse coding
algorithms with applications to object recognition. CBLL-TR-2008-12-01, NYU. 447
Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008b). Fast inference in sparse coding
algorithms with applications to object recognition. Technical report, Computational
and Biological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-200812-01. 462
Kavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009). Learning invariant
features through topographic ﬁlter maps. In CVPR’2009 . 462
Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y.
(2010a). Learning convolutional feature hierarchies for visual recognition. In Advances
in Neural Information Processing Systems 23 (NIPS’10), pages 1090–1098. 299
616

BIBLIOGRAPHY

Kavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Mathieu, M., and LeCun, Y. (2010b). Learning convolutional feature hierarchies for visual recognition.
In NIPS’2010 . 462
Kelley, H. J. (1960). Gradient theory of optimal ﬂight paths. ARS Journal , 30(10),
947–954. 195
Khan, F., Zhu, X., and Mutlu, B. (2011). How do humans teach: On curriculum learning
and teaching dimension. In Advances in Neural Information Processing Systems 24
(NIPS’11), pages 1449–1457. 273
Kim, S. K., McAfee, L. C., McMahon, P. L., and Olukotun, K. (2009). A highly scalable
restricted Boltzmann machine FPGA implementation. In Field Programmable Logic
and Applications, 2009. FPL 2009. International Conference on, pages 367–372. IEEE.
383
Kindermann, R. (1980). Markov Random Fields and Their Applications (Contemporary
Mathematics ; V. 1). American Mathematical Society. 419
Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 . 258
Kingma, D. and LeCun, Y. (2010a). Regularized estimation of image statistics by score
matching. In NIPS’2010 . 465
Kingma, D. and LeCun, Y. (2010b). Regularized estimation of image statistics by score
matching. In J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta,
editors, Advances in Neural Information Processing Systems 23 , pages 1126–1134. 532
Kingma, D., Rezende, D., Mohamed, S., and Welling, M. (2014). Semi-supervised learning
with deep generative models. In NIPS’2014 . 435
Kingma, D. P. (2013). Fast gradient-based inference with continuous latent variable
models in auxiliary form. Technical report, arxiv:1306.0733. 188, 435
Kingma, D. P. and Welling, M. (2014a). Auto-encoding variational bayes. In Proceedings
of the International Conference on Learning Representations (ICLR). 188, 435, 507,
508
Kingma, D. P. and Welling, M. (2014b). Eﬃcient gradient-based inference through transformations between bayes nets and neural nets. Technical report, arxiv:1402.0480. 188,
434, 435
Kirkpatrick, S., Jr., C. D. G., , and Vecchi, M. P. (1983). Optimization by simulated
annealing. Science, 220, 671–680. 271
Kiros, R., Salakhutdinov, R., and Zemel, R. (2014a). Multimodal neural language models.
In ICML’2014 . 95

617

BIBLIOGRAPHY

Kiros, R., Salakhutdinov, R., and Zemel, R. (2014b). Unifying visual-semantic embeddings with multimodal neural language models. arXiv:1411.2539 [cs.LG]. 95, 340
Klementiev, A., Titov, I., and Bhattarai, B. (2012). Inducing crosslingual distributed
representations of words. In Proceedings of COLING 2012 . 408, 484
Knowles-Barley, S., Jones, T. R., Morgan, J., Lee, D., Kasthuri, N., Lichtman, J. W., and
Pﬁster, H. (2014). Deep learning for the connectome. GPU Technology Conference . 23
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and
Techniques. MIT Press. 354, 433, 440
Konig, Y., Bourlard, H., and Morgan, N. (1996). REMAP: Recursive estimation and maximization of A posteriori probabilities – application to transition-based connectionist
speech recognition. In NIPS’95 . MIT Press, Cambridge, MA. 391
Koren, Y. (2009). 1 the bellkor solution to the netﬂix grand prize. 227
Koutnik, J., Greﬀ, K., Gomez, F., and Schmidhuber, J. (2014). A clockwork RNN. In
ICML’2014 . 326, 347
Kočiský, T., Hermann, K. M., and Blunsom, P. (2014). Learning Bilingual Word Representations by Marginalizing Alignments. In Proceedings of ACL. 408
Krause, O., Fischer, A., Glasmachers, T., and Igel, C. (2013). Approximation properties
of DBNs with binary hidden units and real-valued visible units. In ICML’2013 . 495
Krizhevsky, A. (2010).
Convolutional deep belief networks on CIFAR10.
Technical report, University of Toronto.
Unpublished Manuscript:
http://www.cs.utoronto.ca/ kriz/conv-cifar10-aug2010.pdf. 379
Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny
images. Technical report, University of Toronto. 19, 415
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012a). ImageNet classiﬁcation with deep
convolutional neural networks. In Advances in Neural Information Processing Systems
25 (NIPS’2012). 21, 24, 93, 306, 385
Krizhevsky, A., Sutskever, I., and Hinton, G. (2012b). ImageNet classiﬁcation with deep
convolutional neural networks. In NIPS’2012 . 22, 190, 461
Kuhn, H. W. and Tucker, A. W. (1951). Nonlinear programming. In Proceedings of the
Second Berkeley Symposium on Mathematical Statistics and Probability, pages 481–
492, Berkeley, Calif. University of California Press. 90
Kumar, M. P., Packer, B., and Koller, D. (2010). Self-paced learning for latent variable
models. In NIPS’2010 . 273
Laﬀerty, J., McCallum, A., and Pereira, F. C. N. (2001). Conditional random ﬁelds:
Probabilistic models for segmenting and labeling sequence data. In C. E. Brodley and
A. P. Danyluk, editors, ICML 2001 . Morgan Kaufmann. 349, 357
618

BIBLIOGRAPHY

Lang, K. J. and Hinton, G. E. (1988). The development of the time-delay neural network architecture for speech recognition. Technical Report CMU-CS-88-152, CarnegieMellon University. 302, 309, 336
Lappalainen, H., Giannakopoulos, X., Honkela, A., and Karhunen, J. (2000). Nonlinear
independent component analysis using ensemble learning: Experiments and discussion.
In Proc. ICA. Citeseer. 455
Larochelle, H. and Bengio, Y. (2008a). Classiﬁcation using discriminative restricted Boltzmann machines. In ICML’2008 . 226, 448, 591
Larochelle, H. and Bengio, Y. (2008b). Classiﬁcation using discriminative restricted
Boltzmann machines. In ICML’08 , pages 536–543. ACM. 486
Larochelle, H. and Murray, I. (2011). The Neural Autoregressive Distribution Estimator.
In AISTATS’2011 . 329, 332
Larochelle, H., Erhan, D., and Bengio, Y. (2008). Zero-data learning of new tasks. In
AAAI Conference on Artiﬁcial Intelligence. 482
Lasserre, J. A., Bishop, C. M., and Minka, T. P. (2006). Principled hybrids of generative
and discriminative models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR’06), pages 87–94, Washington, DC, USA. IEEE Computer
Society. 223, 486
Le, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A. (2010). Tiled
convolutional neural networks. In J. Laﬀerty, C. K. I. Williams, J. Shawe-Taylor,
R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems
23 (NIPS’10), pages 1279–1287. 292
Le, Q., Ranzato, M., Monga, R., Devin, M., Corrado, G., Chen, K., Dean, J., and Ng,
A. (2012). Building high-level features using large scale unsupervised learning. In
ICML’2012 . 21, 24
Le Roux, N. and Bengio, Y. (2010). Deep belief networks are compact universal approximators. Neural Computation, 22(8), 2192–2207. 495
Le Roux, N., Manzagol, P.-A., and Bengio, Y. (2008). Topmoumoute online natural
gradient algorithm. In NIPS 20 . MIT Press. 241
LeCun, Y. (1985). Une procédure d’apprentissage pour Réseau à seuil assymétrique. In
Cognitiva 85: A la Frontière de l’Intelligence Artiﬁcielle, des Sciences de la Connaissance et des Neurosciences, pages 599–604, Paris 1985. CESTA, Paris. 195
LeCun, Y. (1987). Modèles connexionistes de l’apprentissage. Ph.D. thesis, Université de
Paris VI. 16, 444
LeCun, Y., Jackel, L. D., Boser, B., Denker, J. S., Graf, H. P., Guyon, I., Henderson, D.,
Howard, R. E., and Hubbard, W. (1989). Handwritten digit recognition: Applications
of neural network chips and automatic learning. IEEE Communications Magazine ,
27(11), 41–46. 302
619

BIBLIOGRAPHY

LeCun, Y., Bottou, L., Orr, G. B., and Müller, K.-R. (1998a). Eﬃcient backprop. In
Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524.
Springer Verlag. 365
LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998b). Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11), 2278–2324. 14, 24
LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998c). Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11), 2278–2324. 16, 19,
349, 357, 358, 359, 392
LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998d). Gradient based learning
applied to document recognition. Proc. IEEE . 305
LeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010). Convolutional networks and applications in vision. In Circuits and Systems (ISCAS), Proceedings of 2010 IEEE
International Symposium on, pages 253–256. IEEE. 306
Lee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net model for visual area
V2. In NIPS’07 . 226, 448
Lee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009). Convolutional deep belief
networks for scalable unsupervised learning of hierarchical representations. In L. Bottou
and M. Littman, editors, ICML 2009 . ACM, Montreal, Canada. 298, 580, 581
Lee, Y. J. and Grauman, K. (2011). Learning the easy things ﬁrst: self-paced visual
category discovery. In CVPR’2011 . 273
Leibniz, G. W. (1676). Memoir using the chain rule. (Cited in TMME 7:2&3 p 321-332,
2010). 194
Lenat, D. B. and Guha, R. V. (1989). Building large knowledge-based systems; representation and inference in the Cyc project. Addison-Wesley Longman Publishing Co., Inc.
2
Leprieur, H. and Haﬀner, P. (1995). Discriminant learning with minimum memory loss
for improved non-vocabulary rejection. In EUROSPEECH’95 , Madrid, Spain. 356
L’Hôpital, G. F. A. (1696). Analyse des inﬁniment petits, pour l’intelligence des lignes
courbes. Paris: L’Imprimerie Royale. 194
Lin, T., Horne, B. G., Tino, P., and Giles, C. L. (1996). Learning long-term dependencies
is not as diﬃcult with NARX recurrent neural networks. IEEE Transactions on Neural
Networks, 7(6), 1329–1338. 336, 337
Linde, N. (1992). The machine that changed the world, episode 3. Documentary miniseries. 2
Lindsey, C. and Lindblad, T. (1994). Review of hardware neural networks: a user’s
perspective. In Proc. Third Workshop on Neural Networks: From Biology to High
Energy Physics, pages 195––202, Isola d’Elba, Italy. 383
620

BIBLIOGRAPHY

Linnainmaa, S. (1976). Taylor expansion of the accumulated rounding error. BIT Numerical Mathematics, 16(2), 146–160. 195
Long, P. M. and Servedio, R. A. (2010). Restricted Boltzmann machines are hard to approximately evaluate or simulate. In Proceedings of the 27th International Conference
on Machine Learning (ICML’10). 557
Lovelace, A. (1842). Notes upon L. F. Menabrea’s “Sketch of the Analytical Engine
invented by Charles Babbage”. 1
Lowerre, B. (1976). The Harpy Speech Recognition System. Ph.D. thesis. 350, 356, 362
Lukoševičius, M. and Jaeger, H. (2009). Reservoir computing approaches to recurrent
neural network training. Computer Science Review, 3(3), 127–149. 334
Luo, H., Carrier, P.-L., Courville, A., and Bengio, Y. (2013). Texture modeling with
convolutional spike-and-slab RBMs and deep extensions. In AISTATS’2013 . 95
Lyness, J. N. and Moler, C. B. (1967). Numerical diﬀerentiation of analytic functions.
SIAM J.Numer. Anal., 4, 202––210. 184
Lyu, S. (2009). Interpretation and generalization of score matching. In UAI’09 . 531
Maass, W., Natschlaeger, T., and Markram, H. (2002). Real-time computing without
stable states: A new framework for neural computation based on perturbations. Neural
Computation, 14(11), 2531–2560. 334
MacKay, D. (2003). Information Theory, Inference and Learning Algorithms. Cambridge
University Press. 56
Maclaurin, D., Duvenaud, D., and Adams, R. P. (2015). Gradient-based hyperparameter
optimization through reversible learning. arXiv preprint arXiv:1502.03492 . 372
Mao, J., Xu, W., Yang, Y., Wang, J., Huang, Z., and Yuille, A. L. (2015). Deep captioning
with multimodal recurrent neural networks. In ICLR’2015 . arXiv:1410.1090. 95
Marlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010). Inductive principles for
restricted Boltzmann machine learning. In Proceedings of The Thirteenth International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS’10), volume 9, pages
509–516. 526, 531, 559
Marr, D. and Poggio, T. (1976). Cooperative computation of stereo disparity. Science,
194. 302
Martens, J. (2010). Deep learning via Hessian-free optimization. In L. Bottou and
M. Littman, editors, Proceedings of the Twenty-seventh International Conference on
Machine Learning (ICML-10), pages 735–742. ACM. 267
Martens, J. and Medabalimi, V. (2014). On the expressive eﬃciency of sum product
networks. arXiv:1411.7717 . 496
621

BIBLIOGRAPHY

Martens, J. and Sutskever, I. (2011). Learning recurrent neural networks with Hessianfree optimization. In Proc. ICML’2011 . ACM. 344
Mase, S. (1995). Consistency of the maximum pseudo-likelihood estimator of continuous
state space Gibbsian processes. The Annals of Applied Probability, 5(3), pp. 603–612.
529
Matan, O., Burges, C. J. C., LeCun, Y., and Denker, J. S. (1992). Multi-digit recognition
using a space displacement neural network. In NIPS’91 , pages 488–495, San Mateo
CA. Morgan Kaufmann. 358
McCullagh, P. and Nelder, J. (1989). Generalized Linear Models . Chapman and Hall,
London. 165
McCulloch, W. S. and Pitts, W. (1943). A logical calculus of ideas immanent in nervous
activity. Bulletin of Mathematical Biophysics, 5, 115–133. 13
Mead, C. and Ismail, M. (2012). Analog VLSI implementation of neural systems, volume 80. Springer Science & Business Media. 383
Mesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow, I., Lavoie, E.,
Muller, X., Desjardins, G., Warde-Farley, D., Vincent, P., Courville, A., and Bergstra,
J. (2011). Unsupervised and transfer learning challenge: a deep learning approach. In
JMLR W&CP: Proc. Unsupervised and Transfer Learning , volume 7. 190, 481
Mesnil, G., Rifai, S., Dauphin, Y., Bengio, Y., and Vincent, P. (2012). Surﬁng on the
manifold. Learning Workshop, Snowbird. 584
Miikkulainen, R. and Dyer, M. G. (1991). Natural language processing with modular
PDP networks and distributed lexicon. Cognitive Science, 15, 343–399. 394
Mikolov, T. (2012). Statistical Language Models based on Neural Networks. Ph.D. thesis,
Brno University of Technology. 172, 345
Mikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky, J. (2011a). Empirical evaluation and combination of advanced language modeling techniques. In Proc.
12th annual conference of the international speech communication association (INTERSPEECH 2011). 405
Mikolov, T., Deoras, A., Povey, D., Burget, L., and Cernocky, J. (2011b). Strategies for
training large scale neural network language models. In Proc. ASRU’2011 . 273, 405
Mikolov, T., Le, Q. V., and Sutskever, I. (2013). Exploiting similarities among languages
for machine translation. Technical report, arXiv:1309.4168. 484
Minka, T. (2005). Divergence measures and message passing. Microsoft Research Cambridge UK Tech Rep MSRTR2005173 , 72(TR-2005-173). 536
Minsky, M. L. and Papert, S. A. (1969). Perceptrons . MIT Press, Cambridge. 13
622

BIBLIOGRAPHY

Misra, J. and Saha, I. (2010). Artiﬁcial neural networks in hardware: A survey of two
decades of progress. Neurocomputing, 74(1), 239–255. 383
Mitchell, T. M. (1997). Machine Learning. McGraw-Hill, New York. 92
Mnih, A. and Kavukcuoglu, K. (2013). Learning word embeddings eﬃciently with noisecontrastive estimation. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K. Weinberger, editors, Advances in Neural Information Processing Systems 26 , pages
2265–2273. Curran Associates, Inc. 404, 534
Mnih, A. and Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. In ICML’2012 , pages 1751–1758. 404
Mnih, V. and Hinton, G. (2010). Learning to detect roads in high-resolution aerial images.
In Proceedings of the 11th European Conference on Computer Vision (ECCV). 95
Mobahi, H. and Fisher III, J. W. (2015). A theoretical analysis of optimization by gaussian
continuation. In AAAI’2015 . 272
Mohamed, A., Dahl, G., and Hinton, G. (2012). Acoustic modeling using deep belief
networks. IEEE Trans. on Audio, Speech and Language Processing , 20(1), 14–22. 392
Montúfar, G. (2014). Universal approximation depth and errors of narrow belief networks
with discrete units. Neural Computation, 26. 495
Montúfar, G. and Ay, N. (2011). Reﬁnements of universal approximation results for
deep belief networks and restricted Boltzmann machines. Neural Computation, 23(5),
1306–1319. 495
Montufar, G. and Morton, J. (2014). When does a mixture of products contain a product
of mixtures? SIAM Journal on Discrete Mathematics, 29(1), 321–347. 494
Montufar, G. F., Pascanu, R., Cho, K., and Bengio, Y. (2014). On the number of linear
regions of deep neural networks. In NIPS’2014 . 17, 493, 496, 497
Mor-Yosef, S., Samueloﬀ, A., Modan, B., Navot, D., and Schenker, J. G. (1990). Ranking
the risk factors for cesarean: logistic regression analysis of a nationwide study. Obstet
Gynecol , 75(6), 944–7. 2
Morin, F. and Bengio, Y. (2005). Hierarchical probabilistic neural network language
model. In AISTATS’2005 . 400, 402
Mozer, M. C. (1992). The induction of multiscale temporal structure. In NIPS’91 , pages
275–282, San Mateo, CA. Morgan Kaufmann. 337, 338, 347
Murphy, K. P. (2012). Machine Learning: a Probabilistic Perspective. MIT Press, Cambridge, MA, USA. 92, 138
Murray, B. U. I. and Larochelle, H. (2014). A deep and tractable density estimator. In
ICML’2014 . 172, 333, 334
623

BIBLIOGRAPHY

Nadas, A., Nahamoo, D., and Picheny, M. A. (1988). On a model-robust training method
for speech recognition. IEEE Transactions on Acoustics, Speech and Signal Processing,
ASSP-36(9), 1432–1436. 356
Nair, V. and Hinton, G. (2010a). Rectiﬁed linear units improve restricted Boltzmann
machines. In ICML’2010 . 164, 461
Nair, V. and Hinton, G. E. (2010b). Rectiﬁed linear units improve restricted Boltzmann
machines. In L. Bottou and M. Littman, editors, Proceedings of the Twenty-seventh
International Conference on Machine Learning (ICML-10), pages 807–814. ACM. 14
Narayanan, H. and Mitter, S. (2010). Sample complexity of testing the manifold hypothesis. In NIPS’2010 . 152, 501
Neal, R. M. (1992). Connectionist learning of belief networks. Artiﬁcial Intelligence, 56,
71–113. 582
Neal, R. M. (1996). Bayesian Learning for Neural Networks. Lecture Notes in Statistics.
Springer. 231
Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11(2),
125–139. 538, 539
Neal, R. M. (2005). Estimating ratios of normalizing constants using linked importance
sampling. 540
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. (2011). Reading digits in natural images with unsupervised feature learning. Deep Learning and
Unsupervised Feature Learning Workshop, NIPS. 19
Ney, H. and Kneser, R. (1993). Improved clustering techniques for class-based statistical
language modelling. In European Conference on Speech Communication and Technology (Eurospeech), pages 973–976, Berlin. 395
Niesler, T. R., Whittaker, E. W. D., and Woodland, P. C. (1998). Comparison of
part-of-speech and automatically derived category-based language models for speech
recognition. In International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 177–180. 397
Niranjan, M. and Fallside, F. (1990). Neural networks and radial basis functions in
classifying static speech patterns. Computer Speech and Language, 4, 275–289. 164
Nocedal, J. and Wright, S. (2006). Numerical Optimization. Springer. 85, 90
Olshausen, B. and Field, D. J. (2005). How close are we to understanding V1? Neural
Computation, 17, 1665–1699. 14
Olshausen, B. A. and Field, D. J. (1996). Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381, 607–609. 226, 305,
447, 500
624

BIBLIOGRAPHY

Olshausen, B. A. and Field, D. J. (1997). Sparse coding with an overcomplete basis set:
a strategy employed by V1? Vision Research, 37, 3311–3325. 389, 391, 460
Opper, M. and Archambeau, C. (2009). The variational gaussian approximation revisited.
Neural computation, 21(3), 786–792. 188
Parker, D. B. (1985). Learning-logic. Technical Report TR-47, Center for Comp. Research
in Economics and Management Sci., MIT. 195
Pascanu, R. (2014). On recurrent and deep networks.
Montréal. 244, 245

Ph.D. thesis, Université de

Pascanu, R. and Bengio, Y. (2012). On the diﬃculty of training recurrent neural networks.
Technical Report arXiv:1211.5063, Universite de Montreal. 172
Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the diﬃculty of training recurrent
neural networks. In ICML’2013 . 172, 249, 334, 338, 345, 346, 347
Pascanu, R., Montufar, G., and Bengio, Y. (2013b). On the number of inference regions
of deep feed forward networks with piece-wise linear activations. Technical report, U.
Montreal, arXiv:1312.6098. 189
Pascanu, R., Gülçehre, Ç., Cho, K., and Bengio, Y. (2014a). How to construct deep
recurrent neural networks. In ICLR’2014 . 17, 231, 325, 326, 340, 393, 496, 497
Pascanu, R., Montufar, G., and Bengio, Y. (2014b). On the number of inference regions
of deep feed forward networks with piece-wise linear activations. In ICLR’2014 . 494
Pearl, J. (1985). Bayesian networks: A model of self-activated memory for evidential
reasoning. In Proceedings of the 7th Conference of the Cognitive Science Society, University of California, Irvine, pages 329–334. 417
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann. 49
Petersen, K. B. and Pedersen, M. S. (2006). The matrix cookbook. Version 20051003. 28
Pinto, N., Cox, D. D., and DiCarlo, J. J. (2008). Why is real-world visual object recognition hard? PLoS Comput Biol, 4. 389, 581
Pinto, N., Stone, Z., Zickler, T., and Cox, D. (2011). Scaling up biologically-inspired
computer vision: A case study in unconstrained face recognition on facebook. In Computer Vision and Pattern Recognition Workshops (CVPRW), 2011 IEEE Computer
Society Conference on, pages 35–42. IEEE. 298
Pollack, J. B. (1990). Recursive distributed representations. Artiﬁcial Intelligence, 46(1),
77–105. 326
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.
USSR Computational Mathematics and Mathematical Physics , 4(5), 1–17. 253
625

BIBLIOGRAPHY

Poole, B., Sohl-Dickstein, J., and Ganguli, S. (2014). Analyzing noise in autoencoders
and deep networks. CoRR, abs/1406.1831. 211
Poon, H. and Domingos, P. (2011). Sum-product networks: A new deep architecture. In
UAI’2011 , Barcelona, Spain. 189, 496
Poundstone, W. (2005). Fortune’s Formula: The untold story of the scientiﬁc betting
system that beat the casinos and Wall Street . Macmillan. 57
Powell, M. (1987). Radial basis functions for multivariable interpolation: A review. 164
Presley, R. K. and Haggard, R. L. (1994). A ﬁxed point implementation of the backpropagation learning algorithm. In Southeastcon’94. Creative Technology Transfer-A Global
Aﬀair., Proceedings of the 1994 IEEE , pages 136–138. IEEE. 383
Price, R. (1958). A useful theorem for nonlinear devices having gaussian inputs. IEEE
Transactions on Information Theory, 4(2), 69–72. 187
Quiroga, R. Q., Reddy, L., Kreiman, G., Koch, C., and Fried, I. (2005). Invariant visual
representation by single neurons in the human brain. Nature , 435(7045), 1102–1107.
300
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in
speech recognition. Proceedings of the IEEE , 77(2), 257–286. 354, 391
Rabiner, L. R. and Juang, B. H. (1986). An introduction to hidden Markov models. IEEE
ASSP Magazine , pages 257–285. 308, 354
Raiko, T., Yao, L., Cho, K., and Bengio, Y. (2014). Iterative neural autoregressive
distribution estimator (NADE-k). Technical report, arXiv:1406.1485. 333
Raina, R., Madhavan, A., and Ng, A. Y. (2009). Large-scale deep unsupervised learning
using graphics processors. In L. Bottou and M. Littman, editors, ICML 2009 , pages
873–880, New York, NY, USA. ACM. 24, 378
Rall, L. B. (1981). Automatic Diﬀerentiation: Techniques and Applications. Lecture
Notes in Computer Science 120, Springer. 184
Ramsey, F. P. (1926). Truth and probability. In R. B. Braithwaite, editor, The Foundations of Mathematics and other Logical Essays, chapter 7, pages 156–198. McMaster
University Archive for the History of Economic Thought. 50
Ranzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007a). Eﬃcient learning of
sparse representations with an energy-based model. In NIPS’2006 . 12, 16, 460, 472,
473
Ranzato, M., Huang, F., Boureau, Y., and LeCun, Y. (2007b). Unsupervised learning
of invariant feature hierarchies with applications to object recognition. In Proceedings
of the Computer Vision and Pattern Recognition Conference (CVPR’07). IEEE Press.
299
626

BIBLIOGRAPHY

Ranzato, M., Boureau, Y., and LeCun, Y. (2008). Sparse feature learning for deep belief
networks. In NIPS’2007 . 460
Rao, C. (1945). Information and the accuracy attainable in the estimation of statistical
parameters. Bulletin of the Calcutta Mathematical Society, 37, 81–89. 126
Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to
parallelizing stochastic gradient descent. In NIPS’2011 . 380
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and
approximate inference in deep generative models. In ICML’2014 . 188, 434, 435
Richard Socher, Milind Ganjoo, C. D. M. and Ng, A. Y. (2013). Zero-shot learning
through cross-modal transfer. In 27th Annual Conference on Neural Information Processing Systems (NIPS 2013). 482
Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a). Contractive autoencoders: Explicit invariance during feature extraction. In ICML’2011 . 468, 470,
503
Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X.
(2011b). Higher order contractive auto-encoder. In European Conference on Machine
Learning and Principles and Practice of Knowledge Discovery in Databases (ECML
PKDD). 448
Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X.
(2011c). Higher order contractive auto-encoder. In ECML PKDD . 468
Rifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011d). The manifold
tangent classiﬁer. In NIPS’2011 . 516
Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012). A generative process for
sampling contractive auto-encoders. In ICML’2012 . 584
Ringach, D. and Shapley, R. (2004). Reverse correlation in neurophysiology. Cognitive
Science, 28(2), 147–166. 302
Roberts, S. and Everson, R. (2001). Independent component analysis: principles and
practice. Cambridge University Press. 455
Robinson, A. J. and Fallside, F. (1991). A recurrent error propagation network speech
recognition system. Computer Speech and Language, 5(3), 259–274. 24, 391
Rockafellar, R. T. (1997). Convex analysis. princeton landmarks in mathematics. 88
Romero, A., Ballas, N., Ebrahimi Kahou, S., Chassang, A., Gatta, C., and Bengio, Y.
(2015). Fitnets: Hints for thin deep nets. In ICLR’2015, arXiv:1412.6550 . 271
Rosen, J. B. (1960). The gradient projection method for nonlinear programming. part
i. linear constraints. Journal of the Society for Industrial and Applied Mathematics,
8(1), pp. 181–217. 88
627

BIBLIOGRAPHY

Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage
and organization in the brain. Psychological Review, 65, 386–408. 12, 13, 24
Rosenblatt, F. (1962). Principles of Neurodynamics. Spartan, New York. 13, 24
Roweis, S. and Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear
embedding. Science, 290(5500). 152, 153, 504
Rumelhart, D., Hinton, G., and Williams, R. (1986a). Learning representations by backpropagating errors. Nature, 323, 533–536. 12, 16, 22, 195, 394
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986b). Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland, editors,
Parallel Distributed Processing, volume 1, chapter 8, pages 318–362. MIT Press, Cambridge. 19, 24, 195
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986c). Learning representations
by back-propagating errors. Nature, 323, 533–536. 158, 308
Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (1986d). Parallel
Distributed Processing: Explorations in the Microstructure of Cognition. MIT Press,
Cambridge. 15, 195
Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (1986e). Parallel
Distributed Processing: Explorations in the Microstructure of Cognition, volume 1.
MIT Press, Cambridge. 158
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,
A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2014a). ImageNet Large
Scale Visual Recognition Challenge. 19
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,
A., Khosla, A., Bernstein, M., et al. (2014b). Imagenet large scale visual recognition
challenge. arXiv preprint arXiv:1409.0575 . 25
Rust, N., Schwartz, O., Movshon, J. A., and Simoncelli, E. (2005). Spatiotemporal
elements of macaque V1 receptive ﬁelds. Neuron, 46(6), 945–956. 301
Sainath, T., rahman Mohamed, A., Kingsbury, B., and Ramabhadran, B. (2013). Deep
convolutional neural networks for LVCSR. In ICASSP 2013 . 392
Salakhutdinov, R. and Hinton, G. (2009a). Deep Boltzmann machines. In Proceedings of
the International Conference on Artiﬁcial Intelligence and Statistics, volume 5, pages
448–455. 21, 24, 473, 566, 569, 574, 576
Salakhutdinov, R. and Hinton, G. (2009b). Deep Boltzmann machines. In Proceedings
of the Twelfth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2009), volume 8. 573, 577, 589

628

BIBLIOGRAPHY

Salakhutdinov, R. and Hinton, G. E. (2008). Using deep belief nets to learn covariance
kernels for Gaussian processes. In NIPS’07 , pages 1249–1256, Cambridge, MA. MIT
Press. 487
Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief
networks. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008 ,
volume 25, pages 872–879. ACM. 539
Saul, L. K., Jaakkola, T., and Jordan, M. I. (1996). Mean ﬁeld theory for sigmoid belief
networks. Journal of Artiﬁcial Intelligence Research, 4, 61–76. 24
Savich, A. W., Moussa, M., and Areibi, S. (2007). The impact of arithmetic representation
on implementing mlp-bp on fpgas: A study. Neural Networks, IEEE Transactions on,
18(1), 240–252. 383
Saxe, A. M., Koh, P. W., Chen, Z., Bhand, M., Suresh, B., and Ng, A. (2011). On
random weights and unsupervised feature learning. In Proc. ICML’2011 . ACM. 298
Saxe, A. M., McClelland, J. L., and Ganguli, S. (2013). Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. In ICLR. 266
Schmidhuber, J. (1992). Learning complex, extended sequences using the principle of
history compression. Neural Computation, 4(2), 234–242. 326
Schmidhuber, J. (1996). Sequential neural text compression. IEEE Transactions on
Neural Networks, 7(1), 142–146. 394
Schölkopf, B. and Smola, A. (2002). Learning with kernels . MIT Press. 148
Schölkopf, B., Smola, A., and Müller, K.-R. (1998). Nonlinear component analysis as a
kernel eigenvalue problem. Neural Computation, 10, 1299–1319. 152, 504
Schölkopf, B., Burges, C. J. C., and Smola, A. J. (1999). Advances in Kernel Methods —
Support Vector Learning. MIT Press, Cambridge, MA. 16, 164, 191
Schulz, H. and Behnke, S. (2012).
ICANN’2012 , pages 620–628. 470

Learning two-layer contractive encodings.

In

Schuster, M. and Paliwal, K. (1997). Bidirectional recurrent neural networks. IEEE
Transactions on Signal Processing, 45(11), 2673–2681. 323
Schwenk, H. (2007). Continuous space language models. Computer speech and language,
21, 492–518. 395, 399
Schwenk, H. (2010). Continuous space language models for statistical machine translation.
The Prague Bulletin of Mathematical Linguistics , 93, 137–146. 395, 405
Schwenk, H. (2014). Cleaned subset of wmt ’14 dataset. 19
Schwenk, H. and Bengio, Y. (1998). Training methods for adaptive boosting of neural
networks. In NIPS’97 , pages 647–653. MIT Press. 227
629

BIBLIOGRAPHY

Schwenk, H. and Gauvain, J.-L. (2002). Connectionist language modeling for large vocabulary continuous speech recognition. In International Conference on Acoustics, Speech
and Signal Processing (ICASSP), volume 1, pages 765–768. 395
Schwenk, H. and Gauvain, J.-L. (2005). Building continuous space language models for
transcribing european languages. In Interspeech, pages 737–740. 395
Schwenk, H., Costa-jussà, M. R., and Fonollosa, J. A. R. (2006). Continuous space language models for the iwslt 2006 task. In International Workshop on Spoken Language
Translation, pages 166–173. 395, 405
Seide, F., Li, G., and Yu, D. (2011). Conversational speech transcription using contextdependent deep neural networks. In Interspeech 2011 , pages 437–440. 22
Sermanet, P., Chintala, S., and LeCun, Y. (2012). Convolutional neural networks applied
to house numbers digit classiﬁcation. CoRR, abs/1204.3968. 389
Sermanet, P., Kavukcuoglu, K., Chintala, S., and LeCun, Y. (2013). Pedestrian detection
with unsupervised multi-stage feature learning. In Proc. International Conference on
Computer Vision and Pattern Recognition (CVPR’13). IEEE. 22, 190
Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical
Journal , 27(3), 379—-423. 57
Shannon, C. E. (1949). Communication in the presence of noise. Proceedings of the
Institute of Radio Engineers, 37(1), 10–21. 57
Shilov, G. (1977). Linear Algebra. Dover Books on Mathematics Series. Dover Publications. 28
Siegelmann, H. (1995). Computation beyond the Turing limit. Science, 268(5210), 545–
548. 311
Siegelmann, H. and Sontag, E. (1991). Turing computability with neural nets. Applied
Mathematics Letters, 4(6), 77–80. 311
Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets.
Journal of Computer and Systems Sciences, 50(1), 132–150. 249
Simard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices for convolutional
neural networks. In ICDAR’2003 . 306
Simard, P. and Graf, H. P. (1994). Backpropagation without multiplication. In Advances
in Neural Information Processing Systems , pages 232–239. 383
Simard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent prop - A formalism
for specifying selected invariances in an adaptive network. In NIPS’1991 . 210, 515,
516
Simard, P. Y., LeCun, Y., and Denker, J. (1993). Eﬃcient pattern recognition using a
new transformation distance. In NIPS’92 . 514
630

BIBLIOGRAPHY

Simard, P. Y., LeCun, Y. A., Denker, J. S., and Victorri, B. (1998). Transformation
invariance in pattern recognition — tangent distance and tangent propagation. Lecture
Notes in Computer Science, 1524. 514
Sjöberg, J. and Ljung, L. (1995). Overtraining, regularization and searching for a minimum, with application to neural networks. International Journal of Control, 62(6),
1391–1407. 221
Smolensky, P. (1986). Information processing in dynamical systems: Foundations of
harmony theory. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed
Processing, volume 1, chapter 6, pages 194–281. MIT Press, Cambridge. 424, 437
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of
machine learning algorithms. In NIPS’2012 . 372
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning, C. D. (2011a).
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In
NIPS’2011 . 326, 328
Socher, R., Manning, C., and Ng, A. Y. (2011b). Parsing natural scenes and natural
language with recursive neural networks. In Proceedings of the Twenty-Eighth International Conference on Machine Learning (ICML’2011). 326
Socher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning, C. D. (2011c).
Semi-supervised recursive autoencoders for predicting sentiment distributions. In
EMNLP’2011 . 326
Socher, R., Perelygin, A., Wu, J. Y., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C.
(2013). Recursive deep models for semantic compositionality over a sentiment treebank.
In EMNLP’2013 . 326, 328
Solla, S. A., Levin, E., and Fleisher, M. (1988). Accelerated learning in layered neural
networks. Complex Systems, 2, 625–639. 168
Sontag, E. D. and Sussman, H. J. (1989). Backpropagation can give rise to spurious local
minima even for networks without hidden layers. Complex Systems, 3, 91–106. 242
Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37, 332–341.
184
Spitkovsky, V. I., Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog:
how ”less is more” in unsupervised dependency parsing. In HLT’10 . 273
Srivastava, N. and Salakhutdinov, R. (2012). Multimodal learning with deep Boltzmann
machines. In NIPS’2012 . 485
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).
Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15, 1929–1958. 229, 231, 576
631

BIBLIOGRAPHY

Steinkrau, D., Simard, P. Y., and Buck, I. (2005). Using gpus for machine learning algorithms. 2013 12th International Conference on Document Analysis and Recognition,
0, 1115–1119. 378
Stewart, L., He, X., and Zemel, R. S. (2007). Learning ﬂexible features for conditional
random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
30(8), 1415–1426. 349
Supancic, J. and Ramanan, D. (2013). Self-paced learning for long-term tracking. In
CVPR’2013 . 273
Sussillo, D. (2014). Random walks: Training very deep nonlinear feed-forward networks
with smart initialization. CoRR, abs/1412.6558. 266, 268
Sutskever, I. (2012). Training Recurrent Neural Networks. Ph.D. thesis, Department of
computer science, University of Toronto. 335, 336, 344
Sutskever, I. and Tieleman, T. (2010). On the Convergence Properties of Contrastive
Divergence. In Y. W. Teh and M. Titterington, editors, Proc. of the International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 9, pages 789–
795. 524
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of
initialization and momentum in deep learning. In ICML. 255, 335, 336, 344
Sutskever, I., Vinyals, O., and Le, Q. V. (2014a). Sequence to sequence learning with
neural networks. Technical report, arXiv:1409.3215. 23, 95, 340, 341
Sutskever, I., Vinyals, O., and Le, Q. V. (2014b). Sequence to sequence learning with
neural networks. In NIPS’2014 . 324, 407
Swersky, K. (2010). Inductive Principles for Learning Restricted Boltzmann Machines.
Master’s thesis, University of British Columbia. 466
Swersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas, N. (2011). On
autoencoders and score matching for energy based models. In ICML’2011 . ACM. 532
Swersky, K., Snoek, J., and Adams, R. P. (2014). Freeze-thaw bayesian optimization.
arXiv preprint arXiv:1406.3896 . 373
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014a). Going deeper with convolutions. Technical
report, arXiv:1409.4842. 21, 22, 24, 190, 235, 270, 288
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and
Fergus, R. (2014b). Intriguing properties of neural networks. ICLR, abs/1312.6199.
234
Taigman, Y., Yang, M., Ranzato, M., and Wolf, L. (2014). Deepface: Closing the gap to
human-level performance in face veriﬁcation. In CVPR’2014 . 93
632

BIBLIOGRAPHY

Tang, Y. and Eliasmith, C. (2010). Deep networks for robust visual recognition. In
Proceedings of the 27th International Conference on Machine Learning, June 21-24,
2010, Haifa, Israel. 211
Taylor, G. and Hinton, G. (2009). Factored conditional restricted Boltzmann machines
for modeling motion style. In L. Bottou and M. Littman, editors, ICML 2009 , pages
1025–1032. ACM. 408
Taylor, G., Hinton, G. E., and Roweis, S. (2007). Modeling human motion using binary
latent variables. In NIPS’06 , pages 1345–1352. MIT Press, Cambridge, MA. 408, 409
Tenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global geometric framework
for nonlinear dimensionality reduction. Science, 290(5500), 2319–2323. 152, 153, 476,
477, 504
Thrun, S. (1995). Learning to play the game of chess. In NIPS’1994 . 516
Tibshirani, R. J. (1995). Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society B , 58, 267–288. 205
Tieleman, T. (2008). Training restricted Boltzmann machines using approximations to
the likelihood gradient. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors,
ICML 2008 , pages 1064–1071. ACM. 526, 563
Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal components analysis.
Journal of the Royal Statistical Society B , 61(3), 611–622. 454
Tom Schaul, Ioannis Antonoglou, D. S. (2014). Unit tests for stochastic optimization. In
International Conference on Learning Representations . 260
Torabi, A., Pal, C., Larochelle, H., and Courville, A. (2015). Using descriptive video
services to create a large data source for video annotation research. arXiv preprint
arXiv: 1503.01070 . 142
Tu, K. and Honavar, V. (2011). On the utility of curricula in unsupervised learning of
probabilistic grammars. In IJCAI’2011 . 273
Uria, B., Murray, I., and Larochelle, H. (2013). Rnade: The real-valued neural autoregressive density-estimator. In NIPS’2013 . 331, 333
van der Maaten, L. and Hinton, G. E. (2008a). Visualizing data using t-SNE. J. Machine
Learning Res., 9. 395, 476, 504, 508
van der Maaten, L. and Hinton, G. E. (2008b). Visualizing data using t-SNE. Journal of
Machine Learning Research, 9, 2579–2605. 477
Vanhoucke, V., Senior, A., and Mao, M. Z. (2011). Improving the speed of neural networks
on cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop.
377, 384
633

BIBLIOGRAPHY

Vapnik, V. N. (1982). Estimation of Dependences Based on Empirical Data. SpringerVerlag, Berlin. 106
Vapnik, V. N. (1995). The Nature of Statistical Learning Theory . Springer, New York.
106
Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative
frequencies of events to their probabilities. Theory of Probability and Its Applications,
16, 264–280. 106
Vincent, P. (2011a). A connection between score matching and denoising autoencoders.
Neural Computation, 23(7). 465, 466, 468, 584
Vincent, P. (2011b). A connection between score matching and denoising autoencoders.
Neural Computation, 23(7), 1661–1674. 532, 585
Vincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In NIPS’2002 . MIT Press.
506
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and
composing robust features with denoising autoencoders. In ICML 2008 . 211, 463
Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol, P.-A. (2010). Stacked
denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. J. Machine Learning Res., 11. 463
Vinyals, O., Kaiser, L., Koo, T., Petrov, S., Sutskever, I., and Hinton, G. (2014a). Grammar as a foreign language. Technical report, arXiv:1412.7449. 340
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014b). Show and tell: a neural
image caption generator. arXiv 1411.4555. 340
Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and tell: a neural image
caption generator. In CVPR’2015 . arXiv:1411.4555. 95
Viola, P. and Jones, M. (2001). Robust real-time object detection. In International
Journal of Computer Vision. 382
Von Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by retinal
projections directed to the auditory pathway. Nature , 404(6780), 871–876. 14
Wager, S., Wang, S., and Liang, P. (2013). Dropout training as adaptive regularization.
In Advances in Neural Information Processing Systems 26 , pages 351–359. 232
Waibel, A., Hanazawa, T., Hinton, G. E., Shikano, K., and Lang, K. (1989). Phoneme
recognition using time-delay neural networks. IEEE Transactions on Acoustics, Speech,
and Signal Processing, 37, 328–339. 309, 385, 391
Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. (2013). Regularization of
neural networks using dropconnect. In ICML’2013 . 232
634

BIBLIOGRAPHY

Wang, S. and Manning, C. (2013). Fast dropout training. In ICML’2013 . 232
Warde-Farley, D., Goodfellow, I. J., Lamblin, P., Desjardins, G., Bastien, F., and Bengio,
Y. (2011). pylearn2. http://deeplearning.net/software/pylearn2. 379
Warde-Farley, D., Goodfellow, I. J., Courville, A., and Bengio, Y. (2014). An empirical
analysis of dropout in piecewise linear networks. In ICLR’2014 . 232
Wawrzynek, J., Asanovic, K., Kingsbury, B., Johnson, D., Beck, J., and Morgan, N.
(1996). Spert-ii: A vector microprocessor system. Computer, 29(3), 79–86. 383
Weinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of image manifolds by
semideﬁnite programming. In CVPR’2004 , pages 988–995. 152, 504
Werbos, P. J. (1981). Applications of advances in nonlinear sensitivity analysis. In
Proceedings of the 10th IFIP Conference, 31.8 - 4.9, NYC , pages 762–770. 195
Weston, J., Ratle, F., and Collobert, R. (2008). Deep learning via semi-supervised embedding. In W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008 , pages
1168–1175, New York, NY, USA. ACM. 486
Weston, J., Bengio, S., and Usunier, N. (2010). Large scale image annotation: learning
to rank with joint word-image embeddings. Machine Learning, 81(1), 21–35. 328
Weston, J., Chopra, S., and Bordes, A. (2014). Memory networks. arXiv preprint
arXiv:1410.3916 . 343
Widrow, B. and Hoﬀ, M. E. (1960). Adaptive switching circuits. In 1960 IRE WESCON
Convention Record, volume 4, pages 96–104. IRE, New York. 13, 19, 21, 24
Wikipedia (2015). List of animals by number of neurons — wikipedia, the free encyclopedia. [Online; accessed 4-March-2015]. 21, 24
Williams, C. K. I. and Rasmussen, C. E. (1996). Gaussian processes for regression. In
NIPS’95 , pages 514–520. MIT Press, Cambridge, MA. 191
Williams, R. J. (1992). Simple statistical gradient-following algorithms connectionist
reinforcement learning. Machine Learning, 8, 229–256. 188, 343
Wolpert, D. and MacReady, W. (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1, 67–82. 249
Wolpert, D. H. (1996). The lack of a priori distinction between learning algorithms.
Neural Computation, 8(7), 1341–1390. 110
Wu, R., Yan, S., Shan, Y., Dang, Q., and Sun, G. (2015). Deep image: Scaling up image
recognition. arXiv:1501.02876. 22, 380
Wu, Z. (1997). Global continuation for distance geometry problems. SIAM Journal of
Optimization, 7, 814–836. 271
635

BIBLIOGRAPHY

Xiong, H. Y., Barash, Y., and Frey, B. J. (2011). Bayesian prediction of tissue-regulated
splicing using RNA sequence and cellular context. Bioinformatics, 27(18), 2554–2562.
231
Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S.,
and Bengio, Y. (2015a). Show, attend and tell: Neural image caption generation with
visual attention. In ICML’2015 . 95
Xu, K., Ba, J. L., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R. S.,
and Bengio, Y. (2015b). Show, attend and tell: Neural image caption generation with
visual attention. arXiv:1502.03044. 340
Xu, L. and Jordan, M. I. (1996). On convergence properties of the EM algorithm for
gaussian mixtures. Neural Computation, 8, 129–151. 355
Younes, L. (1998). On the convergence of Markovian stochastic algorithms with rapidly
decreasing ergodicity rates. In Stochastics and Stochastics Models, pages 177–228. 524,
563
Zaremba, W. and Sutskever, I. (2014). Learning to execute. arXiv 1410.4615. 273
Zaremba, W. and Sutskever, I. (2015). Reinforcement learning neural turing machines.
arXiv preprint arXiv:1505.00521 . 343
Zaslavsky, T. (1975). Facing Up to Arrangements: Face-Count Formulas for Partitions
of Space by Hyperplanes. Number no. 154 in Memoirs of the American Mathematical
Society. American Mathematical Society. 494
Zeiler, M. D. and Fergus, R. (2014). Visualizing and understanding convolutional networks. In ECCV’14 . 6
Zhou, J. and Troyanskaya, O. G. (2014). Deep supervised and convolutional generative
stochastic network for protein secondary structure prediction. In ICML’2014 . 590, 591
Zöhrer, M. and Pernkopf, F. (2014). General stochastic networks for classiﬁcation. In
NIPS’2014 . 590

636

Index
Lp norm, 35
k-means, 296, 487
k-nearest neighbors,
boldindex135, 487
0-1 loss, 97
, 50, 298
Absolute value rectiﬁcation, 162
Active constraint, 90
Adagrad, 254
ADALINE, see Adaptive Linear Element
Adaptive Linear Element, 13, 21, 24
Adversarial example, 232
Aﬃne, 102
AIS, see annealed importance sampling
Almost everywhere, 67
Ancestral sampling, 437
ANN, see Artiﬁcial neural network
Annealed importance sampling, 533, 569
Approximate inference, 430
Artiﬁcial intelligence, 1
Artiﬁcial neural network, see Neural network
Asymptotically unbiased, 115
Audio, 293
Autoencoder, 4
Automatic diﬀerentiation, 182
Back-propagation, 172
Back-Propagation Through Time, 311
Bagging, 223
Bayes error,
boldindex108
Bayes’ rule, 66
Bayesian hyperparameter optimization, 369

Bayesian network, see directed graphical model
Bayesian statistics,
boldindex126
Beam Search, 359
Beam search, 347
Belief network, see directed graphical model
Bernoulli distribution, 59
Bias, 115
Boltzmann distribution, 420
Boltzmann machine, 420
Boltzmann Machines, 549
BPTT, see Back-Propagation Through Time
CAE, see contractive auto-encoder
Calculus of variations, 545
Categorical distribution, see multinoulli distribution60
CD, see contrastive divergence
Centering trick (DBM), 573
Central limit theorem, 62
Chain rule of probability, 54
Chess, 2
Chord, 426
Chordal graph, 426
Classical dynamical system, 306
Classical regularization, 197
Classiﬁcation, 93
Cliﬀs, 241
Clipping the gradient, 342
Clique potential, see factor (graphical model)
CNN, see convolutional neural network
Collider, see explaining away
Color images, 293
Computer vision, 381
Concept drift, 478

637

INDEX

Conditional computation, see dynamic structure
Conditional independence, vi, 55
Conditional probability, 53
Connectionism, 15, 373
Connectionist temporal classiﬁcation, 346
consistency, 122
Constrained optimization, 88
Context-speciﬁc independence, 423
Continuation methods, 269
Contractive auto-encoder, 465, 514
Contractive autoencoders, 445
Contrast, 382
Contrastive divergence, 520, 569, 572
Convolution, 272, 576
Convolutional network, 14
Convolutional neural network, 222,
boldindex272
Coordinate descent, 261, 572
Correlation, 56
Cost function, see objective function
Covariance, vi, 55
Covariance matrix, 56
Cross entropy,
boldindex59, 163
Cross-correlation, 274
Cross-validation, 113
CTC, see connectionist temporal classiﬁcation
Curriculum-learning, 271
curse of dimensionality, 143
Cyc, 2
D-separation, 422
DAE, see denoising auto-encoder
Data generating distribution,
boldindex103, 122
Data generating process, 103
Data parallelism, 376
Dataset, 97
Dataset augmentation, 382, 387
DBM, see deep Boltzmann machine
Decision tree,
boldindex136
Decision trees, 487
Decoder, 4

Deep belief network, 24, 538, 550, 559, 577
Deep Blue, 2
Deep Boltzmann machine, 21, 24, 538, 550,
562, 572, 577
Deep learning, 1, 5
Denoising auto-encoder, 459
Denoising autoencoders, 186
Denoising score matching, 528
Density estimation, 96
Derivative, vi, 79
Design matrix,
boldindex99
Detector layer, 280
Diagonal matrix, 36
Dirac delta function, 63
Directed graphical model, 69, 414
Directional derivative, 83
Distributed Representation, 486
Distributed representation, 15
domain adaptation, 476
Dot product, 31
Double exponential distribution, see Laplace
distribution
Doubly block circulant matrix, 276
Dream sleep, 519, 548
DropConnect, 229
Dropout, 186, 226, 364, 365, 572
Dynamic structure, 378
E-step, 541
Early stopping, 215–219
EBM, see energy-based model
Echo state network, 21, 24, 331
Eﬀective number of parameters, 201
Eﬃciency, 125
Eigendecomposition, 37
Eigenvalue, 38
Eigenvector, 38
ELBO, see evidence lower bound
Element-wise product, see Hadamard product, see Hadamard product
EM, see expectation maximization
Embedding, 502
Empirical distribution, 63
Empirical risk, 235
Empirical risk minimization, 235

638

INDEX

Encoder, 4
Energy function, 420
Energy-based model, 420, 562
Ensemble methods, 223
Equality constraint, 89
Equivariance, 279
Error function, see objective function
ESN, see echo state network
Euclidean norm, 35
Euler-Lagrange equation, 546
Evidence lower bound, 540–543, 561
Example, 97
Expectation, 55
Expectation maximization, 541
Expected value, see expectation
Explaining away, 424
Exponential distribution,
boldindex63

Gibbs sampling, 438
Global contrast normalization, 383
GPU, see Graphics processing unit
Gradient, 83
Gradient clipping, 342
Gradient descent, 83
Graph, v
Graph Transformer, 356
Graph transformer, 353
Graphical model, see structured probabilistic model
Graphics processing unit, 374
Greedy layer-wise unsupervised pre-training,
469
Grid search, 364
Hadamard product, v, 31
Hard tanh, 162
Harmonium, see Restricted Boltzmann machine433
Harmony theory, 421
Helmholtz free energy, see evidence lower
bound
Hessian matrix, vi, 84, 259
Hidden layer, 6
Hidden Markov model, 305
HMM, see hidden Markov model
Hyperbolic tangent, 161
Hyperparameter optimization, 364
Hyperparameters, 112, 362, 364
Hypothesis space, 104, 110

Factor (graphical model), 417
Factor analysis, 450
Factor graph, 428
Factors of variation, 4
Feature, 97
Feedforward deep network, 155
Finite diﬀerences, 372
Forward-Backward algorithm, 347
Fourier transform, 293, 295
Fovea, 299
Frequentist probability, 50
Frequentist statistics,
boldindex126
Functional derivatives, 545
Gabor function, 300
Gaussian distribution, see Normal distribution60
Gaussian kernel, 134
Gaussian mixture, 64
GCN, see Global contrast normalization
Generalization, 102
Generalized Lagrange function, see Generalized Lagrangian
Generalized Lagrangian, 89
Generative adversarial networks, 186
Gibbs distribution, 418

i.i.d assumptions, 232
i.i.d., 114
i.i.d. assumptions, 103
Identity matrix, 32
Immorality, 426
Independence, vi, 54
Independent and identically distributed, 114
Independent component analysis, 451
Inequality constraint, 89
Inference, 413, 430, 538, 540–544, 547
Initialization, 262
Integral, vi
Invariance, 280
Isomap, 473

639

INDEX

Jacobian matrix, vi, 68, 83
Joint probability, 52

Manifold learning, 150, 498
Manifold Tangent Classiﬁer, 513
MAP inference, 543
Karush-Kuhn-Tucker conditions, 90
Marginal probability, 53
Karush–Kuhn–Tucker, 88
Markov chain, 348, 437
Kernel (convolution), 273, 274
Markov network, see undirected model416
Kernel machine, 487
Markov property, 348
Kernel trick, 133
Markov random ﬁeld, see undirected model416
KKT, see Karush–Kuhn–Tucker
Matrix, iv, v, 29
KKT conditions, see Karush-Kuhn-Tucker Matrix inverse, 32
conditions
Matrix product, 30
KL divergence, see Kllback-Leibler diver- Max pooling, 280
gence59
Maximum likelihood,
Knowledge base, 2
boldindex122
Kullback-Leibler divergence, vi,
Maxout, 162
boldindex59
Mean ﬁeld, 569, 572
Mean squared error, 101
Lagrange multipliers, 88, 90, 546
Measure theory, 67
Lagrangian, see Gneralized Lagrangian89
Measure zero, 67
Laplace distribution,
Method of steepest descent, see gradient deboldindex63
scent
Latent variable, 446
Missing inputs, 93
LCN, see local contrast normalization
Mixing (Markov chain), 439
Leaky units, 334
Mixture distribution, 64
Line search, 83
Mixture of experts, 487
Linear combination, 33
MLP, see multilayer perception
Linear dependence, 34
MNIST, 18, 19, 572
Linear factor models, 449
Model averaging, 223
Linear regression,
Model capacity, 363
boldindex100, 102, 132
Model compression, 377
Liquid state machine, 331
Model parallelism, 376
Local conditional probability distribution,
Moore-Penrose Pseudoinverse, 41
414
Moore-Penrose pseudoinverse, 207
Local contrast normalization, 384
Moralized graph, 426
Logistic regression, 2, 133
MP-DBM, see multi-prediction DBM
Logistic sigmoid, 7, 65
MRF (Markov Random Field), see undiLong short-term memory, 335
rected model416
Loop, 426
MSE, see mean squared error101
Loss function, see objective function
Multi-modal learning, 482
LSTM, 22, see lng short-term memory335
Multi-prediction DBM, 571, 573
Multi-task learning, 230, 478
M-step, 541
Multilayer perception, 5
Machine learning, 2
Multilayer perceptron, 24,
Main diagonal, 30
boldindex155
Manifold, 150
Multinomial distribution, 60
Manifold hypothesis, 498
Multinoulli distribution, 60
Manifold hypothesis, 151
640

INDEX

Naive Bayes, 2, 71
Nat, 57
natural image, 410
Nearest neighbor regression,
boldindex107
Negative deﬁnite, 84
Negative phase, 517, 519
Neocognitron, 14, 21, 24
Nesterov momentum, 252
Netﬂix Grand Prize, 226
Neural network, 12
Neuroscience, 13
Noise-contrastive estimation, 529
Non-parametric model,
boldindex107
Norm, vii, 35
Normal distribution, 60, 62
Normal equations,
boldindex101, 102, 104, 201
Normalized initialization, 264
Numerical diﬀerentiation, 182, see ﬁnite differences
Object detection, 381
Object recognition, 381
Objective function, 79
Oﬀset, 157
One-shot learning, 480
Orthodox statistics, see frequentist statistics
Orthogonal matrix, 37
Orthogonality, 37
Overﬁtting, 363
Parallel distributed processing, 15
Parameter initialization, 262
Parameter sharing, 277
Parameter tying , Parameter sharing221
Parametric model,
boldindex107
Partial derivative, 82
Partition function, 419, 515, 569
PCA, see principal components analysis
PCD, see stochastic maximum likelihood
Perceptron, 13, 24
Perplexity, 125

Persistent contrastive divergence, see stochastic maximum likelihood
Point Estimator, 114
Pooling, 272, 576
Positive deﬁnite, 84
Positive phase, 517, 519
Pre-training, 469
Precision (of a normal distribution), 62
Predictive sparse decomposition, 296, 444,
456, 458
Preprocessing, 381
Primary visual cortex, 297
Principal components analysis, 43, 386, 450,
538
Principle components analysis, 138–140, 152
Prior probability distribution,
boldindex126
Probabilistic max pooling, 576
Probability density function, 52
Probability distribution, 51
Probability function estimation, 96
Probability mass function, 51
Product rule of probability, see chain rule
of probability
PSD, see predictive sparse decomposition
Pseudolikelihood, 524
Quadrature pair, 301
Radial basis function, 161
Random search, 366
Random variable, 51
Ratio matching, 528
RBF, 161
RBM, see restricted Boltzmann machine
Receptive ﬁeld, 277
Rectiﬁed linear unit, 161
Rectiﬁer, 161
Recurrent network, 24
Recurrent neural network, 308
Recursive neural networks, 306
Regression, 94
Regularization,
boldindex111, 111, 194, 364
Reinforcement learning, 186
ReLU, 161

641

INDEX

Representation learning, 3
Restricted Boltzmann machine, 433, 538,
550, 552, 572, 573, 575, 576
Ridge regression, see weight decay198
Risk, 235
Sample mean, 116
Scalar, iv, v, 28
Score matching, 527
Second derivative, 83
Second derivative test, 84
Self-information, 57
Semi-supervised learning, 483
Separable convolution, 295
Separation (probabilistic modeling), 422
Set, v
SGD, see stochastic gradient descent
Shannon entropy, vi, 57, 546
Sigmoid, vii, see logistic sigmoid, 161
Sigmoid belief network, 24
Simple cell, 298
Simulated annealing, 269
Singular value, see singular value decomposition
Singular value decomposition, 40, 139, 140
Singular vector, see singular value decomposition
SML, see stochastic maximum likelihood
Softmax, 161, 165
Softplus, vii, 65, 162
Spam detection, 2
Sparse coding, 444, 453, 538
Sparse initialization, 265
Sparse representations, 222, 455
Spearmint, 369
spectral radius, 332
Speech recognition, 388
Sphering, see Whitening, 384
Spike and slab restricted Boltzmann machine, 575
Square matrix, 34
ssRBM, see spike and slab restricted Boltzmann machine
Standard deviation, 55
Statistic, 114
Statistical learning theory, 103

Steepest descent, see gradient descent
Stochastic gradient descent, 13, 237,
boldindex249, 572
Stochastic maximum likelihood, 521, 569,
572
Stochastic pooling, 230
Structure learning, 430
Structured output, 94
Structured probabilistic model, 69, 409
Student-t, 445
Sum rule of probability, 53
Sum-product network, 493
Supervised learning,
boldindex98
Support vector machine, 133
Surrogate loss function, 235
SVD, see singular value decomposition
Symbolic diﬀerentiation, 182
Symmetric matrix, 37, 40
t-SNE, 473
Tangent Distance, 511
Tangent plane, 502
Tangent-Prop, 512
Tanh, 161
TDNN, see time-delay neural network
Teacher forcing, 310
Tensor, iv, v, 30
Test set, 103
Tikhonov regularization, see weight decay
Tiled convolution, 290
Time-delay neural network, 300, 306
Time-delay neural networks, 333
Toeplitz matrix, 276
Trace operator, 42
Training error, 102
Transcription, 94
Transfer learning, 476
Translation, 94
Transpose, v, 30
Triangle inequality, 35
Triangulated graph, see chordal graph
Unbiased, 115
Undirected graphical model, 69
Undirected model, 416

642

INDEX

Uniform distribution, 52
Unit norm, 37
Unit vector, 37
Universal approximation theorem, 186
Universal approximator, 492
Unnormalized probability distribution, 417
Unsupervised learning,
boldindex98, 137
Unsupervised pre-training, 469
V-structure, see explaining away
V1, 297
Variance, vi, 55
Variational autoencoder, 186
Variational derivatives, see functional derivatives
Variational free energy, see evidence lower
bound
Vector, iv, v, 29
Visible layer, 6
Viterbi algorithm, 347
Viterbi decoding, 350
Volumetric data, 293
Weight decay, 110,
boldindex198, 365
Weights, 13, 100
Whitening, 384, 386
ZCA, see zero-phase components analysis
Zero-data learning, 480
Zero-phase components analysis, 386
Zero-shot learning, 480

643


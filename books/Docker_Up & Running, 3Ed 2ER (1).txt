Docker: Up & Running
THIRD EDITION

Shipping Reliable Containers in Production
With Early Release ebooks, you get books in their earliest form—the
authors’ raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.

Sean P. Kane and Karl Matthias

Docker: Up and Running
by Sean P. Kane and Karl Matthias
Copyright © 2023 Sean P. Kane and Karl Matthias. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North,
Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales
promotional use. Online editions are also available for most titles
(https://oreilly.com). For more information, contact our
corporate/institutional sales department: 800-998-9938 or
corporate@oreilly.com.
Acquisitions Editor: John Devins
Development Editor: Michele Cronin
Production Editor: Elizabeth Faerm
Copyeditor: TO COME
Proofreader: TO COME
Indexer: TO COME
Interior Designer: David Futato
Cover Designer: Randy Comer
Illustrator: Kate Dullea
Technical Reviewers: Ksenia Burlachenko, Chelsea Frank, Mihai Todor,
and Rachid Zarouali

July 2023: Third Edition
Revision History for the Early Release
2022-10-25: First Release
2023-1-23: Second Release
See https://oreilly.com/catalog/errata.csp?isbn=9781492036739 for release
details.
The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Docker:
Up and Running, the cover image, and related trade dress are trademarks of
O’Reilly Media, Inc.
The views expressed in this work are those of the author, and do not represent
the publisher’s views. While the publisher and the author have used good
faith efforts to ensure that the information and instructions contained in this
work are accurate, the publisher and the author disclaim all responsibility for
errors or omissions, including without limitation responsibility for damages
resulting from the use of or reliance on this work. Use of the information and
instructions contained in this work is at your own risk. If any code samples or
other technology this work contains or describes is subject to open source
licenses or the intellectual property rights of others, it is your responsibility to
ensure that your use thereof complies with such licenses and/or rights.
978-1-098-13176-0
[LSI]

Chapter 1. Introduction
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 1st chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
Docker was first introduced to the world—with no pre-announcement and
little fanfare—by Solomon Hykes, founder and CEO of a company then
called dotCloud, in a five-minute lightning talk at the Python Developers
Conference in Santa Clara, California on March 15, 2013. At the time of this
announcement, only about 40 people outside of dotCloud had been given the
opportunity to play with Docker.
Within a few weeks of this announcement, there was a surprising amount of
press. The project was quickly open-sourced and made publicly available on
GitHub, where anyone could download and contribute to the project. Over
the next few months, more and more people in the industry started hearing
about Docker and how it was going to revolutionize the way software was
built, delivered, and run. And within a year, almost no one in the industry
was unaware of Docker, but many were still unsure what it was exactly, and
why people were so excited about it.
Docker is a tool that promises to easily encapsulate the process of creating a
distributable artifact for any application, deploying it at scale into any

environment, and streamlining the workflow and responsiveness of agile
software organizations.

The Promise of Docker
Initially, many people who were unfamiliar with Docker viewed it as some
sort of virtualization platform, but in reality, it was the first widely accessible
tool to build on top of a much newer technology called containerization.
Docker and Linux containers have had a significant impact on a wide range
of industry segments that include tools and technologies like Vagrant, KVM,
OpenStack, Mesos, Capistrano, Ansible, Chef, Puppet, and so on. There is
something very telling about the list of products that Docker competes with,
and maybe you’ve spotted it already. Looking over this list most engineers
would recognize that these tools span a lot of different use-cases, yet all of
these workflows have been forever changed by Docker. This is largely
because Docker has significantly altered everyone’s expectations of how a
CI/CD1 workflow should function. Instead of each step involving a timeconsuming process managed by specialists, most people expect a DevOps
pipeline to be fully automated and flow from one step to the next without any
human intervention. The technologies in that list are also generally acclaimed
for their ability to improve productivity, and that’s exactly what has given
Docker so much buzz. Docker sits right in the middle of some of the most
enabling technologies of the last decade and can bring significant
improvements to almost every step of the pipeline.
If you were to do a feature-by-feature comparison of Docker and the reigning
champion in any of these individual areas (e.g. configuration management),
Docker would very likely look like a middling competitor. It’s stronger in
some areas than others, but what Docker brings to the table is a feature set
that crosses a broad range of workflow challenges. By combining the ease of
application testing and deployment tools like Vagrant and Capistrano with
the ease of administrating virtualization systems, and then providing
interfaces that make workflow automation and orchestration easy to
implement, Docker provides a very enabling feature set.

Lots of new technologies come and go, and a dose of skepticism about the
newest rage is always healthy. When Docker was a new technology it would
have been easy to dismiss Docker as just another technology that solves a few
very specific problems for developers or operations teams. If you look at
Docker as a pseudo-virtualization or deployment technology alone, it might
not seem very compelling. But Docker is much more than it seems on the
surface.
It is hard and often expensive to get communication and processes right
between teams of people, even in smaller organizations. Yet we live in a
world where the communication of detailed information between teams is
increasingly required to be successful. Discovering and implementing a tool
that reduces the complexity of that communication while aiding in the
production of more robust software is a big win. And that’s exactly why
Docker merits a deeper look. It’s no panacea, and the way that you
implement Docker within your organization requires some critical thought,
but Docker and Linux containers provide a good approach to solving some
real-world organizational problems and helping to enable companies to ship
better software faster. Delivering a well-designed Linux container workflow
can lead to happier technical teams and real savings for the organization’s
bottom line.
So where are companies feeling the most pain? Shipping software at the
speed expected in today’s world is hard to do well, and as companies grow
from one or two developers to many teams of developers, the burden of
communication around shipping new releases becomes much heavier and
harder to manage. Developers have to understand a lot of complexity about
the environment they will be shipping software into, and production
operations teams need to increasingly understand the internals of the software
they ship. These are all generally good skills to work on because they lead to
a better understanding of the environment as a whole and therefore encourage
the designing of robust software, but these same skills are very difficult to
scale effectively as an organization’s growth accelerates.
The details of each company’s environment often require a lot of
communication that doesn’t directly build value for the teams involved. For

example, requiring developers to ask an operations team for release 1.2.1 of a
particular library slows them down and provides no direct business value to
the company. If developers could simply upgrade the version of the library
they use, write their code, test with the new version, and ship it, the delivery
time would be measurably shortened and fewer risks would be involved in
deploying the change. If operations engineers could upgrade software on the
host system without having to coordinate with multiple teams of application
developers, they could move faster. Docker helps to build a layer of isolation
in software that reduces the burden of communication in the world of
humans.
Beyond helping with communication issues, Docker is opinionated about
software architecture in a way that encourages more robustly crafted
applications. Its architectural philosophy centers on atomic or throwaway
containers. During deployment, the whole running environment of the old
application is thrown away with it. Nothing in the environment of the
application will live longer than the application itself, and that’s a simple idea
with big repercussions. It means that applications are not likely to
accidentally rely on artifacts left by a previous release. It means that
ephemeral debugging changes are less likely to live on in future releases that
picked them up from the local filesystem. And it means that applications are
highly portable between servers because all of the state has to be included
directly into the deployment artifact and be immutable, or sent to an external
dependency like a database, cache, or file server.
All of this leads to applications that are not only more scalable but more
reliable as well. Instances of the application container can come and go with
little impact on the uptime of the frontend site. These are proven architectural
choices that have been successful for non-Docker applications, but the design
choices enforced by Docker mean that containerized applications are
required to follow these best practices. And that’s a very good thing.

Benefits of the Docker Workflow
It’s hard to cohesively categorize all of the things Docker brings to the table.

When implemented well, it benefits organizations, teams, developers, and
operations engineers in a multitude of ways. It makes architectural decisions
simpler because all applications essentially look the same on the outside from
the hosting system’s perspective. It makes tooling easier to write and share
between applications. Nothing in this world comes with benefits and no
challenges, but Docker is surprisingly skewed toward the benefits. Here are
some more of the benefits you get with Docker and Linux containers:
Packaging software in a way that leverages the skills developers already
have
Many companies have had to create positions for release and build
engineers in order to manage all the knowledge and tooling required to
create software packages for their supported platforms. Linux tools like
rpm, mock, dpkg, and pbuilder can be complicated to use, and each
one must be learned independently. Docker wraps up all your
requirements together into one packaging format, known as the OCI2
standard.
Bundling application software and required OS filesystems together in a
single standardized image format
In the past, you typically needed to package not only your application but
also many of the dependencies that it relied on, including libraries and
daemons. However, you could never ensure that 100 percent of the
execution environment was identical. For natively compiled code, this
meant that your build system needed to have exactly the same versions of
shared libraries as your production environment. All of this made
packaging difficult to master, and hard for many companies to
accomplish reliably. Often someone running Scientific Linux would
resort to trying to deploy a community package tested on Red Hat
Enterprise Linux, hoping that the package was close enough to what they
needed. With Docker, you deploy your application along with every
single file required to run it. Docker’s layered images make this an
efficient process that ensures that your application is running in the
expected environment.

Using packaged artifacts to test and deliver the exact same artifact to all
systems in all environments
When developers commit changes to a version control system, a new
Docker image can be built, which can go through the whole testing
process and be deployed to production without having to be recompiled
or repackaged at any step in the process, unless that is specifically
desired.
Abstracting software applications from the hardware without sacrificing
resources
Traditional enterprise virtualization solutions like VMware are typically
used when people need to create an abstraction layer between the physical
hardware and the software applications that run on it, at the cost of
resources. The hypervisors that manage the VMs and each VM’s running
kernel use a percentage of the hardware system’s resources, which are
then no longer available to the hosted applications. A container, on the
other hand, is just another process that typically talks directly to the
underlying Linux kernel and therefore can utilize more resources, up until
the system or quota-based limits are reached.
When Docker was first released, Linux containers had been around for quite
a few years, and many of the other technologies that Docker is built on are
not entirely new. However, Docker’s unique mix of strong architectural and
workflow choices combines into a whole that is much more powerful than the
sum of its parts. Docker single-handedly made Linux containers, which have
been publicly available since 2008, approachable and useful for all computer
engineers. Docker fits containers relatively easily into the existing workflow
and processes of real companies. And the problems discussed earlier have
been felt by so many people that interest in the Docker project accelerated
much faster than anyone could have reasonably expected.
From a standing start in 2013, Docker has seen rapid iteration and now has a
huge feature set and is deployed in a vast number of production
infrastructures across the planet. It has become one of the foundation layers

for any modern distributed system and has inspired many others to expand on
the approach. A large number of companies now leverage Docker and Linux
containers as a solution to some of the serious complexity issues that they
face in their application delivery processes.

What Docker Isn’t
Docker can be used to solve a wide range of challenges that other categories
of tools have traditionally been enlisted to fix; however, Docker’s breadth of
features often means that it lacks depth in specific functionality. For example,
some organizations will find that they can completely remove their
configuration management tool when they migrate to Docker, but the real
power of Docker is that although it can replace some aspects of more
traditional tools, it is also usually compatible with them or even enhanced in
combination with them. In the following list, we explore some of the tool
categories that Docker doesn’t directly replace but that can often be used in
conjunction to achieve great results:
Enterprise virtualization platform (VMware, KVM, etc.)
A container is not a virtual machine in the traditional sense. Virtual
machines contain a complete operating system, running on top of a
hypervisor that is managed by the underlying host operating system.
Hypervisors create virtual hardware layers that make it possible to run
additional operating systems on top of a single physical computer system.
This makes it very easy to run many virtual machines with radically
different operating systems on a single host. With containers, both the
host and the containers share the same kernel. This means that containers
utilize fewer system resources but must be based on the same underlying
operating system (e.g. Linux).
Cloud platform (OpenStack, CloudStack, etc.)
Like enterprise virtualization, the container workflow shares a lot of
similarities—on the surface—with more traditional cloud platforms. Both
are traditionally leveraged to allow applications to be horizontally scaled

in response to changing demand. Docker, however, is not a cloud
platform. It only handles deploying, running, and managing containers on
preexisting Docker hosts. It doesn’t allow you to create new host systems
(instances), object stores, block storage, and the many other resources that
are often managed with a cloud platform. That being said, as you start to
expand your Docker tooling, you should start to experience more and
more of the benefits that one traditionally associates with the cloud.
Configuration management (Puppet, Chef, etc.)
Although Docker can significantly improve an organization’s ability to
manage applications and their dependencies, it does not directly replace
more traditional configuration management. Dockerfiles are used to
define how a container should look at build time, but they do not manage
the container’s ongoing state, and cannot be used to manage the Docker
host system. Docker can, however, significantly lessen the need for
complex configuration management code. As more and more servers
simply become Docker hosts, the configuration management codebase
that a company uses can become much smaller, and Docker can be used
to ship the more complex application requirements inside of standardized
OCI images.
Deployment framework (Capistrano, Fabric, etc.)
Docker eases many aspects of deployment by creating container images
that encapsulate all the dependencies of an application in a manner that
can be deployed, in all environments, without changes. However, Docker
can’t be used to automate a complex deployment process by itself. Other
tools are usually still needed to stitch together the larger workflow. That
being said, because Docker and other Linux container toolsets, like
Kubernetes, provide a well-defined interface for deployment, the method
required to deploy containers will be consistent on all hosts, and a single
deployment workflow should suffice for most, if not all, of your Dockerbased applications.
Development environment (Vagrant, etc.)

Vagrant is a virtual machine management tool for developers that is often
used to simulate server stacks that closely resemble the production
environment in which an application is destined to be deployed. Among
other things, Vagrant makes it easy to run Linux software on macOS and
Windows-based workstations. Virtual machines managed by tools like
Vagrant, assist developers in trying to avoid the common “It worked on
my machine” scenario that occurs when the software runs fine for the
developer but does not run properly elsewhere. However, as with many of
the previous examples, when you start to fully utilize Docker, there is a
lot less need to mimic a wide variety of production systems in
development, since most production systems will simply be Linux
container servers, which can easily be reproduced locally.
Workload management tool (Mesos, Kubernetes, Swarm, etc.)
An orchestration layer (including the built-in Swarm mode) must be used
to coordinate work across a pool of Linux container hosts, track the
current state of all the hosts and their resources, and keep an inventory of
running containers. These systems are designed to automate the regular
tasks that are needed to keep a production cluster healthy, while also
providing tools that help make the highly-dynamic nature of
containerized workloads easier for human beings to interact with.
Each of the above sections point out an important function that Docker and
Linux containers disrupted and improved. Linux containers provide a way to
run software in a controlled and isolated environment, while the easy-to-use
CLI3 tooling and container image standard that Docker introduced, made
working with containers much easier, and ensured that there was a repeatable
way to build software across the whole fleet.

Important Terminology
Here are a few terms that we will continue to use throughout the book and
whose meanings you should become familiar with:

Docker client
This is the docker command used to control most of the Docker
workflow and talk to remote Docker servers.
Docker server
This is the dockerd command that is used to start the Docker server
process that builds and launches containers via a client.
Docker or OCI images
Docker and OCI images consist of one or more filesystem layers and
some important metadata that represent all the files required to run a
containerized application. A single image can be copied to numerous
hosts. An image typically has a repository address, a name, and a tag. The
tag is generally used to identify a particular release of an image (e.g.
docker.io/superorbital/wordchain:v1.0.1). A Docker image is any image
that is compatible with the Docker toolset, while an OCI image is
specifically an image that meets the Open Container Initiative standard
and is guaranteed to work with any OCI-compliant tool.
Linux container
A container that has been instantiated from a Docker or OCI image. A
specific container can exist only once; however, you can easily create
multiple containers from the same image. The term Docker container is a
misnomer since Docker simply leverages the operating system’s
container functionality.
Atomic or immutable host
An atomic or immutable host is a small, finely tuned OS image, like
Fedora CoreOS, that supports container hosting and atomic OS upgrades.

Wrap-Up

Completely understanding Docker can be challenging when you are coming
at it without a strong frame of reference. In the next chapter, we will lay
down a broad overview of Docker: what it is, how it is intended to be used,
and what advantages it brings to the table when implemented with all this in
mind.

1 Continuous integration and continuous delivery
2 Open Container Image
3 command line interface

Chapter 2. The Docker
Landscape
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 2nd chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
Before you dive into configuring and installing Docker, a broad survey is in
order to explain what Docker is and what it brings to the table. It is a
powerful technology, but not a tremendously complicated one at its core. In
this chapter, we’ll cover the generalities of how Docker and Linux containers
works, what makes them powerful, and some of the reasons you might use
them. If you’re reading this, you probably have your reasons to use
containers, but it never hurts to augment your understanding before you jump
in.
Don’t worry—this chapter should not hold you up for too long. In the next
chapter, we’ll dive right into getting Docker installed and running on your
system.

Process Simplification

Because Docker is a piece of software, it may not be obvious that it can also
have a big positive impact on company and team processes if it is adopted
and implemented well. So, let’s dig in and see how Docker and Linux
containers can simplify both workflows and communication. This usually
starts with the deployment story. Traditionally, the cycle of getting an
application to production often looks something like the following (illustrated
in Figure 2-1):
1. Application developers request resources from operations engineers.
2. Resources are provisioned and handed over to developers.
3. Developers script and tool their deployment.
4. Operations engineers and developers tweak the deployment repeatedly.
5. Additional application dependencies are discovered by developers.
6. Operations engineers work to install the additional requirements.
7. Loop over steps 5 and 6 n more times.
8. The application is deployed.

Figure 2-1. A traditional deployment workflow (without Docker)

Our experience has shown that when you are following traditional processes,
deploying a brand new application into production can take the better part of
a week for a complex new system. That’s not very productive, and even
though DevOps practices work to alleviate many of the barriers, it often still
requires a lot of effort and communication between teams of people. This
process can be both technically challenging and expensive, but even worse, it
can limit the kinds of innovation that development teams will undertake in
the future. If deploying new software is hard, time-consuming, and dependent
on resources from another team, then developers may just build everything
into the existing application in order to avoid suffering the new deployment
penalty, or even worse they may simply avoid solving problems that require
new development efforts.
Push-to-deploy systems like Heroku have shown developers what the world
can look like if you are in control of your application and a majority of your
dependencies. Talking with developers about deployment will often turn up
discussions of how easy things are on Heroku or similar systems. If you’re an
operations engineer, you’ve probably heard complaints about how much
slower your internal systems are compared with deploying on “push-button”
solutions like Heroku, which are built on top of Linux container technology.
Heroku is a whole environment, not just a container engine. While Docker
doesn’t try to be everything that is included in Heroku, it provides a clean
separation of responsibilities and encapsulation of dependencies, which
results in a similar boost in productivity. Docker also allows even more finegrained control than Heroku by putting developers in control of everything,
down to the exact files and package versions that ship alongside their
application. Some of the tooling and orchestrators that have been built on top
of Docker (e.g., Kubernetes, Docker Swarm mode, and Mesos) aim to
replicate the simplicity of systems like Heroku. But even though these
platforms wrap more around Docker to provide a more capable and complex
environment, a simple platform that uses only Docker still provides all of the
core process benefits without the added complexity of a larger system.
As a company, Docker adopts an approach of “batteries included but
removable.” This means that they want their tools to come with everything

most people need to get the job done, while still being built from
interchangeable parts that can easily be swapped in and out to support custom
solutions.
By using an image repository as the hand-off point, Docker allows the
responsibility of building the application image to be separated from the
deployment and operation of the container. What this means in practice is
that development teams can build their application with all of its
dependencies, run it in development and test environments, and then just ship
the exact same bundle of application and dependencies to production.
Because those bundles all look the same from the outside, operations
engineers can then build or install standard tooling to deploy and run the
applications. The cycle described in Figure 2-1 then looks somewhat like this
(illustrated in Figure 2-2):
1. Developers build the Docker image and ship it to the registry.
2. Operations engineers provide configuration details to the container and
provision resources.
3. Developers trigger deployment.

Figure 2-2. A Docker deployment workflow

This is possible because Docker allows all of the dependency issues to be
discovered during the development and test cycles. By the time the
application is ready for its first deployment, that work has already been done.
And it usually doesn’t require as many handoffs between the development
and operations teams. In a well-refined pipeline, this can completely alleviate
the need for anyone other than the development team to be involved in the

creation and deployment of a new service. That’s a lot simpler and saves a lot
of time. Better yet, it leads to more robust software through testing of the
deployment environment before release.

Broad Support and Adoption
Docker is well supported, with the majority of the large public clouds
offering some direct support for it. For example, Docker and Linux
containers have been used in AWS via multiple products like Elastic
Container Service (ECS), Elastic Kubernetes Service (EKS), Fargate, and
Elastic Beanstalk. Linux containers can also be used on Google AppEngine,
Google Kubernetes Engine, Red Hat OpenShift, IBM Cloud, Microsoft
Azure, and many more. At DockerCon 2014, Google’s Eric Brewer
announced that Google would be supporting Docker as its primary internal
container format. Rather than just being good PR for these companies, what
this meant for the Docker community was that a lot of money began to back
the stability and success of the Docker platform.
Further building its influence, Docker’s image format for Linux containers
has become the lingua franca between cloud providers, offering the potential
for “write once, run anywhere” cloud applications. When Docker released
their libswarm development library at DockerCon 2014, an engineer from
Orchard demonstrated deploying a Linux container to a heterogeneous mix of
cloud providers at the same time. This kind of orchestration had not been
easy before because every cloud provider provided a different API or toolset
for managing instances, which were usually the smallest item you could
manage with an API. What was only a promise from Docker in 2014 has
since become fully mainstream as the largest companies continue to invest in
the platform, support, and tooling. With most providers offering some form
of Docker and Linux container orchestration as well as the container runtime
itself, Docker is well-supported for nearly any kind of workload in common
production environments. If all of your tooling is built around Docker and
Linux containers then your applications can be deployed in a cloud-agnostic
manner, allowing for new flexibility that was not previously possible.

That covers tooling, but what about OS vendor support and adoption? The
Docker client runs directly on most major operating systems, and the server
can run on Linux or Windows Server. The vast majority of the ecosystem is
built around Linux servers, but other platforms are increasingly being
supported. The beaten path is and will likely continue to revolve around
Linux servers running Linux containers.

NOTE
It is possible to run Windows containers natively (without a VM) on 64-bit versions of
Windows Server 2016+. However, 64-bit versions of Windows 10+ Professional still
require Hyper-V to provide the Windows Server kernel that is used for Windows
containers. We will dive into a little more detail about this in “Windows Containers”.
It is also worth noting here that Windows can run Linux containers outside a virtual
machine, by leveraging WSL 2 (Windows Subsystem for Linux, version 2).

To support the growing demand for Docker tooling in development
environments, Docker has released easy-to-use implementations for macOS
and Windows. These appear to run natively but are still utilizing a small
Linux virtual machine to provide the Docker server and Linux kernel. Docker
has traditionally been developed on the Ubuntu Linux distribution, but most
Linux distributions and other major operating systems are now supported
where possible. RedHat, for example, has gone all-in on containers and all of
their platforms have first-class support for Docker. With the near-ubiquity of
containers in the Linux realm, we now have distributions like Red Hat’s
Fedora CoreOS, which is built entirely for Linux container workloads.
In the first years after Docker’s release, a set of competitors and service
providers voiced concerns about Docker’s proprietary image format.
Containers on Linux did not have a standard image format, so Docker, Inc.,
created its own according to the needs of its business.
Service providers and commercial vendors were particularly reluctant to
build platforms that might be subject to the whims of a company with
overlapping interests to their own. Docker as a company faced some public

challenges in that period as a result. To gain some goodwill and support
wider adoption in the marketplace, Docker, Inc., decided to help sponsor the
Open Container Initiative (OCI) in June of 2015. The first full specification
from that effort was released in July 2017 and was based in large part on
version 2 of the Docker image format. It is now possible to apply for OCI
certification for both container images and container runtimes.
The primary high-level OCI-certified runtime is:
containerd, which is the default high-level runtime in modern versions
of Docker and Kubernetes.
These lower-level OCI-certified runtimes can be used by containerd to
manage and create containers:
runC, is often used as the default lower-level runtime by containerd.
crun, which is written in C, and designed to be fast and have a small
memory footprint.
Kata Containers from Intel, Hyper, and the OpenStack Foundation, is a
virtualized runtime, that can run a mix of containers and virtual
machines.
gVisor from Google, is a sandboxed runtime, implemented entirely in
user space.
and Nabla Containers, provide another sandboxed runtime designed to
significantly reduce the attack surface of Linux containers.
The space around deploying containers and orchestrating entire systems of
containers continues to expand, too. Many of these are open source and
available both on-premise and as cloud or SaaS1 offerings from various
providers, either in their clouds or yours. Given the amount of investment
continuing to pour into the Linux container space, it’s likely that Docker will
continue to have an important role in the modern internet.

Architecture
Docker is a powerful technology, and that often indicates something that
comes with a high level of complexity. And, under the hood, Docker is fairly
complex; however, its fundamental user-facing structure is indeed a simple
client/server model. Several pieces are sitting behind the Docker API,
including containerd and runc, but the basic system interaction is a
client talking over an API to a server. Underneath this simple exterior,
Docker heavily leverages kernel mechanisms such as iptables, virtual
bridging, cgroups2, namespaces3, Linux capabilities, secure computing mode,
various filesystem drivers, and more. We’ll talk about some of these in [Link
to Come]. For now, we’ll go over how the client and server work and give a
brief introduction to the network layer that sits underneath a Linux container
in Docker.

Client/Server Model
It’s easiest to think of Docker as consisting of two parts: the client and the
server/daemon (see Figure 2-3). Optionally there is a third component called
the registry, which stores Docker images and their metadata. The server does
the ongoing work of building, running, and managing your containers, and
you use the client to tell the server what to do. The Docker daemon can run
on any number of servers in the infrastructure, and a single client can address
any number of servers. Clients drive all of the communication, but Docker
servers can talk directly to image registries when told to do so by the client.
Clients are responsible for telling servers what to do, and servers focus on
hosting and managing containerized applications.

Figure 2-3. Docker client/server model

Docker is a little different in structure from some other client/server software.
It has a docker client and a dockerd server, but rather than being entirely
monolithic, the server then orchestrates a few other components behind the
scenes on behalf of the client, including containerd-shim-runc-v2,
which is used to interact with runc and containerd. Docker cleanly
hides any complexity behind the simple server API, though, so you can just
think of it as a straight-forward client and server for most purposes. Each
Docker host will normally have one Docker server running that can manage
any number of containers. You can then use the docker command-line tool
to talk to the server, either from the server itself or, if properly secured, from
a remote client. We’ll talk more about that shortly.

Network Ports and Unix Sockets
The docker command-line tool and dockerd daemon can talk to each

other over Unix sockets and network ports. Docker, Inc., has registered three
ports with IANA for use by the Docker daemon and client: TCP ports 2375
for unencrypted traffic, port 2376 for encrypted SSL connections, and port
2377 for Docker Swarm mode. Using a different port is easily configurable
for scenarios where you need to use different settings. The default setting for
the Docker installer is to only use a Unix socket for communication with the
local Docker daemon. This ensures that the system defaults to the most
secure installation possible. This is also easily configurable, but it is highly
recommended that network ports are not used with Docker, due to the lack of
user authentication and role-based-access controls within the Docker daemon.
The Unix socket can be located in different paths on different operating
systems, but in most cases, it can be found here: /var/run/docker.sock. If you
have strong preferences for a different location, you can usually specify this
at install time or simply change the server configuration afterward and restart
the daemon. If you don’t, then the defaults will probably work for you. As
with most software, following the defaults will save you a lot of trouble if
you don’t need to change them.

Robust Tooling
Among the many things that have led to Docker’s strong adoption is its
simple and powerful tooling. Since its initial release, its capabilities have
been expanding ever wider, thanks to efforts from the Docker community at
large. The tooling that Docker ships with supports building Docker images,
basic deployment to individual Docker daemons, a distributed mode called
Swarm mode, and all the functionality needed to manage a remote Docker
server. Beyond the included Swarm mode, community efforts have focused
on managing whole fleets (or clusters) of Docker servers and scheduling and
orchestrating container deployments.

NOTE
One point of confusion for people who search for Docker Swarm information on the
internet is that at least three different things share that name. There was an older, now
deprecated project that was a standalone application, called “Docker Swarm (or Classic

Swarm),” and there is a newer, built-in Swarm that we refer to here as “Swarm mode”
which leverages another underlying library called Swarmkit. You’ll need to carefully
look at the search results to identify the relevant ones. Over time this conflict will
hopefully become moot as the older product fades into history.

Docker has also launched its own orchestration toolset, including Compose,
Docker Desktop and Swarm mode, which creates a cohesive deployment
story for developers. Docker’s offerings in the production orchestration space
have been largely overshadowed by Google’s Kubernetes, although it should
be noted that Kubernetes relied heavily on Docker until v1.24 was released in
early 2022. But Docker’s orchestration tools remain useful, with Compose
being particularly handy for local development.
Because Docker provides both a command-line tool and a remote REST API,
it is easy to add further tooling in any language. The command-line tool lends
itself well to shell scripting and anything the client can do can also be done
programmatically via the REST API. The docker CLI4 is so well known that
many other Linux container CLI tools, like podman and nerdctl, mimic its
arguments for compatibility and easy adoption.

Docker Command-Line Tool
The command-line tool docker is the main interface that most people will
have with Docker. The Docker client is a Go program that compiles and runs
on all common architectures and operating systems. The command-line tool
is available as part of the main Docker distribution on various platforms and
also compiles directly from the Go source. Some of the things you can
typically do with the Docker command-line tool include, but are not limited
to:
Building a container image.
Pulling images from a registry to a Docker daemon or pushing them up
to a registry from the Docker daemon.
Starting a container on a Docker server either in the foreground or

background.
Retrieving the Docker logs from a remote server.
Interactively running a command inside a running container on a remote
server.
Monitoring statistics about your container.
Getting a process listing from your container.
You can probably see how these can be composed into a workflow for
building, deploying, and observing applications. But the Docker commandline tool is not the only way to interact with Docker, and it’s not necessarily
the most powerful.

Docker Engine API
Like many other pieces of modern software, the Docker daemon has an
application programming interface (API). This is in fact what the Docker
command-line tool uses to communicate with the daemon. But because the
API is documented and public, it’s quite common for external tooling to use
the API directly. This enables all manners of tooling, from mapping deployed
Linux containers to servers, to automated deployments, to distributed
schedulers. While it’s very likely that beginners will not initially want to talk
directly to the Docker API, it’s a great tool to have available. As your
organization embraces Docker over time, you will increasingly find the API
to be a good integration point for this tooling.
Extensive documentation for the API is on the Docker site. As the ecosystem
has matured, robust implementations of Docker API libraries have emerged
for all popular languages. Docker maintains SDKs for Python and Go, there
are additional libraries maintained by third parties that are worth considering.
For example, over the years we have used these Go and Ruby libraries, and
have found them to be both robust and rapidly updated as new versions of
Docker are released.
Most of the things you can do with the Docker command-line tooling are

supported relatively easily via the API. Two notable exceptions are the
endpoints that require streaming or terminal access: running remote shells or
executing the container in interactive mode. In these cases, it’s often easier to
use one of these solid client libraries or the command-line tool.

Container Networking
Even though Linux containers are largely made up of processes running on
the host system itself, they usually behave quite differently from other
processes at the network layer. Docker initially supported a single networking
model, but now supports a robust assortment of configurations that handle
most application requirements. Most people run their containers in the default
configuration, called bridge mode. So let’s take a look at how it works.
To understand bridge mode, it’s easiest to think of each of your Linux
containers as behaving like a host on a private network. The Docker server
acts as a virtual bridge and the containers are clients behind it. A bridge is
just a network device that repeats traffic from one side to another. So you can
think of it like a mini–virtual network with each container acting like a host
attached to that network.
The actual implementation, as shown in Figure 2-4, is that each container has
a virtual Ethernet interface connected to the Docker bridge and an IP address
allocated to the virtual interface. Docker lets you bind and expose individual
or groups of ports on the host to the container so that the outside world can
reach your container on those ports. The traffic is largely managed by the
vpnkit library .
Docker allocates the private subnet from an unused RFC 1918 private subnet
block. It detects which network blocks are unused on the host and allocates
one of those to the virtual network. That is bridged to the host’s local network
through an interface on the server called docker0. This means that, by
default, all of the containers are on a network together and can talk to each
other directly. But to get to the host or the outside world, they go over the
docker0 virtual bridge interface.

Figure 2-4. The network on a typical Docker server

There is a dizzying array of ways in which you can configure Docker’s
network layer, from allocating your own network blocks to configuring your
own custom bridge interface. People often run with the default mechanisms,
but there are times when something more complex or specific to your
application is required. You can find much more detail about Docker
networking in the documentation, and we will cover more details in the [Link
to Come].

NOTE
When developing your Docker workflow, you should get started with the default
networking approach. You might later find that you don’t want or need this default
virtual network. Networking is configurable per container, and you can switch off the
whole virtual network layer entirely for a container using the --net=host switch to
docker container run. When running in that mode, Linux containers use the
host’s own network devices and addresses and no virtual interfaces or bridges are
provisioned. Note that host networking has security implications you might need to
consider. Other network topologies are possible and discussed in [Link to Come].

Getting the Most from Docker
Like most tools, Docker has a number of great use cases, and others that
aren’t so good. You can, for example, open a glass jar with a hammer. But
that has its downsides. Understanding how to best use the tool, or even
simply determining if it’s the right tool, can get you on the correct path much
more quickly.
To begin with, Docker’s architecture is aimed squarely at applications that
are either stateless or where the state is externalized into data stores like
databases or caches. Those are the easiest to containerize. Docker enforces
some good development principles for this class of application, and we’ll talk
later about how that’s powerful. But this means doing things like putting a
database engine inside Docker is a bit like swimming against the current. It’s
not that you can’t do it, or even that you shouldn’t do it; it’s just that this is
not the most obvious use case for Docker, so if it’s the one you start with,
you may find yourself disappointed early on. Databases that run well in
Docker are often now deployed this way, but this is not the simple path.
Some good applications for beginning with Docker include web frontends,
backend APIs, and short-running tasks like maintenance scripts that might
normally be handled by cron.
If you focus first on building an understanding of running stateless or
externalized-state applications inside containers, you will have a foundation

on which to start considering other use cases. We strongly recommend
starting with stateless applications and learning from that experience before
tackling other use cases. Note that the community is continuously working on
how to better support stateful applications in Docker, and there are likely to
be many developments in this area.

Containers Are Not Virtual Machines
A good way to start shaping your understanding of how to leverage Docker is
to think of Linux containers not as virtual machines but as very lightweight
wrappers around a single Unix process. During actual implementation, that
process might spawn other processes, but on the other hand, one statically
compiled binary could be all that’s inside your container (see [Link to Come]
for more information). Containers are also ephemeral: they may come and go
much more readily than a traditional virtual machine.
Virtual machines are by design a stand-in for real hardware that you might
throw in a rack and leave there for a few years. Because a real server is what
they’re abstracting, virtual machines are often long-lived in nature. Even in
the cloud where companies often spin virtual machines up and down on
demand, they usually have a running lifespan of days or more. On the other
hand, a particular container might exist for months, or it may be created, run
a task for a minute, and then be destroyed. All of that is OK, but it’s a
fundamentally different approach than the one virtual machines are typically
used for.
To help drive this differentiation home, if you run Docker on a Mac or
Windows system you are leveraging a Linux virtual machine to run
dockerd, the Docker server. However, on Linux, dockerd can be run
natively and therefore there is no need for a virtual machine to be run
anywhere on the system (see Figure 2-5).

Figure 2-5. Typical Docker installations

Limited Isolation
Containers are isolated from each other, but that isolation is probably more
limited than you might expect. While you can put limits on their resources,
the default container configuration just has them all sharing CPU and
memory on the host system, much as you would expect from co-located Unix
processes. This means that unless you constrain them, containers can
compete for resources on your production machines. That might be fine for
your use case, but it impacts your design decisions. Limits on CPU and
memory use are encouraged through Docker, but in most cases, they are not
the default like they would be with a virtual machine.
It’s often the case that many containers share one or more common
filesystem layers. That’s one of the more powerful design decisions in

Docker, but it also means that if you update a shared image, you may also
need to rebuild and redeploy containers that are still utilizing the older image.
Containerized processes are just processes on the Docker server itself. They
are running on the same instance of the Linux kernel as the host operating
system. All container processes show up in the normal ps output on the
Docker server. That is utterly different from a hypervisor, where the depth of
process isolation usually includes running an entirely separate instance of the
operating system kernel for each virtual machine.
This light containment can lead to the tempting option of exposing more
resources from the host, such as shared filesystems to allow the storage of
state. But you should think hard before further exposing resources from the
host into the container unless they are used exclusively by the container.
We’ll talk about the security of containers later, but generally, you might
consider helping to enforce isolation further by applying SELinux5 or
AppArmor policies rather than compromising the existing barriers.

WARNING
By default, many containers use UID 0 to launch processes. Because the container is
contained, this seems safe, but in reality, it isn’t very safe. Because everything is running
on the same kernel, many types of security vulnerabilities or simple misconfiguration
can give the container’s root user unauthorized access to the host’s system resources,
files, and processes. Refer to [Link to Come] for a discussion of how to mitigate this.

Containers Are Lightweight
We’ll get more into the details of how this works later, but creating a new
container can take up very little disk space. A quick test reveals that a newly
created container from an existing image takes a whopping 12 kilobytes of
disk space. That’s pretty lightweight. On the other hand, a new virtual
machine created from a golden image might require hundreds or thousands of
megabytes, since at a minimum it requires a full operating install to exist on
that disk. The new container, on the other hand, is so small because it is just a

reference to a layered filesystem image and some metadata about the
configuration. By default, there is no copy of the data allocated to the
container. Containers are just processes on the existing system that may only
need to read information from the disk, so there may not be a need to copy
any data for the exclusive use of the container, until a time when it needs to
write data that is unique to that container instance.
The lightness of containers means that you can use them for situations where
creating another virtual machine would be too heavyweight or where you
need something to be truly ephemeral. You probably wouldn’t, for instance,
spin up an entire virtual machine to run a curl command to a website from a
remote location, but you might spin up a new container for this purpose.

Toward an Immutable Infrastructure
By deploying most of your applications within containers, you can start
simplifying your configuration management story by moving toward an
immutable infrastructure, where components are replaced entirely rather than
being changed in place. The idea of an immutable infrastructure has gained
popularity in response to how difficult it is, in reality, to maintain a truly
idempotent configuration management codebase. As your configuration
management codebase grows, it can become as unwieldy and unmaintainable
as large, monolithic legacy applications.
With Docker, it is possible to deploy a very lightweight Docker server that
needs almost no configuration management, or in many cases, none at all.
You handle all of your application management simply by deploying and
redeploying containers to the server. When the server needs an important
update to something like the Docker daemon or the Linux kernel, you can
simply bring up a new server with the changes, deploy your containers there,
and then decommission or reinstall the old server.
Container-based Linux distributions like Red Hat’s Fedora CoreOS are
designed around this principle. But rather than requiring you to
decommission the instance, Fedora CoreOS can entirely update itself and
switch to the updated OS. Your configuration and workload largely remain in

your containers and you don’t have to configure the OS very much at all.
Because of this clean separation between deployment and configuration of
your servers, many container-based production systems are now using tools
such as HashiCorp’s Packer to build cloud virtual server images and then
leveraging Docker to nearly or entirely avoid configuration management
systems.

Stateless Applications
A good example of the kind of application that containerizes well is a web
application that keeps its state in a database. Stateless applications are
normally designed to immediately answer a single self-contained request and
have no need to track information between requests from one or more clients.
You might also run something like ephemeral memcached instances in
containers. If you think about your web application, though, it probably has
some local state that you rely on, like configuration files. That might not
seem like a lot of state, but if you bake that configuration into your images, it
means that you’ve limited the reusability of your image and made it more
challenging to deploy into different environments, without maintaining
multiple images for different deployment targets.
In many cases, the process of containerizing your application means that you
move configuration state into environment variables that can be passed to
your application at runtime. Rather than baking the configuration into the
container, you apply the configuration to the container when it is deployed.
This allows you to easily do things like use the same container to run in either
production or staging environments. In most companies, those environments
would require many different configuration settings like the connection URLs
for various external services that the application utilizes.
With containers, you might also find that you are always decreasing the size
of your containerized application as you optimize it down to the bare
essentials required to run. We have found that thinking of anything that you
need to run in a distributed way as a container can lead to some interesting
design decisions. If, for example, you have a service that collects some data,

processes it, and returns the result, you might configure containers on many
servers to run the job and then aggregate the response on another container.

Externalizing State
If Docker works best for stateless applications, how do you best store state
when you need to? Configuration is typically passed by environment
variables, for example. Docker supports environment variables natively, and
they are stored in the metadata that makes up a container configuration. This
means that restarting the container will ensure that the same configuration is
passed to your application each time. It also makes the configuration of the
container easily observable while it’s running, which can make debugging a
lot easier, although there are some security concerns around exposing secrets
in environment variables. It is also possible to store and retrieve your
application configuration inside an external datastore, like consul or postgres.
Databases are often where scaled applications store state and nothing in
Docker interferes with doing that for containerized applications. Applications
that need to store files, however, face some challenges. Storing things to the
container’s filesystem is not performant, will be limited by space, and will
not preserve state when a container is re-created. If you re-deploy a stateful
service without utilizing storage external to the container, you will lose all of
that state. Applications that need to store filesystem state should be
considered carefully before you put them into Docker. If you decide that you
can benefit from Linux containers in these cases, it’s best to design a solution
where the state can be stored in a centralized location that could be accessed
regardless of which host a container runs on. In certain cases, this might
mean using a service like Amazon S3, OpenStack Swift, a local block store,
or even mounting EBS volumes or iSCSI disks inside the container. Docker
volume plug-ins provide some additional options and are briefly discussed in
[Link to Come].

TIP
Although it is possible to externalize state on the host’s local filesystem, it is not

generally encouraged by the community and should be considered an advanced use case.
It is strongly recommended that you start with applications that don’t need persistent
state. There are multiple reasons why this is typically discouraged, but in almost all
cases it is because it introduces dependencies between the container and the host that
interfere with using Docker as a truly dynamic, horizontally scalable application delivery
service. If your container maintains state on the local host filesystem, then it can only be
deployed to the system that houses that local filesystem. Remote volumes that can be
dynamically attached are a good solution, but also an advanced use case.

The Docker Workflow
Like many tools, Docker strongly encourages a particular workflow. It’s a
very enabling workflow that maps well to how many companies are
organized, but it’s probably a little different than what you or your team are
doing now. Having adapted our own organizations’ workflows to the Docker
approach, we can confidently say that this is a change that can have a widereaching positive impact on many teams in your organization. If the
workflow is implemented well, it can help realize the promise of reduced
communication overhead between teams.

Revision Control
The first thing that Docker gives you out of the box is two forms of revision
control. One of them is used to track the filesystem layers that each Docker
image is comprised of and the other is a tagging system for those images.
Filesystem layers
Linux containers are made up of stacked filesystem layers, each identified by
a unique hash, where each new set of changes made during the build process
is laid on top of the previous changes. That’s great because it means that
when you do a new build, you only have to rebuild the layers that follow the
change you’re deploying. This saves time and bandwidth because containers
are shipped around as layers and you don’t have to ship layers that a server
already has stored. If you’ve done deployments with many classic

deployment tools, you know that you can end up shipping hundreds of
megabytes of the same data to a server over and over, with each deployment.
That’s incredibly inefficient, and worse, you can’t be sure exactly what
changed between deployments. Because of the layering effect, and because
Linux containers include all of the application dependencies, with Docker
you can be more confident about the changes that you are shipping to
production.
To simplify this a bit, remember that a Docker image contains everything
required to run your application. If you change one line of code, you certainly
don’t want to waste time rebuilding every dependency that your code requires
into a new image. Instead, by leveraging the build cache, Docker can ensure
that only the layers affected by the code change are rebuilt.
Image tags
The second kind of revision control offered by Docker makes it easy to
answer an important question: what was the previous version of the
application that was deployed? That’s not always easy to answer. There are a
lot of solutions for non-containerized applications, from Git tags for each
release to deployment logs, to tagged builds for deployment, and many more.
If you’re coordinating your deployment with Capistrano, for example, it will
handle this for you by keeping a set number of previous releases on the server
and then using symlinks to make one of them the current release.
But what you find in any scaled production environment is that each
application has a unique way of handling deployment revisions. Many of
them do the same thing, but some may be different. Worse, in heterogeneous
language environments, the deployment tools are often entirely different
between applications and very little is shared. So the question of “What was
the previous version?” can have many answers depending on whom you ask
and about which application. Docker has a built-in mechanism for handling
this: it provides image tagging a standard build step. You can easily leave
multiple revisions of your application on the server so that performing a
rollback is trivial. This is not rocket science, and it’s not functionality that is
hard to find in other deployment tooling, but with container images, it can

easily be made standard across all of your applications, and everyone can
have the same expectations about how things will be tagged for all
applications. This makes communication easier between teams and it makes
tooling much simpler because there is one source of truth for application
releases.

WARNING
In many examples on the internet and in this book, you will see people use the latest
tag for a container image. This is useful when you’re getting started and when you’re
writing examples, as it will always grab the most recent build of an image. But since this
is a floating tag, it is a really bad idea to use latest in most production workflows, as
your dependencies can get updated out from under you, and it is impossible to roll back
to latest because the old version is no longer the one tagged latest. It also makes
it hard to verify if the same image is running on different servers. The rule of thumb is:
don’t use the latest tag in production. It’s not even a good idea to use the latest
tag from upstream images, for the same reasons.
It is highly recommended that you tag your CI/CD builds with something that uniquely
identifies the exact source code commit that was used to build them. In a git workflow,
this could be the git hash related to the commit. Once you are ready to release an image,
the recommendation is that you use semantic versioning, and provide your image with
tags, like 1.4.3, 2.0.0, etc.
Pinning versions requires a bit more work to keep them current, but it will also avoid
many unfortunate and poorly timed surprises during builds and deployments.

Building
Building applications is a black art in many organizations, where a few
people know all the levers to pull and knobs to turn to spit out a well-formed,
shippable artifact. Part of the heavy cost of getting a new application
deployed is getting the build just right. Docker doesn’t solve all of these
problems, but it does provide a standardized tool configuration and toolset for
builds. That makes it a lot easier for people to learn how to build your
applications and to get new builds up and running.

The Docker command-line tool contains a build flag that will consume a
Dockerfile and produce a Docker image. Each command in a Dockerfile
generates a new layer in the image, so it’s easy to reason about what the build
is going to do by looking at the Dockerfile itself. The great part of all of this
standardization is that any engineer who has worked with a Dockerfile can
dive right in and modify the build of any other application. Because the
Docker image is a standardized artifact, all of the tooling behind the build
will be the same regardless of the development language or base image that is
being used or the number of layers needed. The Dockerfile is usually checked
into a revision control system, which also means tracking changes to the
build is simplified. Modern multi-stage Docker builds also allow you to
define the build environment separately from the final artifact image. This
provides huge configure-ability for your build environment just like you’d
have for a production container.
Many Docker builds are a single invocation of the docker image build
command and generate a single artifact, the container image. Because it’s
usually the case that most of the logic about the build is wholly contained in
the Dockerfile, it’s easy to create standard build jobs for any team to use in
build systems like Jenkins. As a further standardization of the build process,
many companies—eBay, for example—have standardized Linux containers
to do the image builds from a Dockerfile. SaaS build offerings like Travis CI
and Codeship also have first-class support for Docker builds.
It is also possible to automate the creation of multiple images that support
different underlying compute architectures, like x86 and ARM, by utilizing
the newer BuildKit support in Docker.

Testing
While Docker itself does not include a built-in framework for testing, the way
containers are built lends some advantages to testing with Linux containers.
Testing a production application can take many forms, from unit testing to
full integration testing in a semi-live environment. Docker facilitates better
testing by guaranteeing that the artifact that passed testing will be the one that

ships to production. This can be guaranteed because we can either use the
Docker SHA for the container, or a custom tag to make sure we’re
consistently shipping the same version of the application.
Since, by design, containers include all of their dependencies, tests run on
containers are very reliable. If a unit test framework says tests were
successful against a container image, you can be sure that you will not
experience a problem with the versioning of an underlying library at
deployment time, for example. That’s not easy with most other technologies,
and even Java WAR (Web application ARchive) files, for example, don’t
include testing of the application server itself. That same Java application
deployed in a Linux container will generally also include an application
server like Tomcat, and the whole stack can be smoke-tested before shipping
to production.
A secondary benefit of shipping applications in Linux containers is that in
places where there are multiple applications that talk to each other remotely
via something like an API, developers of one application can easily develop
against a version of the other service that is currently tagged for the
environment they require, like production or staging. Developers on each
team don’t have to be experts in how the other service works or is deployed
just to do development on their own application. If you expand this to a
service-oriented architecture with innumerable microservices, Linux
containers can be a real lifeline to developers or QA engineers who need to
wade into the swamp of inter-microservice API calls.
A common practice in organizations that run Linux containers in production
is for automated integration tests to pull down a versioned set of Linux
containers for different services, matching the current deployed versions. The
new service can then be integration-tested against the very same versions it
will be deployed alongside. Doing this in a heterogeneous language
environment would previously have required a lot of custom tooling, but it
becomes reasonably simple to implement because of the standardization
provided by Linux containers.

Packaging
Docker builds produce an image that can be treated as a single build artifact,
although technically they may consist of multiple filesystem layers. No
matter which language your application is written in or which distribution of
Linux you run it on, you get a layered Docker image as the result of your
build. And it is all built and handled by the Docker tooling. That build image
is the shipping container metaphor that Docker is named for: a single,
transportable unit that universal tooling can handle, regardless of what it
contains. Like oceanic cargo ships that package everything into steel
containers, your Docker tooling will only ever have to deal with one kind of
package: the Docker image. That’s powerful, because it’s a huge facilitator of
tool reuse between applications, and it means that someone else’s off-theshelf container tools will work with your build images.
Applications that traditionally took a lot of custom configuration to deploy
onto a new host or development system become very portable with Docker.
Once a container is built, it can easily be deployed on any system with a
running Docker server on the same architecture.

Deploying
Deployments are handled by so many kinds of tools in different shops that it
would be impossible to list them here. Some of these tools include shell
scripting, Capistrano, Fabric, Ansible, and in-house custom tooling. In our
experience with multi-team organizations, there are usually one or two people
on each team who know the magical incantation to get deployments to work.
When something goes wrong, the team is dependent on them to get it running
again. As you probably expect by now, Docker makes most of that a nonissue. The built-in tooling supports a simple, one-line deployment strategy to
get a build onto a host and up and running. The standard Docker client
handles deploying only to a single host at a time, but there are a large array of
tools available that make it easy to deploy into a cluster of Docker or other
compatible Linux container hosts. Because of the standardization Docker
provides, your build can be deployed into any of these systems, with low

complexity on the part of the development teams.

The Docker Ecosystem
Over the years, a wide community has formed around Docker, driven by both
developers and system administrators. Like the DevOps movement, this has
facilitated better tools by applying code to operations problems. Where there
are gaps in the tooling provided by Docker, other companies and individuals
have stepped up to the plate. Many of these tools are also open source. That
means they are expandable and can be modified by any other company to fit
their needs.

NOTE
Docker is a commercial company that has contributed much of the core Docker source
code to the open-source community. Companies are strongly encouraged to join the
community and contribute back to the open-source efforts. If you are looking for
supported versions of the core Docker tools, you can find out more about its offerings at
the Docker website.

Orchestration
The first important category of tools that adds functionality to the core
Docker distribution and Linux container experience contains orchestration
and mass deployment tools. Early mass deployment tools like New Relic’s
Centurion, Spotify’s Helios, and the Ansible Docker tooling still work largely
like traditional deployment tools but leverage the container as the distribution
artifact. They take a fairly simple, easy-to-implement approach. You get a lot
of the benefits of Docker without much complexity, but many of these tools
have been replaced by more robust and flexible tools, like Kubernetes.
Fully automatic schedulers like Kubernetes or Apache Mesos with the
Marathon scheduler are more powerful options that take nearly complete
control of a pool of hosts on your behalf. Other commercial entries are
widely available, such as HashiCorp’s Nomad, Mesosphere’s DCOS, and
6

Rancher.6 The ecosystems of both free and commercial options continue to
grow rapidly.
Immutable Atomic Hosts
One additional idea that you can leverage to enhance your Docker experience
is immutable atomic hosts. Traditionally, servers and virtual machines are
systems that an organization will carefully assemble, configure, and maintain
to provide a wide variety of functionality that supports a broad range of usage
patterns. Updates must often be applied via non-atomic operations, and there
are many ways in which host configurations can diverge and introduce
unexpected behavior into the system. Most running systems are patched and
updated in place in today’s world. Conversely, in the world of software
deployments, most people deploy an entire copy of their application, rather
than trying to apply patches to a running system. Part of the appeal of
containers is that they help make applications even more atomic than
traditional deployment models.
What if you could extend that core container pattern down into the operating
system? Instead of relying on configuration management to try to update,
patch, and coalesce changes to your OS components, what if you could
simply pull down a new, thin OS image and reboot the server? And then if
something breaks, easily roll back to the exact image you were previously
using?
This is one of the core ideas behind Linux-based atomic host distributions,
like Red Hat’s Fedora CoreOS, Bottlerocket OS, and others. Not only should
you be able to easily tear down and redeploy your applications, but the same
philosophy should apply for the whole software stack. This pattern helps
provide very high levels of consistency and resilience to the whole stack.
Some of the typical characteristics of an immutable or atomic host are a
minimal footprint, a design focused on supporting Linux containers and
Docker, and atomic OS updates and rollbacks that can easily be controlled
via multi-host orchestration tools on both bare-metal and common
virtualization platforms.

In Chapter 3, we will discuss how you can easily use these immutable hosts
in your development process. If you are also using these hosts as deployment
targets, this process creates a previously unheard-of amount of software stack
symmetry between your development and production environments.
Additional tools
Docker is not just a standalone solution. It has a massive feature set, but there
is always a case where someone needs more than it can deliver on its own.
There is a wide ecosystem of tools to either improve or augment Docker’s
functionality. Some good production tools leverage the Docker API, like
Prometheus for monitoring and Ansible for simple orchestration. Others
leverage Docker’s plug-in architecture. Plug-ins are executable programs that
conform to a specification for receiving and returning data to Docker.

WARNING
Many of the Docker plugins are considered legacy and are being replaced with better
approaches. Ensure that you do adequate research before deciding on a plugin that you
are going to utilize, to ensure that it is the best option and is not going to be unsupported
or quickly replaced.

There are many more good tools that either talk to the API or run as plug-ins.
Many of these have sprung up to make life with Docker easier on the various
cloud providers. These help with seamless integration between Docker and
the cloud. As the community continues to innovate, the ecosystem continues
to grow. There are new solutions and tools available in this space on an
ongoing basis. If you find you are struggling with something in your
environment, look to the ecosystem!

Wrap-Up
There you have it: a quick tour through Docker. We’ll return to this
discussion later on with a slightly deeper dive into the architecture of Docker,

more examples of how to use the community tooling, and an exploration of
some of the thinking behind designing robust container platforms. But you’re
probably itching to try it all out, so in the next chapter, we’ll get Docker
installed and running.

1 Software as a Service
2 Linux Control Groups
3 Linux Namepsaces
4 Command Line Interface
5 Security-Enhanced Linux
6 Some of these commercial offerings have free editions of their platforms.

Chapter 3. Installing Docker
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 3rd chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
We’re now at the point where you hopefully understand roughly what Docker
is and what it isn’t, and it’s time for some hands-on work. Let’s get Docker
installed so we can work with it. The steps required to install Docker vary
depending on the platform you use for development and the Linux
distribution you use to host your applications in production.
In this chapter, we discuss the steps required to get a fully working Docker
development environment set up on most modern desktop operating systems.
First, we’ll install the Docker client on your native development platform,
and then we’ll get a Docker server running on Linux. Finally, we’ll test out
the installation to make sure it works as expected.
Although the Docker client can run on Windows and macOS to control a
Docker server, Linux containers can only be built and launched on a Linux
system. Therefore, non-Linux systems will require a virtual machine or
remote server to host the Linux-based Docker server. Docker Community
Edition, Docker Desktop, and Vagrant, which are all discussed later in this
chapter, provide some approaches to address this issue. It is also possible to

run Windows containers natively on Windows systems, and we will
specifically discuss this in “Windows Containers”, but most of the book’s
focus will be on Linux containers.

NOTE
The Docker ecosystem is changing very rapidly as the technology evolves to become
more robust and solve a broader range of problems. Some features discussed in this book
and elsewhere may become deprecated. To see what has been tagged for deprecation and
eventual removal, refer to the documentation.

TIP
We assume that you are using a traditional Unix shell in most of the code
examples in the book. You can use Powershell, but be aware that some commands
will need adjusting to work in that environment.
If you are in an environment that requires you to use a proxy, make sure that it is
properly configured for docker.

Docker Client
The Docker client natively supports 64-bit versions of Linux, Windows, and
macOS.
The majority of popular Linux distributions can trace their origins to either
Debian or Red Hat. Debian systems utilize the deb package format and
Advanced Package Tool (apt) to install most prepackaged software. On the
other hand, Red Hat systems rely on rpm (Red Hat Package Manager) files
and Yellowdog Updater, Modified (yum), or Dandified yum (dnf) to install
similar software packages. Alpine Linux, which is often used in
environments that require a very small Linux footprint, relies on the Alpine
package manager (apk) to manage software packages.

On macOS and Microsoft Windows, native GUI installers provide the easiest
method to install and maintain prepackaged software. Homebrew for macOS
and Chocolatey for Windows are also very popular options among technical
users.

WARNING
We will be discussing a few approaches to installing Docker in this section. Make sure
that you pick the first one in this list that best matches your needs. Installing more than
one may cause you problems if you are not well versed in how to switch between them
properly.
Choose one of these: Docker Desktop, Docker Community Edition, OS package
manager, or vagrant.

NOTE
You can always find the most recent installation documentation on the Docker website.

Linux
It is strongly recommended that you run Docker on a modern release of your
preferred Linux distribution. It is possible to run Docker on some older
releases, but stability may be a significant issue. Generally, a 3.8 or later
kernel is required, and we advise you to use the newest stable version of your
chosen distribution. The following directions assume you are using a recent,
stable release.

TIP
Although we are not covering it here, Docker Desktop for Linux has been released and
can be used on Linux if you would prefer running the Docker daemon on a local virtual
machine instead of directly on your system.

Ubuntu Linux 22.04 (64-bit)

NOTE
For other versions of Ubuntu, see the Docker Community Edition for Ubuntu.

Let’s take a look at the steps required to install Docker.
These first two commands will ensure that you aren’t running older versions
of Docker. The packages have been renamed a few times, so you’ll need to
specify several possibilities here:
$ sudo apt-get remove docker docker.io containerd runc
$ sudo apt-get remove docker-engine

NOTE
It is safe to ignore apt-get errors that say "Unable to locate package" or that the
"Package is not installed“.

Next, you will need to add the required software dependencies and apt
repository for Docker Community Edition. This lets us fetch and install
packages for Docker and validate that they are signed.
$ sudo apt-get update
$ sudo apt-get install \
ca-certificates \
curl \
gnupg \
lsb-release
$ sudo mkdir -p /etc/apt/keyrings
$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg
|\
sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
$ sudo chmod a+r /etc/apt/keyrings/docker.gpg
$ echo \

"deb [arch=$(dpkg --print-architecture) \
signed-by=/etc/apt/keyrings/docker.gpg] \
https://download.docker.com/linux/ubuntu \
$(lsb_release -cs) stable" |\
sudo tee /etc/apt/sources.list.d/docker.list >
/dev/null

Now that you have the repository set up, run the following commands to
install Docker:
$ sudo apt-get update
$ sudo apt-get install \
docker-ce \
docker-ce-cli \
containerd.io \
docker-compose-plugin

Assuming you don’t get any error messages, you now have Docker installed!
Fedora Linux 36 (64-bit)

NOTE
For other versions of Fedora, see the Docker Community Edition for Fedora.

Let’s take a look at the steps needed to install the correct Docker packages on
your system.
This first command will ensure that you aren’t running older versions of
Docker. As on Ubuntu systems, the package has been renamed a few times,
so you’ll need to specify several possibilities here:
$ sudo dnf remove -y \
docker \
docker-client \
docker-client-latest \
docker-common \

docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-selinux \
docker-engine-selinux \
docker-engine

Next, you will need to add the required software dependencies and dnf
repository for Docker Community Edition.
$ sudo dnf -y install dnf-plugins-core
$ sudo dnf config-manager \
--add-repo \
https://download.docker.com/linux/fedora/docker-ce.repo

Now you can install the current version of Docker Community Edition.
$ sudo dnf install -y \
docker-ce \
docker-ce-cli \
containerd.io \
docker-compose-plugin

macOS, Mac OS X
To install Docker on macOS, you should use the official Docker Desktop
installer.
GUI installer
Download the latest Docker Desktop for Mac installer and then double-click
on the downloaded program icon. Follow all of the installer’s prompts until
the installation is finished.
Docker Desktop for Mac relies on the xhyve project to provide a native
lightweight virtualization layer for the Linux server component, which
macOS requires to launch Linux virtual machines that can build Docker
images and run containers.

Homebrew installation
You can also install the Docker CLI tools using the popular Homebrew
package management system for macOS. If you take this approach, you
should consider installing Vagrant for creating and managing your Linux
VM. We’ll discuss that shortly in “Non-Linux VM-Based Server”.

Microsoft Windows 11
TIP
It is highly recommended that you set up the Windows Subsystem for Linux (WSL2)
before installing Docker Desktop and then select any available options in the Docker
Desktop installer to enable and default to WSL2.
Docker Desktop for Windows can leverage Hyper-V, to provide a native virtualization
layer for the Linux server components, but the Windows Subsystem for Linux (WSL2),
should provide you with the smoothest experience when working with Linux containers.

Download the latest Docker Desktop for Windows installer and then doubleclick on the downloaded program icon. Follow all of the installer prompts
until the installation is finished.

TIP
By default, your Docker Desktop installation on Windows should be set up for Linux
containers, but if you ever get a message that says something like "no matching manifest
for windows/amd64“, then Docker Desktop is likely configured for Windows containers.
Linux containers are still the most common type of Linux container and this book
requires Linux container support. You can easily change your Windows setup by rightclicking on the Docker icon in the Windows taskbar, and selecting "Switch to Linux
containers…“.
You can easily switch back and forth if you need to use both Linux and Windows
containers.

Chocolatey installation
You can also install the Docker CLI tools using the popular Chocolatey
package management system for Windows. If you take this approach, you
should consider installing Vagrant for creating and managing your Linux
VM. We’ll discuss that shortly in “Non-Linux VM-Based Server”.

NOTE
Installation directions for additional environments can be found online.

Docker Server
The Docker server is a separate binary from the client and is used to manage
most of the work that Docker is typically used for. Next we will explore the
most common ways to manage the Docker server.

NOTE
Docker Desktop and Docker Community Edition already set up the server for you, so if
you took that route, you do not need to do anything else besides ensuring that the server
(dockerd) is running.. On Windows and macOS this typically just means starting the
Docker application. On Linux, you may need to run the systemctl commands listed
below to start the server.

systemd-Based Linux
Current Fedora and Ubuntu releases use systemd to manage processes on
the system. Because you have already installed Docker, you can ensure that
the server starts every time you boot the system by typing:
$ sudo systemctl enable docker

This tells systemd to enable the docker service and start it when the

system boots or switches into the default run level. To start the Docker
server, type the following:
$ sudo systemctl start docker

Non-Linux VM-Based Server
If you are using Microsoft Windows or macOS in your Docker workflow,
you will need a virtual machine so that you can set up a Docker server for
testing. Docker Desktop is convenient because it sets up this VM for you
using the native virtualization technology on these platforms. If you are
running an older version of Windows or cannot use Docker Desktop for other
reasons, you should investigate Vagrant to help you create and manage your
Docker server Linux VM.
In addition to Vagrant, it is also possible to use other virtualization tools, like
Lima on macOS or any standard hypervisor, to set up a local Docker server,
depending on your preferences and needs.

WARNING
The example below is not secure and is not intended to be a recommendation. Instead, it
is simply a demonstration of the basic requirements needed to set up a remote Docker
server VM and make use of it. Securing the server is of critical importance.
Using Docker Desktop for development, is often a better option, when possible.

Vagrant
Vagrant provides support for multiple hypervisors and can often be leveraged
to mimic even the most complex environments.
A common use case for leveraging Vagrant during Docker development is to
support testing on images that match your production environment. Vagrant
supports everything from broad distributions like RedHat Enterprise Linux
and Ubuntu to finely focused atomic host distributions like Fedora CoreOS.

You can easily install Vagrant on most platforms by downloading a selfcontained package from vagrantup.com.
You will need to have a hypervisor, like VirtualBox (free/multiplatform/x86only), VMware Workstation Pro/Fusion (paid/multiplatform/most-archs),
HyperV (free/Windows/most-archs), or KVM (free/Linux/most-archs) fully
installed on your system. By default, Vagrant assumes that you are using the
VirtualBox hypervisor, but you can change it by using the --provider flag
when using the vagrant command.
In the following example, you will create a Ubuntu-based Docker host
running the Docker daemon.
Then create a host directory with a name similar to docker-host and
move into that directory:
$ mkdir docker-host
$ cd docker-host

To use Vagrant you need to find a Vagrantbox (VM Image) that is
compatible with your provisioner and architecture. In this example, we will
use a Vagrantbox for the Virtualbox hypervisor.

NOTE
Virtualbox only works on Intel/AMD x86(64) systems and the Vagrantbox that we are
using is specifically built for AMD64 systems.

Go ahead and create a new file called Vagrantfile with the following
contents in it:
puts (<<-EOT)
---------------------------------------------------------------[WARNING] This exposes an unencrypted Docker TCP port on
the VM!!

This is NOT secure and may expose your system to
significant risk
if left running and exposed to the broader network.
---------------------------------------------------------------EOT
$script = <<-SCRIPT
echo \'{"hosts": ["tcp://0.0.0.0:2375",
"unix:///var/run/docker.sock"]}\' | \
sudo tee /etc/docker/daemon.json
sudo mkdir -p /etc/systemd/system/docker.service.d
echo -e \"
[Service]\nExecStart=\nExecStart=/usr/bin/dockerd\" | \
sudo tee /etc/systemd/system/docker.service.d/docker.conf
sudo systemctl daemon-reload
sudo systemctl restart docker
SCRIPT
Vagrant.configure(2) do |config|
# Pick a compatible Vagrantbox
config.vm.box = 'bento/ubuntu-20.04'
# Install Docker if it is not already on the VM image
config.vm.provision :docker
# Configure Docker to listen on an unencrypted local port
config.vm.provision "shell",
inline: $script,
run: "always"
# Port-forward the Docker port to
# 12375 (or another open port) on our host machine
config.vm.network "forwarded_port",
guest: 2375,
host: 12375,
protocol: "tcp",
auto_correct: true
end

You can retrieve a complete copy of the above file by running:
$ git clone https://github.com/bluewhalebook/\
docker-up-and-running-3rd-edition.git --config
core.autocrlf=input
$ cd docker-up-and-running-3rd-edition/chapter_03/vagrant
$ ls Vagrantfile

NOTE
You may need to remove the “\” in the git clone command above and re-assemble
the URL into a single line. It is there because the command is too long for the standard
printed page, and this should work in a standard Unix shell as long as there are no
leading or trailing spaces in either line.

Ensure that you are in the directory with the Vagrantfile and then run the
following command to start the Vagrant VM:

WARNING
This setup is provided as a simple example. It is not secure and should not be left
running without ensuring that the server can not be accessed from the broader network.
Docker maintains documentation on how to secure your Docker endpoint with SSH or
TLS client certificates and some additional information on the attack surface of the
Docker daemon.

$ vagrant up
…
Bringing machine 'default' up with 'virtualbox' provider…
==> default: Importing base box 'bento/ubuntu-20.04'…
==> default: Matching MAC address for NAT networking…
==> default: Checking if box 'bento/ubuntu-20.04' version
'…' is up to date…
==> default: A newer version of the box 'bento/ubuntu20.04' for provider …
==> default: available! You currently have version '…'. The

latest is version
==> default: '202206.03.0'. Run `vagrant box update` to
update.
==> default: Setting the name of the VM:
vagrant_default_1654970697417_18732
==> default: Clearing any previously set network
interfaces…
…
==> default: Running provisioner: docker…
default: Installing Docker onto machine…
==> default: Running provisioner: shell…
default: Running: inline script
default: {"hosts": ["tcp://0.0.0.0:2375",
"unix:///var/run/docker.sock"]}
default: [Service]
default: ExecStart=
default: ExecStart=/usr/bin/dockerd

TIP
On macOS you may see an error like this:
VBoxManage: error: Details: code NS_ERROR_FAILURE
(0x80004005), component MachineWrap, interface IMachine
This is due to the security features in macOS. A quick search should lead you to an
online post that describes the fix.

Once the VM is running, you should be able to connect to the Docker server,
by running the following command, and telling the docker client where it
should connect to with the -H argument:
$ docker -H 127.0.0.1:12375 version
Client:
Cloud integration: v1.0.24
Version:
20.10.14
API version:
1.41
…
Server: Docker Engine - Community

Engine:
Version:
API version:
…

20.10.17
1.41 (minimum version 1.12)

The output will provide you with version information about the various
components that make up the Docker client and server.
Passing in the IP address and port every time you want to run a Docker
command, is not ideal, but luckily Docker can be set up to know about
multiple Docker servers, using the docker context command. To start
let’s check and see what context is currently in use. Take note of the entry
that has an asterisk (*) next to it, which designates the current context.
$ docker context list
NAME
TYPE … DOCKER ENDPOINT
…
default * moby … unix:///var/run/docker.sock …
…

You can create a new context for the Vagrant VM and then make it active by
running the following sequence of commands:
$ docker context create vagrant --docker
host=tcp://127.0.0.1:12375
vagrant
Successfully created context "vagrant"
$ docker context use vagrant
vagrant

If you re-list all the contexts now you should see something like this:
$ docker context list
NAME
TYPE … DOCKER ENDPOINT
…
default
moby … unix:///var/run/docker.sock …
vagrant * moby … tcp://127.0.0.1:12375
…
…

With your current context set to vagrant, running docker version,

without the additional -H argument, will still connect to the correct Docker
server and return the same information as before.
To connect to a shell on the Vagrant-based virtual machine, you can run:
$ vagrant ssh
…
Welcome to Ubuntu 20.04.3 LTS (GNU/Linux 5.4.0-91-generic
x86_64)
…
vagrant@vagrant:~$ exit

Until you have time to secure this setup, it is best to go ahead and shut down
the VM and set your context back to its’ original state.
$ vagrant halt
…
==> default: Attempting graceful shutdown of VM…
$ docker version
Cannot connect to … daemon at tcp://127.0.0.1:12375. Is the
… daemon running?
$ docker context use default
default

TIP
If you are using macOS, you might want to take a look at colima which makes it very
easy to spin up and manage a flexible Docker or Kubernetes virtual machine.

Testing the Setup
Once you have a working client and server set up, you are ready to test that
everything is working. You should be able to run any one of the following
commands on your local system to tell the Docker daemon to download the
latest official container for that distribution and then launch it with a running

Unix shell process.
This step is important to ensure that all the pieces are properly installed and
communicating with each other as expected. It also shows off one of the
features of Docker: we can run containers based on any Linux distribution we
like. In the next few steps, we’ll run Linux containers based on Ubuntu,
Fedora, and Alpine Linux. You don’t need to run them all to prove that this
works; running one of them will suffice.

NOTE
If you are using the docker client on a Linux system, you may need to prepend each
docker command with sudo since the root user may be the only one with docker
access, by default.
Most Docker installs create a docker group that can be used to manage who has access
to the dockerd Unix socket, you can add your user to that group, so that you no longer
need to use the sudo command.

Ubuntu
$ docker container run --rm -ti docker.io/ubuntu:latest
/bin/bash
root@aa9b72ae1fea:/#

TIP
Using docker container run is functionally the same as using docker run.

Fedora
$ docker container run --rm -ti docker.io/fedora:latest
/bin/bash

[root@5c97201e827b /]# exit

Alpine Linux
$ docker container run --rm -ti docker.io/alpine:latest
/bin/sh
/ # exit

NOTE
docker.io/ubuntu:latest, docker.io/fedora:latest, and
docker.io/alpine:latest all represent a Docker image repsitory, followed by
an image name and an image tag.

Exploring the Docker Server
Although the Docker server is often installed, enabled, and run automatically,
it’s useful to see that running the Docker daemon manually on a Linux
system can be as simple as typing something like this:
$ sudo dockerd -H unix:///var/run/docker.sock \
--config-file /etc/docker/daemon.json

NOTE
This section assumes that you are on the actual Linux server or VM that is running the
Docker daemon. If you are using Docker Desktop on a Windows or Mac, you won’t be
able to easily interact with the dockerd executable, as it is intentionally hidden from
the end user, but we’ll show you a trick in just a moment.

This command starts the Docker daemon, creates and listens to a Unix
domain socket (-H unix:///var/run/docker.sock) and reads in
the rest of the configuration from /etc/docker/daemon.json. You’re

not likely to have to start the Docker server yourself, but that’s what going on
behind the scenes. On non-Linux systems, you will typically have a Linuxbased virtual machine that hosts the Docker server. Docker Desktop sets up
this virtual machine for you in the background.

NOTE
If you already have Docker running, executing the daemon again will fail because it
can’t use the same network port twice.

In most cases, it is very easy to SSH into your new Docker server and take a
look around, but the seamless experience of Docker Desktop on a non-Linux
system means it is often not apparent that Docker Desktop is leveraging a
local virtual machine to run the Docker daemon on. Because The Docker
Desktop VM is designed to be very small and very stable, it does not run an
SSH daemon and is, therefore, a bit tricky to access.
If you are curious or just ever have a need to access the underlying VM, you
can do it, but it requires a little advanced knowledge. We will talk about the
command nsenter in much more detail in “nsenter”, but for now, if you
would like to see the virtual machine (or underlying host) you can run these
commands:
$ docker container run --rm -it --privileged --pid=host
debian \
nsenter -t 1 -m -u -n -i sh
/ # cat /etc/os-release
PRETTY_NAME="Docker Desktop"
/ # ps | grep dockerd
1540 root
1:05 /usr/local/bin/dockerd
--containerd /var/run/desktopcontainerd/containerd.sock
--pidfile /run/desktop/docker.pid
--swarm-default-advertise-addr=eth0
--host-gateway-ip 192.168.65.2

/ # exit

This command uses a privileged Debian container that contains the
nsenter command to manipulate the Linux kernel namespaces so that we
can navigate the filesystem of the underlying virtual machine or host.

WARNING
This container is privileged to allow us to navigate the underlying host, but you should
not get into the habit of using privileged containers when adding individual capabilities
or system call privileges will suffice. We discuss this more in [Link to Come].
If you can use a Docker server endpoint, this command will give you access to the
underlying host.

The Docker daemon configuration is typically stored in
/etc/docker/daemon.json, but you may notice that it exists
somewhere like
/containers/services/docker/rootfs/etc/docker/daemon
.json in the Docker Desktop VM. Docker uses sane defaults for all its
settings, so this file may be very small or even completely absent. If you are
using Docker Desktop, you can edit this file by clicking on the Docker icon
and selecting Preferences… → Docker Engine, as shown in Figure 3-1.

Figure 3-1. Docker Desktop server configuration

Wrap-Up
Now that you have a running Docker setup, you can start to look at more than
the basic mechanics of getting it installed. In the next chapter, you’ll explore
how to build and manage Docker images, which provide the basis for every
container you will ever launch with Docker.

TIP
In the rest of the book, when you see docker on the command line, assume you will
need to have the correct configuration in place either as a Docker context, environment
variables or via the -H command-line flag to tell the docker client how to connect to
the dockerd server process.

Chapter 4. Working with Docker
Images
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 4th chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
Every Linux container is based on an image. Images are the underlying
definition of what gets reconstituted into a running container, much like a
virtual disk becomes a virtual machine when you start it up. Docker or OCI
images provide the basis for everything that you will ever deploy and run
with Docker. To launch a container, you must either download a public
image or create your own. You can think of the image as a single asset that
primarily represents the filesystem for the container. However, in reality,
every image consists of one or more linked filesystem layers that generally
have a direct one-to-one mapping to each build step used to create that image.
Because images are built up from individual layers, they put special demands
on the Linux kernel, which must provide the drivers that Docker needs to run
the storage backend. For image management, Docker relies heavily on this
storage backend, which communicates with the underlying Linux filesystem
to build and manage the multiple layers that combine into a single usable

image. The primary storage backends that are supported include overlay2,
btrfs, and device-mapper. Each storage backend provides a fast copy-on-write
(CoW) system for image management. We discuss the specifics of various
backends in [Link to Come]. For now, we’ll use the default backend and
explore how images work, since they make up the basis for almost everything
else that you will do with Docker, including:
Building images
Uploading (pushing) images to an image registry
Downloading (pulling) images from an image registry
Creating and running containers from an image

Anatomy of a Dockerfile
To create a custom Docker image with the default tools, you will need to
become familiar with the Dockerfile. This file describes all the steps that are
required to create an image and would usually be contained within the root
directory of the source code repository for your application.
A typical Dockerfile might look something like the one shown here, which
creates a container for a Node.js-based application:
FROM node:18.13.0
ARG email="anna@example.com"
LABEL "maintainer"=$email
LABEL "rating"="Five Stars" "class"="First Class"
USER root
ENV AP /data/app
ENV SCPATH /etc/supervisor/conf.d
RUN apt-get -y update
# The daemons

RUN apt-get -y install supervisor
RUN mkdir -p /var/log/supervisor
# Supervisor Configuration
COPY ./supervisord/conf.d/* $SCPATH/
# Application Code
COPY *.js* $AP/
WORKDIR $AP
RUN npm install
CMD ["supervisord", "-n"]

Dissecting this Dockerfile will provide some initial exposure to a number of
the possible instructions for controlling how an image is assembled. Each line
in a Dockerfile creates a new image layer that is stored by Docker. This layer
contains all of the changes that are a result of that command being issued.
This means that when you build new images, Docker will only need to build
layers that deviate from previous builds: you can reuse all the layers that
haven’t changed.
Although you could build a Node instance from a plain, base Linux image,
you can also explore Docker Hub for official images for Node. The Node.js
community maintains a series of Docker images and tags that allow you to
quickly determine what versions are available. If you want to lock the image
to a specific point release of Node, you could point it at something like
node:18.13.0. The base image that follows will provide you with an
Ubuntu Linux image running Node 11.11.x.
FROM docker.io/node:18.13.0

The ARG parameter provides a way for you to set variables and their default
values, which are only available during the image build process.
ARG email="anna@example.com"

Applying labels to images and containers allows you to add metadata via
key/value pairs that can later be used to search for and identify Docker
images and containers. You can see the labels applied to any image using the
docker image inspect command. For the maintainer label, we are
leveraging the value of the email build argument that was defined in the
previous line of the Dockerfile. This means that this label can be changed
anytime that we build this image.
LABEL "maintainer"=$email
LABEL "rating"="Five Stars" "class"="First Class"

By default, Docker runs all processes as root within the container, but you
can use the USER instruction to change this:
USER root

CAUTION
Even though containers provide some isolation from the underlying operating system,
they still run on the host kernel. Due to potential security risks, production containers
should almost always be run in the context of an unprivileged user.

Unlike the ARG instruction, the ENV instruction allows you to set shell
variables that can be used by your running application for configuration, in
addition to being available during the build process. The ENV and ARG
instructions can be used to simplify the Dockerfile and help keep it DRYer:1
ENV AP /data/app
ENV SCPATH /etc/supervisor/conf.d

In the following code, you’ll use a collection of RUN instructions to start and
create the required file structure that you need and install some required
software dependencies.

RUN apt-get -y update
# The daemons
RUN apt-get -y install supervisor
RUN mkdir -p /var/log/supervisor

WARNING
While we’re demonstrating it here for simplicity, it is not recommended that you run
commands like apt-get -y update or dnf -y update in your application’s
Dockerfile. This is because it requires crawling the repository index each time you run a
build, which means that your build is not guaranteed to be repeatable since package
versions might change between builds. Instead, consider basing your application image
on another image that already has these updates applied to it and where the versions are
in a known state. It will be faster and more repeatable.

The COPY instruction is used to copy files from the local filesystem into your
image. Most often this will include your application code and any required
support files. Because COPY copies the files into the image, you no longer
need access to the local filesystem to access them once the image is built.
You’ll also start to use the build variables you defined in the previous section
to save you a bit of work and help protect you from typos.
# Supervisor Configuration
COPY ./supervisord/conf.d/* $SCPATH/
# Application Code
COPY *.js* $AP/

TIP
Remember that every instruction creates a new Docker image layer, so it often makes
sense to combine a few logically grouped commands onto a single line. It is even
possible to use the COPY instruction in combination with the RUN instruction to copy a
complex script to your image and then execute that script with only two commands in
the Dockerfile.

With the WORKDIR instruction, you change the working directory in the
image for the remaining build instructions and the default process that
launches with any resulting containers:
WORKDIR $AP
RUN npm install

CAUTION
The order of commands in a Dockerfile can have a very significant impact on ongoing
build times. You should try to order commands so that things that change between every
single build are closer to the bottom. This means that adding your code and similar steps
should be held off until the end. When you rebuild an image, every single layer after the
first introduced change will need to be rebuilt.

And finally, you end with the CMD instruction, which defines the command
that launches the process that you want to run within the container:
CMD ["supervisord", "-n"]

NOTE
Though not a hard and fast rule, it is generally considered a best practice to try to run
only a single process within a container. The core idea is that a container should provide
a single function so that it remains easy to horizontally scale individual functions within
your architecture. In the example, you are using supervisord as a process manager
to help improve the resiliency of the node application within the container and ensure
that it stays running. This can also be useful for troubleshooting your application during
development so that you can restart your service without restarting the whole container.
You could also achieve a similar effect by using the --init command-line argument to
docker container run, which we discuss in “Controlling Processes”.

Building an Image
To build your first image, go ahead and clone a Git repo that contains an
example application called docker-node-hello, as shown here:2
$ git clone https://github.com/spkane/docker-node-hello.git
\
--config core.autocrlf=input
Cloning into 'docker-node-hello'…
remote: Counting objects: 41, done.
remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused
41
Unpacking objects: 100% (41/41), done.
$ cd docker-node-hello

NOTE
Git is frequently installed on Linux and macOS systems, but if you do not already have
Git available, you can download a simple installer from git-scm.com.
The --config core.autocrlf=input option we use helps ensure that the line
endings are not accidentally altered from the Linux standard that is expected.

This will download a working Dockerfile and related source code files into a
directory called docker-node-hello. If you look at the contents while ignoring
the Git repo directory, you should see the following:
$ tree -a -I .git
.
├── .dockerignore
├── .gitignore
├── Dockerfile
├── index.js
├── package.json
└── supervisord
└── conf.d
├── node.conf
└── supervisord.conf

Let’s review the most relevant files in the repo.
The Dockerfile should be the same as the one you just reviewed.
The .dockerignore file allows you to define files and directories that you do
not want to upload to the Docker host when you are building the image. In
this instance, the .dockerignore file contains the following lines:
.git

This instructs docker image build to exclude the .git directory, which
contains the whole source code repository, from the build. The rest of the
files reflect the current state of your source code on the checked-out branch.
You don’t need the contents of the .git directory to build the Docker image,
and since it can grow quite large over time, you don’t want to waste time
copying it every time you do a build. package.json defines the Node.js
application and lists any dependencies that it relies on. index.js is the main
source code for the application.
The supervisord directory contains the configuration files for
supervisord that you will use to start and monitor the application.

NOTE
Using supervisord in this example to monitor the application is overkill, but it is
intended to provide a bit of insight into some of the techniques you can use in a
container to provide more control over your application and its running state.

As we discussed in Chapter 3, you will need to have your Docker server
running and your client properly set up to communicate with it before you
can build a Docker image. Assuming that this is all working, you should be
able to initiate a new build by running the upcoming command, which will
build and tag an image based on the files in the current directory.
Each step identified in the following output maps directly to a line in the
Dockerfile, and each step creates a new image layer based on the previous

step. The first build that you run will take a few minutes because you have to
download the base node image. Subsequent builds should be much faster
unless a new version of our base image tag has been released.

NOTE
The output below is from the new BuildKit included in Docker. If you see significantly
different output, then you are likely still using the older image building code.
You can enable BuildKit in your environment by setting the DOCKER_BUILDKIT
environment variable to 1.
More details can be found on the Docker website.

At the end of the build command, you will notice a period. This refers to the
build context, which tells Docker what files it should upload to the server so
that it can build our image. In many cases, you will simply see a . at the end
of a build command, since a single period represents the current directory.
This build context is what the .dockerignore file is filtering so that we don’t
upload more than we need.

TIP
Docker assumes that the Dockerfile is in the current directory, but if it is not, you can
point directly to it using the -f argument.

Let’s run the build:
$ docker image build -t example/docker-node-hello:latest .
=>
=>
=>
=>
=>

[internal] load
=> transferring
[internal] load
=> transferring
[internal] load

build definition from Dockerfile
dockerfile: 37B
.dockerignore
context: 34B
metadata for

docker.io/library/node:18.13.0
=> CACHED [1/8] FROM
docker.io/library/node:18.13.0@19a9713dbaf3a3899ad…
=> [internal] load build context
=> => transferring context: 233B
=> [2/8] RUN apt-get -y update
=> [3/8] RUN apt-get -y install supervisor
=> [4/8] RUN mkdir -p /var/log/supervisor
=> [5/8] COPY ./supervisord/conf.d/*
/etc/supervisor/conf.d/
=> [6/8] COPY *.js* /data/app/
=> [7/8] WORKDIR /data/app
=> [8/8] RUN npm install
=> exporting to image
=> => exporting layers
=> => writing image
sha256:991844271ca5b984939ab49d81b24d4d53137f04a1bd…
=> => naming to docker.io/example/docker-node-hello:latest

TIP
To improve the speed of builds, Docker will use a local cache when it thinks it is safe.
This can sometimes lead to unexpected issues because it doesn’t always notice that
something changed in a lower layer. In the preceding output, you will notice lines like `
⇒ [2/8] RUN apt-get -y update`. If instead, you see ` ⇒ CACHED [2/8] RUN apt-get -y
update`, you know that Docker decided to use the cache. You can disable the cache for a
build by using the --no-cache argument to the docker image build command.

If you are building your Docker images on a system that is used for other
simultaneous processes, you can limit the resources available to your builds
by using many of the same cgroup methods that we will discuss in Chapter 5.
You can find detailed documentation on the docker image build
arguments in the official documentation.

TIP
Using docker image build is functionally the same as using docker build.

If you have any issues getting a build to work correctly you may want to skip
ahead and read the “Multi-stage builds” and “Troubleshooting Broken
Builds” sections of this chapter.

Running Your Image
Once you have successfully built the image, you can run it on your Docker
host with the following command:
$ docker container run --rm -d -p 8080:8080 example/dockernode-hello:latest

The preceding command tells Docker to create a running container in the
background from the image with the example/docker-nodehello:latest tag, and then map port 8080 in the container to port 8080
on the Docker host. If everything goes as expected, the new Node.js
application should be running in a container on the host. You can verify this
by running docker container ls. To see the running application in
action, you will need to open up a web browser and point it at port 8080 on
the Docker host. You can usually determine the Docker host IP address by
examining the entry from docker context list that is marked with an
asterisk or checking the value of the DOCKER_HOST environment variable if
it happens to be set. If the DOCKER ENDPOINT is set to a Unix socket then
the IP address is most likely 127.0.0.1.
$ docker context list
NAME
TYPE … DOCKER ENDPOINT
…
default * moby … unix:///var/run/docker.sock …
…

Get the IP address and enter something like http://127.0.0.1:8080/ (or your
remote Docker address if it’s different than that) into your web browser
address bar or use a command-line tool like curl. You should see the

following text:
Hello World. Wish you were here.

Build Arguments
If inspect the image that we built, you will be able to see that the maintainer
label was set to anna@example.com.
$ docker inspect example/docker-node-hello:latest | grep
maintainer
"maintainer": "anna@example.com",

If we wanted to change the maintainer label we could simply re-run the
build, and provide a new value for the email ARG via the --build-arg
command line argument, like so:
$ docker image build --build-arg email=me@example.com -t
example/docker-node-hello:latest .
…
=> => naming to docker.io/example/docker-node-hello:latest

After the build has finished, we can check the results by re-inspecting the
new image.
$ docker inspect example/docker-node-hello:latest | grep
maintainer
"maintainer": "me@example.com",

The ARG and ENV instructions can help make _Dockerfile_s very flexible,
while also avoiding a lot of repeated values that can be hard to keep up-todate.

Environment Variables as Configuration
If you read the index.js file, you will notice that part of the file refers to the

variable $WHO, which the application uses to determine who the application
is going to say Hello to:
var DEFAULT_WHO = "World";
var WHO = process.env.WHO || DEFAULT_WHO;
app.get('/', function (req, res) {
res.send('Hello ' + WHO + '. Wish you were here.\n');
});

Let’s quickly cover how you can configure this application by passing in
environment variables when you start it. First, you need to stop the existing
container using two commands. The first command will provide you with the
container ID, which you will need to use in the second command:
$ docker container ls
CONTAINER ID IMAGE
…
b7145e06083f example/centos-node-hello:latest
minutes …

STATUS
Up 4

NOTE
You can format the output of docker container ls by using a Go template so
that you see only the information that you care about. In the preceding example you
might decide to run something like docker container ls --format "table
{{.ID}}\t{{.Image}}\t{{.Status}}" to limit the output to the three fields
you care about. Additionally, running docker container ls --quiet with no
format options will limit the output to only the container ID.

And then, using the container ID from the previous output, you can stop the
running container by typing:
$ docker container stop b7145e06083f
b7145e06083f

TIP
Using docker container ls is functionally equivalent to using docker
container list, docker container ps, or docker ps.
Using docker container stop is also functionally equivalent to using docker
stop.

As seen below, you can then restart the container, after adding a single
instance of the --env argument to the previous docker container
run command:
$ docker container run --rm -d \
--publish mode=ingress,published=8080,target=8080 \
--env WHO="Sean and Karl" \
example/docker-node-hello:latest

If you reload your web browser, you should see that the text on the web page
now reads:
Hello Sean and Karl. Wish you were here.

NOTE
You could shorten the above docker command to the following if you wanted:
$ docker container run --rm -d -p 8080:8080 -e WHO="Sean and
Karl" \
example/docker-node-hello:latest

You can go ahead and stop this container now, using the same method from
above.

Custom Base Images

Base images are the lowest-level images that other Docker images will build
upon. Most often, these are based on minimal installs of Linux distributions
like Ubuntu, Fedora, or Alpine Linux, but they can also be much smaller,
containing a single statically compiled binary. For most people, using the
official base images for their favorite distribution or tool is a great option.
However, there are times when it is preferable to build your own base images
rather than using an image created by someone else. One reason to do this
would be to maintain a consistent OS image across all your deployment
methods for hardware, VMs, and containers. Another would be to get the
image size down substantially. There is no need to ship around an entire
Ubuntu distribution, for example, if your application is a statically built C or
Go application. You might find that you only need the tools you regularly use
for debugging and some other shell commands and binaries. Making the
effort to build such an image could pay off in better deployment times and
easier application distribution.
A common middle-ground between these two approaches is to build images
using Alpine Linux, which is designed to be very small and is popular as a
basis for Docker images. To keep the distribution size very small, Alpine
Linux is based on the modern, lightweight musl standard library, instead of
the more traditional GNU libc. In general, this is not a big issue, since many
packages support musl, but it is something to be aware of. It has the largest
impact on Java-based applications and DNS resolution. It’s widely used in
production, however, because of its diminutive image size. Alpine Linux is
highly optimized for space, which is the reason that it ships with /bin/sh,
instead of /bin/bash, by default. However, you can also install glibc and bash
in Alpine Linux if you need it, and this is often done in the case of JVM
containers.
In the official Docker documentation, there is some good information about
how you can build base images on the various Linux distributions.

Storing Images
Now that you have created a Docker image that you’re happy with, you’ll

want to store it somewhere so that it can be easily accessed by any Docker
host that you want to deploy it to. This is also the normal hand-off point
between building images and storing them somewhere for future deployment.
You don’t normally build the images on a production server and then run
them. This process was described when we talked about handoff between
teams for application deployment. Ordinarily, deployment is the process of
pulling an image from a repository and running it on one or more Linux
servers. There are a few ways you can go about storing your images into a
central repository for easy retrieval.

Public Registries
Docker provides an image registry for public images that the community
wants to share. These include official images for Linux distributions, readyto-go WordPress containers, and much more.
If you have images that can be published on the internet, the best place for
them is a public registry, like Docker Hub. However, there are other options.
When the core Docker tools were first gaining popularity, Docker Hub did
not exist. To fill this obvious void in the community, Quay.io was created.
Since then, Quay.io has gone through a few acquisitions and is now owned
by Red Hat. Cloud vendors like Google and Software-as-a-Service
companies like GitHub also have their own registry offerings. Here we’ll just
talk about the two of them.
Both Docker Hub and Quay.io provide centralized Docker image registries
that can be accessed from anywhere on the internet, and provide a method to
store private images in addition to public ones. Both have nice user interfaces
and the ability to separate team access permissions and manage users. Both
also offer reasonable commercial options for private SaaS hosting of your
images, much in the same way that GitHub sells private registries on their
systems. This is probably the right first step if you’re getting serious about
Docker but are not yet shipping enough code to need an internally hosted
solution.
For companies that use Docker heavily, one of the biggest downsides to these

registries is that they are not local to the network on which the application is
being deployed. This means that every layer of every deployment might need
to be dragged across the internet to deploy an application. Internet latencies
have a very real impact on software deployments, and outages that affect
these registries could have a very detrimental impact on a company’s ability
to deploy smoothly and on schedule. This is mitigated by good image design
where you make thin layers that are easy to move around the internet.

Private Registries
The other option that many companies consider is to host some type of
Docker image registry internally, which can interact with the Docker client to
support pushing, pulling, and searching images. The open-source Distribution
project provides the basic functionality that most other registries build upon.
Other strong contenders in the private registry space include Harbor and Red
Hat Quay. In addition to the basic Docker registry functionality, these
products have solid GUI interfaces and many additional features, like image
verification.

Authenticating to a Registry
Communicating with a registry that stores container images is a part of daily
life with Docker. For many registries, this means you’ll need to authenticate
to gain access to images. But Docker also tries to make it easy to automate
things so it can store your login information and use it on your behalf when
you request things like pulling down a private image. By default, Docker
assumes the registry will be Docker Hub, the public repository hosted by
Docker, Inc.

TIP
Although a bit more advanced, it is worth noting that you can also configure the Docker
daemon to use a custom registry mirror or a pull-through image cache.

Creating a Docker Hub account
For these examples, you will create an account on Docker Hub. You don’t
need an account to download publicly shared images, but you will need to be
logged in to avoid rate limits and upload any containers that you build.
To create your account, use a web browser of your choice to navigate to
Docker Hub.
From there, you can log in via an existing account or create a new login
based on your email address. When you create your account, Docker Hub
sends a verification email to the address that you provided during sign-up.
You should immediately log in to your email account and click the
verification link inside the email to finish the validation process.
At this point, you have created a public registry to which you can upload new
images. The Account Settings option under your profile picture has a
Default Privacy section that will allow you to change your registry
default visibility to private if that is what you need.

WARNING
For much better security, you should create and log in to Docker Hub with a limitedprivilege personal access token.

Logging into a registry
Now let’s log in to the Docker Hub registry using our account:
$ docker login
Login with your Docker ID to push and pull images from
Docker Hub. If you
don't have a Docker ID, head over to https://hub.docker.com
to create one.
Username: <hub_username>
Password: <hub_password/token>
Login Succeeded

NOTE
The command docker login is functionally the same command as docker
login docker.io.

When you get Login Succeeded back from the server, you know you’re
ready to pull images from the registry. But what happened behind the scenes?
It turns out that Docker has written a dotfile for us in our home directory to
cache this information. The permissions are set to 0600 as a security
precaution against other users reading your credentials. You can inspect the
file with something like:
$ ls -la ${HOME}/.docker/config.json
-rw-------@ 1 … 158 Dec 24 10:37
/Users/someuser/.docker/config.json
$ cat ${HOME}/.docker/config.json

On Linux you will see something like this:
{
"auths": {
"https://index.docker.io/v1/": {
"auth":"cmVsaEXamPL3hElRmFCOUE=",
"email":"someuser@example.com"
}
}
}

NOTE
Docker is constantly evolving and has added support for many OS native secret
management systems like the macOS Keychain or Windows Credential Manager. So,
your config.json file might look significantly different than the example. There is also a
set of credentials managers for different platforms that can make your life easier here.

WARNING
The auth value in the Docker client config file is only base64 encoded. It is NOT
encrypted. This is typically only a significant issue on multi-user Linux systems, because
there is not a default system-wide credential manager that just works, and other
privileged users on the system can likely read your docker client config file and access
those secrets. It is possible to configure gpg pr pass to encrypt these files on Linux.

Here you can see the ${HOME}/.docker/config.json file contains
docker.io credentials for the user someuser@example.com in JSON.
Note that this configuration file supports storing credentials for multiple
registries. In this case, you just have one entry, for Docker Hub, but you
could have more if you needed it. From now on, when the registry needs
authentication, Docker will look in ${HOME}/.docker/config.json to see if
you have credentials stored for this hostname. If so, it will supply them. You
will notice that one value is completely lacking here: a timestamp. These
credentials are cached forever or until you tell Docker to remove them,
whichever comes first.
As with logging in, you can also log out of a registry if you no longer want to
cache the credentials:
$ docker logout
Removing login credentials for https://index.docker.io/v1/
$ cat ${HOME}/.docker/config.json
{
"auths": {
}
}

Here you have removed the cached credentials and they are no longer stored
by Docker. Some versions of Docker may even remove this file if it is empty.
If you were trying to log in to something other than the Docker Hub registry,
you could supply the hostname on the command line:

$ docker login someregistry.example.com

This would then add another auth entry into our
${HOME}/.docker/config.json file.
Pushing images into a repository
The first step required to push your image is to ensure that you are logged
into the Docker repository you intend to use. For this example we will focus
on Docker Hub, so ensure that you are logged into Docker Hub with your
preferred credentials.
$ docker login
Login with your Docker ID to push and pull images from
Docker Hub. If you
don't have a Docker ID, head over to https://hub.docker.com
to create one.
Username: <hub_username>
Password: <hub_password/token>
Login Succeeded
Logging in with your password grants your terminal complete
access to
your account.

Once you are logged in, you can upload an image. Earlier you used the
command docker image build -t example/docker-nodehello:latest . to build the docker-node-hello image.
In reality, the Docker client and, for compatibility reasons, many other
container tools actually interpret example/docker-nodehello:latest as docker.io/example/docker-nodehello:latest. docker.io signifies the image registry hostname and
example/docker-node-hello is the repository inside the registry that
contains the images in question.
When you are building an image locally, the registry and repository name can
be anything that you want. However, when you are going to upload your

image to a real registry, you need that to match the login.
You can easily edit the tags on the image that you already created by running
the following command and replacing ${<myuser>} with your Docker
Hub username:
$ docker image tag example/docker-node-hello:latest \
docker.io/${<myuser>}/docker-node-hello:latest

If you need to rebuild the image with the new naming convention or simply
want to give it a try, you can accomplish this by running the following
command in the docker-node-hello working directory that was generated
when you performed the Git checkout earlier in the chapter.

NOTE
For the following examples, you will need to replace ${<myuser>} in all the
examples with the user that you created in Docker Hub. If you are using a different
registry then you will also need to replace docker.io with the hostname of the
registry you are using.

$ docker image build -t docker.io/${<myuser>}/docker-nodehello:latest .
…

On the first build, this will take a little time. If you rebuild the image, you
may find that it is very fast. This is because most, if not all, of the layers
already exist on your Docker server from the previous build. We can quickly
verify that our image is indeed on the server by running docker image
ls ${<myuser>}/docker-node-hello:
$ docker image ls ${<myuser>}/docker-node-hello
REPOSITORY
TAG
IMAGE ID
SIZE
myuser/docker-node-hello
latest
f683df27f02d
hour ago
649MB

CREATED
About an

TIP
It is possible to format the output of docker image ls to make it more concise by
using the --format argument, like this: docker image ls --format="table
{{.ID}}\t{{.Repository}}".

At this point you can upload the image to the Docker repository by using the
docker image push command:
$ docker image push ${<myuser>}/docker-node-hello:latest
Using default tag: latest
The push refers to repository [docker.io/myuser/dockernode-hello]
5f3ee7afc69c: Pushed
…
5bb0785f2eee: Mounted from library/node
latest: digest:
sha256:f5ceb032aec36fcacab71e468eaf0ba8a832cfc8244fbc784d0…

If this image was uploaded to a public repository, anyone in the world can
now easily download it by running the docker image pull command.

TIP
If you uploaded the image to a private repository, then users must log in with credentials
that have access to those repositories using the docker login command before they
will be able to pull the image down to their local system.

$ docker image pull ${<myuser>}/docker-node-hello:latest
Using default tag: latest
latest: Pulling from myuser/docker-node-hello
Digest:
sha256:f5ceb032aec36fcacab71e468eaf0ba8a832cfc8244fbc784d04
0872be041cd5
Status: Image is up to date for myuser/docker-nodehello:latest

docker.io/myuser/docker-node-hello:latest

Exploring images in Docker Hub
In addition to simply using the [Docker Hub website]
(https://hub.docker.com/) to explore what images are available, you can also
use the docker search command to find images that might be useful.
Running docker search node will return a list of images that contain
the word node in either the image name or the description.
$ docker search node
NAME
OFFICIAL AUTOMATED
node
[OK]
mongo-express
[OK]
nodered/node-red
nodered/node-red-docker
[OK]
circleci/node
kindest/node
bitnami/node
[OK]
cimg/node
opendronemap/nodeodm
[OK]
bitnami/node-exporter
[OK]
appdynamics/nodejs-agent
wallarm/node
[OK]
…

DESCRIPTION

STARS

Node.js is a JavaScript-ba… 12267
Web-based MongoDB admin in… 1274
Low-code programming for e… 544
Deprecated - older Node-RE… 356
Node.js is a JavaScript-ba… 130
sigs.k8s.io/kind node imag… 78
Bitnami Node.js Docker Ima… 69
The CircleCI Node.js Docke… 14
Automated build for NodeOD… 10
Bitnami Node Exporter Dock… 9
Agent for monitoring Node.… 5
Wallarm: end-to-end API se… 5

The OFFICIAL header tells you that the image is one of the [official curated
images](https://docs.docker.com/docker-hub/official_images/) on Docker
Hub. This typically means that the image is maintained by the company or
official development community that oversees that application.
AUTOMATED denotes that the image is automatically built and uploaded

by a CI/CD process trigged via commits to the underlying source code
repository. Official images are always automated.

Running a Private Registry
In keeping with the spirit of the open-source community, Docker encourages
the community to share Docker images via Docker Hub by default. There are
times, however, when this is not a viable option due to commercial, legal,
image retention, or reliability concerns.
In these cases, it makes sense to host an internal private registry. Setting up a
basic registry is not difficult, but for production use, you should take the time
to familiarize yourself with all the available configuration options for the
open-source Docker Registry (Distribution).
For this example, we are going to create a very simple secure registry using
SSL and HTTP basic auth.
First, let’s create a few directories and files on our Docker server. If you are
using a virtual machine or cloud instance to run your Docker server, then you
will need to SSH to that server for the next few commands. If you are using
Docker Desktop or Community Edition, then you should be able to run these
on your local system.

TIP
Windows users: You may need to download additional tools, like htppaswd, or alter
the non-Docker commands to accomplish the same tasks on your local system.

First let’s clone a Git repository that contains the basic files required to set up
a simple, authenticated Docker registry.
$ git clone https://github.com/spkane/basic-registry \
--config core.autocrlf=input
Cloning into 'basic-registry'…
remote: Counting objects: 10, done.

remote: Compressing objects: 100% (8/8), done.
remote: Total 10 (delta 0), reused 10 (delta 0), packreused 0
Unpacking objects: 100% (10/10), done.

Once you have the files locally, you can change directories and examine the
files that you have just downloaded.
$ cd basic-registry
$ ls
Dockerfile
config.yaml.sample
README.md
htpasswd.sample

registry.crt.sample
registry.key.sample

The Dockerfile simply takes the upstream registry image from Docker Hub
and copies some local configuration and support files into a new image.
For testing, you can use some of the included sample files, but do not use
these in production.
If your Docker server is available via localhost (127.0.0.1), then you can use
these files unmodified by simply copying each of them like this:
$
$
$
$

cp
cp
cp
cp

config.yaml.sample config.yaml
registry.key.sample registry.key
registry.crt.sample registry.crt
htpasswd.sample htpasswd

If, however, your Docker server is on a remote IP address, then you will need
to do a little additional work.
First copy config.yaml.sample to config.yaml.
$ cp config.yaml.sample config.yaml

Then edit config.yaml and replace 127.0.0.1 with the IP address of your
Docker server so that:
http:
host: https://127.0.0.1:5000

becomes something like this:
http:
host: https://172.17.42.10:5000

NOTE
It is easy to create a registry using a fully qualified domain name (FQDN), like myregistry.example.com, but for this example working with IP addresses is easier
because no DNS is required.

Next, you need to create an SSL keypair for your registry’s IP address:
One way to do this is with the following OpenSSL command. Note that you
will need to set the IP address in this portion of the command
/CN=172.17.42.10 to match your Docker server’s IP address.
openssl req -x509 -nodes -sha256 -newkey rsa:4096 \
-keyout registry.key -out registry.crt \
-days 14 -subj '{/CN=172.17.42.10}'

Finally, you can either use the example htpasswd file by copying it:
$ cp htpasswd.sample htpasswd

or you can create your own username and password pair for authentication by
using a command like the following, replacing ${<username>} and
${<password>} with your preferred values.
docker container run --rm --entrypoint htpasswd g \
-Bbn ${<username>} ${<password>} > htpasswd

If you look at the directory listing again, it should now look like this:
$ ls
Dockerfile

config.yaml.sample

registry.crt

registry.key.sample
README.md
htpasswd
config.yaml
htpasswd.sample

registry.crt.sample
registry.key

If any of these files are missing, review the previous steps, to ensure that you
did not miss one, before moving on.
If everything looks correct, then you should be ready to build and run the
registry.
$ docker image build -t my-registry .
$ docker container run --rm -d -p 5000:5000 --name registry
my-registry
$ docker container logs registry

TIP
If you see errors like docker: Error response from daemon: Conflict.
The container name "/registry" is already in use, then you need
to either change the container name above or remove the existing container with that
name. You can remove the container by running docker container rm
registry.

Testing the private registry
Now that the registry is running, you can test it. The very first thing that you
need to do is authenticate against it. You will need to make sure that the IP
address in the docker login matches the IP address of your Docker
server that is running the registry.

NOTE
myuser is the default username, and myuser-pw! is the default password. If you
generated your own htpasswd, then these will be whatever you choose.

$ docker login 127.0.0.1:5000
Username: <registry_username>
Password: <registry_password>
Login Succeeded

WARNING
This registry container has an embedded SSL key and is not using any external storage,
which means that it contains a secret and when you delete the running container, all your
images will also be deleted. This is by design.
In production, you will want to have your containers pull secrets from a secrets
management system and use some type of redundant external storage, like an object
store. If you want to keep your development registry images between containers, you
could add something like --mount type=bind,source=/tmp/registrydata,target=/var/lib/registry to your docker container run
command to store the registry data on the Docker server.

Now, let’s see if you can push the image you just built into your local private
registry.

TIP
In all of these commands, ensure that you use the correct IP address for your registry.

$ docker image tag my-registry 127.0.0.1:5000/my-registry
$ docker image push 127.0.0.1:5000/my-registry
Using default tag: latest
The push refers to repository [127.0.0.1:5000/my-registry]
f09a0346302c: Pushed
…
4fc242d58285: Pushed
latest: digest:
sha256:c374b0a721a12c41d5b298930d11e658fbd37f22dc2a0fac7d6a
2…

You can then try to pull the same image from your repository.

$ docker image pull 127.0.0.1:5000/my-registry
Using default tag: latest
latest: Pulling from my-registry
Digest:
sha256:c374b0a721a12c41d5b298930d11e658fbd37f22dc2a0fac7d6a
2ecdc0ba5490
Status: Image is up to date for 127.0.0.1:5000/myregistry:latest
127.0.0.1:5000/my-registry:latest

+

TIP
It’s worth keeping in mind that both Docker Hub and Docker Distribution expose an API
endpoint that you can query for useful information. You can find out more information
about the API via the official documentation.

If you have not encountered any errors, then you have a working registry for
development and could build on this foundation to create a production
registry. At this point, you may want to stop the registry for the time being.
You can easily accomplish this by running:
$ docker container stop registry

+

TIP
As you become comfortable with Docker Distribution, you may also want to consider
exploring the CNCF3 open source project, called Harbor, which extends the Docker
Distribution with a lot of security and reliability-focused features.

Optimizing Images

After you have spent a little bit of time working with Docker, you will
quickly notice that keeping your image sizes small and your build times fast
can be very beneficial in decreasing the time required to build and deploy
new versions of your software into production. In this section, we will talk a
bit about some of the considerations you should always keep in mind when
designing your images, and a few techniques that can help achieve these
goals.

Keeping Images Small
In most modern businesses, downloading a single 1 GB file from a remote
location on the internet is not something that people often worry about. It is
so easy to find software on the internet that people will often rely on simply
re-downloading it if they need it again, instead of keeping a local copy for the
future. This may often be acceptable when you truly need a single copy of
this software on a single server, but it can quickly become a scaling problem
when you need the same software on 100+ nodes and you deploy new
releases multiple times a day. Downloading these large files can quickly
cause network congestion and slower deployment cycles that have a real
impact on the production environment.
For convenience, a large number of Linux containers inherit from a base
image that contains a minimal Linux distribution. Although this is an easy
starting place, it isn’t required. Containers only need to contain the files that
are required to run the application on the host kernel, and nothing else. The
best way to explain this is to explore a very minimal container.
Go is a compiled programming language that can easily generate statically
compiled binary files. For this example, we are going to use a very small web
application written in Go that can be found on GitHub.
Let’s go ahead and try out the application so that you can see what it does.
Run the following command and then open up a web browser and point it to
your Docker host on port 8080 (e.g., http://127.0.0.1:8080 for Docker
Desktop and Community Edition):

$ docker container run --rm -d -p 8080:8080 spkane/scratchhelloworld

If all goes well, you should see the following message in your web browser:
Hello World from Go in minimal Linux container. Now
let’s take a look at what files this container comprises. It would be fair to
assume that at a minimum it will include a working Linux environment and
all the files required to compile Go programs, but you will soon see that this
is not the case.
While the container is still running, execute the following command to
determine what the container ID is. The following command returns the
information for the last container that you created:
$ docker container ls -l
CONTAINER ID IMAGE
COMMAND
CREATED
…
ddc3f61f311b spkane/scratch-helloworld "/helloworld" 4
minutes ago
…

You can then use the container ID that you obtained from running the
previous command to export the files in the container into a tarball, which
can be easily examined.
$ docker container export ddc3f61f311b -o web-app.tar

Using the tar command, you can now examine the contents of your
container at the time of the export.
tar -tvf web-app.tar
-rwxr-xr-x 0 0
drwxr-xr-x 0 0
-rwxr-xr-x 0 0
drwxr-xr-x 0 0
drwxr-xr-x 0 0
drwxr-xr-x 0 0
-rwxr-xr-x 0 0
etc/hostname

0
0
0
0
0
0
0

0
0
0
0
0
0
0

Jan
Jan
Jan
Jan
Jan
Jan
Jan

7
7
7
7
7
7
7

15:54
15:54
15:54
15:54
15:54
15:54
15:54

.dockerenv
dev/
dev/console
dev/pts/
dev/shm/
etc/

-rwxr-xr-x 0 0
lrwxrwxrwx 0 0
/proc/mounts
-rwxr-xr-x 0 0
etc/resolv.conf
-rwxr-xr-x 0 0
drwxr-xr-x 0 0
drwxr-xr-x 0 0

0
0

0 Jan
0 Jan

7 15:54 etc/hosts
7 15:54 etc/mtab ->

0

0 Jan

7 15:54

0
0
0

3604416 Jul
0 Jan
0 Jan

2 2014 helloworld
7 15:54 proc/
7 15:54 sys/

The first thing you might notice here is that there are almost no files in this
container, and almost all of them are zero bytes in length. All of the files that
have a zero-length are required to exist in every Linux container and are
automatically bind-mounted from the host into the container when it is first
created. All of these files, except for .dockerenv, are critical files that the
kernel needs to do its job properly. The only file in this container that has any
actual size and is related to our application is the statically compiled
helloworld binary.
The takeaway from this exercise is that your containers are only required to
contain exactly what they need to run on the underlying kernel. Everything
else is unnecessary. Because it is often useful for troubleshooting to have
access to a working shell in your container, people will often compromise
and build their images from a very lightweight Linux distribution like Alpine
Linux.

TIP
If you find yourself exploring image files a lot, you might want to take a look at the tool
[dive](https://github.com/wagoodman/dive), which provides a CLI nice interface for
understanding what an image contains.

To dive into this a little deeper, let’s look at that same container again so that
we can dig into the underlying filesystem and compare it with the popular
alpine base image.
Although we could easily poke around in the alpine image by simply

running docker container run -ti alpine:latest
/bin/sh, we cannot do this with the spkane/scratch-helloworld
image, because it does not contain a shell or SSH. This means that we can’t
use ssh, nsenter, or docker container exec to examine it.
Earlier, we took advantage of the docker container export
command to create a .tar file that contained a copy of all the files in the
container, but this time around we are going to examine the container’s
filesystem by connecting directly to the Docker server, and then looking into
the container’s filesystem itself. To do this, we need to find out where the
image files reside on the server’s disk.
To determine where on the server our files are actually being stored, run
docker image inspect on the alpine:latest image:
$ docker image inspect alpine:latest
[
{
"Id": "sha256:3fd…353",
"RepoTags": [
"alpine:latest"
],
"RepoDigests": [
"alpine@sha256:7b8…f8b"
],
…
"GraphDriver": {
"Data": {
"MergedDir":
"/var/lib/docker/overlay2/ea8…13a/merged",
"UpperDir":
"/var/lib/docker/overlay2/ea8…13a/diff",
"WorkDir":
"/var/lib/docker/overlay2/ea8…13a/work"
},
"Name": "overlay2"
…
}
}

…
]

And then on the spkane/scratch-helloworld:latest image:
$ docker image inspect spkane/scratch-helloworld:latest
[
{
"Id": "sha256:4fa…06d",
"RepoTags": [
"spkane/scratch-helloworld:latest"
],
"RepoDigests": [
"spkane/scratch-helloworld@sha256:46d…a1d"
],
…
"GraphDriver": {
"Data": {
"LowerDir":
"/var/lib/docker/overlay2/37a…84d/diff:
/var/lib/docker/overlay2/28d…ef4/diff",
"MergedDir":
"/var/lib/docker/overlay2/fc9…c91/merged",
"UpperDir":
"/var/lib/docker/overlay2/fc9…c91/diff",
"WorkDir":
"/var/lib/docker/overlay2/fc9…c91/work"
},
"Name": "overlay2"
…
}
}
…
]

NOTE
In this particular example, we are going to use Docker Desktop running on macOS, but
this general approach will work on most Docker servers. However, you can access your
Docker server via whatever method is easiest.

Since we are using Docker Desktop, we need to use our nsenter trick to
enter the SSH-less virtual machine and explore the filesystem.
$ docker container run --rm -it --privileged --pid=host
debian \
nsenter -t 1 -m -u -n -i sh
/ #

Inside the VM, we should now be able to explore the various directories
listed in the GraphDriver section of the docker image inspect
commands.

In this example, if we look at the first entry for the alpine image will see
that it is labeled MergedDir and lists the folder
/var/lib/docker/overlay2/ea86408b2b15d33ee27d78ff44f82104705286221f055ba1331b586
If we list that directory we will get an error, but from listing the parent
directory we quickly discover that we actually want to look at the diff
directory.
/ # ls -lFa /var/lib/docker/overlay2/ea…3a/merged
ls: /var/lib/docker/overlay2/ea..3a/merged: No such file or
directory
/ # ls -lF /var/lib/docker/overlay2/ea…3a/
total 8
drwxr-xr-x
diff/
-rw-r--r-link

18 root

root

4096 Mar 15 19:27

1 root

root

26 Mar 15 19:27

/ # ls -lF /var/lib/docker/overlay2/ea…3a/diff
total 64
drwxr-xr-x

2 root

root

4096 Jan

9 19:37

bin/
drwxr-xr-x
dev/
drwxr-xr-x
etc/
drwxr-xr-x
home/
drwxr-xr-x
lib/
drwxr-xr-x
media/
drwxr-xr-x
mnt/
dr-xr-xr-x
proc/
drwx-----root/
drwxr-xr-x
run/
drwxr-xr-x
sbin/
drwxr-xr-x
srv/
drwxr-xr-x
sys/
drwxrwxrwt
tmp/
drwxr-xr-x
usr/
drwxr-xr-x
var/

2 root

root

4096 Jan

9 19:37

15 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

5 root

root

4096 Jan

9 19:37

5 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

2 root

root

4096 Jan

9 19:37

7 root

root

4096 Jan

9 19:37

11 root

root

4096 Jan

9 19:37

/ # du -sh /var/lib/docker/overlay2/ea…3a/diff
4.5M
/var/lib/docker/overlay2/ea…3a/diff

Now, alpine happens to be a very small base image, weighing in at only
4.5 MB, and it is ideal for building containers on top of. However, we can see
that there is still a lot of stuff in this container before we have started to build
anything from it.
Now, let’s take a look at the files in the spkane/scratch-helloworld

image. In this case, we want to look at the first directory from the
LowerDir entry of the docker image inspect output, which you’ll
notice also ends in a directory called diff.
/ # ls -lFh /var/lib/docker/overlay2/37…4d/diff
total 3520
-rwxr-xr-x
helloworld*

1 root

root

3.4M Jul

2

2014

/ # exit

You’ll notice that there is only a single file in this directory and it is 3.4 MB.
This helloworld binary is the only file shipped in this container and is smaller
than the starting size of the alpine image before any application files have
been added to it.

NOTE
It is possible to run the helloworld application right from that directory on your
Docker server because it does not require any other files. You really don’t want to do
this on anything but a development box, but it can help drive the point home about how
useful these types of statically compiled applications can be.

Multi-stage builds
There is a way you can constrain containers to an even smaller size in many
cases: multi-stage builds. This is how we recommend that you build most
production containers. You don’t have to worry as much about bringing in
extra resources to build your application, and can still run a lean production
container. Multi-stage containers also encourage doing builds inside of
Docker, which is a great pattern for repeatability in your build system.
As the original author of the scratch-helloworld application has written about,
the release of multi-stage build support in Docker itself has made the process
of creating small containers much easier than it used to be. In the past, to do

the same thing that multi-stage delivers for nearly free, you were required to
build one image that compiled your code, extract the resulting binary, and
then build a second image without all the build dependencies that you would
then inject that binary into. This was often difficult to set up and did not
always work out of the box with standard deployment pipelines.
Today, you can now achieve similar results using a Dockerfile as simple as
this one:
# Build container
FROM docker.io/golang:alpine as builder
RUN apk update && \
apk add git && \
CGO_ENABLED=0 go install -a -ldflags '-s'
github.com/spkane/scratch-helloworld@latest
# Production container
FROM scratch
COPY --from=builder /go/bin/scratch-helloworld /helloworld
EXPOSE 8080
CMD ["/helloworld"]

The first thing you’ll notice about this Dockerfile is that it looks a lot like two
_Dockerfile_s that have been combined into one. Indeed this is the case, but
there is more to it. The FROM command has been extended so that you can
name the image during the build phase. In this example, the first line, which
reads FROM docker.io/golang as builder, means that you want
to base your build on the golang image and will be referring to this build
image/stage as builder.
On the fourth line, you’ll see another FROM line, which was not allowed
before the introduction of multi-stage builds. This FROM line uses a special
image name, called scratch, that tells Docker to start from an empty
image, which includes no additional files. The next line, which reads COPY
--from=builder /go/bin/scratch-helloworld
/helloworld, allows you to copy the binary that you built in the builder
image directly into the current image. This will ensure that you end up with

the smallest container possible.
The EXPOSE 8080line is documentation that is intended to inform users
which port(s) and protocols (TCP is the default protocol) the service listens
on.
Let’s try to build this and see what happens. First, create a directory where
you can work and then, using your favorite text editor, paste the content from
the preceding example into a file called Dockerfile.
$ mkdir /tmp/multi-build
$ cd /tmp/multi-build
$ vi Dockerfile

TIP
You can download a copy of this Dockerfile from bluewhalebook/docker-up-andrunning-3rd-edition.

We can now start the multi-stage build.
$ docker image build .
[+] Building 9.7s (7/7) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 37B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/golang:alpine
=> CACHED [builder 1/2] FROM
docker.io/library/golang:alpine@sha256:7cc6257…
=> [builder 2/2] RUN apk update && apk add git &&
CGO_ENABLED=0 go install …
=> [stage-1 1/1] COPY --from=builder /go/bin/scratchhelloworld /helloworld
=> exporting to image
=> => exporting layers
=> => writing image
sha256:bb853f23418161927498b9631f54692cf11d84d6bde3af2d…

You’ll notice that the output looks like most other builds and still ends by
reporting the successful creation of our final, very minimal image.

WARNING
If you are compiling binaries on your local system that use shared libraries, you need to
be careful to ensure that the correct version of those shared libraries are also available to
the process inside the container.

You are not limited to two stages, and in fact, none of the stages need to even
be related to each other. They will be run in order. You could, for example,
have a stage based on the public Go image that builds your underlying Go
application to serve an API, and another stage based on the Angular container
to build your frontend web UI. The final stage could then combine outputs
from both.

TIP
As you start to build more complex images, you may find that being limited to a single
build context is challenging. The docker-buildx plugin which we discuss near the
end of this chapter is capable of supporting multiple build contexts, which can be used to
support some very advanced workflows.

Layers Are Additive
Something that is not apparent until you dig much deeper into how images
are built is that the filesystem layers that make up your images are strictly
additive by design. Although you can shadow/mask files in previous layers,
you cannot delete those files. In practice, this means that you cannot make
your image smaller by simply deleting files that were generated in earlier
steps.

NOTE
If you enable experimental features on your Docker server, it is possible to squash a
bunch of layers into a single layer using docker image build --squash . This
will deleted files actually disappear and will therefore often recover some wasted space,
but it also means that the whole layer must be downloaded by every system that requires
it, even when only a single line of source code was updated, so there are real tradeoffs to
using this approach.

The easiest way to explain the additive nature of image layers is by using
some practical examples. In a new directory download or create the following
file, which will generate an image that launches the Apache web server
running on Fedora Linux:
FROM docker.io/fedora
RUN dnf install -y httpd
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]

and then build it like this:
$ docker image build .
[+] Building 63.5s (6/6) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 130B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/fedora:latest
=> [1/2] FROM docker.io/library/fedora
=> [2/2] RUN dnf install -y httpd
=> exporting to image
=> => exporting layers
=> => writing image
sha256:543d61c956778b8ea3b32f1e09a9354a864467772e6…

Let’s go ahead and tag the resulting image so that you can easily refer to it in
subsequent commands:

$ docker image tag
sha256:543d61c956778b8ea3b32f1e09a9354a864467772e6… size1

Now let’s take a look at our image with the docker image history
command. This command will give us some insight into the filesystem layers
and build steps that our image uses.
$ docker image history size1
IMAGE
CREATED
SIZE …
543d61c95677 About a minute ago
DFOREGROUND… 0B
<missing>
About a minute ago
-y httpd … 273MB
<missing>
6 weeks ago
["/bin/bash"]… 0B
<missing>
6 weeks ago
file:58865512c… 163MB
<missing>
3 months ago
DISTTAG=f36co… 0B
<missing>
15 months ago
maintainer=… 0B

CREATED BY
CMD ["/usr/sbin/httpd" "RUN /bin/sh -c dnf install
/bin/sh -c #(nop)

CMD

/bin/sh -c #(nop) ADD
/bin/sh -c #(nop)

ENV

/bin/sh -c #(nop)

LABEL

You’ll notice that three of the layers added no size to our final image, but two
of them increase the size a great deal. The layer that is 163 MB makes sense,
as this is the base Fedora image that includes a minimal Linux distribution;
however, the 273 MB layer is surprising. The Apache web server shouldn’t
be nearly that large, so what’s going on here, exactly?
If you have experience with package managers like apk, apt, dnf, or yum,
then you may know that most of these tools rely heavily on a large cache that
includes details about all the packages that are available for installation on the
platform in question. This cache uses up a huge amount of space and is
completely useless once you have installed the packages you need. The most
obvious next step is to simply delete the cache. On Fedora systems, you could
do this by editing your Dockerfile so that it looks like this:
FROM docker.io/fedora

RUN dnf install -y httpd
RUN dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]

and then building, tagging, and examining the resulting image:
$ docker image build .
[+] Building 0.5s (7/7) FINISHED
…
=> => writing image
sha256:b6bf99c6e7a69a1229ef63fc086836ada20265a793cb8f2d…
$ docker image tag
sha256:b6bf99c6e7a69a1229ef63fc086836ada20265a793cb8f2d17…
IMAGE
CREATED
CREATED BY
SIZE …
b6bf99c6e7a6 About a minute ago CMD ["/usr/sbin/httpd" "DFOREGROUND… 0B
<missing>
About a minute ago RUN /bin/sh -c dnf clean
all # build… 71.8kB
<missing>
10 minutes ago
RUN /bin/sh -c dnf install
-y httpd … 273MB
<missing>
6 weeks ago
/bin/sh -c #(nop) CMD
["/bin/bash"]… 0B
<missing>
6 weeks ago
/bin/sh -c #(nop) ADD
file:58865512c… 163MB
<missing>
3 months ago
/bin/sh -c #(nop) ENV
DISTTAG=f36co… 0B
<missing>
15 months ago
/bin/sh -c #(nop) LABEL
maintainer=… 0B

If you look carefully at the output from the docker image history
command, you’ll notice that you have created a new layer that adds 71.8kB to
the image, but you have not decreased the size of the problematic layer at all.
What is happening exactly?
The important thing to understand is that image layers are strictly additive in
nature. Once a layer is created, nothing can be removed from it. This means
that you cannot make earlier layers in an image smaller by deleting files in
subsequent layers. When you delete or edit files in subsequent layers, you’re

simply masking the older version with the modified or removed version in
the new layer. This means that the only way you can make a layer smaller is
by removing files before you save the layer.
The most common way to deal with this is by stringing commands together
on a single Dockerfile line. You can do this very easily by taking advantage
of the && operator. This operator acts as a Boolean AND statement and
basically translates into English as “and if the previous command ran
successfully, run this command.” In addition to this, you can also take
advantage of the / operator, which is used to indicate that a command
continues after the newline. This can help improve the readability of long
commands.
With this knowledge in hand, you can rewrite the Dockerfile like this:
FROM docker.io/fedora
RUN dnf install -y httpd && \
dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]

Now you can rebuild the image and see how this change has impacted the
size of the layer that includes the http daemon:
$ docker image build .
[+] Building 0.5s (7/7) FINISHED
…
=> => writing image
sha256:14fe7924bb0b641ddf11e08d3dd56f40aff4271cad7a421fe…
$ docker image tag
sha256:14fe7924bb0b641ddf11e08d3dd56f40aff4271cad7a421fe9b…
IMAGE
CREATED
CREATED BY
SIZE
…
14fe7924bb0b About a minute ago CMD ["/usr/sbin/httpd" "DFOREGROUND"… 0B
<missing>
About a minute ago RUN /bin/sh -c dnf install
-y httpd &… 44.8MB
<missing>
6 weeks ago
/bin/sh -c #(nop) CMD
["/bin/bash"] … 0B
<missing>
6 weeks ago
/bin/sh -c #(nop) ADD

file:58865512ca… 163MB
<missing>
3 months ago
DISTTAG=f36con… 0B
<missing>
15 months ago
maintainer=C… 0B

/bin/sh -c #(nop)

ENV

/bin/sh -c #(nop)

LABEL

In the first two examples, the layer in question was 273 MB in size, but now
that you have removed many unnecessary files that were added to that layer,
you can shrink the layer down to 44.8 MB. This is a very large saving of
space, especially when you consider how many servers might be pulling the
image down during any given deployment.

Utilizing the Layer Cache
The final building technique that we will cover here is related to keeping
build times as fast as possible. One of the important goals of the DevOps
movement is to keep feedback loops as tight as possible. This means that it is
important to try to ensure that problems are discovered and reported as
quickly as possible so that they can be fixed when people are still completely
focused on the code in question and haven’t moved on to other unrelated
tasks.
During any standard build process, Docker uses a layer cache to try to avoid
rebuilding any image layers that it has already built and that do not contain
any noticeable changes. Because of this cache, the order in which you do
things inside your Dockerfile can have a dramatic impact on how long your
builds take on average.
For starters let’s take the Dockerfile from the previous example and
customize it just a bit so that it looks like this:

TIP
Like the other examples, you can also find these files at
github.com/bluewhalebook/docker-up-and-running-3rd-edition.

FROM docker.io/fedora
RUN dnf install -y httpd && \
dnf clean all
RUN mkdir -p /var/www && \
mkdir -p /var/www/html
ADD index.xhtml /var/www/html
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]

Now, in the same directory, let’s also create a new file called index.xhtml that
looks like this:
<html>
<head>
<title>My custom Web Site</title>
</head>
<body>
<p>Welcome to my custom Web Site</p>
</body>
</html>

For the first test, let’s time the build without using the Docker cache at all, by
using the following command:
$ time docker image build --no-cache .
time docker image build --no-cache .
[+] Building 48.3s (9/9) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 238B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/fedora:latest
=> CACHED [1/4] FROM docker.io/library/fedora
=> [internal] load build context
=> => transferring context: 32B
=> [2/4] RUN dnf install -y httpd &&
dnf clean all
=> [3/4] RUN mkdir -p /var/www &&
mkdir -p
/var/www/html
=> [4/4] ADD index.xhtml /var/www/html
=> exporting to image
=> => exporting layers

=> => writing image
sha256:7f94d0d6492f2d2c0b8576f0f492e03334e6a535cac85576c…
real
user
sys

1m21.645s
0m0.428s
0m0.323s

TIP
Windows users should be able to run this command in a WSL2 session or use the
PowerShell Measure-Command function to replace the Unix time command used in
these examples.

The output from the time command tells us that the build without the cache
took about a minute and 21 seconds and only pulled the base image from
layer cache. If you rebuild the image immediately afterward and allow
Docker to use the cache, you will see that the build is very fast.
$ time docker image build .
[+] Building 0.1s (9/9) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 37B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/fedora:latest
=> [1/4] FROM docker.io/library/fedora
=> [internal] load build context
=> => transferring context: 32B
=> CACHED [2/4] RUN dnf install -y httpd &&
dnf clean
all
=> CACHED [3/4] RUN mkdir -p /var/www &&
mkdir -p
/var/www/html
=> CACHED [4/4] ADD index.xhtml /var/www/html
=> exporting to image
=> => exporting layers
=> => writing image
sha256:0d3aeeeeebd09606d99719e0c5197c1f3e59a843c4d7a21af…

real
user
sys

0m0.416s
0m0.120s
0m0.087s

Since none of the layers changed, and the cache could be fully leveraged for
all four build steps, the build took only a fraction of a second to complete.
Now, let’s make a small improvement to the index.xhtml file so that it looks
like this:
<html>
<head>
<title>My custom Web Site</title>
</head>
<body>
<div align="center">
<p>Welcome to my custom Web Site!!!</p>
</div>
</body>
</html>

and then let’s time the rebuild again:
$ time docker image build .
[+] Building 0.1s (9/9) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 37B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/fedora:latest
=> [internal] load build context
=> => transferring context: 214B
=> [1/4] FROM docker.io/library/fedora
=> CACHED [2/4] RUN dnf install -y httpd &&
dnf clean
all
=> CACHED [3/4] RUN mkdir -p /var/www &&
mkdir -p
/var/www/html
=> [4/4] ADD index.xhtml /var/www/html
=> ADD index.xhtml /var/www/html
=> exporting to image

=> => exporting layers
=> => writing image
sha256:daf792da1b6a0ae7cfb2673b29f98ef2123d666b8d14e0b74…
real
user
sys

0m0.456s
0m0.120s
0m0.068s

If you look at the output carefully, you will see that the cache was used for
most of the build. It wasn’t until step 4/4 when Docker needed to copy
index.xhtml, that the cache was invalidated and the layers had to be recreated.
Because the cache could be used for most of the build, the build still did not
exceed a second.
But what would happen if you changed the order of the commands in the
Dockerfile so that they looked like this:
FROM docker.io/fedora
RUN mkdir -p /var/www && \
mkdir -p /var/www/html
ADD index.xhtml /var/www/html
RUN dnf install -y httpd && \
dnf clean all
CMD ["/usr/sbin/httpd", "-DFOREGROUND"]

Let’s quickly time another test build without the cache to get a baseline:
$ time docker image build --no-cache .
[+] Building 51.5s (9/9) FINISHED
…
=> => writing image
sha256:1cc5f2c5e4a4d1cf384f6fb3a34fd4d00e7f5e7a7308d5f1f…
real
user
sys

0m51.859s
0m0.237s
0m0.159s

In this case, the build took 51 seconds to complete, as since we used the -no-cache argument we know that nothing was pulled from the layer cache,

except for the base image. The difference in time from the very first test is
entirely due to fluctuating network speeds and has nothing to do with the
changes that you have made to the Dockerfile.
Now, let’s edit index.xhtml again like so:
<html>
<head>
<title>My custom Web Site</title>
</head>
<body>
<div align="center" style="font-size:180%">
<p>Welcome to my custom Web Site</p>
</div>
</body>
</html>

And now, let’s time the image rebuild, while using the cache:
$ time docker image build .
[+] Building 43.4s (9/9) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 37B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for
docker.io/library/fedora:latest
=> [1/4] FROM docker.io/library/fedora
=> [internal] load build context
=> => transferring context: 233B
=> CACHED [2/4] RUN mkdir -p /var/www &&
mkdir -p
/var/www/html
=> [3/4] ADD index.xhtml /var/www/html
=> [4/4] RUN dnf install -y httpd &&
dnf clean all
=> exporting to image
=> => exporting layers
=> => writing image
sha256:9a05b2d01b5870649e0ad1d7ad68858e0667f402c8087f0b4…
real
user

0m43.695s
0m0.211s

sys

0m0.133s

The first time that you rebuilt the image, after editing the index.xhtml file, it
took only .456 seconds, but this time it took 43.695 seconds, almost exactly
as long as it took to build the whole image without using the cache at all.
This is because you have modified the Dockerfile so that the index.xhtml file
is copied into the image very early in the process. The problem with doing it
this way is that the index.xhtml file changes frequently and will often
invalidate the cache. The other issue is that it is unnecessarily placed before a
very time-consuming step in our Dockerfile: installing the Apache web
server.
The important lesson to take away from all of this is that order matters, and in
general, you should always try to order your Dockerfile so that the most
stable and time-consuming portions of your build process happen first and
your code is added as late in the process as possible.
For projects that require you to install dependencies based on your code using
tools like npm and bundle, it is also a good idea to do some research about
optimizing your Docker builds for those platforms. This often includes
locking down your dependency versions and storing them along with your
code so that they do not need to be downloaded for each and every build.

Directory Caching
One of the many features that BuildKit adds to the image-building experience
is directory caching. Directory caching is an incredibly useful tool for
speeding up build times without saving a lot of files that are unnecessary for
the runtime into your image. In essence, it allows you to save the contents of
a directory inside your image in a special layer that can be bind-mounted at
build-time and then unmounted before the image snapshot is made. This is
often used to handle directories where tools like, Linux software installers
(apt, apk, dnf, etc.), and languages dependency managers (npm,
bundler, pip, etc.), download their databases and archive files.

TIP
If you are unfamiliar with bind mounts and what they are, you can find a [bind mount
overview](https://docs.docker.com/storage/bind-mounts/) in the Docker documentation.

To make use of directory caching, you must have BuildKit enabled. In most
circumstances, this should already be the case, but you can force it from the
client-side, by setting ht environment variable DOCKER_BUILDKIT= to
+1.
export DOCKER_BUILDKIT=1

Let’s explore directory caching by checking out the following git repository
and seeing how utilizing directory caching can significantly improve
consecutive builds while still keeping the resulting image sizes smaller.
$ git clone https://github.com/spkane/open-mastermind.git \
--config core.autocrlf=input
$ cd open-mastermind
$ cat Dockerfile
FROM python:3.9.15-slim-bullseye
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN pip install -r requirements.txt
WORKDIR /app/mastermind
CMD ["python", "mastermind.py"]

This codebase has a very generic Dockerfile checked into the repo. Let’s go
ahead and see how long it takes to build this image, with and without the
layer cache, and let’s also examine how large the resulting image is.
$ time docker build --no-cache -t docker.io/spkane/openmastermind:latest .

[+] Building 67.5s (12/12) FINISHED
…
=> => naming to docker.io/spkane/open-mastermind:latest
0.0s
real
user
sys

0m28.934s
0m0.222s
0m0.248s

$ docker image ls --format "{{ .Size }}" spkane/openmastermind:latest
293MB
$ time docker build -t docker.io/spkane/openmastermind:latest .
[+] Building 1.5s (12/12) FINISHED
…
=> => naming to docker.io/spkane/open-mastermind:latest
0.0s
real
user
sys

0m1.083s
0m0.098s
0m0.095s

From this output, we can see that this image takes just under 29 seconds to
build without the layer cache, and takes just under 2 seconds to build when it
can fully utilize the layer cache. The resulting image size is 293MB in total.

TIP
BuildKit finally has support for modifying or completely disabling the colors used for
the output. This is particularly nice for anyone who uses a dark background in their
terminal. You can configure these colors by setting something like this export
BUILDKIT_COLORS=run=green:warning=yellow:error=red:cancel=c
yan in your environment, or you can completely disable the colors by setting export
NO_COLOR=true.
Note, that the BuildKit version used in various docker components and 3rd party tools
is still being updated, so it might not work yet in every situation.

If you want to test the build, go ahead and run it.
$ docker container run -ti --rm docker.io/spkane/openmastermind:latest

This will launch a terminal-based open source version of the Mastermind
game. There are on-screen directions for the game and as a fallback, you can
always exit by typing [Control-C].
Since this is a Python application, it uses requirements.txt to list all of the
libraries that the application requires, and then the pip application is used in
_Dockerfile+ to install these dependencies.

NOTE
We are installing some unnecessary dependencies simply to make the benefits of
directory caching more obvious.

Go ahead and open up the requirements.txt file and add a line that reads “logsymbols”, so that it looks like this:
colorama
# These are not required - but are used for demonstration
purposes
pandas
flask
log-symbols

Let’s rerun the build now.
$ time docker build -t docker.io/spkane/openmastermind:latest \
--progress=plain .
#1 [internal] load build definition from Dockerfile
…
#9 [5/6] RUN pip install -r requirements.txt

#9
sha256:82dbc10f1bb9fa476d93cc0d8104b76f46af8ece7991eb55393d
6d72a230919e
#9 1.954 Collecting colorama
#9 2.058
Downloading colorama-0.4.5-py2.py3-none-any.whl
(16 kB)
…
real
0m16.379s
user
0m0.112s
sys
0m0.082s

If you look at the full output for step “5/6”, you will notice that all the
dependencies are downloaded again, even though pip would normally have
most of those dependencies cached in /root/.cache. This inefficiency is
because the builder sees that we have made a change that impacts this layer,
and therefore completely recreates the layer, so we lose that cache, even
though we had it stored in the image layer.
Let’s go ahead and improve this situation. To do this we need to leverage the
BuildKit directory cache, and to do that we need to make a few changes to
the Dockerfile, so that it looks like this:
# syntax=docker/dockerfile:1
FROM python:3.9.15-slim-bullseye
RUN mkdir /app
WORKDIR /app
COPY . /app
RUN --mount=type=cache,target=/root/.cache pip install -r
requirements.txt
WORKDIR /app/mastermind
CMD ["python", "mastermind.py"]

There are two important changes in there. First, we added the line:
# syntax=docker/dockerfile:1

This tells Docker that we are going to use a newer version of the Dockerfile
frontend, which provides us with access to BuildKit;s new features.

Then we edited the RUN line to look like this:
RUN --mount=type=cache,target=/root/.cache pip install -r
requirements.txt

This line tells BuildKit to mount a caching layer into the container at
/root/.cache for the duration of this one build step. This will accomplish two
goals for us. It will remove the contents of that directory from the resulting
image, and it will also be re-mounted and available to pip in consecutive
builds.
Let’s go ahead and do a full rebuild of the image with these changes, to
generate the initial cache directory contents. If you follow the output you will
see that pip downloads all the dependencies, exactly as before.
$ time docker build --no-cache -t docker.io/spkane/openmastermind:latest .
[+] Building 15.2s (15/15) FINISHED
…
=> => naming to docker.io/spkane/open-mastermind:latest
0.0s
…
real
0m15.493s
user
0m0.137s
sys
0m0.096s

So, now let’s open up the requirements.txt file and add a line that reads “pyevents”.
colorama
# These are not required - but are used for demonstration
purposes
pandas
flask
log-symbols
py-events

This is where the changes pay off. When we rebuild the image now, we will

see that py-events and its’ dependencies are the only things that are
downloaded, everything else uses the existing cache from our previous build,
which has been mounted into the image for this build step.
$ time docker build -t docker.io/spkane/openmastermind:latest \
--progress=plain .
#1 [internal] load build definition from Dockerfile
…
#14 [stage-0 5/6] RUN -mount=type=cache,target=/root/.cache pip install …
#14
sha256:9bc72441fdf2ec5f5803d4d5df43dbe7bc6eeef88ebee98ed18d
8dbb478270ba
#14 1.711 Collecting colorama
#14 1.714
Using cached colorama-0.4.5-py2.py3-noneany.whl (16 kB)
…
#14 2.236 Collecting py-events
#14 2.356
Downloading py_events-0.1.2-py3-none-any.whl
(5.8 kB)
…
#16 DONE 1.4s
real
user
sys

0m12.624s
0m0.180s
0m0.112s

$ docker image ls --format "{{ .Size }}" spkane/openmastermind:latest
261MB

The build time has shrunk since there is no longer a need to re-download
everything each time and the image size is also 32MB smaller, even though
we have added new dependencies to the image. This is simply because the
cache directory is no longer stored directly in the image that contains the
application.
BuildKit and the new Dockerfile frontends bring a lot of very useful features

to the image-building process that you will want to be aware of. We highly
recommend that you take the time to read through the reference guide and
become acquainted with all the available capabilities.

Troubleshooting Broken Builds
We normally expect builds to just work, especially when we’ve scripted
them, but in the real world things go wrong. Let’s spend a little bit of time
discussing what you can do to troubleshoot a Docker build that is failing.. In
this section, we will explore two options. One that works with the preBuildKit approach to image building and one that works with BuildKit.
For this demonstration, we are going to reuse the docker-hello-node
repo from earlier in the chapter. If required, you can clone it again, like this:
$ git clone https://github.com/spkane/docker-node-hello.git
\
--config core.autocrlf=input
Cloning into 'docker-node-hello'…
remote: Counting objects: 41, done.
remote: Total 41 (delta 0), reused 0 (delta 0), pack-reused
41
Unpacking objects: 100% (41/41), done.
$ cd docker-node-hello

Debugging Pre-BuildKit Images
We need a patient for the next set of exercises, so let’s create a failing build.
To do that, edit the Dockerfile so that the line that reads:
RUN apt-get -y update

now reads:
RUN apt-get -y update-all

WARNING
If you are using Powershell on Windows you will likely need to set the environment
variable that disables BuildKit before running docker image build command
below and then reset it afterward.
PS C:\> $env:DOCKER_BUILDKIT = 0
PS C:\> docker image build -t example/docker-node-hello:latest -no-cache .
PS C:\> $env:DOCKER_BUILDKIT = 1

If you try to build the image now, you should get the following error:
$ DOCKER_BUILDKIT=0 docker image build -t example/dockernode-hello:latest \
--no-cache .
Sending build context to Docker daemon 9.216kB
Step 1/14 : FROM docker.io/node:18.13.0
---> 9ff38e3a6d9d
…
Step 6/14 : ENV SCPATH /etc/supervisor/conf.d
---> Running in e903367eaeb8
Removing intermediate container e903367eaeb8
---> 2a236efc3f06
Step 7/14 : RUN apt-get -y update-all
---> Running in c7cd72f7d9bf
E: Invalid operation update-all
The command '/bin/sh -c apt-get -y update-all' returned a
non-zero code: 100

So, how can we troubleshoot this, especially if we are not developing on a
Linux system? The real trick here is to remember that almost all Docker
images are layered on top of other Docker images and that you can start a
container from any image. Although the meaning is not obvious on the
surface, if you look at the output for step 6, you will see this:

Step 6/14 : ENV SCPATH /etc/supervisor/conf.d
---> Running in e903367eaeb8
Removing intermediate container e903367eaeb8
---> 2a236efc3f06

The first line that reads Running in e903367eaeb8 is telling you that
the build process has started a new container, based on the image created in
step 5. The next line, which reads Removing intermediate
container e903367eaeb8, is telling you that Docker is now removing
the container, after having altered it based on the instruction in step 6. In this
case, it was simply adding a default environment variable via ENV SCPATH
/etc/supervisor/conf.d. The final line, which reads --→
2a236efc3f06, is the one we really care about because this is giving us
the image ID for the image that was generated by step 6. You need this to
troubleshoot the build because it is the image from the last successful step in
the build.
With this information, it is possible to run an interactive container so that you
can try to determine why your build is not working properly. Remember that
every container image is based on the image layers below it. One of the great
benefits of that is that we can just run the lower layer as a container itself,
using a shell to look around!
$ docker container run --rm -ti 2a236efc3f06 /bin/bash
root@b83048106b0f:/#

From inside the container, you can now run any commands that you might
need to determine what is causing your build to fail and what you need to do
to fix your Dockerfile.
root@b83048106b0f:/# apt-get -y update-all
E: Invalid operation update-all
root@b83048106b0f:/# apt-get --help
apt 1.4.9 (amd64)
…

Most used commands:
update - Retrieve new lists of packages
…
root@b83048106b0f:/# apt-get -y update
Get:1 http://security.debian.org/debian-security
stretch/updates … [53.0 kB]
…
Reading package lists… Done
root@b83048106b0f:/# exit
exit

Once the root cause has been determined, the Dockerfile can be fixed, so that
RUN apt-get -y update-all now reads RUN apt-get -y
update, and then rebuilding the image should result in success.
$ DOCKER_BUILDKIT=0 docker image build -t example/dockernode-hello:latest .
Sending build context to Docker daemon 15.87kB
…
Successfully built 69f5e83bb86e
Successfully tagged example/docker-node-hello:latest

Debugging BuildKit Images
When using BuildKit we have to take a slightly different approach to get
access to the point where the build fails, because none of the intermediate
build layers are exported from the build container to the Docker daemon.
The options for debugging BuildKit will almost certainly evolve as we move
forward, but let’s take a look at one approach that works now.
Assuming that the Dockerfile has been reverted to its’ original state, let’s
change the line that reads:
RUN npm install

so, that it now reads:

RUN npm installer

and then attempt to build the image:

TIP
Make sure that you have BuildKit enabled!

$ docker image build -t example/docker-node-hello:debug -no-cache .
[+] Building 51.7s (13/13) FINISHED
=> [internal] load build definition from Dockerfile
0.0s
…
=> [7/8] WORKDIR /data/app
0.0s
=> ERROR [8/8] RUN npm installer
0.4s
______
> [8/8] RUN npm installer:
#13 0.399
#13 0.399 Usage: npm <command>
…
#13 0.402 Did you mean one of these?
#13 0.402
install
#13 0.402
install-test
#13 0.402
uninstall
______
executor failed running [/bin/sh -c npm installer]: exit
code: 1

We see an error as we expected, but how are we going to get access to that
layer, so that we can troubleshoot this?
One approach that works is to leverage multi-stage builds and the -target argument of docker image build.
Let’s start by modifying the Dockerfile in two places. Change this line:

FROM docker.io/node:18.13.0

so that it now reads:
FROM docker.io/node:18.13.0 as deploy

and then immediately before the line that causes the error, we are going to
add a new FROM line:
FROM deploy
RUN npm installer

By doing this we are creating a multi-stage build, where the first stage
contains all of the steps that we know are working and the second stage starts
with our problematic step.
If we try to rebuild this using the same command as before it will still fail.
$ docker image build -t example/docker-node-hello:debug .
[+] Building 51.7s (13/13) FINISHED
…
executor failed running [/bin/sh -c npm installer]: exit
code: 1

So, instead of doing that, let’s tell Docker that we only want to build the first
image in our multi-stage Dockerfile.
$ docker image build -t example/docker-node-hello:debug -target deploy .
[+] Building 0.8s (12/12) FINISHED
=> [internal] load build definition from Dockerfile
0.0s
=> => transferring dockerfile: 37B
0.0s
…
=> exporting to image
0.1s

=> => exporting layers
0.1s
=> => writing image
sha256:a42dfbcfc7b18ee3d30ace944ad4134ea2239a2c0 0.0s
=> => naming to docker.io/example/docker-node-hello:debug
0.0s

Now, we can create a container from this image and do whatever testing we
require.
$ docker container run --rm -ti docker.io/example/dockernode-hello:debug \
/bin/bash
root@17807997176e:/data/app# ls
index.js package.json
root@17807997176e:/data/app# npm install
…
added 18 packages from 16 contributors and audited 18
packages in 1.248s
…
root@17807997176e:/data/app# exit
exit

And then once we understand what is wrong with the Dockerfile we can
revert our debugging changes and fix the npm line so that the whole build
works as expected.

Multi-Architecture Builds
Since the launch of Docker, the amd64/x86\_64 architecture has been the
primary platform that most containers have targeted. However, this has
started to change significantly. More and more developers are using systems
based on the arm64/aarch64 and cloud companies are starting to make ARMbased virtual machines available through their platforms, due to the lowercomputing costs associated with the ARM platform.

This can cause some interesting challenges for anyone who needs to build
and maintain images that will target multiple architectures. How can you
maintain a single, streamlined codebase and pipeline while still supporting all
of these different targets?
Luckily, Docker has released a plugin for the docker CLI, called buildx,
which can help make this process pretty straightforward. In many cases,
docker-buildx will already be installed on your system, and you can
verify this like so:
$ docker buildx version
github.com/docker/buildx v0.9.1
ed00243a0ce2a0aee75311b06e32d33b44729689

TIP
If you need to install the plugin you can follow the directions from the Github repo.

By default docker-buildx will leverage QEMU-based virtualization and
binfmt_misc to support architectures that differ from the underlying system.
This may already be set up on your Linux system, but just in case, it is a good
idea to run the following command when you are first setting up a new
Docker server, just to ensure that the QEMU files are properly registered and
up-to-date.
$ docker container run --rm --privileged multiarch/qemuuser-static \
--reset -p yes
Setting /usr/bin/qemu-alpha-static as binfmt interpreter
for alpha
Setting /usr/bin/qemu-arm-static as binfmt interpreter for
arm
Setting /usr/bin/qemu-armeb-static as binfmt interpreter
for armeb
…
Setting /usr/bin/qemu-aarch64-static as binfmt interpreter

for aarch64
Setting /usr/bin/qemu-aarch64_be-static as binfmt
interpreter for aarch64_be
…

Unlike the original embedded Docker build functionality, which ran directly
on the server, BuildKit can utilize a build container when it builds images,
which means that there is a lot of functional flexibility that can be delivered
with that build container. In the next step, we are going to create a default
buildx container called builder.
If you have an existing buildx container by this name, you
can either remove it by running +docker buildx rm builder+
or you can change the name in the upcoming +docker buildx
create+ command.

With the next two commands, we are going to create the build container, set
it as the default, and then start it up.
$ docker buildx create --name builder --driver dockercontainer --use
builder
$ docker buildx inspect --bootstrap
[+] Building 9.6s (1/1) FINISHED
=> [internal] booting buildkit
9.6s
=> => pulling image moby/buildkit:buildx-stable-1
8.6s
=> => creating container buildx_buildkit_builder0
0.9s
Name:
builder
Driver: docker-container
Nodes:
Name:
Endpoint:
Status:
Buildkit:
Platforms:

builder0
unix:///var/run/docker.sock
running
v0.10.5
linux/amd64, linux/amd64/v2, linux/arm64,

linux/riscv64,
linux/ppc64le, linux/s390x, linux/386,
linux/mips64le,
linux/mips64, linux/arm/v7, linux/arm/v6

For this example, let’s go ahead and download the wordchain git
repository, which contains a useful tool that can generate random and
deterministic word sequences to help with dynamic naming needs.
$ git clone https://github.com/spkane/wordchain.git\
--config core.autocrlf=input
$ cd wordchain

Let’s go ahead and take a look at the included Dockerfile. You’ll notice that it
is a pretty normal multi-stage Dockerfile and does not have anything special
in it related to the platform architecture.
FROM golang:1.18-alpine3.15 AS build
RUN apk --no-cache add \
bash \
gcc \
musl-dev \
openssl
ENV CGO_ENABLED=0
COPY . /build
WORKDIR /build
RUN go install github.com/markbates/pkger/cmd/pkger@latest
&& \
pkger -include /data/words.json && \
go build .
FROM alpine:3.15 AS deploy
WORKDIR /
COPY --from=build /build/wordchain /

USER 500
EXPOSE 8080
ENTRYPOINT ["/wordchain"]
CMD ["listen"]

In the first step, we are going to build our statically-compiled Go binary, and
then in the second step, we are going to package it up into a small
deployment image.

NOTE
The ENTRYPOINT instruction in the Dockerfile is an advanced instruction that allows
you to separate the default process that is run by the container (ENTRYPOINT) from
the command line arguments that are passed to that process (CMD). When
ENTRYPOINT is missing from the Dockerfile the CMD instruction is expected to
contain both the process and all the required command line arguments.

We can go ahead and build this image and sideload it into our local Docker
server by running the following command:
$ docker buildx build --tag wordchain:test --load .
[+] Building 2.4s (16/16) FINISHED
=> [internal] load .dockerignore
0.0s
=> => transferring context: 93B
0.0s
=> [internal] load build definition from Dockerfile
0.0s
=> => transferring dockerfile: 461B
0.0s
…
=> exporting to oci image format
0.3s
=> => exporting layers
0.0s
=> => exporting manifest

sha256:4bd1971f2ed820b4f64ffda97707c27aac3e8eb7 0.0s
=> => exporting config
sha256:ce8f8564bf53b283d486bddeb8cbb074ff9a9d4ce9 0.0s
=> => sending tarball
0.2s
=> importing to docker
0.0s

We can quickly test out the image by running the following commands.
$ docker container run wordchain:test random
witty-stack
$ docker container run wordchain:test random -l 3 -d .
odd.goo
$ docker container run wordchain:test --help
wordchain is an application that can generate a readable
chain
of customizable words for naming things like
containers, clusters, and other objects.
…

As long as you got some random word pairs back with the first two
commands, then everything is working as expected.
Now, to build this image for multiple architectures, we need to simply add
the --platform argument to our build.

NOTE
Typically we would also replace --load with --push, which would push all the
resulting images to the tagged repository, but in this case, we need to simply remove -load, because the Docker server can not load images for multiple platforms at the
moment, and we do not have a repository setup to push these images to. If we did have a
repository and we tagged the images correctly, then we could very easily build and push
all the resulting images in one step, with a command like this:

docker buildx build --platform linux/amd64,linux/arm64 -tag docker.io/spkane/wordchain:latest --push .

Building this image for both the linux/amd64 and the linux/arm64 platforms
can be accomplished like this:
$ docker buildx build --platform linux/amd64,linux/arm64 \
--tag wordchain:test .
[+] Building 114.9s (23/23) FINISHED
…
=> [linux/arm64 internal] load metadata for
docker.io/library/alpine:3.1 2.7s
=> [linux/amd64 internal] load metadata for
docker.io/library/alpine:3.1 2.7s
=> [linux/arm64 internal] load metadata for
docker.io/library/golang:1.1 3.0s
=> [linux/amd64 internal] load metadata for
docker.io/library/golang:1.1 2.8s
…
=> CACHED [linux/amd64 build 5/5] RUN go install
github.com/markbates/pk 0.0s
=> CACHED [linux/amd64 deploy 2/3] COPY --from=build
/build/wordchain / 0.0s
=> [linux/arm64 build 5/5] RUN go install
github.com/markbates/pkger/c 111.7s
=> [linux/arm64 deploy 2/3] COPY --from=build
/build/wordchain /
0.0s
WARNING: No output specified with docker-container driver.
Build result will
only remain in the build cache. To push result
image into registry
use --push or to load image into docker use --load

NOTE
Due to the emulation that is required when building images for non-native architectures
you may notice that some steps take much longer than normal. This is to be expected
due to the additional computational overhead from the emulation.

It is possible to set up Docker so that it will build each image on a worker with a
matching architecture, which should speed things up significantly in many cases. You
can find some information about this within this Docker blog article.

In the output for the build you will notice lines that start with something like
⇒ \[linux/amd64 *\] or ⇒ \[linux/arm64 *\]. Each of these lines represents
the builder working on this build step for the stated platform. Many of these
steps will run in parallel, and due to caching and other considerations, each
build might progress at differing speeds.
Since we did not add --push to our build, you will also notice that we
received a warning at the end of the build. This is because the dockercontainer driver that the builder is using just left everything in the build
cache, which means that we can’t run the resulting images, at this point, we
can only feel confident that the build is working.

TIP
There are a few build arguments that are automatically set by Docker which can be
especially helpful to leverage inside your Dockerfile when you are doing multiarchitecture builds. As an example, TARGETARCH is frequently used, to make sure that
a given build step downloads the correct pre-built binary for the current image’s
platform.

So, when we upload this image to a repository, how does Docker know which
image to use for the local platform? This information is provided to the
Docker server, through something called an image manifest. We can look at
the manifest for docker.io/spkane/workdchain by running:
$ docker manifest inspect docker.io/spkane/wordchain:latest
{
"schemaVersion": 2,
"mediaType":
"application/vnd.docker.distribution.manifest.list.v2+json"

,
"manifests": [
{
"mediaType":
"application/vnd.docker.distribution.manifest.v2+json",
"size": 739,
"digest": "sha256:4bd1…bfc0",
"platform": {
"architecture": "amd64",
"os": "linux"
}
},
{
…
"platform": {
"architecture": "arm64",
"os": "linux"
}
},
…
]
}

If you look through the output you will see that there are blocks, which
identify the image that is required for every platform that the image supports.
This is accomplished via the individual digest entries that are then paired with
a platform block. This manifest file is downloaded by the server when it
requires an image, and then after referencing the manifest, the server will
download the correct image for the local platform. This is why our Dockerfile
works at all. Each FROM line lists a base image that we want to use, but it is
the Docker server that utilizes this manifest file to determine exactly which
image to download for each platform that the build is targeting.

Wrap-Up
At this point, you should feel comfortable pretty comfortable with image
creation for Docker and have a solid understanding of many of the core tools
and functionality that you can leverage to streamline your build pipeline. In

the next chapter, we will start to dig into how you can use your images to
create containerized processes for your projects.

1 Don’t Repeat Yourself.
2 This code was originally forked from GitHub.
3 Cloud Native Computing Foundation

Chapter 5. Working with
Containers
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 5th chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
In the previous chapter, we learned how to build a Docker image and the very
basic steps required for running the resulting image within a container. In this
chapter, we’ll first take a look at the history of container technology and then
dive deeper into running containers and exploring the Docker commands that
control the overall configuration, resources, and privileges that your container
receives.

What Are Containers?
You might be familiar with virtualization systems like VMware or KVM that
allow you to run a complete Linux kernel and operating system on top of a
virtualized layer, commonly known as a hypervisor. This approach provides
very strong isolation between workloads because each virtual machine hosts
its own operating system kernel that sits in a separate memory space on top
of a hardware virtualization layer.

Containers are fundamentally different since they all share a single kernel,
and isolation between workloads is implemented entirely within that one
kernel. This is called operating system virtualization. The libcontainer
README provides a good, short definition of a container: “A container is a
self-contained execution environment that shares the kernel of the host
system and which is (optionally) isolated from other containers in the
system.” One of the major advantages of containers is resource efficiency
because you don’t need a whole operating system instance for each isolated
workload. Since you are sharing a kernel, there is one less layer of indirection
between the isolated task and the real hardware underneath. When a process
is running inside a container, there is only a little bit of code that sits inside
the kernel managing the container. Contrast this with a virtual machine where
there would be a second layer running. In a VM, calls by the process to the
hardware or hypervisor would require bouncing in and out of privileged
mode on the processor twice, thereby noticeably slowing down many calls.

NOTE
[libcontainer](https://github.com/opencontainers/runc/tree/main/libcontainer) is a Go
library that is designed to provide a standard interface for managing Linux containers
from applications.

But the container approach does mean that you can only run processes that
are compatible with the underlying kernel. For example, unlike hardware
virtualization provided by technologies like VMware or KVM, Windows
applications cannot run natively inside a Linux container on a Linux host.
Windows applications can, however, run inside Windows containers on a
Windows host. So containers are best thought of as an OS-specific
technology where you can run any of your favorite applications or daemons
that are compatible with the container server’s kernel. When thinking of
containers, you should try very hard to throw out what you might already
know about virtual machines and instead conceptualize a container as a
wrapper around a normal process that runs on the server.

NOTE
In addition to being able to run containers inside virtual machines, it is completely
feasible to run a virtual machine inside a container. If you do this, then it is indeed
possible to run a Windows application inside a Windows VM that is running inside a
Linux container.

History of Containers
It is often the case that a revolutionary technology is an older technology that
has finally arrived in the spotlight. Technology goes in waves, and some of
the ideas from the 1960s are back in vogue. Similarly, Docker is a newer
technology and it has an ease of use that has made it an instant hit, but it
doesn’t exist in a vacuum. Much of what underpins Docker comes from work
done over the last 30 years in a few different areas. We can easily trace the
conceptual evolution of containers from a simple system call that was added
to the Unix kernel in the late 1970s to the modern container tooling that
powers many huge internet firms, like Google, Twitter, and Facebook. It’s
worth taking some time for a quick tour through how the technology evolved
and led to the creation of Docker because understanding this helps you place
it within the context of other things that you might be familiar with.
Containers are not a new idea. They are a way to isolate and encapsulate a
part of the running system. The oldest technology in this area includes the
very first batch processing systems. When using these early computers, the
system would only run one program at a time, switching to run another
program once the previous program had finished or a pre-defined time span
had elapsed. With this design there was enforced isolation: you could make
sure your program didn’t step on anyone else’s program because it was only
possible to run one thing at a time. Although modern computers still switch
tasks constantly, it is incredibly fast and completely unnoticeable to most
users.
We would argue that the seeds for today’s containers were planted in 1979
with the addition of the chroot system call to Version 7 Unix. chroot

restricts a process’s view of the underlying filesystem to a single subtree. The
chroot system call is commonly used to protect the operating system from
untrusted server processes like FTP, BIND, and Sendmail, which are publicly
exposed and susceptible to compromise.
In the 1980s and 1990s, various Unix variants were created with mandatory
access controls for security reasons.1 This meant you had tightly controlled
domains running on the same Unix kernel. Processes in each domain had an
extremely limited view of the system that precluded them from interacting
across domains. A popular commercial version of Unix that implemented this
idea was the Sidewinder firewall built on top of BSDI Unix, but this was not
possible with most mainstream Unix implementations.
That changed in 2000 when FreeBSD 4.0 was released with a new command,
called jail, which was designed to allow shared-environment hosting
providers to easily and securely create a separation between their processes
and those that belonged to each of their customers. FreeBSD jail expanded
chroot’s capabilities and also restricted everything a process could do with
the underlying system and other jailed processes.
In 2004, Sun released an early build of Solaris 10, which included Solaris
containers, which later evolved into Solaris Zones. This was the first major
commercial implementation of container technology and is still used today to
support many commercial container implementations. In 2005 OpenVZ for
Linux was released by the company Virtuozzo, followed in 2007 by HP’s
Secure Resource Partitions for HP-UX, which was later renamed HP-UX
containers. Finally, in 2008, Linux containers (LXC) were released in version
2.6.24 of the Linux kernel. The phenomenal growth of Linux containers
across the community did not truly start to grow until 2013 with the inclusion
of user namespaces in version 3.8 of the Linux kernel and the release of
Docker one month later.
Companies, like Google, which had to deal with scaling applications for
broad internet consumption and/or hosting untrusted user code, started
pushing container technology in the early 2000s to facilitate distributing their
applications across global data centers reliably and securely. A few

companies maintained their own patched Linux kernels with container
support for internal use, but as the need for these features became more
evident within the Linux community, Google contributed some of its work
supporting containers into the mainline Linux kernel.

Creating a Container
So far we’ve started containers using the handy docker container
run command. But docker container run is really a convenience
command that wraps two separate steps into one. The first thing it does is
create a container from the underlying image. We can accomplish this
separately using the docker container create command. The
second thing docker container run does is execute the container,
which we can also do separately with the docker container start
command.
The docker container create and docker container start
commands both contain all the options that pertain to how a container is
initially set up. In Chapter 4, we demonstrated that with the docker
container run command you could map network ports in the underlying
container to the host using the -p/--publish argument, and that -e/-env could be used to pass environment variables into the container.
This only just begins to touch on the array of things that you can configure
when you first create a container. So let’s take a look at some of the options
that docker supports.

Basic Configuration
Let’s start by exploring some of the ways we can tell Docker to configure our
container when we create it.
Container name
When you create a container, it is built from the underlying image, but
various command-line arguments can affect the final settings. Settings

specified in the Dockerfile are always used as defaults, but you can override
many of them at creation time.
By default, Docker randomly names your container by combining an
adjective with the name of a famous person. This results in names like
ecstatic-babbage and serene-albattani. If you want to give your container a
specific name, you can use the --name argument.
$ docker container create --name="awesome-service"
ubuntu:latest sleep 120

After creating this container, you could then start it by using the docker
container start awesome-service. It will automatically exit after
120 seconds, but you could stop it before then by running docker
container stop awesome-service. We will dive a bit more into
each of these commands a little later in the chapter.

WARNING
You can only have one container with any given name on a Docker host. If you run the
preceding command twice in a row, you will get an error. You must either delete the
previous container using docker container rm or change the name of the new
container.

Labels
As mentioned in Chapter 4, labels are key/value pairs that can be applied to
Docker images and containers as metadata. When new Linux containers are
created, they automatically inherit all the labels from their parent image.
It is also possible to add new labels to the containers so that you can apply
metadata that might be specific to that single container.
$ docker container run --rm -d --name has-some-labels \
-l deployer=Ahmed -l tester=Asako \
ubuntu:latest sleep 1000

You can then search for and filter containers based on this metadata, using
commands like docker container ls.
$ docker container ls -a -f label=deployer=Ahmed
CONTAINER ID IMAGE
COMMAND
… NAMES
845731631ba4 ubuntu:latest "sleep 1000" … has-some-labels

You can use the docker container inspect command to see all the
labels that a container has.
$ docker container inspect has-some-labels
…
"Labels": {
"deployer": "Ahmed",
"tester": "Asako"
},
…

Note that this container runs the command sleep 1000, so after 1,000
seconds it will stop running.
Hostname
By default, when you start a container, Docker copies certain system files on
the host, including /etc/hostname, into the container’s configuration directory
on the host,2 and then uses a bind mount to link that copy of the file into the
container. We can launch a default container with no special configuration
like this:
$ docker container run --rm -ti ubuntu:latest /bin/bash

This command uses the docker container run command, which runs
docker container create and docker container start in
the background. Since we want to be able to interact with the container that
we are going to create for demonstration purposes, we pass in a few useful

arguments. The --rm argument tells Docker to delete the container when it
exits, the -t argument tells Docker to allocate a pseudo-TTY, and the -i
argument tells Docker that this is going to be an interactive session, and we
want to keep STDIN open. If there is no ENTRYPOINT defined in the
image, then the final argument in the command is the executable and
command line arguments that we want to run within the container, which in
this case is the ever-useful /bin/bash. If there is an ENTRYPOINT
defined in the image, then the final argument is passed to the
ENTRYPOINT process as a list of command line arguments to that
command.

NOTE
You might have noticed that the above paragraph talks about -i and -t, but the
command is using the argument -ti. There is a lot of Unix history that explains why
this is, but a quick overview can be found online if you are curious.

If we now run the mount command from within the resulting container,
we’ll see something similar to this:
root@ebc8cf2d8523:/# mount
overlay on / type overlay
(rw,relatime,lowerdir=…,upperdir=…,workdir…)
proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)
tmpfs on /dev type tmpfs (rw,nosuid,mode=755)
shm on /dev/shm type tmpfs
(rw,nosuid,nodev,noexec,relatime,size=65536k)
mqueue on /dev/mqueue type mqueue
(rw,nosuid,nodev,noexec,relatime)
devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,
…,ptmxmode=666)
sysfs on /sys type sysfs (ro,nosuid,nodev,noexec,relatime)
/dev/sda9 on /etc/resolv.conf type ext4
(rw,relatime,data=ordered)
/dev/sda9 on /etc/hostname type ext4
(rw,relatime,data=ordered)
/dev/sda9 on /etc/hosts type ext4

(rw,relatime,data=ordered)
devpts on /dev/console type devpts
(rw,nosuid,noexec,relatime,…,ptmxmode=000)
proc on /proc/sys type proc
(ro,nosuid,nodev,noexec,relatime)
proc on /proc/sysrq-trigger type proc
(ro,nosuid,nodev,noexec,relatime)
proc on /proc/irq type proc
(ro,nosuid,nodev,noexec,relatime)
proc on /proc/bus type proc
(ro,nosuid,nodev,noexec,relatime)
tmpfs on /proc/kcore type tmpfs (rw,nosuid,mode=755)
root@ebc8cf2d8523:/#

NOTE
When you see any examples with a prompt that looks something like root@hashID, it
means that you are running a command within the container instead of on the local host.
- There are occasions when a container will have been configured with a different
hostname instead (e.g., using --name on the CLI), but in the default case, it’s the
container ID hash. - It is also possible to change the user that is used inside the container
using --user, but by default, it will be root.

There are quite a few bind mounts in a container, but in this case, we are
interested in this one:
/dev/sda9 on /etc/hostname type ext4
(rw,relatime,data=ordered)

While the device number will be different for each container, the part we care
about is that the mount point is /etc/hostname. This links the container’s
/etc/hostname to the hostname file that Docker has prepared for the container,
which by default contains the container’s ID and is not fully qualified with a
domain name.
We can check this in the container by running the following:
root@ebc8cf2d8523:/# hostname -f

ebc8cf2d8523
root@ebc8cf2d8523:/# exit

NOTE
Don’t forget to exit the container shell to return to the local host when finished.

To set the hostname specifically, we can use the --hostname argument to
pass in a more specific value.
$ docker container run --rm -ti -hostname="mycontainer.example.com" \
ubuntu:latest /bin/bash

Then, from within the container, we’ll see that the fully qualified hostname is
defined as requested.
root@mycontainer:/# hostname -f
mycontainer.example.com
root@mycontainer:/# exit

Domain Name Service
Just like /etc/hostname, the resolv.conf file that configures Domain Name
Service (DNS) resolution is managed via a bind mount between the host and
container.
/dev/sda9 on /etc/resolv.conf type ext4
(rw,relatime,data=ordered)
[Details about the resolve.conf file]
(https://sslhow.com/understanding-etc-resolv-conf-file-inlinux) can be found online.

By default, this is an exact copy of the Docker host’s resolv.conf file. If you
didn’t want this, you could use a combination of the --dns and --dns-

search arguments to override this behavior in the container:
$ docker container run --rm -ti --dns=8.8.8.8 --dns=8.8.4.4
\
--dns-search=example1.com --dns-search=example2.com \
ubuntu:latest /bin/bash

NOTE
If you want to leave the search domain completely unset, then use --dns-search=.

Within the container, you would still see a bind mount, but the file contents
would no longer reflect the host’s resolv.conf, instead, it would now look like
this:
root@0f887071000a:/# more /etc/resolv.conf
nameserver 8.8.8.8
nameserver 8.8.4.4
search example1.com example2.com
root@0f887071000a:/# exit

MAC address
Another important piece of information that you can configure is the media
access control (MAC) address for the container.
Without any configuration, a container will receive a calculated MAC
address that starts with the 02:42:ac:11 prefix.
If you need to specifically set this to a value, you can do so by running
something similar to this:
$ docker container run --rm -ti --macaddress="a2:11:aa:22:bb:33" \
ubuntu:latest /bin/bash

Normally you will not need to do that. But sometimes you want to reserve a

particular set of MAC addresses for your containers to avoid conflicting with
other virtualization layers that use the same private block as Docker.

WARNING
Be very careful when customizing the MAC address settings. It is possible to cause ARP
contention on your network if two systems advertise the same MAC address. If you have
a strong need to do this, try to keep your locally administered address ranges within
some of the official ranges, like x2-xx-xx-xx-xx-xx, x6-xx-xx-xx-xx-xx, xA-xx-xx-xx-xx-xx,
and xE-xx-xx-xx-xx-xx (with x being any valid hexadecimal character).

Storage Volumes
There are times when the default disk space allocated to a container, or the
container’s ephemeral nature, is not appropriate for the job at hand, so you’ll
need storage that can persist between container deployments.

WARNING
Mounting storage from the Docker host is not generally advisable because it ties your
container to a particular Docker host for its persistent state. But for cases like temporary
cache files or other semi-ephemeral states, it can make sense.

For times like this, you can leverage the --mount/-v command to mount
directories and individual files from the host server into the container. Note
that it is important that you use fully-qualified paths in the --mount/-v
argument. The following example mounts /mnt/session_data to /data within
the container:
$ docker container run --rm -ti \
--mount type=bind,target=/mnt/session_data,source=/data \
ubuntu:latest /bin/bash
root@0f887071000a:/# mount | grep data
/dev/sda9 on /data type ext4 (rw,relatime,data=ordered)

root@0f887071000a:/# exit

TIP
For bind mounts specifically, you can use the -v argument to shorten the command.
When using the -v argument you will notice below that the source and target
files/directories are separated by a colon(:).
It is also important to note that volumes are mounted read-write by default. You can
easily make docker mount the file or directory read-only by adding ,readonly to
end the of the --mount arguments or by :ro to the end of the -v arguments.
docker container run --rm -ti -v /mnt/session_data:/data:ro \
ubuntu:latest /bin/bash

Neither the host mount point nor the mount point in the container needs to
preexist for this command to work properly. If the host mount point does not
exist already, then it will be created as a directory. This could cause you some
issues if you were trying to point to a file instead of a directory.
In the mount options, you can see that the filesystem was mounted read-write
on /data as expected.
SELINUX AND VOLUME MOUNTS
If you have SELinux enabled on your Docker host, you may get a
“Permission Denied” error when trying to mount a volume into your
container. You can handle this by using one of the z options to the
Docker command for mounting volumes:
The lowercase z option indicates that the bind mount content is
shared among multiple containers.
The uppercase Z option indicates that the bind mount content is
private and unshared.

If you are going to share a volume between containers, you can use the z
option to the volume mount:
docker container run --rm -v /app/dhcpd/etc:/etc/dhcpd:z
dhcpd

However, the best option is actually the Z option to the volume mount
command, which will set the directory with the exact MCS label (e.g.,
chcon … -l s0:c1,c2) that the container will be using. This provides for
the best security and will allow only a single container to mount the
volume:
docker container run --rm -v /app/dhcpd/etc:/etc/dhcpd:Z
dhcpd

WARNING
Use extreme caution with the z options. Bind-mounting a system
directory such as /etc or /var with the Z option will very likely render
your system inoperable and require you to use SELinux tools to [relabel
the host machine](https://www.thegeekdiary.com/understanding-selinuxfile-labelling-and-selinux-context/) manually.

If the container application is designed to write into /data, then this data will
be visible on the host filesystem in /mnt/session_data and will remain
available when this container stops and a new container starts with the same
volume mounted.
It is possible to tell Docker that the root volume of your container should be
mounted read-only so that processes within the container cannot write
anything to the root filesystem. This prevents things like log files, which a
developer may be unaware of, from filling up the container’s allocated disk in
production. When it’s used in conjunction with a mounted volume, you can
ensure that data is written only into expected locations.

In the previous example, we could accomplish this simply by adding -read-only=true to the command.
$ docker container run --rm -ti --read-only=true -v
/mnt/session_data:/data \
ubuntu:latest /bin/bash
root@df542767bc17:/# mount | grep " / "
overlay on / type overlay
(ro,relatime,lowerdir=…,upperdir=…,workdir=…)
root@df542767bc17:/# mount | grep data
/dev/sda9 on /data type ext4 (rw,relatime,data=ordered)
root@df542767bc17:/# exit

If you look closely at the mount options for the root directory, you’ll notice
that they are mounted with the ro option, which makes it read-only.
However, the /session_data mount is still mounted with the rw option so that
our application can successfully write to the one volume to which it’s
designed to write.
Sometimes it is necessary to make a directory like /tmp writeable, even when
the rest of the container is read-only. For this use case, you can use the -mount type=tmpfs argument with docker container run, so that
you can mount a tmpfs filesystem into the container. A tmpfs filesystem is
completely in-memory. They will be very fast, but they are also ephemeral
and will utilize additional system memory. Any data in these tmpfs
directories will be lost when the container is stopped. The following example
shows a container being launched with a 256MB tmpfs filesystem mounted at
/tmp:
$ docker container run --rm -ti --read-only=true \
--mount type=tmpfs,destination=/tmp,tmpfs-size=256M \
ubuntu:latest /bin/bash
root@25b4f3632bbc:/# df -h /tmp
Filesystem
Size Used Avail Use% Mounted on
tmpfs
256M
0 256M
0% /tmp
root@25b4f3632bbc:/# grep /tmp /etc/mtab
tmpfs /tmp tmpfs
rw,nosuid,nodev,noexec,relatime,size=262144k 0 0

root@25b4f3632bbc:/# exit

WARNING
Containers should be designed to be stateless whenever possible. Managing storage
creates undesirable dependencies and can easily make deployment scenarios much more
complicated.

Resource Quotas
When people discuss the types of problems they must often cope with when
working in the cloud, the “noisy neighbor” is often near the top of the list.
The basic problem this term refers to is that other applications running on the
same physical system as yours can have a noticeable impact on your
performance and resource availability.
Virtual machines have the advantage that you can easily and very tightly
control how much memory and CPU, among other resources, are allocated to
the virtual machine. When using Docker, you must instead leverage the
cgroup functionality in the Linux kernel to control the resources that are
available to a Linux container. The docker container create and
docker container run commands directly support configuring CPU,
memory, swap, and storage I/O restrictions when you create a container.

NOTE
Constraints are normally applied at the time of container creation. If you need to change
them, you can use the docker container update command or deploy a new
container with the adjustments.

There is an important caveat here. While Docker supports various resource
limits, you must have these capabilities enabled in your kernel for Docker to
take advantage of them. You might need to add these as command-line
parameters to your kernel on startup. To figure out if your kernel supports

these limits, run docker system info. If you are missing any support,
you will get warning messages at the bottom, like:
WARNING: No swap limit support

NOTE
The details regarding getting cgroup support configured for your kernel are distributionspecific, so you should consult the Docker documentation if you need help configuring
things.

CPU shares
Docker has several ways to limit CPU usage by applications in containers.
The original method, and one still commonly used, is the concept of cpu
shares. Below we’ll present other options as well.
The computing power of all the CPU cores in a system is considered to be the
full pool of shares. Docker assigns the number 1024 to represent the full pool.
By configuring a container’s CPU shares, you can dictate how much time the
container gets to use the CPU. If you want the container to be able to use at
most half of the computing power of the system, then you would allocate it
512 shares. Note that these are not exclusive shares, meaning that assigning
all 1024 shares to a container does not prevent all other containers from
running. Rather, it’s a hint to the scheduler about how long each container
should be able to run each time it’s scheduled. If we have one container that
is allocated 1024 shares (the default) and two that are allocated 512, they will
all get scheduled the same number of times. But if the normal amount of CPU
time for each process is 100 microseconds, the containers with 512 shares
will run for 50 microseconds each time, whereas the container with 1024
shares will run for 100 microseconds.
Let’s explore a little bit how this works in practice. For the following
examples, we’ll use a new Docker image that contains the stress
command for pushing a system to its limits.

When we run stress without any cgroup constraints, it will use as many
resources as we tell it to. The following command creates a load average of
around 5 by creating two CPU-bound processes, one I/O-bound process, and
two memory allocation processes. For all of the following examples, we are
running on a system with two CPUs.
Note, that in the command below, everything following the container image
name is related to the stress command, not the docker command.
$ docker container run --rm -ti spkane/train-os \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
120s

WARNING
This should be a reasonable command to run on any modern computer system, but be
aware that it is going to stress the host system. So, don’t do this in a location that can’t
take the additional load, or even a possible failure, due to resource starvation.

If you run the top or htop command on the Docker host, near the end of
the two-minute run, you can see how the system is affected by the load
created by the stress program.
$ top -bn1 | head -n 15
top - 20:56:36 up 3 min, 2 users,
2.02, 0.75
Tasks: 88 total,
5 running, 83
0 zombie
%Cpu(s): 29.8 us, 35.2 sy, 0.0 ni,
0.6 si, 0.0 st
KiB Mem:
1021856 total,
270148
42716 buffers
KiB Swap:
0 total,
0
83764 cached Mem
PID USER
TIME+ COMMAND

PR

NI

VIRT

load average: 5.03,
sleeping,

0 stopped,

32.0 id, 0.8 wa, 1.6 hi,
used,

751708 free,

used,

0 free.

RES

SHR S

%CPU %MEM

810 root
20
0:49.63 stress
813 root
20
0:49.18 stress
812 root
20
0:46.42 stress
814 root
20
0:46.89 stress
811 root
20
0:21.34 stress
1 root
20
0:07.32 systemd
2 root
20
0:00.04 kthreadd
3 root
20
0:00.11 ksoftir…

0

7316

96

0 R

44.3

0.0

0

7316

96

0 R

44.3

0.0

0

138392

46936

996 R

31.7

4.6

0

138392

22360

996 R

31.7

2.2

0

7316

96

0 D

25.3

0.0

0

110024

4916

3632 S

0.0

0.5

0

0

0

0 S

0.0

0.0

0

0

0

0 S

0.0

0.0

NOTE
Docker Desktop users on non-Linux systems may discover that Docker has made the
VM filesystem read-only and it does not contain many useful tools for monitoring the
VM. For these demos where you want to be able to monitor the resource usage of
various processes, you can work around this by doing something like this:
$
/
/
/
/

docker container run --rm -it --pid=host alpine sh
# apk update
# apk add htop
# htop -p $(pgrep stress | tr '\n' ',')
# exit

Be aware that the preceding htop command will give you an error unless stress is
actively running when you launch htop, since no processes will be returned by the
pgrep command.
You will also want to exit and re-run htop each time you run a new stress instance.

If you want to run the same stress command again, with only half the
amount of available CPU time, you can do so like this:

$ docker container run --rm -ti --cpu-shares 512
spkane/train-os \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
120s

The --cpu-shares 512 is the flag that does the magic, allocating 512
CPU shares to this container. Note that the effect might not be noticeable on a
system that is not very busy. That’s because the container will continue to be
scheduled for the same time-slice length whenever it has work to do unless
the system is constrained for resources. So in our case, the results of a top
command on the host system will likely look the same, unless you run a few
more containers to give the CPU something else to do.

WARNING
Unlike virtual machines, Docker’s cgroup-based constraints on CPU shares can have
unexpected consequences. They are not hard limits; they are relative limits, similar to
the nice command. An example is a container that is constrained to half the CPU
shares but is on a system that is not very busy. Since the CPU is not busy, the limit on
the CPU shares would have only a limited effect because there is no competition in the
scheduler pool. When a second container that uses a lot of CPU is deployed to the same
system, suddenly the effect of the constraint on the first container will be noticeable.
Consider this carefully when constraining containers and allocating resources.

CPU pinning
It is also possible to pin a container to one or more CPU cores. This means
that work for this container will be scheduled only on the cores that have
been assigned to this container. That is useful if you want to hard-shard CPUs
between applications or if you have applications that need to be pinned to a
particular CPU for things like cache efficiency.
In the following example, we are running a stress container pinned to the first
of two CPUs, with 512 CPU shares.
$ docker container run --rm -ti \

--cpu-shares 512 --cpuset-cpus=0 spkane/train-os \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
120s

WARNING
The --cpuset-cpus argument is zero-indexed, so your first CPU core is 0. If you tell
Docker to use a CPU core that does not exist on the host system, you will get a Cannot
start container error. On a two-CPU example host, you could test this by using -cpuset-cpus=0-2.

If you run top again, you should notice that the percentage of CPU time
spent in user space (us) is lower than it previously was, since we have
restricted two CPU-bound processes to a single CPU.
%Cpu(s): 18.5 us, 22.0 sy, 0.0 ni, 57.6 id, 0.5 wa, 1.0 hi,
0.3 si, 0.0 st

NOTE
When you use CPU pinning, additional CPU sharing restrictions on the container only
take into account other containers running on the same set of cores.

Using the CPU CFS (Completely Fair Scheduler) within the Linux kernel,
you can alter the CPU quota for a given container by setting the --cpuquota flag to a valid value when launching the container with docker
container run.
Simplifying CPU Quotas
While CPU shares were the original mechanism in Docker for managing
CPU limits, Docker has evolved a great deal since and one of the ways that it
now makes users’ lives easier is by greatly simplifying how CPU quotas can
be set. Instead of trying to set CPU shares and quotas correctly, you can now

simply tell Docker how much CPU you would like to be available to your
container, and it will do the math required to set the underlying cgroups
correctly.
The --cpus command can be set to a floating-point number between 0.01
and the number of CPU cores on the Docker server.
$ docker container run --rm -ti --cpus=".25" spkane/trainos \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
60s

If you try to set the value too high, you’ll get an error message from Docker
(not the stress application) that will give you the correct range of CPU
cores that you have to work with.
$ docker container run --rm -ti --cpus="40.25"
spkane/train-os \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
60s
docker: Error response from daemon: Range of CPUs is from
0.01 to 4.00, as there are only 4 CPUs available.
See 'docker container run --help'.

The docker contrainer update command can be used to
dynamically adjust the resource limits of one or more containers. You could
adjust the CPU allocation on two containers simultaneously, for example,
like so:
docker container update --cpus="1.5" 092c5dc85044
92b797f12af1

TIP
Docker sees CPUs the same way that Linux sees them. Hyper-Threading and cores are
interpreted by Linux and exposed via the special file /proc/cpuinfo. When you use
the --cpus command in Docker you are referring to how many of the entries in this
file you want the container to have access to, whether they refer to a standard core or a

hyper-threaded core.

Memory
We can control how much memory a container can access in a manner
similar to constraining the CPU. There is, however, one fundamental
difference: while constraining the CPU only impacts the application’s priority
for CPU time, the memory limit is a hard limit. Even on an unconstrained
system with 96 GB of free memory, if we tell a container that it may have
access only to 24 GB, then it will only ever get to use 24 GB regardless of the
free memory on the system. Because of the way the virtual memory system
works on Linux, it’s possible to allocate more memory to a container than the
system has actual RAM. In this case, the container will resort to using swap,
just like a normal Linux process.
Let’s start a container with a memory constraint by passing the --memory
option to the docker container run command:
$ docker container run --rm -ti --memory 512m spkane/trainos \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
10s

When you use the --memory option alone, you are setting both the amount
of RAM and the amount of swap that the container will have access to. So by
using --memory 512m here, we’ve constrained the container to 512 MB of
RAM and 512 MB of additional swap space. Docker supports b, k, m, or g,
representing bytes, kilobytes, megabytes, or gigabytes, respectively. If your
system somehow runs Linux and Docker and has multiple terabytes of
memory, then unfortunately you’re going to have to specify it in gigabytes.
If you would like to set the swap separately or disable it altogether, you need
to also use the --memory-swap option. This defines the total amount of
memory and swap available to the container. If we rerun our previous
command, like so:

$ docker container run --rm -ti --memory 512m --memoryswap=768m \
spkane/train-os stress -v --cpu 2 --io 1 --vm 2 --vmbytes 128M \
--timeout 10s

Then we’re telling the kernel that this container can have access to 512 MB
of memory and 256 MB of additional swap space. Setting the --memoryswap option to -1 will disable the swap completely within the container.

WARNING
Again, unlike CPU shares, memory is a hard limit! This is good because the constraint
doesn’t suddenly have a noticeable effect on the container when another container is
deployed to the system. But it does mean that you need to be careful that the limit
closely matches your container’s needs because there is no wiggle room. An out-ofmemory container causes the kernel to behave just like it would if the system were out of
memory. It will try to find a process to kill so that it can free up space. This is a common
failure case where containers have their memory limits set too low. The telltale sign of
this issue is a container exit code of 137 and kernel out-of-memory (OOM) messages in
the Docker server’s dmesg output.

So, what happens if a container reaches its memory limit? Well, let’s give it a
try by modifying one of our previous commands and lowering the memory
significantly:
$ docker container run --rm -ti --memory 100m spkane/trainos \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout
10s

While all of our other runs of the stress container ended with a line like
this:
stress: info: [17] successful run completed in 10s

We see that this run quickly fails with a line similar to this:
stress: FAIL: [1] (451) failed run completed in 0s

This is because the container tries to allocate more memory than it is allowed,
and the Linux out-of-memory (OOM) killer is invoked and starts killing
processes within the cgroup to reclaim memory. In this case, our container
has a single-parent process that has spawned a few children processes, and
when the OOM killer kills one of the children processes the parent process
cleans everything up and exits with an error.
Example 5-1.
Docker has features that allow you to tune and disable the Linux OOM killer
by using the --oom-kill-disable and the --oom-score-adj
arguments to docker container run, but they are not recommended
for almost any use cases.
If you access your Docker server, you can see the kernel message related to
this event by running dmesg. The output will look something like this:
[ 4210.403984] stress invoked oom-killer:
gfp_mask=0x24000c0 …
[ 4210.404899] stress
cpuset=5bfa65084931efabda59d9a70fa8e88 …
[ 4210.405951] CPU: 3 PID: 3429 Comm: stress Not tainted
4.9 …
[ 4210.406624] Hardware name:
BHYVE, BIOS 1.00 03/14/2014
…
[ 4210.408978] Call Trace:
[ 4210.409182] [<ffffffff94438115>] ? dump_stack+0x5a/0x6f
….
[ 4210.414139] [<ffffffff947f9cf8>] ? page_fault+0x28/0x30
[ 4210.414619] Task in /docker-ce/docker/5…3
killed as a result of limit of /docker-ce/docker/5…3
[ 4210.416640] memory: usage 102380kB, limit 102400kB,
failc …
[ 4210.417236] memory+swap: usage 204800kB, limit 204800kB,
…
[ 4210.417855] kmem: usage 1180kB, limit

9007199254740988kB, …
[ 4210.418485] Memory cgroup stats for /docker-ce/docker/5…
3:
cache:0KB rss:101200KB rss_huge:0KB mapped_file:0KB
dirty:0KB
writeback:11472KB swap:102420KB inactive_anon:50728KB
active_anon:50472KB inactive_file:0KB active_file:0KB
unevictable:0KB
…
[ 4210.426783] Memory cgroup out of memory: Kill process
3429…
[ 4210.427544] Killed process 3429 (stress) totalvm:138388kB,
anon-rss:44028kB, file-rss:900kB, shmem-rss:0kB
[ 4210.442492] oom_reaper: reaped process 3429 (stress),
now
anon-rss:0kB, file-rss:0kB, shmem-rss:0kB

This out-of-memory event will also be recorded by Docker and viewable via
docker system events.
$ docker system events
2018-01-28T15:56:19.972142371-08:00 container oom \
d0d803ce32c4e86d0aa6453512a9084a156e96860e916ffc2856fc63ad9
cf88b \
(image=spkane/train-os, name=loving_franklin)

Block I/O
Many containers are just stateless applications and won’t need block I/O
restrictions. But Docker also supports limiting block I/O in a few different
ways via the cgroups mechanism.
The first way is applying some prioritization to a container’s use of block
device I/O. You enable this by manipulating the default setting of the
blkio.weight cgroup attribute. This attribute can have a value of 0
(disabled) or a number between 10 and 1000, the default being 500. This
limit acts a bit like CPU shares, in that the system will divide all of the
available I/O between every process within a cgroup slice by 1000, with the

assigned weights impacting how much available I/O is available to each
process.
To set this weight on a container, you need to pass the --blkio-weight
to your docker container run command with a valid value. You can
also target a specific device using the --blkio-weight-device option.
As with CPU shares, tuning the weights is hard to get right in practice, but we
can make it vastly simpler by limiting the maximum number of bytes or
operations per second that are available to a container via its cgroup. The
following settings let us control that:
--device-read-bps
from a device
--device-read-iops
a device
--device-write-bps
to a device
--device-write-iops
device

Limit read rate (bytes per second)
Limit read rate (IO per second) from
Limit write rate (bytes per second)
Limit write rate (IO per second) to a

You can test how these impact the performance of a container by running
some of the following commands, which use the Linux I/O tester bonnie.
$ time docker container run --rm -ti spkane/train-os:latest
bonnie++ \
-u 500:500 -d /tmp -r 1024 -s 2048 -x 1
…
real 0m27.715s
user 0m0.027s
sys
0m0.030s
$ time docker container run -ti --rm --device-write-iops
/dev/vda:256 \
spkane/train-os:latest bonnie++ -u 500:500 -d /tmp -r
1024 -s 2048 -x 1
…
real 0m58.765s
user 0m0.028s
sys
0m0.029s

$ time docker container run -ti --rm --device-write-bps
/dev/vda:5mb \
spkane/train-os:latest bonnie++ -u 500:500 -d /tmp -r
1024 -s 2048 -x 1
…

TIP
Powershell users should be able to use the Measure-Command function to replace the
Unix time command used in these examples.

In our experience, the --device-read-iops and --device-writeiops arguments are the most effective way to set block I/O limits and are
the ones we recommend.
ulimits
Before Linux cgroups, there was another way to place a limit on the
resources available to a process: the application of user limits via the
ulimit command. That mechanism is still available and still useful for all
of the use cases where it was traditionally used.
The following code is a list of the types of system resources that you can
usually constrain by setting soft and hard limits via the ulimit command:
$ ulimit -a
core file size (blocks, -c) 0
data seg size (kbytes, -d) unlimited
scheduling priority (-e) 0
file size (blocks, -f) unlimited
pending signals (-i) 5835
max locked memory (kbytes, -l) 64
max memory size (kbytes, -m) unlimited
open files (-n) 1024
pipe size (512 bytes, -p) 8
POSIX message queues (bytes, -q) 819200
real-time priority (-r) 0

stack size (kbytes, -s) 10240
cpu time (seconds, -t) unlimited
max user processes (-u) 1024
virtual memory (kbytes, -v) unlimited
file locks (-x) unlimited

It is possible to configure the Docker daemon with the default user limits that
you want to apply to every container. The following command tells the
Docker daemon to start all containers with a soft limit of 50 open files and a
hard limit of 150 open files:
$ sudo dockerd --default-ulimit nofile=50:150

You can then override these ulimits on a specific container by passing in
values using the --ulimit argument.
$ docker container run --rm -d --ulimit nofile=150:300
nginx

There are some additional advanced commands that you can use when
creating containers, but this covers many of the more common use cases. The
Docker client documentation lists all the available options and is updated
with each Docker release.

Starting a Container
Before we got into the details of containers and constraints, we created our
container using the docker container create command. That
container is just sitting there without doing anything. There is a
configuration, but no running process. When we’re ready to start the
container, we can do so using the docker container start command.
Let’s say that we needed to run a copy of Redis, a common key/value store.
We won’t do anything with this Redis container, but it’s a lightweight, longlived process and serves as an example of something we might do in a real
environment. We could first create the container:

$ docker container create -p 6379:6379 redis:2.8
Unable to find image 'redis:7.0' locally
7.0: Pulling from library/redis
3f4ca61aafcd: Pull complete
…
20bf15ad3c24: Pull complete
Digest:
sha256:8184cfe57f205ab34c62bd0e9552dffeb885d2a7f82ce4295c0d
f344cb6f0007
Status: Downloaded newer image for redis:7.0
092c5dc850446324e4387485df7b76258fdf9ed0aedcd53a37299d35fc6
7a042

The result of the command is some output, the last line of which is the full
hash that was generated for the container. We could use that long hash to start
it, but if we failed to note it down, we could also list all the containers on the
system, whether they are running or not, using:
$ docker container ls -a --filter ancestor=redis:2.8
CONTAINER ID IMAGE
COMMAND
CREATED
… NAMES
092c5dc85044 redis:7.0 "docker-entrypoint.s…" 46 seconds
ago elegant_wright

We can confirm the identity of our container by filtering the output by the
image that we used and examining the container’s creation time. We can then
start the container with the following command:
$ docker container start 092c5dc85044

NOTE
Most Docker commands will work with the container name, the full hash, the short hash,
or even just enough of the hash to make it unique. In the previous example, the full hash
for the container is 092c5dc850446324e…a37299d35fc67a042, but the short hash that is
shown in most command output is 092c5dc85044. This short hash consists of the first 12
characters of the full hash. In the previous example, running docker container
start 6b7 would have worked just fine.

That should have started the container, but with it running in the background
we won’t necessarily know if something went wrong. To verify that it’s
running, we can run:
$ docker container ls
CONTAINER ID IMAGE
…
092c5dc85044 redis:7.0
minutes …

COMMAND

…

"docker-entrypoint.s…" …

STATUS
Up 2

And, there it is: running as expected. We can tell because the status says
“Up” and shows how long the container has been running.

Auto-Restarting a Container
In many cases, we want our containers to restart if they exit. Some containers
are very short-lived and come and go quickly. But for production
applications, for instance, you expect them to be up and running at all times
after you’ve told them to run. If you are running a more complex system, a
scheduler may do this for you.
In the simple case, we can tell Docker to manage restarts on our behalf by
passing the --restart argument to the docker container run
command. It takes four values: no, always, or on-failure, or
unless-stopped. If restart is set to no, the container will never
restart if it exits. If it is set to always, the container will restart whenever it
exits, with no regard to the exit code. If restart is set to on-failure,
then whenever the container exits with a nonzero exit code, Docker will try to
restart the container. If we set restart to on-failure:3 then Docker
will try and restart the container three times before giving up. unlessstopped is the most common choice and will restart the container unless it
is intentionally stopped with something like docker container stop.
We can see this in action by re-running our last memory-constrained stress
container without the --rm argument, but with the --restart argument.

$ docker container run -ti --restart=on-failure:3 --memory
100m \
spkane/train-os stress -v --cpu 2 --io 1 --vm 2 --vmbytes 128M \
--timeout 120s

In this example, we’ll see the output from the first run appear on the console
before it dies. If we run a docker container ls immediately after the
container dies, we’ll likely see that Docker has restarted the container.
$ docker container ls
… IMAGE
… STATUS
…
… spkane/train-os … Up Less than a second …

It will continue to fail because we haven’t given it enough memory to
function properly. After three attempts, Docker will give up and we’ll see the
container disappear from the output of docker container ls.

Stopping a Container
Containers can be stopped and started at will. You might think that starting
and stopping a container is analogous to pausing and resuming a normal
process, but it’s not quite the same in reality. When stopped, the process is
not paused; it exits. And when a container is stopped, it no longer shows up
in the normal docker container ls output. On reboot, Docker will
attempt to start all of the containers that were running at shutdown. If you
need to prevent a container from doing any additional work, without actually
stopping the process, then you can pause the Linux container with docker
container pause and unpause, which will be discussed in more detail
later. For now, go ahead and stop the Redis container that we started a little
earlier:
$ docker container stop 092c5dc85044
$ docker container ls
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

Now that we have stopped the container, nothing is in the running container
list! We can start it back up with the container ID, but it would be
inconvenient to have to remember that. So docker container ls has
an additional option (-a) to show all containers, not just the running ones.
$ docker container ls -a
CONTAINER ID IMAGE
STATUS
…
092c5dc85044 redis:7.0 Exited (0) 2 minutes ago …
…

That STATUS field now shows that our container exited with a status code of
0 (no errors). We can start it back up with the same configuration it had
before:
$ docker container start 092c5dc85044
092c5dc85044
$ docker container ls -a
CONTAINER ID IMAGE
STATUS
…
092c5dc85044 redis:7.0 Up 14 seconds …
…

Voilà, our container is back up and running, and configured just as it was
before.

NOTE
Remember that containers exist as a blob of configuration in the Docker system even
when they are not started. That means that as long as the container has not been deleted,
you can restart it without needing to recreate it. Although memory and tmpfs contents
will have been lost, all of the container’s other filesystem contents and metadata,
including environment variables and port bindings, are saved and will still be in place
when you restart the container.

By now we’ve probably thumped on enough about the idea that containers
are just a tree of processes that interact with the system in essentially the

same way as any other process on the server. But it’s important to point it out
here again because it means that we can send Unix signals to our process in
the containers that they can then respond to. In the previous docker
container stop example, we’re sending the container a SIGTERM
signal and waiting for the container to exit gracefully. Containers follow the
same process group signal propagation that any other process group would
receive on Linux.
A normal docker container stop sends a SIGTERM to the process. If
you want to force a container to be killed if it hasn’t stopped after a certain
amount of time, you can use the -t argument, like this:
$ docker container stop -t 25 092c5dc85044

This tells Docker to initially send a SIGTERM signal as before, but then if the
container has not stopped within 25 seconds (default: 10), to send a
SIGKILL signal to forcefully kill it.
Although stop is the best way to shut down your containers, there are times
when it doesn’t work and you’ll need to forcefully kill a container, just as you
might have to do with any process outside of a container.

Killing a Container
When a process is misbehaving, docker container stop might not
cut it. You might just want the container to exit immediately.
In these circumstances, you can use docker container kill . As
you’d expect, it looks a lot like docker container stop:
$ docker container start 092c5dc85044
092c5dc85044
$ docker container kill 092c5dc85044
092c5dc85044

A docker container ls command now shows that the container is no
longer running, as expected:
$ docker container ls
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES

Just because it was killed rather than stopped does not mean you can’t start it
again, though. You can just issue a docker container start like you
would for a nicely stopped container. Sometimes you might want to send
another signal to a container, one that is not stop or kill. Like the Linux
kill command, docker container kill supports sending any Unix
signal. Let’s say we wanted to send a USR1 signal to our container to tell it to
do something like reconnect a remote logging session. We could do the
following:
$ docker container start 092c5dc85044
092c5dc85044
$ docker container kill --signal=USR1 092c5dc85044
092c5dc85044

If our container process was designed to do something with the USR1 signal,
it would now do it. Any standard Unix signal can be sent to a container using
this method.

Pausing and Unpausing a Container
There are a few reasons why we might not want to completely stop our
container. We might want to pause it, leave its resources allocated, and leave
its entries in the process table. That could be because we’re taking a snapshot
of its filesystem to create a new image, or just because we need some CPU on
the host for a while. If you are used to normal Unix process handling, you
might wonder how this works since containerized processes are just
processes.
Pausing leverages the cgroups freezer, which essentially just prevents your

process from being scheduled until you unfreeze it. This will prevent the
container from doing anything while maintaining its overall state, including
memory contents. Unlike stopping a container, where the processes are made
aware that they are stopping via the SIGSTOP signal, pausing a container
doesn’t send any information to the container about its state change. That’s
an important distinction. Several Docker commands use pausing and
unpausing internally as well. Here is how we pause a container:
$ docker container start 092c5dc85044
092c5dc85044
$ docker container pause 092c5dc85044
092c5dc85044

NOTE
To pause and unpause containers in Windows, you must be using Hyper-V or WSL2 as
the underlying virtualization technology.

If we look at the list of running containers, we will now see that the Redis
container status is listed as (Paused).
# docker container ls
CONTAINER ID IMAGE
… STATUS
092c5dc85044 redis:7.0 … Up 25 seconds (Paused)

…
…

Attempting to use the container in this paused state would fail. It’s present,
but nothing is running. We can now resume the container by using the
docker unpause command.
$ docker container unpause 092c5dc85044
092c5dc85044
$ docker container ls
CONTAINER ID IMAGE
… STATUS
…
092c5dc85044 redis:7.0 … Up 49 seconds …

It’s back to running, and docker container ls correctly reflects the
new state. Note that it shows “Up 38 minutes” now because Docker still
considers the container to be running even when it is paused.

Cleaning Up Containers and Images
After running all these commands to build images, create containers, and run
them, we have accumulated a lot of image layers and container folders on our
system.
We can list all the containers on our system using the docker container
ls -a command and then delete any of the containers in the list. We must
stop all containers that are using an image before removing the image itself.
Assuming we’ve done that, we can remove it as follows, using the docker
container rm command:
$ docker container stop 092c5dc85044
092c5dc85044ls
$ docker container rm 092c5dc85044
092c5dc85044

NOTE
It is possible to remove a running container if you use the -f or --force flag with
docker container rm.

We can then list all the images on our system using:
$ docker image ls
REPOSITORY
TAG
ubuntu
latest
188.3MB
redis
7.0
spkane/train-os latest

IMAGE ID
5ba9dab47459

CREATED
3 weeks ago

SIZE

0256c63af7db
78fb082a4d65

2 weeks ago
4 months ago

117MB
254MB

We can then delete an image and all associated filesystem layers by running:
$ docker image rm 0256c63af7db

WARNING
If you try to delete an image that is in use by a container, you will get a Conflict,
cannot delete error. You should stop and delete the container(s) first.

There are times, especially during development cycles when it makes sense to
completely purge all the images or containers from your system. The easiest
way to do this is by running the docker system prune command.
$ docker system prune
WARNING! This will remove:
- all stopped containers
- all networks not used by at least one container
- all dangling images
- all build cache
Are you sure you want to continue? [y/N] y
Deleted Containers:
cbbc42acfe6cc7c2d5e6c3361003e077478c58bb062dd57a230d31bcd01
f6190
…
Deleted Images:
deleted:
sha256:bec6ec29e16a409af1c556bf9e6b2ec584c7fb5ffbfd7c46ec00
b30bf …
untagged:
spkane/squid@sha256:64fbc44666405fd1a02f0ec731e35881465fac3
95e7 …
…
Total reclaimed space: 1.385GB

TIP
To remove all unused images, instead of only dangling images, try docker system

prune -a.

It is also possible to craft more specific commands to accomplish similar
goals.
To delete all of the containers on your Docker hosts, use the following
command:
$ docker container rm $(docker container ls -a -q)

And to delete all the images on your Docker host, this command will get the
job done:
$ docker image rm $(docker images -q)

The docker container ls and docker images commands both
support a filter argument that can make it easy to fine-tune your delete
commands for certain circumstances.
To remove all containers that exited with a nonzero state, you can use this
filter:
$ docker container rm $(docker container ls -a -q --filter
'exited!=0')

And to remove all untagged images, you can type:
$ docker image rm $(docker images -q -f "dangling=true")

NOTE
You can read the official Docker documentation to explore the filtering options. At the
moment there are very few filters to choose from, but more will likely be added over
time.
You can also make your own very creative filters by stringing together commands using
pipes (|) and other similar techniques.

In production systems that see a lot of deployments, you can sometimes end
up with old containers or unused images lying around and filling up disk
space. It can be useful to script the docker system prune command to
run on a schedule (e.g., running under cron or via a systemd timer).

Windows Containers
Up to now we have focused entirely on Docker commands for Linux
containers, since this is the most common use case and works on all Docker
platforms. However, since 2016, the Microsoft Windows platform has
supported running Windows containers that include native Windows
applications and can be managed with the usual set of Docker commands.
Windows containers are not the focus of this book, since they make up only a
very tiny portion of production containers at this point and they aren’t
compatible with the rest of the Docker ecosystem because they require
Windows-specific container images. However, they’re a growing and
important part of the Docker world, so we’ll take a brief look at how they
work. In fact, except for the actual contents of the containers, almost
everything else works the same as Linux containers. In this section, we’ll run
through a quick example of how you can run a Windows container on
Windows 10+ with Hyper-V and Docker.

TIP
For this to work, you must be using Docker Desktop on a compatible 64-bit edition of
Windows 10 or newer.

The first thing you’ll need to do is to switch Docker from Linux containers to
Windows containers. To do this, right-click on the Docker whale icon in your
taskbar and select Switch to Windows Containers…. You should

get a notification that Docker is switching, and this process might take some
time, although usually, it happens almost immediately. Unfortunately, there is
no notification that the switch has been completed. If you right-click on the
Docker icon again, you should now see Switch to Linux
Containers… in place of the original option.

NOTE
If the first time you right-click on the Docker icon, it reads Switch to Linux
Containers…, then you are already configured for Windows containers.

We can test a simple Windows container by opening up PowerShell and
trying to run the following command:
PS C:\> docker container run --rm -it
mcr.microsoft.com/powershell `
pwsh -command `
'Write-Host "Hello World from Windows
`($IsWindows`)"'
Hello World from Windows (True)

This will download and launch a base container for Powershell and then use
scripting to print Hello World from Windows (True) to the screen.

NOTE
If the output from the above command prints Hello World from Windows
(false) then you have not switched over to Windows Container mode, or you are
running this command on a non-Windows platform.

If you want to build a Windows container image that accomplishes roughly
the same task, you can create the following Dockerfile:

# escape=`
FROM mcr.microsoft.com/powershell
SHELL ["pwsh", "-command"]
RUN Add-Content C:\helloworld.ps1 `
'Write-Host "Hello World from Windows"'
CMD ["pwsh", "C:\\helloworld.ps1"]

When you build this Dockerfile it will base the image on
mcr.microsoft.com/powershell, create a small Powershell script,
and then configure the image to run that script when this image is used to
launch a container.

WARNING
You may have noticed that we had to escape the backslash (\) with an additional
backslash in the preceding Dockerfile’s CMD line. This is because Docker has its roots
in Unix and the backslash has a special meaning in Unix shells. So, even though we
changed the escape character for the Dockerfile to match what is used in Powershell by
default (which we set via the SHELL directive, we still need to escape some backslashes
to ensure that Docker does not misinterpret them.

If you build this Dockerfile now, you’ll see something similar to this:
PS C:\> docker image build -t windows-helloworld:latest .
Sending build context to Docker daemon 2.048kB
Step 1/4 : FROM mcr.microsoft.com/powershell
---> 7d8f821c04eb
Step 2/4 : SHELL ["pwsh", "-command"]
---> Using cache
---> 1987fb489a3d
Step 3/4 : RUN Add-Content C:\helloworld.ps1
'Write-Host "Hello World from Windows"'
---> Using cache
---> 37df47d57bf1
Step 4/4 : CMD ["pwsh", "C:\\helloworld.ps1"]

---> Using cache
---> 03046ff628e4
Successfully built 03046ff628e4
Successfully tagged windows-helloworld:latest

And now if you run the resulting image, you’ll see this:
PS C:\> docker container run --rm -ti windowshelloworld:latest
Hello World from Windows

Microsoft maintains good documentation about Windows containers that also
includes an example of building a container that launches a .NET application.

TIP
On the Windows platform, it is also useful to know that you can get improved isolation
for your container by launching it inside a dedicated and very lightweight Hyper-V
virtual machine. You can do this very easily, by simply adding the -isolation=hyperv option to your docker container create and docker
container run commands. There is a small performance and resource penalty for
this, but it does significantly improve the isolation of your container. You can read more
about this in the documentation.

Even if you plan to mostly work with Windows containers, for the rest of the
book you should switch back to Linux containers, so that all the examples
work as expected. When you are done reading and are ready to dive into
building your containers, you can always switch back.

TIP
Remember that you can re-enable Linux containers by right-clicking on the Docker icon,
and selecting Switch to Linux Containers….

Wrap-Up
In the next chapter, we’ll continue our exploration of what Docker brings to
the table. For now, it’s probably worth doing a little experimentation on your
own. We suggest exercising some of the container control commands we
covered here so that you’re familiar with the command-line options and the
overall syntax. Now would even be a great time to try to design and build a
small image and then launch it as a new container. When you are ready to
continue, head on to Chapter 6!

1 SELinux is one current implementation.
2 Typically under /var/lib/docker/containers.

Chapter 6. Exploring Docker
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 6th chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
Now that you have some experience working with containers and images, we
can explore some of Docker’s other capabilities. In this chapter, we’ll
continue to use the docker command-line tool to talk to the running
dockerd server that you’ve configured while visiting some of the other
fundamental commands.
Docker provides commands to do several additional things easily:
Printing the Docker version
Viewing the server information
Downloading image updates
Inspecting containers
Entering a running container
Returning a result
Viewing logs

Monitoring statistics
And much more…
Let’s take a look at these and some of the additional community tooling that
augments Docker’s native capabilities.

Printing the Docker Version
If you completed the last chapter, you have a working Docker daemon on a
Linux server or virtual machine, and you’ve started a base container to make
sure it’s all working. If you haven’t set that up already and you want to try
out the steps in the rest of the book, you’ll want to follow the installation
steps in Chapter 3 before you move on with this section.
The absolute simplest thing you can do with Docker is print the versions of
the various components. It might not sound like much, but this is a useful tool
to have because Docker is built from a multitude of components whose
versions will directly dictate what functionality is available to you. Knowing
how to show the version will also help you troubleshoot certain types of
connection issues between the client and server. For example, the Docker
client might give you a cryptic message about mismatched API versions and
it’s nice to be able to translate that into Docker versions so you know which
component needs updating. Note that this command talks to the remote
Docker server. If the client can’t connect to the server for any reason, the
client will report an error, and then only print out the client version
information. If you find that you are having connectivity problems, you
should probably revisit the steps in the last chapter.

NOTE
You can always directly log in to the Docker server and run docker commands from a
shell on the server if you are troubleshooting issues or simply do not want to use the
docker client to connect to a remote system. On most Docker servers, this will require
either root privileges or membership in the docker group to connect to the Unix

domain socket that Docker is listening on.

Since we just installed all of the Docker components at the same time, when
we run docker version, we should see that all of our versions match:
$ docker version
Client:
Cloud integration:
Version:
API version:
Go version:
Git commit:
Built:
OS/Arch:
Context:
Experimental:

v1.0.24
20.10.17
1.41
go1.17.11
100c701
Mon Jun 6 23:04:45 2022
darwin/amd64
default
true

Server: Docker Desktop 4.10.1 (82475)
Engine:
Version:
20.10.17
API version:
1.41 (minimum version 1.12)
Go version:
go1.17.11
Git commit:
a89b842
Built:
Mon Jun 6 23:01:23 2022
OS/Arch:
linux/amd64
Experimental:
false
containerd:
Version:
1.6.6
GitCommit:
10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1
runc:
Version:
1.1.2
GitCommit:
v1.1.2-0-ga916309
docker-init:
Version:
0.19.0
GitCommit:
de40ad0

Notice how we have different sections representing the client and server. In
this case we have a matching client and server since we just installed them

together. But it’s important to note that this won’t always be the case.
Hopefully, in your production systems, you can manage to keep the same
version running on most systems. But it’s not uncommon for development
environments and build systems to have slightly different versions.
API clients and libraries will usually work across a large number of Docker
versions, depending on which API version they require. In the Server
section, we can see that it’s telling us that the current API version is 1.41 and
the minimum API it will serve is 1.12. This is useful information when
you’re working with third-party clients and now you know how to verify this
information.

Server Information
We can also find out a lot about the Docker server via the Docker client.
Later we’ll talk more about what all of this means, but you can find out
which filesystem backend the Docker server is running, which kernel version
it is on, which operating system it is running on, which plug-ins are installed,
which runtime is being used, and how many containers and images are
currently stored there. docker system info will present you with
something similar to this, which has been shortened for brevity:
$ docker system info
Client:
…
Plugins:
buildx: Docker Buildx (Docker Inc., v0.8.2)
compose: Docker Compose (Docker Inc., v2.6.1)
extension: Manages Docker extensions (Docker Inc.,
v0.2.7)
sbom: View the packaged-based Software Bill Of Materials
(SBOM) …
scan: Docker Scan (Docker Inc., v0.17.0)
Server:
Containers: 11
…
Images: 6

Server Version: 20.10.17
Storage Driver: overlay2
…
Plugins:
Volume: local
Network: bridge host ipvlan macvlan null overlay
Log: awslogs fluentd gcplogs gelf journald json-file
local logentries …
…
Runtimes: io.containerd.runc.v2
io.containerd.runtime.v1.linux runc
Default Runtime: runc
…
Kernel Version: 5.10.104-linuxkit
Operating System: Docker Desktop
OSType: linux
Architecture: x86_64
…

Depending on how your Docker daemon is set up, this might look somewhat
different. Don’t be concerned about that; this is just to give you an example.
Here we can see that our server is a Docker Desktop release running the
5.10.104 Linux kernel and backed with the overlay2 filesystem driver. We
also have a few images and containers on the server. With a fresh install, this
number should be zero.
The information about plug-ins is worth pointing out here. It’s telling us
about all the things this installation of Docker supports. On a fresh install,
things will look more or less like this, depending on which new plug-ins are
distributed with Docker. Docker itself is made up of many different plug-ins
all working together. This is powerful because it means it’s also possible to
install several other plug-ins contributed by members of the community. It’s
useful to be able to see which are installed even if you just want to make sure
Docker has recognized one that you recently added.
In most installations, /var/lib/docker will be the default root directory used to
store images and containers. If you need to change this, you can edit your
Docker startup scripts to launch the daemon, with the --data-root
argument pointing to a new storage location. To test this by hand, you could

run something like this:
$ sudo dockerd \
-H unix:///var/run/docker.sock \
--data-root="/data/docker"

NOTE
By default, the configuration file for the Docker server can be found in
/etc/docker/daemon.json. Most of the arguments that we discuss passing directly to
dockerd can be permanently set in this file. If you are using Docker Desktop, you are
advised to modify this file in the Docker Desktop UI.

We will talk more about runtimes later, but here you can see that we have
three runtimes installed. The runc runtime is the default Docker runtime. If
you think of Linux containers, you are usually thinking about the type of
container that runc builds. On this server, we also have the
io.containerd.runc.v2 and
io.containerd.runtime.v1.linux runtimes installed. We’ll talk
more about some other runtimes in [Link to Come].

Downloading Image Updates
We’re going to use an Ubuntu base image for the following examples. Even
if you have already grabbed the ubuntu:latest base image once, you can
pull it again and it will automatically pick up any updates that have been
published since you last ran it.
This is because latest is a tag that, by convention, is supposed to represent
the latest build of the container. However, the latest tag is controversial,
since it is not permanently pinned to a specific image and can have different
meanings across different projects. Some people use it to point to the most
recent stable release, some people use it to point to the last built produced by
their CI/CD system, and others simply refuse to tag any of their images with

latest. That being said, it is still in wide use and can be useful in preproduction environments where the convenience of using it outweighs the
lack of assurances that a real version provides.
Invoking pull will look like this:
$ docker image pull ubuntu:latest
latest: Pulling from library/ubuntu
405f018f9d1d: Pull complete
Digest:
sha256:b6b83d3c331794420340093eb706a6f152d9c1fa51b262d9bf34
594887c2c7ac
Status: Downloaded newer image for ubuntu:latest
docker.io/library/ubuntu:latest

That command pulled down only the layers that have changed since we last
ran the command. You might see a longer or shorter list, or even an empty
list, depending on when you last pulled the image, what changes have been
pushed to the registry since then, and how many layers the target image
contains.

TIP
It’s good to remember that even though you pulled latest, Docker won’t
automatically keep the local image up to date for you. You’ll be responsible for doing
that yourself. However, if you deploy an image based on a newer copy of
ubuntu:latest, the Docker client will download the missing layers during the
deployment just like you would expect. Keep in mind that this is the behavior of the
Docker client, and other libraries or API tools may not behave this way. It’s highly
recommended that you always deploy production code using a fixed version tag rather
than the latest tag. This helps guarantee that you get the version you expect and there
are no unexpected surprises.

In addition to referring to items in the registry by the latest tag or another
version number tag, you can also refer to them by their content-addressable

tag. Which look like this:
sha256:b6b83d3c331794420340093eb706a6f152d9c1fa51b26
2d9bf34594887c2c7ac. These are generated as a hashed sum of the
contents of the image and are a very precise identifier. This is by far the
safest way to refer to Docker images where you need to make sure you are
getting the exact version you expect because these can’t be moved like a
version tag. The syntax for pulling them from the registry is very similar, but
note the @ in the tag.
docker image pull
ubuntu@sha256:b6b83d3c331794420340093eb706a6f152d…

Note that, unlike most Docker commands where you may shorten the hash,
you cannot do that with SHA256 hashes. You must use the full hash here.

Inspecting a Container
Once you have a container created, running or not, you can now use docker
to see how it was configured. This is often useful in debugging, and also has
some other information that can be useful for identifying a container.
For this example, go ahead and start up a container:
$ docker container run --rm -d -t ubuntu /bin/bash
3c4f916619a5dfc420396d823b42e8bd30a2f94ab5b0f42f052357a68a6
7309b

We can list all our running containers with docker container ls to
ensure everything is running as expected, and to copy the container ID.
$ docker container ls
CONTAINER ID IMAGE
COMMAND
… STATUS
…
NAMES
3c4f916619a5 ubuntu:latest "/bin/bash" … Up 31 seconds …
angry_mestorf

In this case, our ID is 3c4f916619a5. We could also use
angry_mestorf, which is the dynamic name assigned to our container.
Many underlying tools need the unique container ID though, so it’s useful to
get into the habit of looking at that first. As we mentioned earlier, the ID as
shown is the truncated (or short) version, but Docker treats these
interchangeably with the long versions. As is the case in many version
control systems, this hash is just the prefix of a much longer hash. Internally,
the kernel uses a 64-byte hash to identify the container. But that’s painful for
humans to use, so Docker supports the shortened hash.
The output to docker container inspect is pretty verbose, so we’ll
cut it down in the following code block to a few values worth pointing out.
You should look at the full output to see what else you think is interesting:
$ docker container inspect 3c4f916619a5
[{
"Id":
"3c4f916619a5dfc420396d823b42e8bd30a2f94ab5b0f42f052357a68a
67309b",
"Created": "2022-07-17T17:26:53.611762541Z",
…
"Args": [],
…
"Image": "sha256:27941809078cc9b2802deb2b0bb6feed6c…
7f200e24653533701ee",
…
"Config": {
"Hostname": "3c4f916619a5",
…
"Env": [
"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sb
in:/bin"
],
"Cmd": [
"/bin/bash"
],
…
"Image": "ubuntu",

…
},
…
}]

Note that long "Id" string. That’s the full unique identifier of this container.
Luckily we can use the short version, even if that’s still not especially
convenient. We can also see the exact time when the container was created is
much more precise than what docker container ls gives us.
Some other interesting things are shown here as well: the top-level command
in the container, the environment that was passed to it at creation time, the
image on which it’s based, and the hostname inside the container. All of these
are configurable at container creation time if you need to do so. The usual
method for passing configuration to containers, for example, is via
environment variables, so being able to see how a container was configured
via docker container inspect can reveal a lot when you’re
debugging.
You can go ahead and stop the current container by running something like
docker container stop 3c4f916619a5.

Exploring the Shell
Let’s get a container running with just an interactive bash shell so we can
take a look around. We’ll do that, as we did before, by just running
something like:
$ docker container run --rm -it ubuntu:22.04 /bin/bash

That will run an Ubuntu 22.04 LTS container with the bash shell as the toplevel process. By specifying the 22.04 tag, we can be sure to get a particular
version of the image. So, when we start that container, what processes are
running?
root@35fd1ad27228:/# ps -ef

UID
root
root

PID
1
9

PPID
0
1

C STIME TTY
0 17:45 pts/0
0 17:47 pts/0

TIME CMD
00:00:00 /bin/bash
00:00:00 ps -ef

Wow, that’s not much, is it? It turns out that when we told docker to start
bash, we didn’t get anything but that. We’re inside a whole Linux
distribution image, but no other processes started for us automatically. We
only got what we asked for. It’s good to keep that in mind going forward.

WARNING
Linux containers don’t, by default, start anything in the background as a full virtual
machine would. They’re a lot lighter weight than that and therefore don’t start an init
system. You can, of course, run a whole init system if you need to, or the tini init
system that is built into Docker, but you have to ask for it. We’ll talk about that more in
Chapter 7.

That’s how we get a shell running in a container. You should feel free to
poke around and see what else looks interesting inside the container. Note
that you might have a pretty limited set of commands available. You’re in a
base Ubuntu distribution, though, so you can fix that by using apt-get
update, followed by apt-get install … to download more packages.
Note that these are only going to be around for the life of this container.
You’re modifying the top layer of the container, not the base image!
Containers are by nature ephemeral, so anything you do inside this container
won’t outlast it.
When you are done in the container make sure and exit the shell, which
will then naturally stop the container.
root@35fd1ad27228:/# exit

Returning a Result
How in-efficient would it be to spin up a whole virtual machine to run one

command and get the results? You usually wouldn’t want to do this because
it would be very time-consuming and require booting a whole operating
system to simply execute one command. But Docker and Linux containers do
not work the same way as virtual machines: containers are very lightweight
and don’t have to boot up like an operating system. Running something like a
quick background job and waiting for the exit code is a normal use case for a
Linux container. You can think of it as a way to get remote access to a
containerized system and have access to any of the individual commands
inside that container with the ability to pipe data to and from them and return
exit codes.
This can be useful in lots of scenarios: you might, for instance, have system
health checks run this way remotely, or have a series of machines with
processes that you spin up via Docker to process a workload and then return.
The docker command-line tools proxy the results to the local machine. If
you run the remote command in foreground mode and don’t specify doing
otherwise, docker will redirect its stdin to the remote process, and the
remote process’s stdout and stderr to your terminal. The only things we have
to do to get this functionality are to run the command in the foreground and
not allocate a TTY on the remote. This is also the default configuration! No
command-line options are required.
When we run these commands, Docker creates a new container, executes the
command that we requested inside the container’s namespaces and cgroups,
removes the container and then exits, so that nothing is left running or taking
up unnecessary disk space between invocations. The following code should
give you an idea of the types of things that you can do:
$ docker container run --rm ubuntu:22.04 /bin/false
$ echo $?
1
$ docker container run --rm ubuntu:22.04 /bin/true
$ echo $?
0
$ docker container run --rm ubuntu:22.04 /bin/cat

/etc/passwd
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
…
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/usr/sbin/nologin
$ docker container run --rm ubuntu:22.04 /bin/cat
/etc/passwd | wc -l
19

Here we executed /bin/false on the remote server, which will always
exit with a status of 1. Notice how docker proxied that result to us in the
local terminal. Just to prove that it returns other results, we also run
/bin/true, which will always return a 0. And there it is.
Then we actually ask docker to run cat /etc/passwd on the remote
container. What we get is a printout of the /etc/passwd file contained inside
that container’s filesystem. Because that’s just regular output on stdout, we
can pipe it into local commands just like we would anything else.

WARNING
The previous code pipes the output into the local wc command, not a wc command in
the container. The pipe itself is not passed to the container. If you want to pass the whole
command, including the pipes, to the server, you need to invoke a complete shell on the
remote side and pass a quoted command, like bash -c "<your command> |
<something else>". In the previous code, that would be: docker container
run ubuntu:22.04 /bin/bash -c "/bin/cat /etc/passwd | wc l".

Getting Inside a Running Container
You can pretty easily get a shell running in a new container as we
demonstrated earlier with docker container run. But it’s not the

same as getting a new shell inside an existing container that is actively
running your application. Every time you use docker container run,
you get a new container. But if you have an existing container that is running
an application and you need to debug it from inside the container, you need
something else.
Using docker container exec is the Docker-native way to get a new
interactive process in a container, but there is also a more Linux-native way
to do it, called nsenter. Let’s take a look at both of these methods.

docker container exec
First, let’s take a look at the easiest and best way to get inside a running
container. The dockerd server and docker command-line tool support
remotely executing a new process in a running container via the docker
container exec command. So let’s start up a container in background
mode, and then enter it using docker container exec and invoking a
shell. The command you invoke doesn’t have to be a shell: it’s possible to run
individual commands inside the container and see their results outside it
using docker container exec. But if you want to get inside the
container to look around, a shell is the easiest way to do that.
To run docker container exec, we’ll need our container’s ID. For
this demo, let’s create a container that will just run the sleep command for
600 seconds.
$ docker container run -d --rm ubuntu:22.04 sleep 600
9f09ac4bcaa0f201e31895b15b479d2c82c30387cf2c8a46e487908d9c2
85eff

The short ID for this container is 9f09ac4bcaa0. We can now use that to get
inside the container with docker container exec. The command line
for that, unsurprisingly, looks a lot like the command line for docker
container run. We request an interactive session and a pseudo-TTY
with the -i and -t flags:

$ docker container exec -it 9f09ac4bcaa0 /bin/bash
root@9f09ac4bcaa0:/#

Note that we got a command line back that tells us the ID of the container
we’re running inside. That’s pretty useful for keeping track of where we are.
We can now run a normal Linux ps to see what else is running inside our
container. We should see the sleep process that was created when the
container was originally started.
root@9f09ac4bcaa0:/# ps -ef
UID
PID PPID C STIME TTY
root
1
0 0 20:22 ?
root
7
0 0 20:23 pts/0
root
15
7 0 20:23 pts/0

TIME
00:00:00
00:00:00
00:00:00

CMD
sleep 600
/bin/bash
ps -ef

You should type exit to get out of the container when you are done.

WARNING
You can also run additional processes in the background via docker container
exec. You use the -d option just like with docker container run. But you
should think hard about doing that for anything but debugging because you lose the
repeatability of the image deployment if you depend on this mechanism. Other people
would then have to know what to pass to docker container exec to get the
desired functionality. If you’re tempted to do this, you would probably reap bigger gains
from rebuilding your container image to launch both processes in a repeatable way. If
you need to signal to the software inside the container to take some action like rotating
logs or reloading a configuration, it is cleaner to leverage docker container
kill -s <SIGNAL> with the standard Unix signal name to pass information to the
process inside the container.

nsenter
Part of the core util-linux package from kernel.org is nsenter, short
for “Namespace Enter,” which allows you to enter any Linux namespace. In
[Link to Come], we’ll go into more detail on namespaces. But they are the

core of what makes a container a container. Using nsenter, therefore, we
can get into a Linux container from the server itself, even in situations where
the dockerd server is not responding and we can’t use docker
container exec. nsenter can also be used to manipulate things in a
container as root on the server that would otherwise be prevented by
docker container exec. This can be truly useful when you are
debugging. Most of the time, docker container exec is all you need,
but you should have nsenter in your tool belt.
Most Linux distributions ship with a new enough util-linux package
that it will contain nsenter. If you are on a distribution that does not have
it, the easiest way to get ahold of nsenter is to install it via the third-party
Linux container.
This container works by pulling a Docker image from the Docker Hub
registry and then running a Linux container that will install the nsenter
command-line tool into /usr/local/bin. This might seem strange at first, but
it’s a clever way to allow you to install nsenter to any Docker server
remotely using nothing more than the docker command.
Unlike docker container exec, which can be run remotely,
nsenter requires that you run it on the server itself, directly or via a
container. For our purposes, we’ll use a specially crafted container to run
nsenter. As with the docker container exec example, we need to
have a container running.
$ docker container run -d --rm ubuntu:22.04 sleep 600
fd521174d66dc32650d165e0ce7dd97255c7b3624c34cb1d119d9552843
82ddf

docker container exec is pretty simple, but nsenter is a little
inconvenient to use. It needs to have the PID of the actual top-level process in
your container, which is not obvious to find. Let’s go ahead and run
nsenter by hand so you can see what’s going on.
First, we need to find out the ID of the running container, because nsenter

needs to know that to access it. We can easily get this using docker
container ls:
$ docker container ls
CONTAINER ID IMAGE
fd521174d66d
ubuntu:22.04
angry_albattani

COMMAND
…
"sleep 1000" …

NAMES

The ID we want is that first field, fd521174d66d. With that, we can now find
the PID we need. We can do that like this:
$ docker container inspect --format \{{.State.Pid\}}
fd521174d66d
2721

TIP
You can also get the real PIDs of the processes in your container by running the
command docker container top followed by the container ID. In our example
this would look like this:
docker container top fd521174d66d
UID
PID
PPID C STIME TTY TIME
root 2721 2696 0 20:37 ?
00:00:00

CMD
sleep 600

Make sure to update the --target argument in the command below with
the process ID that you got from the previous command, then go ahead and
invoke nsenter:
$ docker container run --rm -it --privileged --pid=host
debian \
nsenter --target 2721 --all
# ps -ef
UID
root

PID
1

PPID
0

C STIME TTY
0 20:37 ?

TIME CMD
00:00:00 sleep 600

root
root
# exit

11
15

0
11

0 20:51 ?
0 20:51 ?

00:00:00 -sh
00:00:00 ps -ef

If the result looks a lot like docker container exec, that’s because it
does almost the same thing under the hood!
The command line argument --all is telling nsenter that we want to
enter all of the namespaces used by the process specified with --target.
If you want to troubleshoot a container that does not have a Unix shell then
things get a little trickier, but it is still possible. For this example, we can run
a container that has a single executable in it.
$ docker container run --rm -d --name outyet-small \
--publish mode=ingress,published=8090,target=8080 \
spkane/outyet:1.9.4-small
4f6de24d4c9c794c884afa758ef5b33ea38c01f8ec9314dcddd9fadc25c
1a443

Let’s take a quick look at the processes that are running in this container.
$ docker container top outyet-small
UID PID
PPID C STIME TTY TIME
CMD
root 61033 61008 0 22:43 ?
00:00:00 /outyet -version
1.9.4 -poll 600s …

If you try and launch a Unix shell in the container you will get an error.
$ docker container exec -it outyet-small /bin/sh
OCI runtime exec failed: exec failed: unable to start
container process: exec:
"/bin/sh": stat /bin/sh: no such file or directory:
unknown

We can then launch a second container which includes a shell and some other
useful tools in a way that the new container can see the processes in the first
container, is using the same network stack as the first container, and has some
extra privileges which will be helpful for our debugging.

$ docker container run --rm -it --pid=container:outyetsmall \
--net=container:outyet-small --cap-add sys_ptrace \
--cap-add sys_admin spkane/train-os /bin/sh
sh-5.1#

If you type ls in this container you will see the file system the
spkane/train-os image, which contains /bin/sh and all of our
debugging tools, but it does not contain any of the files from our outyetsmall container.
sh-5.1# ls
bin
dev home
usr
boot etc lib
var

lib64

media

opt

root

sbin

sys

lost+found

mnt

proc

run

srv

tmp

However, if you type ps -ef you will notice that you see all of the
processes from the original container. This is because we told Docker to
attach to use the namespace from the outyet-small container by passing in -pid=container:outyet-small.
sh-5.1# ps -ef
UID PID PPID C
root
1
0 0
-poll 600s …
root 29
0 0
root 36
29 0

STIME TTY
22:43 ?

TIME
CMD
00:00:00 /outyet -version 1.9.4

22:47 pts/0 00:00:00 /bin/sh
22:49 pts/0 00:00:00 ps -ef

And because we are using the same network stack, you can even curl the
port that the outyet service from the first container is bound to.
sh-5.1# curl localhost:8080
<!DOCTYPE html><html><body><center>
<h2>Is Go 1.9.4 out yet?</h2>
<h1>

<a
href="https://go.googlesource.com/go/&#43;/go1.9.4">YES!
</a>
</h1>
<p>Hostname: 155914f7c6cd</p>
</center></body></html>

At this point, you could use strace or whatever else you wanted to debug
your application and then finally exit the new debug container, leaving
your original container still running on the server.

WARNING
If you run strace you will need to type [Control-C] to exit the strace process.

sh-5.1# strace -p 1
strace: Process 1 attached
futex(0x963698, FUTEX_WAIT, 0, NULL^Cstrace: Process 1
detached
<detached …>
sh-5.1# exit
exit

You’ll notice that we could not see the file system in this use case. If you
need to view or copy files from the container you can make use of the
docker container export command to retrieve a tarball of the
container’s filesystem.
$ docker container export outyet-small -o export.tar

You can then use tar to view or extract the files.
$ tar -tvf export.tar
-rwxr-xr-x 0 0
0

0 Jul 17 16:04 .dockerenv

drwxr-xr-x 0 0
0
0 Jul
-rwxr-xr-x 0 0
0
0 Jul
drwxr-xr-x 0 0
0
0 Jul
drwxr-xr-x 0 0
0
0 Jul
drwxr-xr-x 0 0
0
0 Jul
-rwxr-xr-x 0 0
0
0 Jul
-rwxr-xr-x 0 0
0
0 Jul
lrwxrwxrwx 0 0
0
0 Jul
/proc/mounts
-rwxr-xr-x 0 0
0
0 Jul
drwxr-xr-x 0 0
0
0 Apr
drwxr-xr-x 0 0
0
0 Apr
-rw-r--r-- 0 0
0
261407 Mar
etc/ssl/certs/ca-certificates.crt
-rwxr-xr-x 0 0
0
5640640 Apr
drwxr-xr-x 0 0
0
0 Jul
drwxr-xr-x 0 0
0
0 Jul

17
17
17
17
17
17
17
17

16:04
16:04
16:04
16:04
16:04
16:04
16:04
16:04

dev/
dev/console
dev/pts/
dev/shm/
etc/
etc/hostname
etc/hosts
etc/mtab ->

17 16:04 etc/resolv.conf
24 2021 etc/ssl/
24 2021 etc/ssl/certs/
13 2018
24 2021 outyet
17 16:04 proc/
17 16:04 sys/

When you are finished go ahead and delete export.tar and then stop the
outyet-small container with docker container stop outyetsmall.

NOTE
You can explore the container’s filesystem from the Docker server by navigating
directly to where the filesystem resides on the server’s storage system. This will
typically look something like /var/lib/docker/overlay/fd5…, but will vary based on the
Docker setup, storage backend, and container hash. You can determine your Docker root
directory by running docker system info.

docker volume
Docker supports a volume subcommand that makes it possible to list all of
the volumes stored in your root directory and then discover additional
information about them, including where they are physically stored on the
server.
These volumes are not bind-mounted, but instead, they are special data

containers that provide a useful method for persisting data.
If we run a normal docker command that bind mounts a directory, we’ll
notice that it does not create any Docker volumes.
$ docker volume ls
DRIVER

VOLUME NAME

$ docker container run --rm -d -v /tmp:/tmp ubuntu:latest
sleep 120
6fc97c50fb888054e2d01f0a93ab3b3db172b2cd402fc1cd616858b2b51
38857
$ docker volume ls
DRIVER

VOLUME NAME

However, you can easily create a new volume with a command like this:
$ docker volume create my-data

If you then list all your volumes, you should see something like this:
# docker volume ls
DRIVER
local

VOLUME NAME
my-data

# docker volume inspect my-data
[
{
"CreatedAt": "2022-07-31T16:19:42Z",
"Driver": "local",
"Labels": {},
"Mountpoint": "/var/lib/docker/volumes/mydata/_data",
"Name": "my-data",
"Options": {},
"Scope": "local"
}
]

Now you can start a container with this data volume attached to it, by running
the following:
$ docker container run --rm \
--mount source=my-data,target=/app \
ubuntu:latest touch /app/my-persistent-data

That container created a file in the data volume and then immediately exited.
If we now mount that data volume to a different container, we will see that
our data is still there.
$ docker container run --rm \
--mount source=my-data,target=/app \
fedora:latest ls -lFa /app/my-persistent-data
-rw-r--r-- 1 root root 0 Jul 31 16:24 /app/my-persistentdata

And finally, you can delete the data volume when you are done with it by
running:
$ docker volume rm my-data
my-data

NOTE
If you try to delete a volume that is in use by a container (whether it is running or not),
you’ll get an error like this:
Error response from daemon: unable to remove volume:
remove my-data: volume is in use - [
d0763e6e8d79e55850a1d3ab21e9d…,
4b40d52978ea5e784e66ddca8bc22…]

These commands should help you to explore your containers in great detail.
Once we’ve explained namespaces more in [Link to Come], you’ll get a
better understanding of exactly how all these pieces interact and combine to
create a container.

Logging
Logging is a critical part of any production application. When things go
wrong, logs can be a critical tool in restoring service, so they need to be done
well. There are some common ways in which we expect to interact with
application logs on Linux systems, some better than others. If you’re running
an application process on a box, you might expect the output to go to a local
log file that you could read through. Or perhaps you might expect the output
to simply be logged to the kernel buffer where it can be read from dmesg.
Or, as on many modern Linux distributions with systemd, you might
expect logs to be available from journalctl. Because of the container’s
restrictions and how Docker is constructed, none of these will work without
at least some configuration on your part. But that’s OK because logging has
first-class support in Docker.
Docker makes logging easier in a few critical ways. First, it captures all of the
normal text output from applications in the containers it manages. Anything
sent to stdout or stderr in the container is captured by the Docker
daemon and streamed into a configurable logging backend. Secondly, like
many other parts of Docker, this system is pluggable and there are lots of
powerful options available to you as plug-ins. But let’s not dive into the deep
end just yet.

docker container logs
We’ll start with the simplest Docker use case: the default logging mechanism.
There are limitations to this mechanism, which we’ll explain in a minute, but
for the simple case it works well, and it’s very convenient. If you are running
Docker in development, this is probably the only logging strategy you’ll use

there. This logging method has been there from the very beginning and is
well understood and supported. The mechanism is the json-file method.
The docker container logs command exposes most users to this.
As is implied by the name, when you run the default json-file logging
plug-in, your application’s logs are streamed by the Docker daemon into a
JSON file for each container. This lets us retrieve logs for any container at
any time.
We can display some logs, by starting an nginx container:
$ docker container run --rm -d --name nginx-test --rm
nginx:latest

and then :
$
…
2022/07/31 16:36:05 [notice]
method
2022/07/31 16:36:05 [notice]
2022/07/31 16:36:05 [notice]
20210110 (Debian 10.2.1-6)
2022/07/31 16:36:05 [notice]
linuxkit
…

1#1: using the "epoll" event
1#1: nginx/1.23.1
1#1: built by gcc 10.2.1
1#1: OS: Linux 5.10.104-

This is nice because Docker allows you to get the logs remotely, right from
the command line, on demand. That’s very useful for low-volume logging.

NOTE
To limit the log output to more recent logs, you can use the --since option to display
only logs after a specified RFC 3339 date (e.g., 2002-10-02T10:00:00-05:00), Unix
timestamp (e.g., 1450071961), standard timestamp (e.g., 20220731), or Go duration
string (e.g., 5m45s). You may also use --tail followed by the number of lines you
would like to tail.

The actual files backing this logging are on the Docker server itself, by
default in /var/lib/docker/containers/<container_id>/ where the
<container_id> is replaced by the actual container ID. If you take a look
at the file named <container_id>-json.log, you’ll see that it’s a file
with each line representing a JSON object. It will look something like this:
{"log":"2022/07/31 16:36:05 [notice] 1#1: using the
\"epoll\" event method\n",
"stream":"stderr","time":"2022-0731T16:36:05.189234362Z"}

That log field is exactly what was sent to stdout on the process in
question; the stream field tells us that this was stdout and not stderr,
and the precise time that the Docker daemon received it is provided in the
time field. It’s an uncommon format for logging, but it’s structured rather
than just a raw stream, which is beneficial if you want to do anything with the
logs later.
Like a log file, you can also tail the Docker logs live with docker
container logs -f:
$ docker container logs -f nginx-test
…
2022/07/31 16:36:05 [notice] 1#1: start
2022/07/31 16:36:05 [notice] 1#1: start
2022/07/31 16:36:05 [notice] 1#1: start
2022/07/31 16:36:05 [notice] 1#1: start

worker
worker
worker
worker

process
process
process
process

35
36
37
38

This looks identical to the usual docker container logs, but the
client then blocks, waiting on and displaying any new logs to appear, much
like the Linux command line tail -f. You can type [Control-C] to
exit the logs stream at any time.
$ docker container stop nginx-test ---

TIP

By configuring the tag log option similar to --log-opt tag="
{{.ImageName}}/{{.ID}}", it is possible to change the default log tag (which
every log line will start with) to something more useful. By default, Docker logs will be
tagged with the first 12 characters of the container ID.

For single-host logging, this mechanism is pretty good. Its shortcomings are
around log rotation, access to the logs remotely once they’ve been rotated,
and disk space usage for high-volume logging. Despite being backed by a
JSON file, this mechanism performs well enough that most production
applications can log this way if that’s the solution that works for you. But if
you have a more complex environment, you’re going to want something
more robust, and with centralized logging capabilities.

WARNING
The default settings for dockerd do not currently enable log rotation. You’ll want to
make sure you specify the --log-opt max-size and --log-opt max-file
settings via the command line or the deamon.json configuration file if you are
running in production. Those settings limit the largest file size before rotation and the
maximum number of logfiles to keep, respectively. max-file does not do anything
unless you’ve also set max-size to tell Docker when to rotate the logs. Note that when
this is enabled, the docker container logs mechanism will return data only from
the current logfile.

More Advanced Logging
For those times when the default mechanism isn’t enough—and at scale, it’s
probably not—Docker also supports configurable logging backends. This list
of plug-ins is constantly growing. Currently supported are the json-file
we described earlier, as well as syslog, fluentd, journald, gelf,
awslogs, splunk, gcplogs, local and logentries, which are used
for sending logs to various popular logging frameworks and services.
That’s a big list of plug-ins we just threw out there. The supported option that

currently is the simplest for running Docker at scale is the option to send your
container logs to syslog directly from Docker. You can specify this on the
Docker command line with the --log-driver=syslog option or set it as
the default in the daemon.json file for all containers.

TIP
The daemon.json file is the configuration for the dockerd server. It can usually be
found in the /etc/docker/ directory on the server. For Docker Desktop, this file can be
edited in Preferences → Docker Engine from the UI. If you change this file, you will
need to restart Docker Desktop or the dockerd daemon.

There are also several third-party plug-ins available. We’ve seen mixed
results from third-party plug-ins, primarily because they complicate installing
and maintaining Docker. However, you may find that there is a third-party
implementation that’s perfect for your system, and it might be worth the
installation and maintenance hassle.

WARNING
Some caveats apply to all of the logging drivers. For example, Docker supports only one
at a time. This means that you can use the syslog or gelf logging driver, but not
along with the json-file driver. Unless you run json-file or journald, you
will lose the ability to use the docker container logs command! This may not
be expected and is a big consideration when you are changing the driver.
Some plugins are designed to send the logs to a remote endpoint and keep a local JSON
copy for the docker container logs command, but you will need to determine if
the plugin that you want to use supports this. There are too many gotchas to go through
for each driver, but you should keep in mind the tradeoff between guaranteed delivery of
logs and the potential for breaking your Docker deployment. UDP-based solutions or
other nonblocking options are recommended.

Traditionally, most Linux systems have some kind of syslog receiver,

whether it be syslog, rsyslog, or any of the many other options. This
protocol in its various forms has been around for a long time and is fairly
well supported by most deployments. When migrating to Docker from a
traditional Linux or Unix environment, many companies already have syslog
infrastructure in place, which means this is often the easiest migration path as
well.

NOTE
Many newer Linux distributions are based on the systemd init system and therefore
use journald for logging by default, which is different from syslog.

While syslog is a traditional solution, it has its problems. The Docker syslog
driver supports TLS, TCP, and UDP connection options, which sounds great,
but you should be cautious about streaming logs from Docker to a remote log
server over TCP or TLS. The problem with this is that they are both run on
top of connection-oriented TCP sessions, and Docker tries to connect to the
remote logging server at the time of container startup. If it fails to make the
connection, it will block trying to start the container. If you are running this
as your default logging mechanism, this can strike at any time on any
deployment.
This is not a particularly usable state for production systems and thus it is
recommended that you use the UDP option for syslog logging if you intend to
use the syslog driver. This does mean your logs are not encrypted and do
not have guaranteed delivery. There are various philosophies around logging,
and you’ll need to balance your need for logs against the reliability of your
system. We tend to recommend erring on the side of reliability, but if you run
in a secure audit environment you may have different priorities.

TIP
You can log directly to a remote syslog-compatible server from a single container by
setting the log option syslog-address similar to this: --log-opt syslog-

address=udp://192.168.42.42:123.

One final caveat to be aware of regarding most of the logging plug-ins: they
are blocking by default, which means that logging back-pressure can cause
issues with your application. You can change this behavior by setting -log-opt mode=non-blocking, and then setting the maximum buffer
size for logs to something like --log-opt max-buffer-size=4m.
Once these are set, the application will no longer block when that buffer fills
up. Instead, the oldest loglines in memory will be dropped. Again, reliability
needs to be weighed here against your businesses need to receive all the logs.

WARNING
Some third-party libraries and programs write to the filesystem for various (and
sometimes unexpected) reasons. If you are trying to design clean containers that do not
write directly into the container filesystem, you should consider utilizing the --readonly and --mount type=tmpfs options to docker container run that we
discussed in Chapter 4. Writing logs inside the container is not recommended. It makes
them hard to get to, prevents them from being preserved beyond the container lifespan,
and can wreak havoc with the Docker filesystem backend.

Monitoring Docker
Among the most important requirements for production systems is that they
are observable and measurable. A production system where you are blind to
how it’s behaving won’t serve you well. In modern operations environments,
we want to monitor everything meaningful and report as many useful
statistics as we can. Docker supports container health checks and some nice,
basic reporting capabilities via docker container stats and
docker system events. We’ll show you those and then look at a
community offering from Google that does some nice graphing output, and a
—currently experimental—feature of Docker that exports container metrics
to the Prometheus monitoring system.

Container Stats
Let’s start with the CLI tools that ship with Docker itself. The docker CLI
has an endpoint for viewing important statistics of running containers. The
command-line tool can stream from this endpoint and every few seconds
report back on one or more listed containers, giving basic statistics
information about what’s happening. docker container stats, like
the Linux top command, takes over the current terminal and updates the
same lines on the screen with the current information. It’s hard to show that
in print so we’ll just give an example, but this updates every few seconds by
default.
Command-line stats
Start an active container:
$ docker container run --rm -d --name stress \
docker.io/spkane/train-os:latest \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M -timeout 60s

Then run the stats command to look at the new container:
$ docker container stats stress
CONTAINER ID NAME
CPU %
MEM USAGE/LIMIT
MEM % NET I/O
BLOCK I/O PIDS
1a9f52f0855f stress 476.50% 36.09MiB/7.773GiB 0.45%
1.05kB/0B 0B/0B
6

You can type [Control-C] to exit the stats stream at any time.

TIP
You can use the --no-stream option to get a single point in time set of statistics that
will not update and will return you back to the command line after the command
completes.

Let’s break that rather dense output down into some manageable chunks.
What we have is:
1. The container ID (but not the name).
2. The amount of CPU it’s currently consuming. 100% is equivalent to one
whole CPU core.
3. The amount of memory it has in use is followed by the maximum
amount it’s allowed to use.
4. Network and block I/O statistics.
5. The number of active processes inside the container.
Some of these will be more useful than others for debugging, so let’s take a
look at what you can do with them.
One of the more helpful pieces of output here is the percentage of memory
used versus the limit that was set for the container. One common problem
with running production containers is that overly aggressive memory limits
can cause the Linux kernel’s OOM (out of memory) killer to stop the
container over and over again. The stats command can help you identify
and troubleshoot these types of issues.
Concerning I/O statistics, if you run all of your applications in containers,
then this summary can make it very clear where your I/O is going from the
system. Before containers, this was much harder to figure out!
The number of active processes inside the container helps debug as well. If
you have an application that is spawning children without reaping them, this
can expose it pretty quickly.
One great feature of docker container stats is that it can show not
just one container, but all of them in a single summary. That can be pretty
revealing, even on boxes where you think you know what they are doing.
That is all useful and easy to digest because it’s human formatted and
available on the command line. But there is an additional endpoint on the
Docker API that provides a lot more information than is shown in the client.

We’ve steered away from directly utilizing the API in this book so far, but in
this case, the data provided by the API is so much richer than the client, that
we’ll go ahead and use curl to make an API request and see what our
container is doing. It’s nowhere near as nice to read, but there is a lot more
detail.

NOTE
Remember that basically everything that the docker client can do can be done directly
through the Docker APIs. This means that you can programmatically do very similar
things in your applications if there is a need.

The example below is a good intro to calling the API directly.
Stats API endpoint
The /stats/ endpoint that we’ll hit on the API will continue to stream stats
to us as long as we keep the connection open. Since as humans, we can’t
easily parse the JSON, we’ll just ask for one line and then use the tool jq to
“pretty-print” it. For this command to work, you’ll need to have jq installed
(version 2.6 or later). If you don’t and you still want to see the JSON output,
you can skip the pipe to jq, but you’ll get plain, ugly JSON back. If you
already have a favorite JSON pretty printer, you should feel free to use that
instead.
Most Docker daemons will be installed with the API available only on the
Unix domain socket and not published on TCP. So we’ll use curl from the
Docker server host itself to call the API. If you plan to monitor this endpoint
in production, you would need to expose the Docker API on a TCP port,
usually over SSL and requiring credentials. This is not something that we
recommend, but the Docker documentation will walk you through this.

NOTE
If you are not on the Docker server or using Docker Desktop locally, you may need to

inspect the contents of the DOCKER_HOST environment variable, using something like
echo $DOCKER_HOST, to discover the hostname or IP address of the Docker server
that you are using.

First, start up a container that you can read stats from:
$ docker container run --rm -d --name stress \
docker.io/spkane/train-os:latest \
stress -v --cpu 2 --io 1 --vm 2 --vm-bytes 128M -timeout 60s

Now that the container is running, you can get an ongoing stream of statistics
about the container in JSON format by running something like curl with
your container’s name or hash.

NOTE
In the following examples, we are running curl against the Docker socket, but you
could just as easily run it against the Docker port if it is available.

$ curl --no-buffer -XGET --unix-socket /var/run/docker.sock
\
http://docker/containers/stress/stats

NOTE
This JSON stream of statistics will not stop on its own. So for now, we can use the
[Control-C] key combination to stop it.

To get a single group of statistics, we can run something similar to this:
$ curl -s -XGET --unix-socket /var/run/docker.sock \
http://docker/containers/stress/stats | head -n 1 | jq

And finally, if we have [jq](https://stedolan.github.io/jq/) or another tool
capable of pretty-printing JSON, we can make this output human-readable, as
shown here:
$ curl -s -XGET --unix-socket /var/run/docker.sock \
http://docker/containers/stress/stats | head -n 1 | jq
{
"read": "2022-07-31T17:41:59.10594836Z",
"preread": "0001-01-01T00:00:00Z",
"pids_stats": {
"current": 6,
"limit": 18446744073709552000
},
"blkio_stats": {
"io_service_bytes_recursive": [
{
"major": 254,
"minor": 0,
"op": "read",
"value": 0
},
…
]
},
"num_procs": 0,
"storage_stats": {},
"cpu_stats": {
"cpu_usage": {
"total_usage": 101883204000,
"usage_in_kernelmode": 43818021000,
"usage_in_usermode": 58065183000
…
},
},
"memory_stats": {
"usage": 183717888,
"stats": {
"active_anon": 0,
"active_file": 0,
…

},
"limit": 8346021888
},
"name": "/stress",
"id":
"9be7c9de26864ac97e07fc3d8e3ffb5bb52cc2ba49f569d4ba8d407f87
47851f",
"networks": {
"eth0": {
"rx_bytes": 1046,
"rx_packets": 9,
…
}
}
}

There is a lot of information in there. We’ve cut it down to prevent wasting
any more trees or electrons than necessary, but even so, there is a lot to
digest. The main idea is to let you see how much data is available from the
API about each container. We won’t spend much time going into the details,
but you can get quite detailed memory usage information, as well as block
I/O and CPU usage information.
If you are doing your own monitoring, this is a great endpoint to hit as well.
A drawback, however, is that it’s one endpoint per container, so you can’t get
the stats about all containers from a single call.

Container Health Checks
As with any other application, when you launch a container it is possible that
it will start and run, but never actually enter a healthy state where it could
receive traffic. Production systems also fail and your application may become
unhealthy at some point during its life, so you need to be able to deal with
that.
Many production environments have standardized ways to health-check
applications. Unfortunately, there’s no clear standard for how to do that
across organizations and so it’s unlikely that many companies do it in the

same way. For this reason, monitoring systems have been built to handle that
complexity so that they can work in a lot of different production systems. It’s
a clear place where a standard would be a big win.
To help remove this complexity and standardize on a universal interface,
Docker has added a health-check mechanism. Following the shipping
container metaphor, Linux containers should really look the same to the
outside world no matter what is inside the container, so Docker’s healthcheck mechanism not only standardizes health checking for containers, but
also maintains the isolation between what is inside the container and what it
looks like on the outside. This means that containers from Docker Hub or
other shared repositories can implement a standardized health-checking
mechanism and it will work in any other Docker environment designed to run
production containers.
Health checks are a build-time configuration item and are created with a
HEALTHCHECK definition in the Dockerfile. This directive tells the Docker
daemon what command it can run inside the container to ensure the container
is in a healthy state. As long as the command exits with a code of zero (0),
Docker will consider the container to be healthy. Any other exit code will
indicate to Docker that the container is not in a healthy state, at which point
appropriate action can be taken by a scheduler or monitoring system.
We will be using the following project to explore Docker Compose in a few
chapters. But, for the moment, it includes a useful example of Docker health
checks. Go ahead and pull down a copy of the code and then navigate into the
rocketchat-hubot-demo/mongodb/docker/ directory:
$ git clone https://github.com/spkane/rocketchat-hubotdemo.git \
--config core.autocrlf=input
$ cd rocketchat-hubot-demo/mongodb/docker

In this directory, you will see a Dockerfile and a script called dockerhealthcheck. If you view the Dockerfile, this is all that you will see:
FROM docker.io/bitnami/mongodb:4.4

# Approxiamate Upstream Dockerfile:
# https://github.com/bitnami/bitnami-docker-mongodb/
# blob/879452aa052d33744384d43949958a3204ad5d29/4.4/debian10/Dockerfile
COPY docker-healthcheck /usr/local/bin/
# Useful Information:
#
https://docs.docker.com/engine/reference/builder/#healthche
ck
# https://docs.docker.com/compose/compose-file/#healthcheck
HEALTHCHECK CMD ["docker-healthcheck"]

It is very short because we are basing this on the upstream Mongo image, and
our image inherits a lot of things from that including the entry point, default
command, and port to expose:
EXPOSE 27017
ENTRYPOINT [ "/opt/bitnami/scripts/mongodb/entrypoint.sh" ]
CMD [ "/opt/bitnami/scripts/mongodb/run.sh" ]

NOTE
Be aware that docker will forward traffic to a container’s ports even when the container
and underlying processes are still spinning up.

So, in our Dockerfile we are only adding a single script that can health-check
our container, and defining a health-check command that runs that script.
You can build the container like this:
$ docker image build -t mongo-with-check:4.4 .
=> [internal] load build definition from Dockerfile
0.0s
=> => transferring dockerfile: 37B
0.0s
=> [internal] load .dockerignore

0.0s
=> => transferring context: 2B
0.0s
=> [internal] load metadata for
docker.io/bitnami/mongodb:4.4
0.5s
=> [internal] load build context
0.0s
=> => transferring context: 40B
0.0s
=> CACHED [1/2] FROM
docker.io/bitnami/mongodb:4.4@sha256:9162…ae209
0.0s
=> [2/2] COPY docker-healthcheck /usr/local/bin/
0.0s
=> exporting to image
0.0s
=> => exporting layers
0.0s
=> => writing image sha256:a6ef…da808
0.0s
=> => naming to docker.io/library/mongo-with-check:4.4
0.0s

And then run the container and looking at the docker container ls
output:
$ docker container run -d --rm --name mongo-hc mongo-withcheck:4.4
5a807c892428ab0641232c82bd477fc8d1142c9e15c27d5946b8bfe7056
e2695
$ docker container ls
… IMAGE
… STATUS
PORTS
…
… mongo-with-check:4.4 … Up 1 second (health: starting)
27017/tcp …

You should notice that the STATUS column now has a health section in
parentheses. Initially, this will display health: starting as the
container is starting up. You can change the amount of time that Docker
waits for the container to initialize using the --health-start-period

argument to docker container run. The status will change to
healthy once the container is up and the health check is successful. It
might take this container 40+ seconds to transition into a healthy state.
$ docker container ls
… IMAGE
… STATUS
PORTS
…
… mongo-with-check:4.4 … Up 32 seconds (healthy) 27017/tcp
…

You can query this status directly, using the docker container
inspect command.
$ docker container inspect -format='{{.State.Health.Status}}' mongo-hc
healthy
$ docker container inspect --format='{{json
.State.Health}}' mongo-hc | jq
{
"Status": "healthy",
"FailingStreak": 0,
"Log": [
…
]
}

If your container began failing its health check, the status would change to
unhealthy and you could then determine how to handle the situation.
$ docker container ls
… IMAGE
… STATUS
PORTS
…
… mongo-with-check:4.4 … Up 9 minutes (unhealthy) 27017/tcp
…

At this point, you can stop the container by simply running docker

container stop mongo-hc.

TIP
As with most systems, you can configure a lot of details about your health checks,
including how often Docker checks the health (--health-interval), how many
failures are required to cause the container to be marked unhealthy (--healthretries), and more. You can even disable the health check completely (--nohealthcheck) if needed.

This feature is very useful, and you should strongly consider using it in all of
your containers. This will help you improve both the reliability of your
environment and the visibility you have into how things are running in it. It is
also supported by many production schedulers and monitoring systems, so it
should be easy to implement.

WARNING
As always, the usefulness of a health check is largely determined by how well-written it
is, and how good a job it does at accurately determining the state of the service.

docker system events
The dockerd daemon internally generates an events stream around the
container lifecycle. This is how various parts of the system find out what is
going on in other parts. You can also tap into this stream to see what lifecycle
events are happening for containers on your Docker server. This, as you
probably expect by now, is implemented in the docker CLI tool as another
command-line argument. When you run this command, it will block and
continually stream messages to you. Behind the scenes, this is a long-lived
HTTP request to the Docker API that returns messages in JSON blobs as they
occur. The docker CLI tool decodes them and prints some data to the
terminal.

This events stream is useful in monitoring scenarios or in triggering
additional actions, like wanting to be alerted when a job completes. For
debugging purposes, it allows you to see when a container died even if
Docker restarts it later. Down the road, this is a place where you might also
find yourself directly implementing some tooling against the API.
In one terminal go ahead and run the events command:
$ docker system events

You will notice that nothing happens.
In another terminal do ahead and launch the following short-lived container:
$ docker container run --rm --name sleeper debian:latest
sleep 5

In the original terminal that is running the events command, you should
now see something like this:
…09:59.606… container create d6… (image=debian:latest,
name=sleeper)
…09:59.610… container attach d6… (image=debian:latest,
name=sleeper)
…09:59.631… network connect ea… (container=d60b…,
name=bridge, type=bridge)
…09:59.827… container start d6… (image=debian:latest,
name=sleeper)
…10:04.854… container die d6… (exitCode=0,
image=debian:latest, name=sleeper)
…10:04.907… network disconnect ea… (container=d60b…,
name=bridge, type=bridge)
…10:04.922… container destroy d6… (image=debian:latest,
name=sleeper)

You can type [Control-C] to exit the events stream at any time.

TIP

As with the Docker statistics, you can access the docker system events via curl using a
command like curl --no-buffer -XGET --unix-socket
/var/run/docker.sock http://docker/events.

In this example, we ran a short-lived container that simply counted 5 seconds
and then exited.
The container create, container attach, network connect, and container start
events are all the steps required to get the container into a running state.
When the container exits, the events stream logs a container die, network
disconnect, and container destroy message. Each one of these marks a step in
completely tearing down the container. Docker also helpfully tells us the ID
of the image that the container is running on. This can be useful for tying
deployments to events, for example, because a deployment usually involves a
new image.
If you have a server where containers are not staying up, the docker
system events stream is pretty helpful in seeing what’s going on and
when. But if you’re not watching it at the time, Docker very helpfully caches
some of the events and you can still get at them for some time afterward. You
can ask it to display events after a time with the --since option, or before
with the --until option. You can also use both to limit the window to a
narrow scope of time when an issue you are investigating may have occurred.
Both options take ISO time formats like those in the previous example (e.g.,
2018-02-18T14:03:31-08:00).

TIP
There are a few specific event types that you should go out of your way to monitor.
These include:
container oom
Appears when a container runs out of memory.
container exec_create, container exec_start, container exec_die

Appear when someone has used docker container exec to enter a
container, which could signal a security incident.

cAdvisor
docker container stats and docker system events are
useful but don’t yet get us graphs to look at. And graphs are pretty helpful
when we’re trying to see trends. Of course, other people have filled some of
this gap. When you begin to explore the options for monitoring Docker, you
will find that many of the major monitoring tools now provide some
functionality to help you improve the visibility into your containers’
performance and ongoing state.
In addition to the commercial tooling provided by companies like DataDog,
GroundWork, and New Relic, there are plenty of options for free, opensource tools like Prometheus or even Nagios. We’ll talk about Prometheus in
the next section. Soon after Docker was introduced, Google released their
internal container monitoring tool as a well-maintained open source project
on GitHub, called cAdvisor. Although cAdvisor can be run outside of
Docker, by now you’re probably not surprised to hear that the easiest
implementation of cAdvisor is to simply run it as a Linux container.
To install cAdvisor on most Linux systems, all you need to do is run this
code:

WARNING
This command is intended to be run directly on a Linux Docker server. It will not work
properly when run from a Windows or macOS system.

$ docker container run \
--volume=/:/rootfs:ro \
--volume=/var/run:/var/run:ro \
--volume=/sys:/sys:ro \

--volume=/var/lib/docker/:/var/lib/docker:ro \
--volume=/dev/disk/:/dev/disk:ro \
--publish=8080:8080 \
--detach=true \
--name=cadvisor \
--privileged \
--rm \
--device=/dev/kmsg \
gcr.io/cadvisor/cadvisor:latest
Unable to find image 'cadvisor/cadvisor:latest' locally
Pulling repository cadvisor/cadvisor
f0643dafd7f5: Download complete
…
ba9b663a8908: Download complete
Status: Downloaded newer image for cadvisor/cadvisor:latest
f54e6bc0469f60fd74ddf30770039f1a7aa36a5eda6ef5100cddd9ad5fd
a350b

NOTE
On RHEL-based systems, you may need to add the following line to the docker
container run command shown here: --volume=/cgroup:/cgroup \.

Once you have done this, you will be able to navigate to your Docker host on
port 8080 to see the cAdvisor web interface (e.g., http://172.17.42.10:8080/)
and the various detailed charts it has for the host and individual containers
(see Figure 6-1).

Figure 6-1. cAdvisor CPU graphs

cAdvisor provides a REST API endpoint, which can easily be queried for
detailed information by your monitoring systems:
$ curl http://172.17.42.10:8080/api/v2.1/machine/

You can find details about the cAdvisor API via the official documentation.
The amount of detail provided by cAdvisor should be sufficient for many of
your graphing and monitoring needs.

Prometheus Monitoring
The Prometheus monitoring system has become a popular solution for
monitoring distributed systems. It works largely on a pull model, where it
reaches out and gathers statistics from endpoints on a timed basis. Docker has
an endpoint that was built for Prometheus and makes it easy to integrate your
container stats into a Prometheus monitoring system. At the time of this
writing, the endpoint is currently experimental and not enabled in the
dockerd server by default. Our brief experience with it shows that it seems
to work well, and it’s a pretty slick solution, as we’ll show you. We should
point out that this solution is for monitoring the dockerd server, in contrast
to the other solutions, which exposed information about the containers.
To export metrics to Prometheus, we need to reconfigure the dockerd
server to enable the experimental features, and additionally to expose the
metrics listener on a port of our choice. This is nice because we don’t have to
expose the whole Docker API on a TCP listener to get metrics out of the
system—a security win at the expense of a little more configuration. To do
that, we can either provide the --experimental and --metricsaddr= options on the command line, or we can put them into the
daemon.json file that the daemon uses to configure itself. Because many
current distributions run systemd and changing configurations there is
highly dependent on your installation, we’ll use the daemon.json option since

it’s more portable. We’ll demonstrate this on Ubuntu Linux 22.04 LTS. On
this distribution, the file is usually not present to begin with. So let’s put one
there using your favorite editor.

TIP
As previously mentioned, the daemon.json file for Docker Desktop can be edited in
Preferences → Docker Engine from the UI. If you change this file, you will need to
restart Docker Desktop or the dockerd daemon.

Adjust or add the following lines to the daemon.json file:
{
"experimental": true,
"metrics-addr": "0.0.0.0:9323"
}

You should now have a file that contains only what you just pasted and
nothing else.

WARNING
Any time you make a service available on the network, you need to consider what
security risks you might introduce. We believe the benefit of making metrics available is
worth the tradeoff, but you should think through the repercussions in your scenario. For
example, making them available on the public internet is probably not a good idea in
almost all cases.

When we restart Docker we’ll now have a listener on all addresses on port
9323. That’s where we’ll have Prometheus connect to get our metrics. But
first, we need to restart the dockerd server. Docker Desktop automatically
takes care of the restart for you, but if you are on the Linux Docker server,
then you can run something like sudo systemctl restart docker
to restart the daemon. You should not get any errors returned from the restart.

If you do, you likely have something set incorrectly in the daemon.json file.
Now you can test the metrics endpoint with curl.
$ curl -s http://localhost:9323/metrics | head -15
# HELP builder_builds_failed_total Number of failed image
builds
# TYPE builder_builds_failed_total counter
builder_builds_failed_total{reason="build_canceled"} 0
builder_builds_failed_total{reason="build_target_not_reacha
ble_error"} 0
builder_builds_failed_total{reason="command_not_supported_e
rror"} 0
builder_builds_failed_total{reason="dockerfile_empty_error"
} 0
builder_builds_failed_total{reason="dockerfile_syntax_error
"} 0
builder_builds_failed_total{reason="error_processing_comman
ds_error"} 0
builder_builds_failed_total{reason="missing_onbuild_argumen
ts_error"} 0
builder_builds_failed_total{reason="unknown_instruction_err
or"} 0
# HELP builder_builds_triggered_total Number of triggered
image builds
# TYPE builder_builds_triggered_total counter
builder_builds_triggered_total 0
# HELP engine_daemon_container_actions_seconds The number
of seconds it
# takes to process each container action
# TYPE engine_daemon_container_actions_seconds histogram

If you run this locally, you should get very similar output. It might not be
identical, and that’s OK as long as you get something that is not an error
message.
So now we have a place where Prometheus can get to our statistics. But we
need to have Prometheus running somewhere, right? We can easily do that by
spinning up a container. But first, we need to write a simple config. We’ll put
it in /tmp/prometheus/prometheus.yaml. You can use your favorite editor to

put the following into the file:
# Scrape metrics every 5 seconds and name the monitor
'stats-monitor'
global:
scrape_interval: 5s
external_labels:
monitor: 'stats-monitor'
# We're going to name our job 'DockerStats' and we'll
connect to the docker0
# bridge address to get the stats. If your docker0 has a
different IP address
# then use that instead. 127.0.0.1 and localhost will not
work.
scrape_configs:
- job_name: 'DockerStats'
static_configs:
- targets: ['172.17.0.1:9323']

NOTE
For Docker Desktop, you can also use host.docker.internal:9323 or
gateway.docker.internal:9323 in place of the 172.17.0.1:9323 shown
here. Both of these hostnames will point to the container’s IP address.

As noted in the file, you should use the IP address of your docker0 bridge
here, or the IP address of your ens3 or eth0 interface since localhost and
127.0.0.1 are not routable from the container. The address we used here
is the usual default for docker0, so it’s probably the right one for you.
Now that we’ve written that out, we need to start up the container using this
config:
$ docker container run --rm -d -p 9090:9090 \
-v /tmp/prometheus/prometheus.yaml:/etc/prometheus.yaml
\
prom/prometheus --config.file=/etc/prometheus.yaml

That will run the container and volume mount the config file we made into
the container so that it will find the settings we need it to have to monitor our
Docker endpoint. If it starts up cleanly, you should now be able to open your
browser and navigate to port 9090 on your host. There you will get a
Prometheus window something like Figure 6-2.

Figure 6-2. Prometheus event graph

Here we’ve selected one of the metrics, the
engine_daemon_events_total, and graphed it over a short period.
You can easily query any of the other metrics in the drop-down. Further work
and exploration with Prometheus would allow you to define alerts and
alerting policies based on these metrics as well. And it is easy to monitor so
much more than just the dockerd server. You can also expose metrics for
Prometheus from your applications. If you’re intrigued and want to look at
something more advanced, you might take a look at DockProm, which
leverages Grafana to make nice dashboards and also queries your container
metrics like those in the Docker API /stats endpoint.

Exploration
This should give you all the basics that you need to start running containers.
It’s probably worth downloading a container or two from the Docker Hub
registry and exploring a bit on your own to get used to the commands we just
learned. There are many other things you can do with Docker, including but
not limited to:
Copying files in and out of the container with docker container
cp
Saving an image to a tarball with docker image save
Loading an image from a tarball with docker image import
Docker has a huge feature set that you will likely grow into over time. Each
new release adds more functionality as well. We’ll get into a lot more detail
later on about many of the other commands and features, but keep in mind
that Docker’s whole feature set is very large.

Wrap-Up
In the next chapter, we will dive into some of the more technical details about

how Docker works and how you can use this knowledge to debug your
containerized applications.

Chapter 7. Debugging
Containers
A NOTE FOR EARLY RELEASE READERS
With Early Release ebooks, you get books in their earliest form—the
author’s raw and unedited content as they write—so you can take
advantage of these technologies long before the official release of these
titles.
This will be the 7th chapter of the final book. Please note that the GitHub
repo will be made active later on.
If you have comments about how we might improve the content and/or
examples in this book, or if you notice missing material within this
chapter, please reach out to the author at mcronin@oreilly.com.
Once you’ve shipped an application to production, there will come a day
when it’s not working as expected. It’s always nice to know ahead of time
what to expect when that day comes. It’s also important to have a good
understanding of debugging containers before moving on to more complex
deployments. Without debugging skills, it will be difficult to see where
orchestration systems have gone wrong. So let’s take a look at debugging
containers.
In the end, debugging a containerized application is not all that different from
debugging a normal process on a system except that the tools are somewhat
different. Docker provides some pretty nice tooling to help you out! Some of
these map to regular system tools, and some go further.
It is also critical to understand that your application is not running in a
separate system from the other Docker processes. They share a kernel, and
depending on your container configuration, they may share other things like a

storage subsystem and network interfaces. This means that you can get a lot
of information about what your container is doing from the system.
If you’re used to debugging applications in a virtual machine environment,
you might think you would need to enter the container to inspect an
application’s memory or CPU use, or to debug its system calls. However, this
is not so! Despite feeling in many ways like a virtualization layer, processes
in containers are just processes on the Linux host itself. If you want to see a
process list across all of the Linux containers on a machine, you could log in
to the server and run ps with your favorite command-line options. However,
you can use the docker container top command from anywhere, to
see the list of processes running in your container from the viewpoint of the
underlying Linux kernel. Let’s take a more detailed look at some of the
things that you can do when debugging a containerized application that do
not require the use of docker container exec or nsenter.

Process Output
One of the first things you’ll want to know when debugging a container is
what is running inside it. As we mentioned above, Docker has a built-in
command for doing just that: docker container top. This is not the
only way to see what’s going on inside a container, but it is by far the easiest
to use. Let’s see how that works:
$ docker container run --rm -d --name nginx-debug --rm
nginx:latest
796b282bfed33a4ec864a32804ccf5cbbee688b5305f094c6fbaf20009a
c2364
$ docker container top nginx-debug
UID
root
nginx
uuidd
uuidd
uuidd

PID PPID
2027 2002
-g daemon
2085 2027
2086 2027
2087 2027

C STIME
0 12:35
off;
0 12:35
0 12:35
0 12:35

TTY TIME CMD
?
00:00 nginx: master process
?
?
?

00:00 nginx: worker process
00:00 nginx: worker process
00:00 nginx: worker process

uuidd
uuidd
uuidd
uuidd
uuidd

2088
2089
2090
2091
2092

2027
2027
2027
2027
2027

0
0
0
0
0

12:35
12:35
12:35
12:35
12:35

?
?
?
?
?

00:00
00:00
00:00
00:00
00:00

nginx:
nginx:
nginx:
nginx:
nginx:

worker
worker
worker
worker
worker

process
process
process
process
process

$ docker container stop nginx-debug

To run docker container top, we need to pass it the name or ID of
our container, and then we receive a nice listing of what is running inside our
container, ordered by PID just as we’d expect from Linux ps output.
There are some oddities here, though. The primary one is the name-spacing
of user IDs and filesystems.
It is important to understand that the username for a particular user ID (UID)
can be completely different between each container and the host system. It is
even possible that a specific UID has no named user in the container or host’s
/etc/passwd file associated with it at all. This is because Unix does not
require a UID to have a named user associated with it and Linux namespaces,
which we discuss much more in [Link to Come], provide some isolation
between the container’s concept of valid users and those on the underlying
host.
Let’s look at a more concrete example of this. Let’s consider a production
Docker server running Ubuntu 22.04 and a container running on it that has an
Ubuntu distribution inside. If you run the following commands on the Ubuntu
host, you would see that UID 7 is named lp:
$ id 7
uid=7(lp) gid=7(lp) groups=7(lp)

NOTE
There is nothing special about the UID number we are using here. You don’t need to
take any particular note of it. It was chosen simply because it is used by default on both
platforms but represents a different username.

If we then enter the standard Fedora container on that Docker host, you will
see that UID 7 is set to halt in /etc/passwd. By running the following
commands, you can see that the container has a completely different
perspective of who UID 7 is:
$ docker container run --rm -it fedora:latest /bin/bash
root@c399cb807eb7:/# id 7
uid=7(halt) gid=0(root) groups=0(root)
root@c399cb807eb7:/# grep x:7: /etc/passwd
halt:x:7:0:halt:/sbin:/sbin/halt
root@409c2a8216b1:/# exit

If we then run ps aux on the theoretical Ubuntu Docker server while that
container was running as UID 7 (-u 7), we would see that the Docker host
would show the container process as being run by lp instead of halt:
$ docker container run --rm -d -u 7 fedora:latest sleep 120
55…c6
$ ps aux | grep sleep
lp
2388 0.2
120
vagrant
2419 0.0
-color=auto sleep

0.0

2204

0.0

5892

784 ?

… 0:00 sleep

1980 pts/0 … 0:00 grep -

This could be particularly confusing if a well-known user like nagios or
postgres were configured on the host system but not in the container, yet
the container ran its process with the same ID. This namespacing can make
the ps output look quite strange. It might, for example, look like the
nagios user on your Docker host is running the postgresql daemon that
was launched inside a container, if you don’t pay close attention.

TIP
One solution to this is to dedicate a nonzero UID to your containers. On your Docker
servers, you can create a container user as UID 5000 and then create the same user
in your base container images. If you then run all your containers as UID 5000 (-u
5000), not only will you improve the security of your system by not running container
processes as UID 0, but you will also make the ps output on the Docker host easier to
decipher by displaying the container user for all of your running container processes.
Some systems use the nobody or daemon user for the same purpose, but we prefer
container for clarity. There is a little more detail about how this works in [Link to
Come].

Likewise, because the process has a different view of the filesystem, paths
that are shown in the ps output are relative to the container and not the host.
In these cases, knowing it is in a container is a big win.
So that’s how you use the Docker tooling to look at what’s running in a
container. But that’s not the only way, and in a debugging situation, it might
not be the best way. If you hop onto a Docker server and run a normal Linux
ps to see what’s running, you get a full list of everything containerized and
not containerized just as if they were all equivalent processes. There are some
ways to look at the process output to make things a lot clearer. For example,
you can facilitate debugging by looking at the Linux ps output in tree form
so that you can see all of the processes descended from Docker. Here’s what
that might look like when you use the BSD command-line flags to look at a
system that is currently running two containers; we’ll chop the output to just
the part we care about:

NOTE
Note that Docker Desktop’s virtual machine contains minimal versions of most Linux
tools, and some of these commands may not produce the same output that you will get if
you use a standard Linux server, as the Docker daemon host.

$ ps axlfww
… /usr/bin/containerd
…
… /usr/bin/dockerd -H fd:// -containerd=/run/containerd/containerd.sock
… \_ /usr/bin/docker-proxy -proto tcp -host-ip 0.0.0.0 host-port 8080 \
-container-ip 172.17.0.2 -container-port 8080
… \_ /usr/bin/docker-proxy -proto tcp -host-ip :: -hostport 8080 \
-container-ip 172.17.0.2 -container-port 8080
…
… /usr/bin/containerd-shim-runc-v2 -namespace moby -id 97…
3d -address /run/…
… \_ sleep 120
…
… /usr/bin/containerd-shim-runc-v2 -namespace moby -id 69…
7c -address /run/…

NOTE
Many of the ps commands in the preceding example work only on Linux distributions
with the full ps command. Some stripped-down versions of Linux, like Alpine, run the
Busybox shell, which does not have full ps support and won’t show some of this output.
We recommend running a full distribution on your host systems like Ubuntu or Fedora
CoreOS.

Here you can see that we’re running one instance of containerd, which is
the main container runtime used by the Docker daemon. dockerd has two
docker-proxy sub-processes running at the moment, which we will
discuss in more detail in “Network Inspection”.
Each process that is using containerd-shim-runc-v2 represents a
single container and all of the processes that are running inside that container.
In this example, we have two containers. They show up as containerdshim-runc-v2 followed by some additional information about the
process, including the container ID. In this case, we are running one instance

of Google’s cadvisor, and one instance of sleep in another container.
Each container that has ports mapped, will have at least one dockerproxy process that is used to map the required network ports between the
container and the host Docker server. In this example both docker-proxy
processes are related to cadvisor. One is mapping the ports for IPv4
addresses and the other is mapping ports for IPv6 addresses.
Because of the tree output from ps, it’s pretty clear which processes are
running in which containers. If you’re a bigger fan of Unix SysV commandline flags, you can get a similar, but not as nice-looking, tree output with ps
-ejH:
$ ps -ejH
…
…
…
…
…
…
…
…
…
…
…

containerd
dockerd
docker-proxy
docker-proxy
containerd-shim
cadvisor
containerd-shim
sleep

You can get a more concise view of the docker process tree by using the
pstree command. Here, we’ll use pidof to scope it to the tree belonging
to docker:
$ pstree `pidof dockerd`
dockerd─┬─docker-proxy───7*[{docker-proxy}]
├─docker-proxy───6*[{docker-proxy}]
└─10*[{dockerd}]

This doesn’t show us PIDs and therefore is useful only for getting a sense of
how things are connected. But this is conceptually clear output when there

are a lot of processes running on a host. It’s far more concise and provides a
nice high-level map of how things connect. Here we can see the same
containers that were shown in the previous ps output, but the tree is
collapsed so we get multipliers like 7* when there are 7 duplicate processes.
We can get a full tree with PIDs if we run pstree, as shown here:
$ pstree -p `pidof dockerd`
dockerd(866)─┬─docker-proxy(3050)─┬─{docker-proxy}(3051)
│
├─{docker-proxy}(3052)
│
├─{docker-proxy}(3053)
│
├─{docker-proxy}(3054)
│
├─{docker-proxy}(3056)
│
├─{docker-proxy}(3057)
│
└─{docker-proxy}(3058)
├─docker-proxy(3055)─┬─{docker-proxy}(3059)
│
├─{docker-proxy}(3060)
│
├─{docker-proxy}(3061)
│
├─{docker-proxy}(3062)
│
├─{docker-proxy}(3063)
│
└─{docker-proxy}(3064)
├─{dockerd}(904)
├─{dockerd}(912)
├─{dockerd}(913)
├─{dockerd}(914)
├─{dockerd}(990)
├─{dockerd}(1014)
├─{dockerd}(1066)
├─{dockerd}(1605)
├─{dockerd}(1611)
└─{dockerd}(2228)

This output provides us with a very good look at all the processes attached to
Docker and what they are running.
If you wanted to inspect a single container and its processes then you could
determine the container’s main process ID and then use pstree to see all
the related sub-processes.

$ ps aux | grep containerd-shim-runc-v2
root
3072 … /usr/bin/containerd-shim-runc-v2 -namespace
moby -id 69…7c …
root
4489 … /usr/bin/containerd-shim-runc-v2 -namespace
moby -id f1…46 …
vagrant 4651 … grep --color=auto shim
$ pstree -p 3072
containerd-shim(3072)─┬─cadvisor(3092)─┬─{cadvisor}(3123)
│
├─{cadvisor}(3124)
│
├─{cadvisor}(3125)
│
├─{cadvisor}(3126)
│
├─{cadvisor}(3127)
│
├─{cadvisor}(3128)
│
├─{cadvisor}(3180)
│
├─{cadvisor}(3181)
│
└─{cadvisor}(3182)
├─{containerd-shim}(3073)
├─{containerd-shim}(3074)
├─{containerd-shim}(3075)
├─{containerd-shim}(3076)
├─{containerd-shim}(3077)
├─{containerd-shim}(3078)
├─{containerd-shim}(3079)
├─{containerd-shim}(3080)
├─{containerd-shim}(3121)
└─{containerd-shim}(3267)

Process Inspection
If you’re logged in to the Docker server, you can inspect running processes
using all of the standard debugging tools. Common debugging tools like
strace work as expected. In the following code, we’ll inspect an nginx
process running inside a container:
$ docker container run --rm -d --name nginx-debug --rm
nginx:latest
$ docker container top nginx-debug

UID
PID
root
22983
daemon off;
systemd+ 23032
systemd+ 23033

PPID … CMD
22954 … nginx: master process nginx -g
22983 … nginx: worker process
22983 … nginx: worker process

$ sudo strace -p 23032
strace: Process 23032 attached
epoll_pwait(10,

WARNING
If you run strace you will need to type [Control-C] to exit the strace process.

You can see that we get the same output that we would from noncontainerized processes on the host. Likewise, an lsof shows us that the
files and sockets open in a process work as expected:
$ sudo lsof -p 22983
COMMAND
PID USER … NAME
nginx
22983 root … /
nginx
22983 root … /
nginx
22983 root … /usr/sbin/nginx
nginx
22983 root … /usr/sbin/nginx (stat: No such file or
directory)
nginx
22983 root … /lib/aarch64-linux-gnu/libnss_files2.31.so (stat: …
nginx
22983 root … /lib/aarch64-linux-gnu/libc-2.31.so
(stat: …
nginx
22983 root … /lib/aarch64-linux-gnu/libz.so.1.2.11
(path inode=…)
nginx
22983 root … /usr/lib/aarch64-linuxgnu/libcrypto.so.1.1 (stat: …
nginx
22983 root … /usr/lib/aarch64-linuxgnu/libssl.so.1.1 (stat: …
nginx
22983 root … /usr/lib/aarch64-linux-gnu/libpcre28.so.0.10.1 (stat: …
nginx
22983 root … /lib/aarch64-linux-

gnu/libcrypt.so.1.1.0 (path …
nginx
22983 root … /lib/aarch64-linux-gnu/libpthread2.31.so (stat: …
nginx
22983 root … /lib/aarch64-linux-gnu/libdl-2.31.so
(stat: …
nginx
22983 root … /lib/aarch64-linux-gnu/ld-2.31.so
(stat: …
nginx
22983 root … /dev/zero
nginx
22983 root … /dev/null
nginx
22983 root … pipe
nginx
22983 root … pipe
nginx
22983 root … pipe
nginx
22983 root … protocol: UNIX-STREAM
nginx
22983 root … pipe
nginx
22983 root … pipe
nginx
22983 root … protocol: TCP
nginx
22983 root … protocol: TCPv6
nginx
22983 root … protocol: UNIX-STREAM
nginx
22983 root … protocol: UNIX-STREAM
nginx
22983 root … protocol: UNIX-STREAM

Note that the paths to the files are all relative to the container’s view of the
backing filesystem, which is not the same as the host view. Therefore, the
host might not be able to find a file of the file version on the host might not
match the one the container sees. In this case, it’s probably best to enter the
container using docker container exec to look at the files with the
same view that the processes inside it have.
It’s possible to run the GNU debugger (gdb) and other process inspection
tools in the same manner as long as you’re root and have proper
permissions to do so.
It is worth mentioning here that it is also possible to run a new debugging
container that can see the processes of an existing container and therefore
provide additional tools to debug issues. We will discuss the underlying
details of this command later, in the [Link to Come] and [Link to Come]
sections.
$ docker container run -ti --rm --cap-add=SYS_PTRACE \
--pid=container:nginx-debug spkane/train-os:latest bash

[root@e4b5d2f3a3a7
USER PID %CPU %MEM
root
1 0.0 0.2
daemon off;
101
30 0.0 0.1
101
31 0.0 0.1
root 136 0.0 0.1
root 152 0.0 0.2

/]# ps aux
… TIME COMMAND
… 0:00 nginx: master process nginx -g
…
…
…
…

0:00
0:00
0:00
0:00

nginx: worker process
nginx: worker process
bash
ps aux

[root@e4b5d2f3a3a7 /]# strace -p 1
strace: Process 1 attached
rt_sigsuspend([], 8
[Control-C]
strace: Process 1 detached
<detached …>
[root@e4b5d2f3a3a7 /]# exit
$ docker container stop nginx-debug

WARNING
You will need to type [Control-C] to exit the strace process.

Controlling Processes
When you have a shell directly on the Docker server, you can, in many ways,
treat containerized processes just like any other process running on the
system. If you’re remote, you might send signals with docker
container kill because it’s expedient. But if you’re already logged in
to a Docker server for a debugging session or because the Docker daemon is
not responding, you can just kill the process like you would any other.
Note that unless you kill the top-level process in the container (PID 1 inside
the container), killing a process will not terminate the container itself. That
might be desirable if you were killing a runaway process but it might leave

the container in an unexpected state. Developers probably expect that all the
processes are running if they can see their container in docker
container ls and it could also confuse a scheduler like Mesos or
Kubernetes or any other system that is health-checking your application.
Keep in mind that containers are supposed to look to the outside world like a
single bundle. If you need to kill off something inside the container, it’s best
to replace the whole container. Containers offer an abstraction that tools
interoperate with. They expect the internals of the container to be predictable
and remain consistent.
Terminating processes is not the only reason to send signals. And since
containerized processes are just normal processes in many respects, they can
be passed the whole array of Unix signals listed in the manpage for the Linux
kill command. Many Unix programs will perform special actions when
they receive certain predefined signals. For example, nginx will reopen its
logs when receiving a SIGUSR1 signal. Using the Linux kill command,
you can send any Unix signal to a container process on the local server.

TIP
Unless you run an orchestrator like Kubernetes that can handle multiple containers in a
larger abstraction like a pod, we consider it a best practice to run some kind of process
control in your production containers. Whether it be [tini]
(https://github.com/krallin/tini), [upstart](https://upstart.ubuntu.com/), [runit]
(http://smarden.org/runit/), [s6](https://skarnet.org/software/s6/), or something else, this
approach allows you to treat containers atomically even when they contain more than
one process. You should, however, try very hard not to run more than one thing inside
your container, to ensure that your container is scoped to handle one well-defined task
and does not grow into a monolithic container.
In either case, you will want docker container ls to reflect the presence of the
whole container so that you don’t need to worry about whether an individual process
inside it has died. If you can assume that the presence of a container and absence of error
logs means that things are working, you can treat docker container ls output as
the truth about what’s happening on your Docker systems. It also means any
orchestration system you use can do the same.
It is also a good idea to ensure that you understand the complete behavior of your

preferred process control service, including memory or disk utilization, Unix single
handling, and so on, since this can impact your container’s performance and behavior.
Generally, the lightest-weight systems are the best.

Because containers work just like any other process, it’s important to
understand how they can interact with your application in less than helpful
ways. There are some special needs in a container for processes that spawn
background children—that is, anything that forks and daemonizes so the
parent no longer manages the child process lifecycle. Jenkins build containers
are one common example where people see this go wrong. When daemons
fork into the background, they become children of PID 1 on Unix systems.
Process 1 is special and is usually an init process of some kind.
PID 1 is responsible for making sure that children are reaped. In your
container, by default, your main process will be PID 1. Since you probably
won’t be handling the reaping of children from your application, you can end
up with zombie processes in your container. There are a few solutions to this
problem. The first is to run an init system in the container of your own
choosing—one that is capable of handling PID 1 responsibilities. s6,
runit, and others described in the preceding note can be easily used inside
the container.
But Docker itself provides an even simpler option that solves just this one
case without taking on all the capabilities of a full init system. If you provide
the --init flag to docker container run, Docker will launch a very
small init process based on the tini project that will act as PID 1 inside the
container on startup. Whatever you specify in your Dockerfile as the CMD is
passed to tini and otherwise works in the same way you would expect. It
does, however, replace anything you might have in the ENTRYPOINT
section of your Dockerfile.
When you launch a Linux container without the --init flag, you get
something like this in your process list:
$ docker container run --rm -it alpine:3.16 sh

/ # ps -ef
PID

USER
1 root
5 root

TIME
COMMAND
0:00 sh
0:00 ps -ef

/ # exit

Notice that in this case, the CMD we launched is PID 1. That means it is
responsible for child reaping. If we are launching a container where that is
important, we can pass --init to make sure that when the parent process
exits, children are reaped.
$ docker container run --rm -it --init alpine:3.16 sh
/ # ps -ef
PID

USER
1 root
5 root
6 root

TIME
0:00
0:00
0:00

COMMAND
/sbin/docker-init -- sh
sh
ps -ef

/ # exit

Here, you can see that the PID 1 process is /sbin/docker-init. That
has in turn launched the shell binary for us as specified on the command line.
Because we now have an init system inside the container, the PID 1
responsibilities fall to it rather than the command we used to invoke the
container. In most cases, this is what you want. You may not need an init
system, but it’s small enough that you should consider having at least tini
inside your containers in production.
In general, you probably only need an init process inside your container, if
you are running multiple parent processes, or you have processes that do not
respond to Unix signals properly.

Network Inspection

Compared to process inspection, debugging containerized applications at the
network level can be more complicated. Unlike traditional processes running
on the host, Linux containers can be connected to the network in multiple
ways. If you are running the default setup, as the vast majority of people are,
then your containers are all connected to the network via the default bridge
network that Docker creates. This is a virtual network where the host is the
gateway to the rest of the world. We can inspect these virtual networks with
the tooling that ships with Docker. You can get it to show you which
networks exist by calling the docker network ls command:
$ docker network ls
NETWORK ID
f9685b50d57c
8acae1680cbd
fb70d67499d3

NAME
bridge
host
none

DRIVER
bridge
host
null

SCOPE
local
local
local

Here we can see the default bridge network: the host network, which is for
any containers running in host network mode, where containers share the
same network namespace as the host; and the none network, which disables
network access entirely for the container. If you use docker compose or
other orchestration tools, they may create additional networks here with
different names.
But seeing which networks exist doesn’t make it any easier to see what’s on
those networks. So, you can see which containers are attached to any
particular named network with the docker network inspect
command. This produces a fair amount of output. It shows you all of the
containers that are attached to the specified network and a number of details
about the network itself. Let’s take a look at the default bridge network:
$ docker network inspect bridge
[
{
"Name": "bridge",
…

"Driver": "bridge",
"EnableIPv6": false,
…
"Containers": {
"69e9…c87c": {
"Name": "cadvisor",
…
"IPv4Address": "172.17.0.2/16",
"IPv6Address": ""
},
"a2a8…e163": {
"Name": "nginx-debug",
…
"IPv4Address": "172.17.0.3/16",
"IPv6Address": ""
}
},
"Options": {
"com.docker.network.bridge.default_bridge":
"true",
…
"com.docker.network.bridge.host_binding_ipv4":
"0.0.0.0",
"com.docker.network.bridge.name": "docker0",
…
},
"Labels": {}
}
]

We’ve excluded some of the details here to shrink the output a bit. But what
we can see is that there are two containers on the bridge network, and they
are attached to the docker0 bridge on the host. We can also see the IP
addresses of each container(IPv4Address and IPv6Address), and the
host network address they are bound to (host_binding_ipv4). This is
useful when you are trying to understand the internal structure of the bridged
network. Note that if you have containers on different networks, they may not
have connectivity to each other, depending on how the networks were
configured.

TIP
In general, we recommend leaving your containers on the default bridge network until
you have a good reason not to or are running docker compose or a scheduler that
manages container networks on its own. In addition, naming your containers in some
identifiable way helps here because we can’t see the image information. The name and
ID are the only references we have in this output that can tie us back to a docker
container ls listing. Some schedulers don’t do a good job of naming containers,
which is too bad because it can be really helpful for debugging.

As we’ve seen, containers will normally have their own network stack and
their own IP address, unless they are running in host networking mode, which
we will discuss further in [Link to Come]. But what about when we look at
them from the host machine itself? Because containers have their own
network and addresses, they won’t show up in all netstat output on the
host. But we know that the ports you map to your containers are bound to the
host. Running netstat -an on the Docker server works as expected, as
shown here:
$ sudo netstat -an
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address
Foreign Address
State
tcp
0
0 0.0.0.0:8080
0.0.0.0:*
LISTEN
tcp
0
0 127.0.0.53:53
0.0.0.0:*
LISTEN
tcp
0
0 0.0.0.0:22
0.0.0.0:*
LISTEN
tcp
0
0 192.168.15.158:22
192.168.15.120:63920
ESTABLISHED
tcp6
0
0 :::8080
:::*
LISTEN
tcp6
0
0 :::22
:::*
LISTEN
udp
0
0 127.0.0.53:53
0.0.0.0:*
udp
0
0 192.168.15.158:68
0.0.0.0:*

raw6
7
…

0

0 :::58

:::*

Here we can see all of the interfaces that we’re listening on. Our container is
bound to port 8080 on IP address 0.0.0.0. That shows up. But what happens
when we ask netstat to show us the process name that’s bound to the
port?
$ sudo netstat -anp
Active Internet connections (servers and established)
Proto … Local Address
Foreign Address
PID/Program name
tcp
… 0.0.0.0:8080
0.0.0.0:*
1516/docker-proxy
tcp
… 127.0.0.53:53
0.0.0.0:*
692/systemd-resolve
tcp
… 0.0.0.0:22
0.0.0.0:*
780/sshd: /usr/sbin
tcp
… 192.168.15.158:22
192.168.15.120:63920
1348/sshd: vagrant
tcp6
… :::8080
:::*
1522/docker-proxy
tcp6
… :::22
:::*
780/sshd: /usr/sbin
udp
… 127.0.0.53:53
0.0.0.0:*
692/systemd-resolve
udp
… 192.168.15.158:68
0.0.0.0:*
690/systemd-network
raw6
… :::58
:::*
690/systemd-network

…
…
…
…
…
…
…
…
…
…

We see the same output, but notice what is bound to the port: dockerproxy. That’s because, in its default configuration, Docker has a proxy
written in Go that sits between all of the containers and the outside world.
That means that when we look at this output, all containers running via
Docker will be associated with docker-proxy. Notice that there is no clue
here about which specific container docker-proxy is handling. Luckily,

docker container ls shows us which containers are bound to which
ports, so this isn’t a big deal. But it’s not obvious, and you probably want to
be aware of it before you’re debugging a production failure. Still, passing the
p flag to netstat is helpful in identifying which ports are tied to
containers.

NOTE
If you’re using host networking in your container, then this layer is skipped. There is no
docker-proxy, and the process in the container can bind to the port directly. It also
shows up as a normal process in netstat -anp output.

Other network inspection commands work largely as expected, including
tcpdump, but it’s important to remember that docker-proxy is there, in
between the host’s network interface and the container, and that the
containers have their own network interfaces on a virtual network.

Image History
When you’re building and deploying a single container, it’s easy to keep
track of where it came from and what images it’s sitting on top of. But this
rapidly becomes unmanageable when you’re shipping many containers with
images that are built and maintained by different teams. How can you tell
what layers are actually underneath the one your container is running on?
Your container’s image version hopefully shows you which build you’re
running of the application, but that doesn’t reveal anything about the images
it’s built on. docker image history does just that. You can see each
layer that exists in the inspected image, the sizes of each layer, and the
commands that were used to build it:
$ docker image history redis:latest
IMAGE
SIZE

… CREATED BY
COMMENT

e800a8da9469 … /bin/sh -c #(nop) CMD ["redis-server"]
0B
<missing>
… /bin/sh -c #(nop) EXPOSE 6379
0B
<missing>
… /bin/sh -c #(nop) ENTRYPOINT ["dockerentry…
0B
<missing>
… /bin/sh -c #(nop) COPY
file:e873a0e3c13001b5…
661B
<missing>
… /bin/sh -c #(nop) WORKDIR /data
0B
<missing>
… /bin/sh -c #(nop) VOLUME [/data]
0B
<missing>
… /bin/sh -c mkdir /data && chown redis:redis
…
0B
<missing>
… /bin/sh -c set -eux;
savedAptMark="$(aptm…
32.4MB
<missing>
… /bin/sh -c #(nop) ENV
REDIS_DOWNLOAD_SHA=f0…
0B
<missing>
… /bin/sh -c #(nop) ENV
REDIS_DOWNLOAD_URL=ht…
0B
<missing>
… /bin/sh -c #(nop) ENV REDIS_VERSION=7.0.4
0B
<missing>
… /bin/sh -c set -eux; savedAptMark="$(aptma…
4.06MB
<missing>
… /bin/sh -c #(nop) ENV GOSU_VERSION=1.14
0B
<missing>
… /bin/sh -c groupadd -r -g 999 redis &&
usera…
331kB
<missing>
… /bin/sh -c #(nop) CMD ["bash"]
0B
<missing>
… /bin/sh -c #(nop) ADD
file:6039adfbca55ed34a…
74.3MB

Using docker image history can be useful, for example, when you
are trying to determine why the size of the final image is much larger than
expected. The layers are listed in order, with the first one at the bottom of the
list and the last one at the top.
Here we can see that the command output has been truncated in a few cases.
For long commands, adding the --no-trunc option to the preceding
command will let you see the complete command that was used to build each

layer. Just be aware that --no-trunc will make the output much larger and
more difficult to visually scan in most cases.

Inspecting a Container
In Chapter 4, we showed you how to read the docker container
inspect output to see how a container is configured. But underneath that is
a directory on the host’s disk that is dedicated to the container. Usually this is
/var/lib/docker/containers. If you look at that directory, it contains very long
SHA hashes, as shown here:
$ sudo ls /var/lib/docker/containers
106ead0d55af55bd803334090664e4bc821c76dadf231e1aab7798d1baa
19121
28970c706db0f69716af43527ed926acbd82581e1cef5e4e6ff152fce1b
79972
3c4f916619a5dfc420396d823b42e8bd30a2f94ab5b0f42f052357a68a6
7309b
589f2ad301381b7704c9cade7da6b34046ef69ebe3d6929b9bc24785d74
88287
959db1611d632dc27a86efcb66f1c6268d948d6f22e81e2a22a57610b50
70b4d
a1e15f197ea0996d31f69c332f2b14e18b727e53735133a230d54657ac6
aa5dd
bad35aac3f503121abf0e543e697fcade78f0d30124778915764d85fb10
303a7
bc8c72c965ebca7db9a2b816188773a5864aa381b81c3073b9d3e52e977
c55ba
daa75fb108a33793a3f8fcef7ba65589e124af66bc52c4a070f645fffbb
c498e
e2ac800b58c4c72e240b90068402b7d4734a7dd03402ee2bce3248cc6f4
4d676
e8085ebc102b5f51c13cc5c257acb2274e7f8d1645af7baad0cb6fe8eef
36e24
f8e46faa3303d93fc424e289d09b4ffba1fc7782b9878456e0fe11f1f68
14e4b

That’s a bit daunting. But those are just the container IDs in long form. If you

want to look at the configuration for a particular container, you just need to
use docker container ls to find its short ID, and then find the
directory that matches:
$ docker container ls
CONTAINER ID
IMAGE
COMMAND
…
c58bfeffb9e6
gcr.io/cadvisor/cadvisor:v0.44.1-test
"/usr/bin/cadvisor…" …

You can view the short ID from docker container ls, then match it to
the ls /var/lib/docker/containers output to see that you want
the directory beginning with c58bfeffb9e6. Command-line tab
completion is helpful here. If you need exact matching, you can do a
docker container inspect c58bfeffb9e6 and grab the long ID
from the output. This directory contains some pretty interesting files related
to the container:
$ cd /var/lib/docker/containers/\
c58bfeffb9e6e607f3aacb4a06ca473535bf9588450f08be46baa230ab4
3f1d6
$ ls -la
total 48
drwx--x--- 4 root root 4096 Aug 20 10:38 .
drwx--x--- 30 root root 4096 Aug 20 10:25 ..
-rw-r----- 1 root root 635 Aug 20 10:34 c58bf…f1d6json.log
drwx------ 2 root root 4096 Aug 20 10:24 checkpoints
-rw------- 1 root root 4897 Aug 20 10:38 config.v2.json
-rw-r--r-- 1 root root 1498 Aug 20 10:38 hostconfig.json
-rw-r--r-- 1 root root
13 Aug 20 10:24 hostname
-rw-r--r-- 1 root root 174 Aug 20 10:24 hosts
drwx--x--- 2 root root 4096 Aug 20 10:24 mounts
-rw-r--r-- 1 root root 882 Aug 20 10:24 resolv.conf
-rw-r--r-- 1 root root
71 Aug 20 10:24 resolv.conf.hash

As we discussed in Chapter 5, this directory contains some files that are bindmounted directly into your container, like hosts, resolv.conf, and hostname. If
you are running the default logging mechanism, then this directory is also
where Docker stores the JSON file containing the log that is shown with the
docker container logs command, the JSON configuration that backs
the docker container inspect output (config.v2.json), and the
networking configuration for the container (hostconfig.json). The
resolv.conf.hash file is used by Docker to determine when the container’s file
has diverged from the current one on the host so it can be updated.
This directory can also be really helpful in the event of severe failure. Even if
we’re not able to enter the container, or if docker is not responding, we can
look at how the container was configured. It’s also pretty useful to understand
where those files are mounted from inside the container. Keep in mind that
it’s not a good idea to modify these files. Docker expects them to contain
reality, and if you alter that reality, you’re asking for trouble. But it’s another
avenue for information on what’s happening in your container.

Filesystem Inspection
Docker, regardless of the backend actually in use, has a layered filesystem
that allows it to track the changes in any given container. This is how the
images are assembled when you do a build, but it is also useful when you’re
trying to figure out if a Linux container has changed anything and, if so,
what. A common problem with containerized applications is that they may
continue to write things into the container’s filesystem. Normally you don’t
want your containers to do that, to the extent possible, and it can help
debugging to figure out if your processes have been writing into the
container. Sometimes this is helpful in turning up stray logfiles that exist in
the container as well. As with most of the core tools, this kind of inspection is
built into the docker command-line tooling and is also exposed via the API.
Let’s take a look at what this shows us. Let’s launch a quick container and
use its name to explore this.

$ docker container run --rm -d --name nginx-fs --rm
nginx:latest
1272b950202db25ee030703515f482e9ed576f8e64c926e4e535ba11f75
36cc4
$ sudo docker container diff 1272b950202d
$
C
A
C
C
C
A
A
A
A
A
C
C
C
C

docker diff nginx-fs
/run
/run/nginx.pid
/var
/var/cache
/var/cache/nginx
/var/cache/nginx/scgi_temp
/var/cache/nginx/uwsgi_temp
/var/cache/nginx/client_temp
/var/cache/nginx/fastcgi_temp
/var/cache/nginx/proxy_temp
/etc
/etc/nginx
/etc/nginx/conf.d
/etc/nginx/conf.d/default.conf

$ docker container stop nginx-fs

Each line begins with either A or C, which is shorthand for added or changed,
respectively. We can see that this container is running nginx, that the
nginx configuration file has been written to, and that some temporary files
have been created in a new directory named /var/cache/nginx. Being
able to find out how the container filesystem is being used can be very useful
when you are trying to optimize and harden your container’s filesystem
usage.
Further detailed inspection requires exploring the container with docker
container export, docker container exec or nsenter and
the like, to see exactly what is in the filesystem. But docker container
diff gives you a good place to start.

Wrap-Up
At this point, you should have a good idea of how to deploy and debug
individual containers in development and production, but how do you start to
scale this for larger application ecosystems? In the next chapter, we’ll take a
look at one of the simpler Docker orchestration tools: Docker Compose. This
tool is a nice bridge between a single Linux container and a production
orchestration system. It delivers a lot of value in development environments
and throughout the DevOps pipeline.

About the Authors
Sean Kane is the founder of techlabs.sh and a Principal Production
Operations engineer at SuperOrbital. Sean specializes in engineering,
teaching, and writing about modern devops processes, including Kubernetes,
Docker, Terraform, and more. He has had a long career in production
operations, with many diverse roles across a broad range of industries. Sean
is the lead inventor on a container-related patent and spends a lot of his spare
time writing, teaching, and speaking about technology. He is an avid traveler,
hiker, and camper and lives in the US Pacific Northwest with his wife,
children, and dogs.
Karl Matthias is VP of Architecture at Community.com and has previously
worked at several well known tech companies, where he has held a number
of very senior engineering and leadership roles for more than 25 years. He is
an enthusiast of hard problems, distributed systems, Go, Ruby, Elixir,
scalable datastores, automated infrastructure, and repeatable systems.

